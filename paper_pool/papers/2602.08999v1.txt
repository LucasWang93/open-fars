Title: CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion

Abstract: With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue

Body: CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion I INTRODUCTION I-A Contributions II RELATED WORK II-A Interactive Visual Grounding and Clarification Dialogues II-B Ambiguity Detection in Grounded Instructions III METHODS III-A CLUE architecture for IVG III-A 1 Model III-A 2 Dataset III-B CLUE architecture for Ambiguity detection III-B 1 Model III-B 2 Dataset IV RESULTS AND DISCUSSION IV-A Ambiguity detection IV-A 1 Ablation study IV-A 2 Setup IV-A 3 Baselines IV-A 4 Results IV-A 5 Generalization IV-A 6 Comparison with SOTA IV-B Interactive Visual Grounding IV-B 1 Setup IV-B 2 Baseline IV-B 3 Results V CONCLUSION AND FUTUR WORK CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion Mouad Abrini and Mohamed Chetouani Institut des Systèmes Intelligents et de Robotique (ISIR), Sorbonne Université Paris, France {mouad.abrini, mohamed.chetouani}@isir.upmc.fr Abstract With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM’s cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue I INTRODUCTION Robots operating in human spaces must interpret underspecified language instructions and act safely. A central capability is interactive visual grounding (IVG): given a natural-language instruction (e.g., “Pick up the apple”), the robot must identify the intended referent and, when necessary, ask for clarification (see Fig 1 ). While recent IVG systems can localize objects from referring expressions [ 28 , 15 , 13 , 9 ] and even engage in dialog [ 20 , 10 , 16 , 24 ] , an open question is how to detect that the current instruction is ambiguous in the specific visual scene. Classical referring expression comprehension (REC) methods, from modular attention networks to modern transformer detectors, largely assume uniqueness, optimizing for a single best match [ 28 , 15 ] . Interactive approaches in HRI typically decide to ask using heuristics (e.g., multiple plausible candidates) or token-level uncertainty (e.g., entropy/confidence from a language or policy model), as seen in planners that query based on grounding confidence [ 27 ] , and LLM-driven ambiguity classifiers like CLARA [ 18 ] . Recent dialog-trained systems further learn when to ask from demonstration, e.g., ClearVQA [ 12 ] and uncertainty-timed clarification [ 23 ] . These signals can be effective, but are policy-level (answer vs. ask) and only indirectly grounded in the spatial structure of the scene: they rarely indicate where confusion arises or which distractors compete for attention. We argue that ambiguity in grounded instructions arises in the joint image-language representation, where textual tokens align with multiple plausible visual candidates. In such cross-modal spaces, the decoder’s text queries attend to visual patches that encode scene structure and attributes. When several objects fit the instruction, the alignment becomes non-selective, distributing mass across competing regions. We exploit this property by extracting the model’s cross-modal self-attention weights and training a convolutional network on these maps to classify ambiguity. This turns a latent alignment pattern into a spatial signal that both (i) indicates when the instruction is underspecified and (ii) localizes where the confusion lies for better interpretability. Figure 1: Problem illustration: when an instruction is underspecified, the robot should detect it and ask for clarification (AI generated, then edited) I-A Contributions • Spatial ambiguity detection: a CNN over cross-modal attention maps that explicitly detects and localizes referential ambiguity. • A synthetic dataset generated using the Isaac Sim Simulator [ 17 ] for multimodal ambiguity detection. • End-to-end disambiguation (IVG): a fine-tuned VLM on InViG only [ 29 ] (real-world dataset) yielding state-of-the-art IVG performance, outperforming TiO [ 26 ] trained on InViG-only. Figure 2: Overall CLUE architecture. An RGB image is encoded by SigLIP and projected by an MLP. The text prefix is tokenized and passed with the image tokens into a Gemma2 decoder equipped with LoRA adapters. The decoder both (i) autoregressively generates clarification questions and (ii) exposes cross-modal attention maps from mid-layers to a lightweight CNN ambiguity detect (Fig. 4 ). If the detector predicts ambiguous, the model asks a follow-up and updates the dialog context; otherwise it emits location tokens and triggers object detection. The loop repeats until disambiguation. II RELATED WORK II-A Interactive Visual Grounding and Clarification Dialogues Ambiguity in referring expressions has long been addressed by interactive systems that can detect unclear references and ask users for clarification. Early human-robot interaction work integrated dialog or gestures to resolve ambiguity. For example, [ 20 ] combined user pointing gestures with a dialog system to disambiguate spoken commands, and [ 10 ] used a referring expression model to find candidate objects and then engaged in conversation to identify the correct target [ 16 ] . Similarly, [ 1 ] proposed generating spatial heatmaps and follow-up questions to locate the intended object, and [ 24 ] showed that clarification questions from a robot improved the grounding of object references. In essence, if a user says “Bring me that cup,” an interactive agent must recognize the referential ambiguity (e.g. multiple cups are present) and ask a targeted question to clarify which cup is meant. Modern systems explicitly predict such ambiguity and generate clarification questions. For instance, [ 27 ] presents an attribute-guided POMDP planner that uses vision-language grounding confidence to decide when to ask a question to differentiate between look-alike objects. Recent vision-language models are explicitly trained to ask when a question is ambiguous: [ 12 ] introduces the ClearVQA benchmark and methods that detect ambiguity, generate targeted clarification questions, and then answer after interaction. Likewise, [ 23 ] analyzed human dialogs and model uncertainty to determine the “right time” to ask for clarification, aligning the agent’s questions with moments of referential uncertainty. [ 29 ] introduce InViG, a large-scale benchmark with ∼ \sim 500K human-robot interactions specifically targeting ambiguity in open-ended scenes. It evaluates how an agent should respond with clarifying questions in a natural manner rather than relying on fixed templates. Figure 3: IVG with notation. An RGB image (AI generated in this example) is encoded by SigLIP and projected to the decoder via an MLP, the text prefix is tokenized and concatenated with image tokens. The Gemma2 LLM takes X ( 1 ) = ′′ i ​ m ​ a ​ g ​ e ​ clarify ′′ ∥ C ( 1 ) X^{(1)}=^{\prime\prime} image \text{clarify}^{\prime\prime}\|C^{(1)} , where the running context after the first turn is C ( 1 ) = U ​ ‖ R 1 ‖ ​ H 1 C^{(1)}=U\|R_{1}\|H_{1} (initial user request U U , assistant question R 1 R_{1} , human reply H 1 H_{1} ). The model autoregressively generates either a clarification segment (left stream, labeled R 1 R_{1} ) or location tokens G = locXXXX ​ … G= \text{locXXXX} ... (right stream) that decode to a bounding box. Green blocks denote image features, pink blocks denote text tokens. [ 26 ] builds on that line by proposing TiO, an end-to-end model for interactive visual grounding in the wild and learns to gather information actively during the interaction. The main limitation of these kinds of methods is that they rely heavily on the statistical presence of the dialog rounds. For example, the InViG dataset contains at least one dialog round, so the model will always ask a clarification question, no matter what, even if the initial instruction is unambiguous. All these approaches typically rely on model confidence scores or predicted likelihoods to trigger clarifications. In contrast, our method enables triggering clarifications using a spatial, cross-modal attention signal from a VLM, rather than token likelihoods or confidence heuristics. II-B Ambiguity Detection in Grounded Instructions To detect ambiguity, models typically trigger clarification via heuristics (e.g., multiple candidate objects) or learned uncertainty (token/policy entropy, confidence). For example, the model proposed in [ 18 ] classifies commands as clear/ambiguous/infeasible using an LLM-based confidence signal and asks for clarification when needed. Interactive grounding also uses dialog/gestures or heatmap/question strategies to resolve reference ambiguity [ 20 , 10 , 16 , 1 , 24 ] , and recent work learns when to ask from data; for instance, by planning over grounding confidence [ 27 ] or using model uncertainty to guide questioning [ 23 ] . To evaluate such capabilities, benchmarks like ClearVQA [ 12 ] have been introduced. In an earlier work [ 8 ] for HRI, the authors used an existing object detector (DETR [ 4 ] ) connected to a dense image captioning model (DenseCAP [ 14 ] ), on which they applied GradCAM [ 19 ] . Although this method is quite promising, it is limited by the performance of the sub-components themselves and is not specifically tailored for ambiguity detection. Furthermore, even if we wanted to finetune (fully supervised) this further using the saliency maps, we would be limited by the computational complexity of GradCAM. In fact, if we define a loss that depends on the saliency map and backpropagate, we would be computing second order gradients, which is slow and memory hungry. Beyond that, recent work [ 6 ] fine-tuned an end-to-end foundation model (Molmo 7B [ 7 ] ) to predict a yes/no response for ambiguity detection. However, for the balanced class ratio dataset they used, the performance was not very high (around 0.68 F1). [ 5 ] introduced a benchmark for ambiguity detection, named “Focus Ambiguity”, which is precisely the type of ambiguity we are tackling in this paper. They showed that even large models do not achieve high performance when it comes to ambiguity classification. Figure 4: Ambiguity detector. The image is encoded by SigLIP and projected with an MLP. The text prefix (e.g. ”Get the apple”) is tokenized and passed to the Gemma2 decoder. From the 14th layer, we read text to image cross-attention for the selected prefix query tokens Q Q and keep the first 1024 keys (image tokens), yielding per-token 32 × \times 32 maps that are mean-aggregated into a single spatial map. This map is passed to a lightweight CNN to produce the ambiguity probability p a ​ m ​ b p_{amb} . III METHODS Whenever applicable to fine-tuning the VLM’s language model, we use Low-Rank Adaptation (LoRA) [ 11 ] . Concretely, we freeze the visual encoder, the multimodal projector, and the tokenizer, and we insert rank-r LoRA adapters into the decoder’s projection layers (attention q/k/v/o and MLP up/down/gate). Unless stated otherwise, we use r = 16 r=16 and α = 32 \alpha=32 , along with a dropout rate of 0.05. We use two different adapters for each task: Adapter A for ambiguity detection and adapter B for IVG, as represented in Fig 2 . III-A CLUE architecture for IVG III-A 1 Model We implement IVG as a transformer decoder based on PaliGemma mix 2 (paligemma2-3b-mix-448) [ 21 ] . This model was pre-trained to handle different tasks by using a different initial token. For example, if one needs to do REC, the text instruction should be ” detect the red cup”. If we want to do image captioning, we should use ” caption en ”. Since it was also trained for open-ended object detection, its vocabulary is extended with discrete tokens { lock } k = 0 1024 \left\{ \mathrm{lock} \right\}_{k=0}^{1024} that encode normalized box coordinates in [ 0 , 1024 ) [0,1024) . In this line, we reserve a special conditioning token ” clarify ” for our task: we attend to image tokens and the running text dialog context and autoregressively emit either a clarification question or a sequence of grounding location tokens. Figure 3 shows an illustration of how the model is used. Let C be the running dialog context (the text prefix). Concretely, C ( k ) C^{(k)} is the concatenation (noted ∥ \| ) of the initial user request U U plus all turns (assistant/robot questions R i R_{i} and human answers H i H_{i} ) up to turn k k . At token step t, conditioned on image I I , the linearized prefix X X , and the previously generated suffix tokens y t y_{ t} , the decoder predicts the next token y t y_{t} . The interactive exchange is a sequence of assistant/robot (model) prompts and user replies. A typical dialog is defined as: D = { ( R 1 , H 1 ) , ( R 2 , H 2 ) , ⋯ , ( R K , H K ) } D=\{(R_{1},H_{1}),(R_{2},H_{2}),\cdots,(R_{K},H_{K})\} For training, We construct a supervised dataset S S of prefix → \rightarrow target pairs by cumulatively revealing pairs and alternating the prediction target: ( U → R 1 ) , \displaystyle(U\rightarrow R_{1}), ( U ∥ R 1 ​ H 1 → R 2 ) , \displaystyle\quad(U\|R_{1}H_{1}\rightarrow R_{2}), … , \displaystyle\quad\ldots, ( U ∥ R 1 ​ H 1 ​ ⋯ ​ R K − 1 ​ H K − 1 → R K ) \displaystyle\quad(U\|R_{1}H_{1}\cdots R_{K-1}H_{K-1}\rightarrow R_{K}) The final grounding target would be: ( U ∥ R 1 ​ H 1 ​ ⋯ ​ R K ​ H K → G ) (U\|R_{1}H_{1}\cdots R_{K}H_{K}\rightarrow G) Each training example is linearized as input X = ( " image clarify " ∥ prefix ) X=\left(\texttt{" image clarify "}\|\text{prefix}\right) and target suffix Y Y (either a textual response R t R_{t} or localization tokens G G ). We apply token-level cross-entropy only over the suffix tokens: ℒ I ​ V ​ G ​ ( θ ) = 1 T ​ ∑ t = 1 T [ − log ⁡ p θ ​ ( y t | I , X , y t ) ] \mathcal{L}_{IVG}(\theta)=\frac{1}{T}\sum_{t=1}^{T}\left[-\log p_{\theta}(y_{t}|I,X,y_{ t})\right] Where T T is the number of tokens in the target suffix Y Y for this example. At test time, given the initial user instruction U U and image I I , we set C 0 = U C_{0}=U and iterate. 