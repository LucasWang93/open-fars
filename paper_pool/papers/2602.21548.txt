Title: DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference

Abstract: The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput.
  We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path -- which inherently avoids network congestion and avoids interference with latency-critical model execution communications -- with a global scheduler that dynamically balances load across prefill and decode engines.
  Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87times on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96times without violating SLO.

Body: DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference 1 Introduction 2 Background 2.1 LLM Inference Preliminary 2.2 Agentic Use of LLMs 2.3 Modern AI Data Center Architecture 3 Bottleneck Motivation 4 DualPath System Overview 4.1 Dual-Path Loading 4.2 Bottleneck-Free Analysis 4.3 Practical Challenges 5 CNIC-Centric Traffic Manager 5.1 Traffic Isolation 5.2 CNIC-Assisted KV-Cache Copy 6 Adaptive Request Scheduler 6.1 Inter-Engine Scheduling 6.2 Intra-Engine Scheduling 7 Evaluation 7.1 Implementation 7.2 Experimental Setup 7.3 Offline Batch Inference 7.4 Online Serving 7.5 Ablation Study 7.6 Large-Scale Scalability 8 Discussion 8.1 Potential Future Work 8.2 Working Set Analysis 9 Related Work 10 Conclusion A Appendix A.1 Traffic Isolation Configuration Details A.2 27B Model Specifications A.3 Agent Task Structure A.4 Experimental Configurations A.5 KV-Cache Block Layout DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference Yongtong Wu 1,3 Shaoyuan Chen 2,3 Yinmin Zhong 1,3 Rilin Huang 1 Yixuan Tan 3 Wentao Zhang 3 Liyue Zhang 3 Shangyan Zhou 3 Yuxuan Liu 3 Shunfeng Zhou 3 Mingxing Zhang 2 Xin Jin 1 Panpan Huang 3 1 School of Computer Science, Peking University 2 Tsinghua University 3 DeepSeek-AI Abstract. The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput. We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path — which inherently avoids network congestion and avoids interference with latency-critical model execution communications — with a global scheduler that dynamically balances load across prefill and decode engines. Our evaluation on three models with realistic agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87 × \times on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96 × \times without violating SLO. 1. Introduction Large Language Models (LLMs) are rapidly evolving from single-turn chatbots (OpenAI, 2025b ; DeepSeek-AI, 2025d ) and standalone reasoners (OpenAI, 2025b ) into agentic systems that can autonomously plan, invoke tools, and solve real-world tasks through multi-turn interactions (Chowa et al. , 2026 ; Wang et al. , 2024 ; Xi et al. , 2025 ; Jiang et al. , 2024 ; Mohammadi et al. , 2025 ) . In such settings, an LLM no longer serves isolated prompts; instead, it participates in long-running sessions where context accumulates over time (Lin et al. , 2025 ) . As agentic applications become increasingly prevalent, multi-turn LLM inference has emerged as a critical workload in production systems, ranging from coding assistants (Yang et al. , 2024 ; Wu et al. , 2023 ) to autonomous task agents (Zhou et al. , 2023 ; Li et al. , 2024 ) . This paradigm shift in applications has driven a significant transformation in LLM inference workloads: from traditional human-LLM interaction to human-LLM-environment interaction, called the agentic paradigm . The typical pattern of human-model interaction involves users providing input, engaging in a few rounds of interaction with the LLM, and consuming the results generated by the LLM. By contrast, an agentic LLM may interact with an external environment, through tools such as a web browser and Python interpreter, over dozens or even hundreds of turns. Although each individual tool call or feedback is short (often hundreds of tokens), the context accumulates across turns and can grow to extreme lengths. As a result, agentic workloads become highly I/O-bound: the multi-turn, short-append pattern leads to very high KV-Cache hit rates — typically ≥ 95 % \geq 95\% (Chen et al. , 2026 ) — making the efficiency of KV-Cache loading, rather than pure computation, the dominant performance factor. Figure 1 . Existing bottleneck (left) and DualPath (right). To improve throughput under agentic workloads, existing LLM inference systems have converged on a common set of architectural patterns: layer-wise prefill (Xiong et al. , 2024 ; Du et al. , 2025 ) , prefill–decode (PD) disaggregation (Zhong et al. , 2024 ; Patel et al. , 2025 ; Zhao et al. , 2025a ) , and external KV-Cache storage (Gao et al. , 2024 ; Liu et al. , 2025 ; Qin et al. , 2025 ) . In these systems, prefill engines load the KV-Cache in a layer-wise manner to accommodate as many requests as possible within a single batch. When prefill completes, decoding engines typically receive KV-Cache from prefill engines via a high-performance RDMA network. The decoding engines then generate tokens and store their KV-Cache in distributed storage to enable reuse across turns. However, this architecture also introduces a critical limitation. As shown in Figure 1 , prefill engines must load large volumes of KV-Cache from remote storage. As a result, prefill-side storage network bandwidth becomes the throughput bottleneck of the entire system, even though decoding engines often have substantial unused storage network bandwidth. This imbalance reveals a fundamental inefficiency in existing designs: storage network bandwidth is unevenly utilized across engines. The bandwidth of prefill engines are persistently saturated, while decoding engines remain underutilized. Simply provisioning more bandwidth to prefill engines is costly and often impractical in general-purpose clusters. Therefore, it is promising to exploit and combine the available I/O bandwidth of all engines, rather than overloading prefill engines alone, to accelerate KV-Cache loading for agentic LLM workloads. Prior studies have attempted to alleviate the KV-Cache loading bottleneck. Mooncake (Qin et al. , 2025 ) caches KV-Cache in a distributed DRAM pool and employs an affinity-aware scheduler to maximize the DRAM KV-Cache hit rate. However, it cannot be used in memory-constrained scenarios, such as the rollout phase in RL, where DRAM is occupied to hold large training state that is offloaded from HBM. It is also not cost-effective in scenarios with enormous working sets (e.g., online serving), considering the cost comparison between DRAM and SSD. Other attempts reduce the amount of KV-Cache data to retrieve (Gao et al. , 2025 ) and reduce the retrieval overhead (Hu et al. , 2025 ; Yan et al. , 2025 ) . However, they do not solve the inherent inefficiency caused by storage I/O imbalance between different engines. In this paper, we present DualPath, a new LLM inference system that rethinks KV-Cache loading in modern inference architectures for agentic workloads. The key insight behind DualPath is that KV-Cache loading does not have to be prefill-centric. While existing systems always load KV-Cache directly from storage into prefill engines, they cannot utilize the remote storage bandwidth of decoding engines. DualPath leverages this observation by enabling dual-path KV-Cache loading : in addition to the conventional storage-to-prefill path, KV-Cache can be loaded into decoding engines and then transferred to prefill engines via high-performance RDMA. By dynamically selecting between these paths, DualPath redistributes network load and alleviates prefill-side bandwidth pressure. Realizing this design raises two challenges. First, introducing an extra loading path introduces complex traffic patterns and potential interference with collective primitives in model execution, which can degrade overall performance if unmanaged. Second, the system must decide online which loading path to use under dynamic and heterogeneous workloads, and ensure load balance across both GPUs and NICs simultaneously. To address these challenges, DualPath adopts (1) an optimized dual-path loading data path design, which introduces no inherent congestion under common P/D ratios, (2) a NIC-centric traffic management approach to isolate KV-Cache traffic from latency-sensitive model inference communications, and (3) a dynamic scheduling policy that jointly balances computation and network utilization across prefill and decoding engines. We implement DualPath on top of a modern inference stack and evaluate it using representative agentic workloads with long contexts and high cache reuse. Experiments show that DualPath significantly improves system throughput and the first token latency, while maintaining the latency between tokens. In agentic inference scenarios, DualPath increases end-to-end throughput by up to 1.87 × \times for offline inference, and improves the online serving throughput by 1.96 × \times on average. In summary, this paper makes three contributions: • We identify the I/O-bound nature of multi-turn, agentic LLM workloads and show that KV-Cache loading dominates system performance under modern LLM inference architectures. • We present DualPath, an inference system that introduces dual-path KV-Cache loading and leverages decoding-engine bandwidth to resolve prefill-side bottlenecks. • We design and evaluate a workload-aware scheduling algorithm that dynamically balances computation and network resources, significantly improving balance on realistic workloads. 2. Background 2.1. LLM Inference Preliminary LLM inference is becoming one of the most important system workloads recently. Popular LLMs utilize decoder-only transformer architecture, comprising stacked blocks with attention layers and feed-forward networks (FFNs). Attention layers enable token interactions within requests, while FFNs process tokens independently. The model predicts subsequent tokens based on preceding ones, storing attention keys and values as KV-Cache in HBM to avoid recomputations. PD-disaggregated Inference. Prefill–decode (PD) disaggregation (Zhong et al. , 2024 ; Patel et al. , 2025 ) separates the prefill phase from the decode phase, assigning them to dedicated prefill engines (PEs) and decode engines (DEs), respectively. The two phases exhibit distinct compute and memory patterns: prefill is compute-intensive and batched, while decode is memory-bound and latency-sensitive. With PD disaggregation, PEs load the hit KV-Cache and perform prefilling; then, they transfer the KV-Cache to DEs, which perform autoregressive decoding. This design reduces interference between phases, enables stage-specific optimizations, and improves scalability, making it the de facto architecture for modern LLM serving. To support multi-turn conversations, the KV-Cache is often stored in distributed storage for reuse across turns. Layerwise Prefill. Long-context prefill is bottlenecked by HBM capacity, as both activations and the KV-Cache for the entire batch must reside within it, forcing limited batch sizes and leading to poor GPU utilization. LayerKV (Xiong et al. , 2024 ) and PrefillOnly (Du et al. , 2025 ) address this problem by exploiting the strong locality in prefill computations: each layer requires only its own layer-specific KV-Cache. Consequently, the KV-Cache can be allocated and freed per layer, and the GPU holds only one layer’s KV-Cache for the forward batch. This increases the effective batch size (in tokens) by approximately a factor equal to the number of layers, boosting prefill throughput. 2.2. Agentic Use of LLMs Figure 2 . Agent trajectory example. LLMs increasingly power agentic applications that perform multi-turn reasoning and interact with an environment (via e.g., terminal commands, code execution, or asking for human feedback) over long sessions. As shown in Figure 2 , in a typical turn , the model receives a prompt composed by the previous context plus some newly appended tokens (often tool output or user input) and generates the next action or response. A single agent run is a trajectory of dozens or even hundreds of turns: the context grows turn-by-turn and can reach up to one million tokens (Anthropic, 2026 ; DeepMind, 2026 ) . Because most of the context, typically ¿95% tokens in our traces, is reused across rounds, the vast majority of tokens in each round can hit the KV-Cache; only the newly appended context needs prefill computation. Due to the extreme length of agent trajectories, DRAM and HBM-based KV-Cache storage like Mooncake (Qin et al. , 2025 ) can only store a small proportion of KV-Caches, necessitating the use of larger yet cheaper external SSD-based KV-Cache storage (DeepSeek-AI, 2025a ) . The agentic LLM inference workload is also prevalent in agent LLM training, which often adopts reinforcement learning (RL) approaches. In a typical RL training loop, the agent LLM first undergoes a rollout phase, where it is prompted to generate a large number of multi-step agent trajectories. These trajectories are then scored by a separate reward model. Finally, the LLM parameters are updated to increase the likelihood of high-scoring outputs and reduce the likelihood of low-scoring ones. During the rollout phase, substantial data (like reward model and optimizer states) is offloaded to host DRAM, further constraining the available DRAM for KV-Cache. This reinforces the need for external, high-capacity KV-Cache storage that can accommodate long agentic rollout contexts efficiently. 2.3. Modern AI Data Center Architecture Modern AI data centers are purpose-built logical supercomputers engineered to handle large-scale generative AI training and inference workloads. For example, in a standard NVIDIA DGX SuperPOD (NVIDIA, 2023 ) , each node is equipped with 8 Hopper GPUs interconnected via high-speed NVLink. Each GPU is paired with a dedicated 400 Gbps compute NIC ( CNIC , also known as east-west NIC), which maximizes inter-node communication bandwidth. Independent of the compute fabric, each node also features a storage NIC ( SNIC , also known as south-north NIC) up to 400 Gbps, providing fast access to datasets, model checkpoints, and on-disk KV cache. A fundamental principle of this architecture is that the compute network and the storage network are isolated from each other (Zhao et al. , 2025a ) . This separation is essential to maximize both storage and application performance. By isolating high-intensity east-west compute traffic between GPUs from storage traffic, the architecture prevents interference between them, and drastically reduces compute communication latency. This design also ensures that the inter-GPU communication remains highly reliable and predictable even when performing data-intensive tasks such as reading large datasets or writing multi-tera