Title: HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation

Abstract: Data across modalities such as images, text, and graphs often contains hierarchical and relational structures, which are challenging to model within Euclidean geometry. Hyperbolic geometry provides a natural framework for representing such structures. Building on this property, this work introduces HexFormer, a hyperbolic vision transformer for image classification that incorporates exponential map aggregation within its attention mechanism. Two designs are explored: a hyperbolic ViT (HexFormer) and a hybrid variant (HexFormer-Hybrid) that combines a hyperbolic encoder with an Euclidean linear classification head. HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit. Experiments across multiple datasets demonstrate consistent performance improvements over Euclidean baselines and prior hyperbolic ViTs, with the hybrid variant achieving the strongest overall results. Additionally, this study provides an analysis of gradient stability in hyperbolic transformers. The results reveal that hyperbolic models exhibit more stable gradients and reduced sensitivity to warmup strategies compared to Euclidean architectures, highlighting their robustness and efficiency in training. Overall, these findings indicate that hyperbolic geometry can enhance vision transformer architectures by improving gradient stability and accuracy. In addition, relatively simple mechanisms such as exponential map aggregation can provide strong practical benefits.

Body: HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation 1 Introduction 2 Related Work 2.1 Hyperbolic Geometry in Deep Learning 2.2 Hyperbolic Vision Transformers 2.3 Aggregation and Training Dynamics in Hyperbolic Models 3 Preliminaries 3.1 The Lorentz Model 3.2 Tangent Space 3.3 Exponential and Logarithmic Maps 4 Methodology 4.1 Model variants and component overview Hyperbolic fully-connected layers (LorentzFC): Patch embeddings and Lorentz batch normalization: Classification Head: 4.2 Hyperbolic attention and aggregation Score computation: Exponential aggregation: 5 Experiments 5.1 Training Strategy 5.2 Comparison of Hyperbolic Vision Transformers 5.3 Warmup Dependency and Gradient Stability 5.4 Vision Transformer Variants 5.5 Aggregation Strategies 6 Conclusion A Appendix A.1 Training Strategy and Experiments Set-up Data Augmentation. Weight Initialization and Loss: Learning Rate Scheduler: Optimizer: Hyperparameters and Tuning: Curvature Selection: A.2 Additional Experiments Hyperbolicity: Choice of Datasets: A.2.1 Additional Experiments on different activation function and on different optimizer A.2.2 Experiments on weak/flat Hierarchical Datasets: A.2.3 Gradients Stability A.2.4 Epoch-wise Performance Analysis A.3 Centroid vs Exponential Map aggregation A.3.1 Failed Experiments with Centroid A.4 Runtime, Memory, and Limitations A.5 Use of Large Language Models in this Paper HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation Haya Alyoussef 1 , Ahmad Bdeir 2 , Diego Coello de Portugal Mecke 1 , Tom Hanika 1 , Niels Landwehr 2 , Lars Schmidt-Thieme 1 1 Information Systems and Machine Learning Lab (ISMLL) 2 Data Science Department Hildesheim University Hildesheim, Germany Abstract Data across modalities such as images, text, and graphs often contains hierarchical and relational structures, which are challenging to model within Euclidean geometry. Hyperbolic geometry provides a natural framework for representing such structures. Building on this property, this work introduces HexFormer, a hyperbolic vision transformer for image classification that incorporates exponential map aggregation within its attention mechanism. Two designs are explored: a hyperbolic ViT (HexFormer) and a hybrid variant (HexFormer-Hybrid) that combines a hyperbolic encoder with an Euclidean linear classification head. HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit. Experiments across multiple datasets demonstrate consistent performance improvements over Euclidean baselines and prior hyperbolic ViTs, with the hybrid variant achieving the strongest overall results. Additionally, this study provides an analysis of gradient stability in hyperbolic transformers. The results reveal that hyperbolic models exhibit more stable gradients and reduced sensitivity to warmup strategies compared to Euclidean architectures, highlighting their robustness and efficiency in training. Overall, these findings indicate that hyperbolic geometry can enhance vision transformer architectures by improving gradient stability and accuracy. In addition, relatively simple mechanisms such as exponential map aggregation can provide strong practical benefits. Code is available at: https://github.com/HayaAlyoussef/HexFormer.git 1 Introduction Hyperbolic geometry has emerged as a powerful tool in deep learning due to its ability to model hierarchical and relational structures more effectively than Euclidean spaces (Nickel and Kiela, 2017 ; Bdeir et al. , 2023 ) . Vision Transformers (ViTs) form the backbone of many state-of-the-art vision models (Sim√©oni et al. , 2025 ; Ravi et al. , 2024 ) . However, the reliance on Euclidean representations suggests they may struggle to capture complex, hierarchical structures in ways that hyperbolic spaces naturally can (Chamberlain et al. , 2017 ) . Recent works have attempted to improve the performance by integrating hyperbolic geometry into transformer architectures. Early efforts mapped ViT outputs into hyperbolic space for metric learning (Ermolov et al. , 2022 ) , while more recent approaches such as HVT (Fein-Ashley et al. , 2024 ) and LViT from the HyperCore framework (He et al. , 2025a ) incorporated hyperbolic operations into transformer modules. Although these methods reported improvements, they either restricted hyperbolic integration to limited components or did not investigate training dynamics in detail. This work introduces a hyperbolic Vision Transformer based on the Lorentz model for image classification. The proposed approach consistently outperforms Euclidean baselines across multiple datasets, activation functions, and model scales, while also surpassing prior hyperbolic ViTs such as HVT (Fein-Ashley et al. , 2024 ) and LViT (He et al. , 2025a ) . Beyond accuracy gains, hyperbolic models also exhibit more stable training dynamics, reducing sensitivity to warmup schedules and hyperparameter tuning. The main contributions are as follows: ‚Ä¢ Hyperbolic ViT with novel attention (HexFormer) : A transformer architecture entirely formulated in the Lorentz model of hyperbolic space is developed. In addition, a new attention mechanism incorporating a simple exponential map aggregation is introduced as an alternative to centroid based averaging, providing more stable and effective feature aggregation. This model yields better performance compared to Euclidean ViTs and prior hyperbolic ViT models. ‚Ä¢ Hybrid encoder-classifier design (HexFormer-Hybrid) : A model that combines a hyperbolic encoder with an Euclidean linear classification head. This further improves performance over both hyperbolic and Euclidean models. ‚Ä¢ Analysis of stable and scalable training : A detailed study of training dynamics demonstrates that hyperbolic ViTs achieve improved gradient stability and robustness to warmup strategies, reducing the need for extensive fine-tuning. Extensive experiments demonstrate that both hyperbolic and hybrid models achieve consistent improvements over Euclidean ViTs, with the hybrid variant delivering the strongest results overall. These findings indicate that hyperbolic geometry provides a principled and scalable approach to enhancing transformer architectures in terms of both representational capacity and training stability. 2 Related Work 2.1 Hyperbolic Geometry in Deep Learning Hyperbolic embeddings have been employed extensively to encode hierarchical relationships in data (Nickel and Kiela, 2017 ; Bdeir et al. , 2023 ; Chen et al. , 2021 ) . Seminal work projected taxonomies into the Poincar√© ball (Nickel and Kiela, 2017 ) . However, previous works have shown that the Poincar√© ball suffers from training instabilities (Mishne et al. , 2024 ) . In computer vision, convolutional neural networks have been adapted to hyperbolic Lorentz space, yielding performance gains in classification and segmentation (Bdeir et al. , 2023 ) . Hyperbolic Lorentz representations have also benefited graph learning and fully connected architectures, particularly when modeling relational data (Yang et al. , 2024 ) . 2.2 Hyperbolic Vision Transformers Vision transformers (Dosovitskiy et al. , 2020 ) have shown state-of-the-art performance for image classification tasks. Nevertheless, they require a lot of data and tend to not perform so well with smaller datasets (Pandey et al. , 2023 ) . Due to this, a natural extension is to improve the ViT‚Äôs architecture to reduce the amount of data needed by changing the representation space. Several approaches have explored integrating hyperbolic geometry into Vision Transformers: Hyp-ViT projects a vanilla ViT‚Äôs output into the Poincar√© ball and employs a pairwise cross-entropy loss in hyperbolic distance for metric learning, outperforming Euclidean baselines in retrieval tasks (Ermolov et al. , 2022 ) . HVT implements a Vision Transformer on ImageNet with self-attention adapted via M√∂bius transformations and hyperbolic distance within the Poincar√© ball framework (Fein-Ashley et al. , 2024 ) . HyperCore introduces modules for constructing hyperbolic foundation models in both Lorentz and Poincar√© geometries. It includes a hyperbolic ViT in Lorentz space (LViT), demonstrating improvements over Euclidean ViTs in image classification (He et al. , 2025a ) . However, these lifts are only observed when finetuning from a bigger dataset, but not when training only on the downstream task. 2.3 Aggregation and Training Dynamics in Hyperbolic Models Most existing hyperbolic ViT variants use centroid based aggregation in attention mechanisms (Chen et al. , 2021 ; Fein-Ashley et al. , 2024 ; He et al. , 2025a ) . These can introduce distortion in curved spaces when processing big values (Mishne et al. , 2024 ) . Aggregation via exponential maps has not been explored in this context, although logarithmic and exponential maps have been used in hyperbolic models, they have not been applied directly as the aggregation step in attention. Despite the intuitive approach it is an alternative to the expensive fr√©chet mean. For instance, HVT (Fein-Ashley et al. , 2024 ) maps queries, keys, and values into tangent space and performs the entire attention computation there, which mitigates the benefits from the exponentially growing hyperbolic distance metric. This omission can lead to suboptimal aggregation, as it fails to fully leverage the geometric properties of hyperbolic space. HexFormer approach calculates the scores based on the hyperbolic distance and uses them to aggregate the points on the tangent plane, which allows us leveraging both approaches. Furthermore, the training behavior of hyperbolic transformers (specifically gradient stability and sensitivity to warmup) remains underexamined. Prior work on hyperbolic models have demonstrated the potential of integrating both Euclidean and non-Euclidean geometry for improving performance (Ermolov et al. , 2022 ; Bdeir et al. , 2023 ) . This motivates exploring hybrid approaches that leverage the strengths of both geometries. 3 Preliminaries Hyperbolic geometry is a non-Euclidean geometry with constant negative curvature K 0 K 0 . Several equivalent models can represent hyperbolic space, such as the Poincar√© ball model (Ganea et al. , 2018 ) , the Poincar√© half-plane model (Tifrea et al. , 2018 ) , the Klein model (Gulcehre et al. , 2018 ) , and the Lorentz (hyperboloid) model (Nickel and Kiela, 2018 ) . These models are isometric to one another, meaning transformations between them preserve hyperbolic distances (Ramsay and Richtmyer, 1995 ) . The Lorentz model is frequently adopted in hyperbolic deep learning due to its numerical stability and the closed-form expressions of its distance function, exponential map, and logarithmic map (Chen et al. , 2021 ; Yang et al. , 2024 ) . 3.1 The Lorentz Model The n n -dimensional Lorentz model is the Riemannian manifold ùïÉ K n = ( ‚Ñí n , ùî§ x K ) \mathbb{L}^{n}_{K}=(\mathcal{L}^{n},\mathfrak{g}_{x}^{K}) . It is defined in the ( n + 1 ) (n+1) -dimensional Minkowski space based on the Riemannian metric tensor, using diag ‚Äã ( ‚àí 1 , 1 , ‚Ä¶ , 1 ) \mathrm{diag}(-1,1,\ldots,1) . The hyperboloid is then given by ‚Ñí n ‚âî { x ‚àà ‚Ñù n + 1 ‚à£ ‚ü® x , x ‚ü© ‚Ñí = 1 / K , x t 0 } , \mathcal{L}^{n}\coloneqq\{x\in\mathbb{R}^{n+1}\mid\langle x,x\rangle_{\mathcal{L}}=1/K,\;x_{t} 0\}, where K ‚àà ‚Ñù ‚àí K\in\mathbb{R}^{-} is the negative curvature and the Lorentzian inner product is given as ‚ü® x , y ‚ü© ‚Ñí ‚âî ‚àí x t ‚Äã y t + x s ‚ä§ ‚Äã y s = x ‚ä§ ‚Äã diag ‚Äã ( ‚àí 1 , 1 , ‚Ä¶ , 1 ) ‚Äã y . \langle x,y\rangle_{\mathcal{L}}\coloneqq-x_{t}y_{t}+x_{s}^{\top}y_{s}=x^{\top}\,\mathrm{diag}(-1,1,\ldots,1)\,y. which corresponds to the upper sheet of the two-sheeted hyperboloid. The first coordinate x t x_{t} is often referred to as the time-like component, while the remaining n n coordinates represent spatial dimensions x s x_{s} , reflecting the connection to the geometry of special relativity. Each point in ùïÉ K n \mathbb{L}^{n}_{K} has the form x = [ x t x s ] , x ‚àà ‚Ñù n + 1 , x t ‚àà ‚Ñù , x s ‚àà ‚Ñù n {x}=\left[\begin{smallmatrix}x_{t}\\ {x}_{s}\\ \end{smallmatrix}\right],{x}\in\mathbb{R}^{n+1},x_{t}\in\mathbb{R},{x}_{s}\in\mathbb{R}^{n} and x t = ‚Äñ x s ‚Äñ 2 ‚àí 1 / K x_{t}=\sqrt{||{x}_{s}||^{2}-1/K} (Chen et al. , 2021 ; Bdeir et al. , 2023 ) . 3.2 Tangent Space At each point x ‚àà ùïÉ K n x\in\mathbb{L}^{n}_{K} , the tangent space is the Euclidean subspace orthogonal to x x under the Lorentzian inner product: ùíØ x ‚Äã ùïÉ K n ‚âî { y ‚àà ‚Ñù n + 1 ‚à£ ‚ü® y , x ‚ü© ‚Ñí = 0 } . \mathcal{T}_{x}\mathbb{L}^{n}_{K}\coloneqq\{y\in\mathbb{R}^{n+1}\mid\langle y,x\rangle_{\mathcal{L}}=0\}. Although the ambient space is Minkowski, each tangent space is Euclidean. This property allows standard Euclidean operations to be performed in ùíØ x ‚Äã ùïÉ K n \mathcal{T}_{x}\mathbb{L}^{n}_{K} before mapping results back to the manifold via the exponential map. The tangent space at the origin is denoted as ùíØ 0 ‚Äã ùïÉ K n \mathcal{T}_{0}\mathbb{L}^{n}_{K} . (Chen et al. , 2021 ) . 3.3 Exponential and Logarithmic Maps The exponential and logarithmic maps connect the hyperbolic manifold ùïÉ K n \mathbb{L}^{n}_{K} with its tangent spaces ùíØ x ‚Äã ùïÉ K n \mathcal{T}_{x}\mathbb{L}^{n}_{K} . For x ‚àà ùïÉ K n x\in\mathbb{L}^{n}_{K} and z ‚àà ùíØ x ‚Äã ùïÉ n z\in\mathcal{T}_{x}\mathbb{L}^{n} , the exponential map exp x K ‚Å° ( z ) : ùíØ x ‚Äã ùïÉ K n ‚Üí ùïÉ K n \exp^{K}_{x}(z):\mathcal{T}_{x}\mathbb{L}^{n}_{K}\rightarrow\mathbb{L}^{n}_{K} , maps tangent vectors to hyperbolic spaces: exp x K ‚Å° ( z ) = cosh ‚Å° ( Œ± ) ‚Äã x + sinh ‚Å° ( Œ± ) ‚Äã z Œ± , Œ± = ‚àí K ‚Äã ‚Äñ z ‚Äñ ‚Ñí , ‚Äñ z ‚Äñ ‚Ñí = ‚ü® z , z ‚ü© ‚Ñí . \exp^{K}_{x}(z)=\cosh(\alpha)\,x+\sinh(\alpha)\,\frac{z}{\alpha},\quad\alpha=\sqrt{-K}\,\|z\|_{\mathcal{L}},\quad\|z\|_{\mathcal{L}}=\sqrt{\langle z,z\rangle_{\mathcal{L}}}. Conversely, for y ‚àà ùïÉ K n y\in\mathbb{L}^{n}_{K} , the logarithmic map log x K ‚Å° ( y ) : ùïÉ K n ‚Üí ùíØ x ‚Äã ùïÉ K n \log^{K}_{x}(y):\mathbb{L}^{n}_{K}\rightarrow\mathcal{T}_{x}\mathbb{L}^{n}_{K} , os the inverse of exponential map that gives the tangent vector at x x pointing toward y y : log x K ‚Å° ( y ) = cosh ‚àí 1 ‚Å° ( Œ≤ ) Œ≤ 2 ‚àí 1 ‚Äã ( y ‚àí Œ≤ ‚Äã x ) , Œ≤ = K ‚Äã ‚ü® x , y ‚ü© ‚Ñí . \log^{K}_{x}(y)=\frac{\cosh^{-1}(\beta)}{\sqrt{\beta^{2}-1}}\,(y-\beta x),\qquad\beta=K\langle x,y\rangle_{\mathcal{L}}. Where log x K ‚Å° ( exp x K ‚Å° ( z ) ) = z \log^{K}_{x}(\exp^{K}_{x}(z))=z . These operators provide a consistent mechanism to switch between the curved geometry of ùïÉ K n \mathbb{L}^{n}_{K} and its Euclidean tangent approximations, and are essential for optimization and representation learning in hyperbolic neural networks (Chen et al. , 2021 ; Yang et al. , 2024 ) . 4 Methodology Figure 1: Overview of HexFormer architecture. Hyperbolic embedding, transformer encoder, and classification head operate on the Lorentz manifold. Residual connections are applied to space-like components only. This section describes the Hyperbolic Vision 