Title: EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents

Abstract: Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.

Body: EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents 1 Introduction 2 Related Work 3 Proposed Capture System 3.1 Stage I: Scene Reconstruction 3.2 Stage II: Sequence Processing 3.3 Stage III: Sequence Calibration 3.4 Stage IV: Motion Optimization 4 Evaluation 4.1 Ablation Study on Loss Functions 4.2 Comparison on Capture Methods 5 Downstream Tasks 5.1 Monocular Human Scene Reconstruction 5.2 Physics-based Character Animation 5.2.1 Human Object Interaction Skill Training 5.2.2 Scene-aware Motion Tracking 5.3 Real-world Humanoid Robot Control 6 Conclusion 7 Limitations and Future Work. 8 Acknowledge 9 More Details of EmbodMocap 9.1 Capture technique 9.2 Human Labor Analysis 10 More Details of Monocular Human-Scene Reconstruction Pipeline 11 More Details of Human-Object Interaction Skills 11.1 Follow Skill 11.2 Sit Skill 11.3 Climb Skill 11.4 Lie Skill 11.5 Prone Skill 11.6 Support Skill 12 More Details of Scene-Aware Imitation Policy 12.1 Representations 12.2 Reward 13 More Details of Captured Dataset Used in Main Paper 13.1 Qualitative Demonstrations EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents Wenjia Wang 1‚àó Liang Pan 1‚àó Huaijin Pi 1 Yuke Lou 1 Xuqian Ren 2 Yifan Wu 1 Zhouyingcheng Liao 1 Lei Yang 3 Rishabh Dabral 4 Christian Theobalt 4 Taku Komura 1 (*: Core contributor.) 1 The University of Hong Kong 2 Tampere University 3 The Chinese University of Hong Kong 4 Max-Planck Institute for Informatics Abstract Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research. Figure 1 : Introducing EmbodMocap , a portable and low-cost system for simultaneous 4D human and scene reconstruction, deployable anywhere using two moving iPhones. The dataset captured by EmbodMocap benefits three crucial embodied AI tasks: monocular human scene reconstruction, physics-based character animation, and real-world humanoid motion control. Project page . 1 Introduction Embodied Artificial Intelligence (Embodied AI) aims to build agents that can perceive, understand, and act within real-world environments. Progress in this field relies on datasets that capture both human motion and the surrounding 3D scene, enabling physically grounded perception and action learning. Such scene-aware data allows modeling of realistic human‚Äìscene interactions, simulation of lifelike behaviors, and training of humanoids to operate seamlessly in complex environments. They serve as a foundation for advancing embodied reasoning and control across robotics, virtual reality, and computer vision. However, collecting high-quality human‚Äìscene data remains difficult. Precise 3D motion and scene geometry cannot be automatically obtained from internet videos due to occlusions and depth ambiguity. Existing capture systems that provide high-quality human‚Äìscene data typically rely on multi-view camera rigs [ 11 , 74 ] , wearable motion suits [ 21 , 34 ] , or LiDAR scanners [ 5 , 18 ] , which are costly, complex, and limited to controlled studio environments. These constraints hinder scalable and scene-aware data acquisition, limiting the ability of embodied AI models to learn from natural human behavior in diverse indoor and outdoor environments. In this paper, we propose EmbodMocap, an efficient and affordable framework for capturing metrically accurate 4D human and scene using only two iPhones. Our key idea is to jointly calibrate and optimize dual RGB-D inputs to reconstruct both humans and scenes within a unified world coordinate frame. Specifically, we first reconstruct the static scene from a single RGB-D sequence to define the world scale, then capture synchronized dual-view RGB-D videos of human motion, and finally perform geometric alignment and motion optimization to recover world-anchored human poses. In contrast to existing systems that rely on multi-camera rigs or wearable sensors, our approach achieves high-quality, scene-consistent reconstruction using only moving consumer devices. This design enables scalable, in-the-wild data collection that preserves precise human motion and authentic scene context, supporting realistic human‚Äìscene interaction modeling for embodied AI research. Based on the data collected with EmbodMocap, we demonstrate the reliability and versatility of our capture pipeline through three representative applications. The first application verifies geometric consistency, where we fine-tune reconstruction models to jointly recover humans and scenes in world coordinates. The second validates physical realism, showing that the captured motions enable scalable training of physics-based character skills and scene-aware motion tracking. The third demonstrates embodied transferability, where our data support humanoid robot training through a sim-to-real motion tracking framework [ 42 , 25 ] . These results highlight that EmbodMocap enables scalable and physically grounded data acquisition for embodied AI. In summary, our contributions can be summarized as follows: ‚Ä¢ We introduce EmbodMocap, a portable and affordable data collection pipeline that produces high-quality multi-modal data for embodied AI applications. ‚Ä¢ We validate our capture pipeline‚Äôs effectiveness across three key embodied AI tasks: monocular human-scene reconstruction, physics-based character animation, and real-world humanoid motion control. ‚Ä¢ We provide a scalable and accessible solution that lowers the barrier for embodied AI research, opening new possibilities for real-world applications and further advancements in the field. 2 Related Work Table 1 : Comparison of 4D Human Scene datasets based on different features. Datasets Publication Device Outcome Mocap Suit Scanner Static Cam. Dyna. Cam. Total Cost($) Mesh Dyna.Anno. Outdoor PROX [ 11 ] ICCV2019 - Structure Sensor Kinetic-One - 2K ‚úì ‚úó ‚úó RICH [ 18 ] CVPR 2022 - Leica RTC360 6-8 √ó \times Cameras 1 √ó \times Camera 20K+ ‚úì ‚úì ‚úì EgoBody [ 74 ] ECCV2022 - 1 √ó \times IPhone 5 √ó \times Azure Kinect Hololens2 9K ‚úì ‚úì ‚úó SLOPER4D [ 5 ] CVPR2023 Noitom PN+NUC11 Ouster-os1 LiDAR - DJI-Action2+TLS 20K ‚úì ‚úì ‚úì EMDB [ 21 ] ICCV 2023 EM Sensors - - 1 √ó \times IPhone 15K ‚úó ‚úì ‚úì Nymeria [ 34 ] ECCV2024 2 √ó \times XSens+Aria Wistband - - 2 √ó \times Project Aria 60K+ ‚úó ‚úì ‚úì EmbodMocap - - 1 √ó \times IPhone - 2 √ó \times IPhone 1K ‚úì ‚úì ‚úì Datasets for 4D Human Scene Capture. Early motion datasets, such as AMASS [ 35 , 9 ] , focus on pure human motion, unifying multiple motion capture sources into a large-scale repository. While invaluable for studying human motion, these datasets lack the 3D scene context essential for understanding human‚Äìscene interactions. Recent 4D datasets, like PROX [ 11 ] , RICH [ 18 ] , and EgoBody [ 74 ] , combine scanned 3D scenes with motion capture using multi-view camera systems, while EMDB [ 21 ] and SPLOPER4D [ 5 ] , employ IMUs or electromagnetic sensors for motion recording in large-scale environments. Nymeria [ 34 ] extends this further with Project Aria glasses and optical marker-based systems for wide-area motion capture. However, these approaches face notable limitations: marker-based and multi-camera systems are expensive and restricted to small studio environments, while IMU and EM-based methods, though more flexible, require extensive manual alignment and post-processing to synchronize motion with 3D scenes. And the wearable devices will influence the human appearance in RGB images. In contrast, our approach uses minimal equipment, operates in diverse environments without static camera setups, and avoids wearable devices, preserving the naturalness of RGB images for authentic human‚Äìscene interaction capture. Table 1 compares these datasets. Monocular Human Scene Reconstruction. Early works [ 3 , 41 , 20 , 7 , 23 ] on RGB-based human mesh recovery focus on reconstructing 3D pose and shape but often ignore scene context [ 60 ] or camera information [ 24 , 63 ] , leading to inconsistencies under camera motion. Recent methods address this by combining motion cues [ 73 ] , SLAM or visual odometry [ 72 , 66 , 54 ] , and human motion priors [ 73 , 53 ] to recover global trajectories in world coordinates. Emerging models move toward jointly reconstructing humans and 3D scenes with spatial intelligence models [ 61 , 62 ] . For example, HSFM [ 37 ] combines Dust3R [ 62 ] with multi-view correspondence to jointly recover human meshes, scene point clouds, and camera parameters from multi-cameras. HAMSt3R [ 48 ] integrates DensePose [ 8 ] and multi-view scene reconstruction in one model, with an optimization to get human poses, while JOSH [ 28 ] uses MASt3R-SLAM [ 38 ] and joint optimization to achieve globally consistent 4D human-scene reconstructions. This trend emphasizes the simultaneous prediction of human motion and scene geometry, which futher requires multi-model data pairs with high-quality annotations. In our paper, we propose a monocular human scene reconstruction pipeline combined with 2 feedforward models, and finetuned it on our proposed dataset to prove the efficiency of our paired data. Training Humanoid from Video Data. Recent advances in physics-based animation and reinforcement learning enable humanoid agents to perform realistic and physically consistent motions using control policies learned from marker-based motion capture data. These methods have shown strong realism in tasks like motion tracking [ 42 , 32 ] , locomotion [ 44 , 43 , 33 ] , and human‚Äìscene interaction [ 40 , 64 ] , and have been extended to real-world applications in motion tracking [ 15 , 19 , 13 ] , locomotion [ 16 ] , and scene interaction [ 14 , 2 ] . However, marker-based methods require dedicated studios, expensive hardware, and extensive manual effort, making them costly and hard to scale. Adapting captured motions to new scenes or robot morphologies also demands complex retargeting and re-simulation. To address this, recent works like VideoMimic [ 1 ] , ASAP [ 13 ] , and HDMI [ 67 ] train humanoid control directly from in-the-wild video data. By using monocular motion capture methods such as TRAM [ 66 ] and GVHMR [ 53 ] , they estimate human motion from videos and retarget it to virtual humanoids for training in physical simulators. This video-driven paradigm leverages diverse real-world data but struggles with capturing complex skills or scene geometries due to occlusion and depth ambiguities. In this paper, we propose a method for high-precision human motion and scene reconstruction that overcomes these limitations. 3 Proposed Capture System Figure 2 : EmbodMocap: We propose an affordable dataset capture and processing system. From left to right, the four stages (Stage-I to Stage-IV) illustrate our core logic: leveraging high-quality camera matrices provided by SpectacularAI [ 55 ] and aligning sequence coordinates to the scene‚Äôs world frame. For detailed explanations, please refer to Sec. 3 . We aim to capture metrically accurate human motion and scene geometry using only two iPhones. As shown in Fig. 2 , our capture process consists of four sequential stages that progressively reconstruct and align the scene, cameras, and human motion within a unified world coordinate frame. We first reconstruct a metrically accurate static scene and establish the world reference using a single iPhone RGB-D sequence (Sec. 3.1 ). Then, we use two synchronized iPhones to record dual-view RGB-D videos of human motion and extract per-frame camera poses and human priors with off-the-shelf perception models (Sec. 3.2 ). Next, we align the dual-view camera trajectories to the reconstructed scene through a combination of COLMAP registration and multi-view geometric optimization (Sec. 3.3 ). Finally, we refine the SMPL parameters by triangulating dual-view 2D keypoints into 3D space and optimizing human poses and translations in the world coordinate system (Sec. 3.4 ). 3.1 Stage I: Scene Reconstruction In this stage, we aim to reconstruct a metrically accurate, Z-up scene mesh that serves as the reference world coordinate system. We first use a single iPhone to capture an RGB-D video of the scene, along with synchronized IMU data. The recorded data are processed by the SpectacularAI SDK (SAI) [ 55 ] , which automatically selects keyframes according to the accumulated camera translation and estimates corresponding camera parameters ( ùë≤ s , ùëπ s , n , ùëª s , n ) (\bm{K}_{s},\bm{R}_{s,n},\bm{T}_{s,n}) in Z-up world coordinates with metric scale. These trajectories establish a consistent world frame for all subsequent stages. Based on the recovered poses, we refine the iPhone LiDAR depth maps using PromptDA [ 26 ] , unproject them into 3D space, and integrate the point clouds through TSDF fusion [ 4 ] to obtain a dense and metrically accurate global mesh ‚Ñ≥ g \mathcal{M}_{g} . Note that the depth maps are truncated based on a threshold determined by the effective range of the iPhone‚Äôs depth sensor. Specifically, we use a threshold of 3.5m for indoor scenes and 5m for outdoor scenes. We further apply lightweight post-processing such as outlier removal and small-component filtering to clean the mesh. Finally, we extract SIFT features from the same SAI keyframes and run COLMAP [ 50 ] with fixed camera parameters to build a sparse structure database. This database preserves the metric scale and serves as a reference for registering dual-view sequences in later stages. 3.2 Stage II: Sequence Processing After reconstructing the static scene in St