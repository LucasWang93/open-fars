Title: SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training

Abstract: Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.

Body: SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training 1 Introduction 2 Background 2.1 LLM Structure 2.2 Mixed Precision Training 2.3 Quantization 3 Overall Procedure 3.1 Collecting Statistics and Analyzing Divergence 3.2 Deciding the Optimal Layer-wise Quantization Scheme 4 Quantify the Quantization Impact 4.1 Preliminary 4.2 Loss Divergence in Forward Pass 4.3 Weight Divergence in Backward Pass 4.3.1 Impact on Weight Gradient 4.3.2 Impact on Weight Divergence 5 Find Optimal Quantization Policy 5.1 Quality Loss and Efficiency Metrics 5.2 Integer Linear Programming Problem 5.3 Incorporating Pipeline Parallelism 6 Evaluation 6.1 Experiment Setup 6.2 Results 6.2.1 SNIP Outperforms Other Quantization Schemes 6.3 In-Depth Analysis 7 Related Work 8 Conclusion A Proof of the Theoretical Results A.1 Proof of Theorem 4.1 A.2 Proof of Theorem 4.2 \setcctype by SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training Yunjie Pan University of Michigan Ann Arbor Michigan USA panyj@umich.edu , Yongyi Yang University of Michigan Ann Arbor Michigan USA NTT Research, Inc. Sunnyvale California USA yongyi@umich.edu , Hanmei Yang University of Massachusetts Amherst Amherst Massachusetts USA hanmeiyang@umass.edu and Scott Mahlke University of Michigan Ann Arbor Michigan USA mahlke@umich.edu (2026) Abstract. Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead. Mixed-precision training, Quantization, Large language models, Neural network † † journalyear: 2026 † † copyright: cc † † conference: Proceedings of the 31st ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2; March 22–26, 2026; Pittsburgh, PA, USA † † booktitle: Proceedings of the 31st ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS ’26), March 22–26, 2026, Pittsburgh, PA, USA † † doi: 10.1145/3779212.3790223 † † isbn: 979-8-4007-2359-9/2026/03 † † ccs: Computer systems organization Architectures † † ccs: Computing methodologies Neural networks 1. Introduction Large Language Models (LLMs) have revolutionized natural language processing (NLP), powering applications such as conversational AI, code generation, and reasoning (Brown et al. , 2020 ; Achiam et al. , 2023 ; Touvron et al. , 2023 ; Dubey et al. , 2024 ; Liu et al. , 2024 ; Team et al. , 2023 ; Bai et al. , 2023 ) . Despite their widespread utility, training LLMs demands extraordinary computational resources. For example, training Llama 3 (Dubey et al. , 2024 ) with 8 billion parameters requires 1.46 million GPU hours on NVIDIA H100 GPUs, resulting in 420 tons of CO2 emissions. The immense computational resources, runtime, and environmental impact underscore the urgent need for efficient training strategies. Mixed precision training (Micikevicius et al. , 2018 ) has emerged as a pivotal technique to mitigate the costs, allowing most compute-intensive operations to be executed in low-precision formats (e.g. FP8 (Liu et al. , 2024 ; Peng et al. , 2023 ; Micikevicius et al. , 2022 ) , FP4 (Wang et al. , 2025 ) ), while retaining higher precision for critical operations. The NVIDIA Hopper GPU supports the FP8 format (NVidia, 2022 ) , while the more recent Blackwell GPU extends support to the FP4 format (Nvidia, 2024 ) . For the FP8 format, the GEMM operation achieves twice the TFLOPS of BF16, resulting in a 1.3x to 1.4x speedup in end-to-end LLM training. Similarly, an FP4 GEMM on the Blackwell GPU doubles the TFLOPS compared to FP8, which translates into an additional 1.4x speedup over FP8 training latency (Tseng et al. , 2025 ) . However, most existing mixed precision training frameworks adopt a uniform precision policy, assuming identical precision requirements for all layers, thereby missing the opportunity for greater efficiency gains through fine-grained precision configurations (Liu et al. , 2024 ; Peng et al. , 2023 ; Micikevicius et al. , 2022 ; Wang et al. , 2025 ) . While prior research has investigated layer-wise quantization for machine learning models, these efforts are largely focused on inference rather than training and come with limitations. These methods often rely on local quantization metrics such as KL-divergence (Wang et al. , 2019 ; Chen et al. , 2024b ) , absolute quantization error, or relative quantization error, which fail to fully capture the broader training dynamics. Additionally, some prior work employs computationally expensive approaches, including reinforcement learning (HAQ (Wang et al. , 2019 ) ), Hessian-based sensitivity analysis (OBC (Frantar and Alistarh, 2022 ) ), or iterative precision refinement (BitSET (Pan et al. , 2023 ) ). These techniques are primarily tailored for inference and do not address the comprehensive needs of model quality during LLM pretraining, thus limiting their effectiveness in a training context. To address these limitations, we propose SNIP, a fine-grained mixed precision training framework that dynamically determines the quantization per layer to ensure training quality and efficiency. Rather than relying on exhaustive search or purely heuristic-based decisions, SNIP introduces a novel optimization proxy quality metric to quantify the impact of quantization. Figure 1. Illustration of the gap of the training loss between high-precision (BF16) and low-precision (FPX). The gap consists of two parts: (1) Forward Loss Divergence: the increase in training loss directly introduced by quantization during the forward pass, and (2) Backward Weight Divergence: the accumulation of errors in weight updates during the backward pass due to quantization, which can compound over iterations and layers, impacting model convergence. SNIP quantifies training quality loss through two key metrics, as illustrated in Figure 1 : loss divergence and weight divergence. Loss divergence occurs in the forward pass when quantization increases training loss. Weight divergence occurs in the backward pass when quantization errors in gradients distort parameter updates, resulting in the trained model deviating from the models trained with full precision. Building upon these metrics, we formulate the problem of determining layer-specific quantization schemes as an integer linear programming (ILP) problem. The ILP systematically minimizes quality loss while meeting efficiency constraints, ensuring a globally optimized quantization policy. Figure 2. System overview of SNIP, showing its integration into LLM training. SNIP periodically collects statistics on activations, weights, and optimizers, then asynchronously analyzes divergence metrics, solves an ILP problem, and updates layer-wise quantization. Figure 3. Comparison of accuracy versus efficiency (the fraction of FP4 FLOPs) for the TinyLlama 1B model. The FP8 baseline achieves the highest accuracy but lowest efficiency, while the FP4 configuration maximizes efficiency at the cost of accuracy. SNIP demonstrates a balance between high accuracy and a significant reduction in FLOPs by selectively applying FP4 to certain layers based on the loss and weight divergence metrics, outperforming other methods such as random and other heuristic-based approaches. Beyond its theoretical advancements, SNIP designs a practical system that integrates seamlessly into LLM training pipelines, as shown in Figure 2 . The system operates asynchronously alongside standard training, periodically updating layer-wise quantization decisions without interrupting the primary training loop. The system follows a structured workflow: (1) collect statistics on activations, gradients, and optimizer states, (2) analyze these metrics to estimate the impact of precision scaling, (3) formulate an optimization problem to determine the optimal per-layer precision settings, and (4) update quantization assignments asynchronously. This adaptive approach ensures that SNIP remains effective across different training phases and model architectures, making it broadly applicable to various LLM workloads. Figure 3 highlights the trade-off between the accuracy and efficiency of different precision selection methods for the TinyLlama 1B model. Efficiency is measured as the fraction of FLOPs executed in FP4, with the remainder using FP8 precision. SNIP consistently outperforms all other quantization methods at different efficiency levels, including random (randomly selecting layers for FP4), min-abs-err (selecting layers that minimize absolute quantization error for FP4), min-rel-err (selecting layers that minimize relative quantization error for FP4), E-layer-type (empirically apply FP4 to non-sensitive layer types), and E-layer-id (empirically apply FP4 for middle layers). Remarkably, even at 80% FP4 FLOPs, SNIP maintains nearly full-precision accuracy, whereas alternative quantization methods fail to converge. The contributions of this paper are: • We propose SNIP a fine-grained mixed-precision quantization framework specifically for LLM pretraining that seamlessly integrates into existing pipelines. SNIP periodically determines the optimal quantization policy while minimizing the model quality loss. Unlike heuristic-based methods, SNIP provides a global optimization strategy that dynamically adapts to different training stages and supports various quantization techniques. • We present a novel theoretical perspective on how quantization errors influence overall LLM training quality. Specifically, we introduce two quantization quality metrics, loss divergence for the forward pass and weight divergence for the backward pass, which quantify the impact of precision scaling on training stability. These metrics enable efficient precision selection with minimal overhead. • We evaluate SNIP on 1B, 3B, 7B, and 70B models using up to 80% FP4 FLOPs, demonstrating that it consistently improves training efficiency with subbyte precision while maintaining near full-precision model accuracy. 2. Background 2.1. LLM Structure Figure 4. The transformer block structure of Llama-like LLM. The blocks in blue (Q, K, V, O, Gate, Up and Down) are linear layers. Figure 5. The overall mixed precision training framework for linear layers, including the forward and backward pass. Most LLMs are built upon the Transformer (Vaswani, 2017 ) framework, which consists of embedding layers that convert tokenized inputs into dense vector representations, transformer blocks that are the core computation units, stacked N N times to increase the model capacity, and output projection layer that maps the final hidden representations of tokens back to the vocabulary space. We illustrate the transformer block structure of Llama-like LLMs in Figure 4 . Each transformer block contains multi-head self-attention, which uses three types of linear layers Query (Q), Key (K), and Value (V) to project inputs to an intermediate representation. After computing the attention scores, the output is projected back to the original dimension using the Output linear layer (O). Subsequently, the processed tensors are passed through a Feedforward Neural Network (FFN), which comprises the Gate and Up linear layers for intermediate transformations. After the non-linear activation (e.g. SwiGLU), the intermediate representation is projected back by the Down linear layer. Following prior works (Liu et al. , 2024 ; Wang et al. , 2025 ) , we focus on the quantization of those linear layers in transformer blocks because they take the majority (¿90%) of the FLOPs during training (Casson, 2023 ) . Figure 6. An overview of the SNIP training workflow. The top plot illustrates the training loss over steps, with periodic updates to the FPX quantization scheme highlighted in yellow boxes. The process consists of six steps: (1) collecting statistics during a normal iteration, (2) injecting noise during the backward pass and dumping gradients, (3) injecting noise during the forward pass and dumping gradients, (4) analyzing loss and weight divergence, (5) solving the Integer Linear Programming (ILP) problem to determine the optimal FPX quantization scheme, and (6) updating the FPX scheme. 2.2. Mixed Precision Training Mixed precision training (Micikevicius et al. , 2018 ) addresses training costs and model accuracy by using reduced precision, typically BF16, in linear or convolutional layers while retaining higher precision for critical operations. State-of-the-art advancements such as DeepSeek-V3 (Liu et al. , 2024 ) have extended these techniques to include FP8 training for large language models (LLMs). Meanwhile, Wang et al. (Wang et al. , 2025 ) have integrated FP4 into mixed precision training. The FP4 approaches, while employing FP4 across all layers, necessitate complex gradient calculations that add overhead, and rely on irregular sparse GEMM to handle outliers, which further adds significant overhead. Adopting a similar idea on mixed precision training for LLMs, we present the overall framework used in our experiments in Figure 5 . In this framework, the most compute-intensive operators (GEMM) within linear layers are executed in low precision ( FPX ), where FPX refers to low-precision formats such as FP8 or FP4. Before the GEMM operation, the input activations, weights, and output gradients are quantized to low precision, while the output of the GEMM operator remains in BF16. To ensure numerical stability, we maintain a master copy of the weights in FP32, following DeepSeek-V3 (Liu et al. , 2024 ) . Other types of layers, such as RMSNorm, SwiGLU, Softmax, and Attention, continue to use higher precision (BF16) to preserve accuracy while optimizi