Title: HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation

Abstract: With the advancement of language models, unified multimodal understanding and generation have made significant strides, with model architectures evolving from separated components to unified single-model frameworks. This paper explores an efficient training paradigm to build a single transformer for unified multimodal understanding and generation. Specifically, we propose a multimodal warmup strategy utilizing prior knowledge to extend capabilities. To address cross-modal compatibility challenges, we introduce feature pre-scaling and multimodal AdaLN techniques. Integrating the proposed technologies, we present the HaploOmni, a new single multimodal transformer. With limited training costs, HaploOmni achieves competitive performance across multiple image and video understanding and generation benchmarks over advanced unified models. All codes will be made public at https://github.com/Tencent/HaploVLM.

Body: HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation 1 Introduction 2 Related Work 3 Method 3.1 Preliminaries Multimodal LLMs. Diffusion Transformer. 3.2 Model Design HaploOmni-attention Mechanism. HaploOmni Block. Pre Post Connector. Feature Pre-scaling. Inference Mode. 3.3 Training Procedure Stage 1: Multimodal Warmup. Stage 2: Connector Alignment. Stage 3: Unified Tuning. 4 Experiments 4.1 Datasets and Metrics Datasets. Metrics. 4.2 Implementation Details 4.3 Main Results Visual Understanding. Video Generation. 4.4 Ablation Study 4.5 Qualitative Results 5 Conclusion A Appendix A.1 Datasets. A.2 Metrics. A.3 Implementation. A.4 Broader Impact HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation Yicheng Xiao 1,2 , Lin Song üñÇ 2 ‚Å£ ‚àó superscript üñÇ 2 {}^{2*}\textsuperscript{\Letter} start_FLOATSUPERSCRIPT 2 ‚àó end_FLOATSUPERSCRIPT , Rui Yang 3 , Cheng Cheng 4 , Zunnan Xu 1 , Zhaoyang Zhang 2 , Yixiao Ge 2 , Xiu Li üñÇ 1 superscript üñÇ 1 {}^{1}\textsuperscript{\Letter} start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT , Ying Shan 2 1 Tsinghua Shenzhen International Graduate School, Tsinghua University 2 ARC Lab, Tencent PCG 3 The University of Hong Kong 4 Xi‚Äôan JiaoTong University xiaoyc23@mails.tsinghua.edu.cn ronnysong@tencent.com Equal contribution. üñÇ Corresponding author. Abstract With the advancement of language models, unified multimodal understanding and generation have made significant strides, with model architectures evolving from separated components to unified single-model frameworks. This paper explores an efficient training paradigm to build a single transformer for unified multimodal understanding and generation. Specifically, we propose a multimodal warmup strategy utilizing prior knowledge to extend capabilities. To address cross-modal compatibility challenges, we introduce feature pre-scaling and multimodal AdaLN techniques. Integrating the proposed technologies, we present the HaploOmni, a new single multimodal transformer. With limited training costs, HaploOmni achieves competitive performance across multiple image and video understanding and generation benchmarks over advanced unified models. All codes will be made public at https://github.com/Tencent/HaploVLM . 1 Introduction In recent years, large-scale language models (LLMs) [ 15 , 63 , 1 ] have exhibited remarkable capabilities across diverse domains, prompting researchers to extensively investigate their potential applications in multimodal contexts. There is an increasing focus on developing unified approaches that simultaneously address both multimodal understanding and generation capabilities. The former research can be categorized into three phases in terms of implementation architecture, progressing from segregated to unified frameworks. In the first phase, tool-based methods like InstructGPT [ 43 ] and HuggingGPT [ 49 ] employ LLMs to allocate task-specific tools. While these methods offer simplicity and ease of use, their reliance on text-tool interactions limits their flexibility and controllability. In the second phase, methodologies incorporate separate encoders and decoders in conjunction with LLMs, exemplified by Seed [ 16 ] , Emu-2 [ 52 ] , and VILA-U [ 57 ] , achieving multimodal input-output compatibility through feature interaction mechanisms. Although these approaches have achieved commendable results on general multimodal benchmarks, their segregated processes result in insufficient modal integration, constraining their capability to handle fine-grained understanding and generation tasks. Method Video Single Und. Gen. SEED POPE MVBench VBench Support Transformer Data Data SEED-X [ 16 ] ‚úó ‚úó 152M 152M - 84.2 - - TokenFlow [ 47 ] ‚úó ‚úó 10M 60M 68.7 86.8 - - Janus-Pro [ 8 ] ‚úó ‚úó 41M 98M 72.1 87.4 - - Show-o [ 62 ] ‚úó ‚úî 36M 611M - 73.8 - - ViLA-U [ 57 ] ‚úî ‚úó 7M 16M 59.0 85.8 38.9 73.4 HaploOmni (ours) ‚úî ‚úî 4M 3M 74.6 88.3 52.9 78.1 Table 1 : Characteristics comparison with some other unified models. Video support means that the models can process video inputs and generate videos. ‚ÄúUnd. Data‚Äù and ‚ÄúGen. Data‚Äù indicate the number of training data for understanding and generation tasks, respectively. In the third phase, the latest approaches utilize a unified single-transformer framework. One subset, including Chameleon [ 53 ] and Show-o [ 62 ] , achieves model unification through image discretization tokens. Another subset, exemplified by Transfusion [ 70 ] , employs hybrid text autoregressive and image diffusion modeling processes for unification. Compared to the encoder-decoder methods, these single-transformer methods are more streamlined and enable cross-modal early-fusion and late-fusion, thereby enhancing fine-grained multimodal representation capabilities [ 70 ] . However, existing methods adopt from-scratch training approaches. Due to the absence of prior knowledge, their overall performance falls short of encoder-decoder methods while incurring substantial training costs. Consequently, this paper explores a new perspective: efficiently constructing a single multimodal transformer by leveraging knowledge from specialized models to achieve high-performance unified multimodal understanding and generation. To achieve it, we propose a new training paradigm for single multimodal transformers. Considering that the natural language possesses more abstract and higher-level semantic representations compared to natural images [ 39 ] , we propose a multimodal warmup process that depth-wise partitions a transformer decoder into three components: visual encoding, text encoding-decoding, and visual decoding. These components are initialized using corresponding prior models and subsequently fine-tuned independently to accommodate identity mapping across other modalities. Following the warmup phase, the model undergoes unified training for multimodal understanding and generation in an end-to-end manner. Furthermore, we find that different modalities exhibit varying preferences for feature scaling, significantly impacting training effectiveness and stability. Inspired by the diffusion transformer, we propose feature pre-scaling strategies and Multimodal AdaLN. The former pre-establishes initial feature transformation scales for different modalities based on statistical information, while the latter enables the model to autonomously select normalization parameters for various inputs. With the proposed techniques, we present the HaploOmni , a cost-efficient yet high-performance single transformer for multimodal understanding and generation. As demonstrated in Table 1 , we evaluate our method on image and video multimodal understanding and generation benchmarks. Compared with previous models, our HaploOmni achieves superior performance across multiple image understanding datasets, including SEEDBench [ 28 ] and POPE [ 32 ] . Additionally, it significantly outperforms unified video-text models such as VILA-U in both MVBench [ 30 ] video understanding and VBench [ 22 ] generation benchmarks. 2 Related Work Text-to-video generation models aim to automatically produce visually and logically consistent videos based on textual descriptions of scenes, objects, and actions. Most text-to-video models [ 20 , 21 , 19 , 5 ] are built on latent diffusion models with a U-Net architecture. The field achieved a significant milestone with the introduction of diffusion transformers [ 44 ] , as demonstrated by the impressive Sora [ 42 ] . Following this breakthrough, the majority of studies have adopted diffusion transformers to develop open-source text-to-video models. For example, CogVideoX [ 66 ] introduces an expert transformer to improve the fusion of visual and textual modalities. Unified multi-modal LLMs are capable of performing both understanding and generation tasks within a single framework. Several efforts [ 16 , 57 , 54 , 60 ] have been made to unify vision understanding and generation. For instance, SEED [ 16 ] and Emu [ 52 ] predict the next multimodal element by regressing visual embeddings or classifying textual tokens. These models primarily rely on an additional diffusion-based image decoder [ 46 ] . Similarly, Chameleon [ 53 ] and Emu3 [ 54 ] discretize visual features and train token-based autoregressive models on mixed image and text data. VILA-U [ 57 ] improves the vision tokenizer by introducing a unified vision tower that aligns discrete visual features with text during pre-training. In addition, TransFusion [ 70 ] and Show-o [ 62 ] attempt to integrate diffusion and autoregressive approaches within a single transformer. However, most unified models still lag behind task-specific architectures, likely because generation tasks require low-level features while understanding tasks demand high-level features. To address this limitation, Janus [ 55 ] employs separate tokenizers for understanding and generation tasks. Similarly, TokenFlow [ 47 ] defines dual codebooks with shared mappings to enable flexible combinations of low and high-level features. Despite recent advances, current approaches are limited by their inability to effectively trade off between performance and training resources. In this paper, we introduce a method for efficiently constructing a unified single transformer achieving comparable performance across both understanding and generation tasks. Figure 1 : Illustration of our HaploOmni-attention mechanism and HaploOmni Block. We implement a hybrid masking strategy that applies causal attention to text features and timestep tokens while adopting bidirectional attention for processing visual signals and latent noise. Drawing from the standard transformer module, we develop the HaploOmni block through the implementation of multimodal AdaLN. 3 Method In this section, we begin by introducing the preliminaries, followed by a detailed elaboration of our unified single transformer (HaploOmni) and the novel training paradigm we propose. This approach leverages knowledge from specialized models to efficiently construct HaploOmni, enabling high-performance unified multimodal understanding and generation. 3.1 Preliminaries Multimodal LLMs. Given a visual signal (image/video) and a series of corresponding text requests, a common approach for answer generation is to use a multimodal large language model [ 39 , 63 , 9 ] , which typically integrates a vision encoder and a language model. Generally, the raw visual input is transformed into a discrete or continuous feature space, which is then combined with text embeddings generated by a linguistic tokenizer. An auto-regressive LLM then processes the mixed multimodal sequence { x t } t = 1 T ‚àí 1 superscript subscript subscript ùë• ùë° ùë° 1 ùëá 1 \{x_{t}\}_{t=1}^{T-1} { italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT to predict the next tokens by modeling the conditional probability: P ‚Å¢ ( x 1 , x 2 , ‚ãØ , x T ) = ‚àè t = 1 T P ‚Å¢ ( x t ‚à£ x 1 , x 2 , ‚ãØ , x t ‚àí 1 ) . ùëÉ subscript ùë• 1 subscript ùë• 2 ‚ãØ subscript ùë• ùëá superscript subscript product ùë° 1 ùëá ùëÉ conditional subscript ùë• ùë° subscript ùë• 1 subscript ùë• 2 ‚ãØ subscript ùë• ùë° 1 P\left(x_{1},x_{2},\cdots,x_{T}\right)=\prod_{t=1}^{T}P\left(x_{t}\mid x_{1},x% _{2},\cdots,x_{t-1}\right). italic_P ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚ãØ , italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) = ‚àè start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_P ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚à£ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚ãØ , italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ) . (1) Then, the NTP loss is defined using cross-entropy and the conditional probability described above, utilized to optimize the LLM during the training phase. Figure 2 : The progressive training stages of our HaploOmni, including multimodal warmup, connector alignment and unified tuning. Diffusion Transformer. Diffusion models, such as the denoising diffusion probabilistic model (DDPM), generate data by progressively transforming noise into a target distribution over a series of timesteps. The Diffusion Transformer (DiT) integrates the transformer architecture into this generative process, enabling it to learn the reversal of the incremental noise-adding procedure in the forward process. At each timestep t ùë° t italic_t , the model estimates the noise œµ t subscript italic-œµ ùë° \epsilon_{t} italic_œµ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT added to the data at the previous timestep. The objective function for training the Diffusion Transformer can be written as: ‚Ñí diff = ùîº ‚Å¢ [ ‚Äñ œµ t ‚àí œµ ^ Œ∏ ‚Å¢ ( ùê± t , t ) ‚Äñ 2 ] subscript ‚Ñí diff ùîº delimited-[] superscript norm subscript italic-œµ ùë° subscript ^ italic-œµ ùúÉ subscript ùê± ùë° ùë° 2 \mathcal{L}_{\textit{diff}}=\mathbb{E}\left[\|\epsilon_{t}-\hat{\epsilon}_{% \theta}(\mathbf{x}_{t},t)\|^{2}\right] caligraphic_L start_POSTSUBSCRIPT diff end_POSTSUBSCRIPT = blackboard_E [ ‚à• italic_œµ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - over^ start_ARG italic_œµ end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) ‚à• start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] (2) where ùê± t subscript ùê± ùë° \mathbf{x}_{t} bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the noisy data at timestep t ùë° t italic_t , ùê± 0 subscript ùê± 0 \mathbf{x}_{0} bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is the raw image or video data, and œµ ^ Œ∏ ‚Å¢ ( ‚ãÖ ) subscript ^ italic-œµ ùúÉ ‚ãÖ \hat{\epsilon}_{\theta}(\cdot) over^ start_ARG italic_œµ end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( ‚ãÖ ) is the model‚Äôs estimate of the noise at each timestep, parameterized by the network Œ∏ ùúÉ \theta italic_Œ∏ . The attention mechanism in the transformer architecture enables the model to refine the noisy data by conditioning on both the input noise and the context provided by previous timesteps. This iterative refinement process allows the model to generate high-quality samples from noise in both the image and text domains, making it particularly suitable for multimodal generative tasks. Moreover, it lays a solid foundation for adapting the pre-trained DiT model to the inference paradigm of LLM, enabling the efficient development of a unified transformer model for multimodal understanding and generation. 3.2 Model Design Overall, to streamline the training of the unified single transformer for multimodal understanding and generation, we first partition it into three components: pre-decoder, base-decoder, and post-decoder. All parts consist of multiple HaploOmni Blocks as shown in Fig. 1 and then are initialized using corresponding prior models, ViT [ 14 ] for visual encoding, a pre-trained LLM [ 15 ] for text encoding-decoding, and DiT [ 66 ]