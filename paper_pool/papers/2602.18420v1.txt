Title: SPQ: An Ensemble Technique for Large Language Model Compression

Abstract: This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/

Body: SPQ: An Ensemble Technique for Large Language Model Compression 1 Introduction 2 Related Work 2.1 SVD-Based Compression 2.2 Structured Pruning 2.3 Quantization 2.4 Combined Compression Methods 3 Contributions 4 SPQ Framework 4.1 Variance-Retained Low-Rank SVD 4.2 Activation-Based Structured Pruning 4.3 Post-Training Linear Quantization 4.4 LoRA Fine-tuning 5 Experimental Design 5.1 Base Settings 5.2 SPQ Hyperparameters 6 Experimental Results 6.1 SVD Experiments 6.2 Pruning Experiments 6.3 Quantization Experiments 6.4 Pairwise Combinations of Compression Methods 6.5 SVD + Pruning + Quantization Experiments 7 Discussion Conclusion 8 Limitation Future Work 9 Bibliographical References SPQ: An Ensemble Technique for Large Language Model Compression Abstract This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 ‚Üí \rightarrow 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9√ó speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ‚Äôs robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/ Keywords: LLM Compression, SVD, Pruning, Quantization \NAT@set@cites SPQ: An Ensemble Technique for Large Language Model Compression Jiamin Yao, Eren Gultepe ‚àó ‚Ä† ‚Ä† thanks: *Corresponding author Dept. of Computer Science, Southern Illinois University Edwardsville, Edwardsville, USA jamieyao0508@gmail.com, egultep@siue.edu Abstract content 1. Introduction Recent advances in large language models (LLMs) have enabled impressive natural language understanding and generation, but their growing size incurs substantial computational and memory costs, making deployment on resource-constrained or real-time systems challenging. Efficient compression techniques that reduce memory while preserving performance are therefore essential. In this work, we explore an ensemble compression technique that we refer to as SVD-Pruning-Quantization (SPQ), which combines i) variance-based truncation of SVD, ii) activation-based structured pruning, and iii) 8-bit linear quantization. Each component in SPQ is carefully tailored to leverage the strengths of the different compression methods by applying them to where they are most effective. SVD exploits the low-rank structure in attention layers, while pruning removes redundant neurons in the feedforward multilayer perceptron (MLP) layers, thus providing a layer-aware alignment between model structure and compression method. However, SVD and pruning alone degrade perplexity under aggressive compression. To mitigate this effect, we integrate 8-bit linear quantization, which provides an additional compression layer while preserving model performance. By combining these three techniques, SPQ achieves substantial weight memory savings while maintaining or even improving language modeling quality. Our results demonstrate that careful orchestration of heterogeneous compression methods can yield highly compact models suitable for deployment in memory-constrained environments without significantly degrading performance. 2. Related Work SVD-based compression, structured pruning, and quantization are three commonly used approaches for LLM compression, with each individually offering a distinct advantage, and recently there has been interest in combined methods. 2.1. SVD-Based Compression Singular Value Decomposition (SVD) has been widely used for neural network compression by approximating large weight matrices with low-rank factors. Early work showed its effectiveness on convolutional networks (Denton et al. , 2014 ) , and later studies confirmed its utility in classical tasks (Wang and Zhu, 2017 ) . Low-rank factorization also reduces transformer complexity in ALBERT (Lan et al. , 2019 ) and Linformer (Wang et al. , 2020 ) . Task-aware methods like Tacoere (Guan et al. , 2024 ) exploit cluster structure in transformer weights, while activation-aware SVD (ASVD) (Yuan et al. , 2023 ) leverages activation statistics for guided truncation. Adapting SVD to LLMs, SVD-LLM (Wang et al. , 2024b ) and SVD-LLM v2 (Wang et al. , 2025b ) use reconstruction loss-aware decompositions with heterogeneous per-layer ranks, and both apply a LoRA fine-tuning step after decomposition to restore accuracy. Further refinements include gradient-friendly decompositions (Wang et al. , 2025a ) , adaptive rank allocation (Li et al. , 2025b ) , and applications to Mixture-of-Experts models (Li et al. , 2025a ) . Overall, these advances highlight SVD‚Äôs flexibility, but also its sensitivity to truncation thresholds and layer heterogeneity. 2.2. Structured Pruning Structured pruning removes redundant components in a hardware-friendly manner, such as filters, channels, or attention heads. Early CNN work ranked filter importance using weight magnitudes (Li et al. , 2016 ) , pruned neurons based on average activation (Hu et al. , 2016 ) , or applied a Taylor-based saliency criterion (Molchanov et al. , 2016 ) . For LLMs, SlimLLM (Guo et al. , 2025 ) jointly estimates head and channel importance and uses layer-wise sparsity with lightweight regression for quality recovery. LLM-Pruner (Ma et al. , 2023 ) leverages gradient-based scores and applies LoRA-based tuning with minimal data, while Wang et al. (Wang et al. , 2024a ) proposed a retraining-free framework that prunes pretrained models before fine-tuning. As surveyed in He and Xiao ( 2023 ) , structured pruning achieves significant FLOP reductions, though some fine-tuning is often needed to maintain accuracy. 2.3. Quantization Quantization reduces numerical precision to compress models and accelerate inference. Early efforts, such as Deep Compression (Han et al. , 2015 ) , demonstrated the effectiveness of quantization in CNNs, while Q8BERT (Zafrir et al. , 2019 ) extended these ideas to transformers. Martynov et al. (Martynov et al. , 2024 ) investigate cross-domain properties of transformer weights to design near-lossless quantization schemes, pushing the limits of precision reduction without sacrificing accuracy. Quantization-aware training (QAT) enables efficient integer-only deployment on edge devices (Jacob et al. , 2018 ) , while post-training quantization (PTQ) achieves 4‚Äì8 bit precision without retraining (Banner et al. , 2019 ) . For LLMs, GPTQ (Frantar et al. , 2022 ) performs layer-wise calibration with minimal error and Activation-aware Weight Quantization (AWQ) (Lin et al. , 2024 ) preserves critical activations by using mixed precision and block-wise quantization for stable 4-bit inference. 2.4. Combined Compression Methods Ensemble approaches combine complementary techniques to achieve synergistic compression. Early work such as Deep Compression (Han et al. , 2015 ) showed that pruning, quantization, and Huffman coding together can dramatically reduce CNN model size without accuracy loss. Following this idea, several methods extend integration to LLMs: Ren and Zhu (Ren and Zhu, 2023 ) prune redundant heads or neurons before applying low-rank factorization, while SVD-based quantization methods like SVDq (Yankun et al. , 2025 ) and SVDQuant (Li et al. , 2024 ) use low-rank structure to better support mixed-precision or 4-bit quantization. SVD-LLM V2 (Wang et al. , 2025b ) also integrates SVD-based compression with GPTQ quantization to further improve accuracy under tight memory budgets. QLoRA (Dettmers et al. , 2023 ) combines 4-bit quantization with low-rank adapters for efficient fine-tuning at near full-precision quality. 3. Contributions Below we highlight how our SPQ framework is different than previous individual or partially-combined compression techniques. SVD in SPQ vs. Prior SVD. In contrast to the latest SVD-based method (SVD-LLM v2), which depends on loss approximations for weight matrix types, our SVD variance truncation method is only applied to the attention layers, which were identified experimentally as the most amenable to perplexity retention. This variance-based cutoff yields interpretable, per-layer rank ratios without requiring truncation-loss estimation or group-level allocation. Pruning in SPQ vs. Prior Pruning. Prior pruning methods (e.g., SlimLLM, LLM-Pruner) rely on complex importance estimation and multi-structure pruning, increasing implementation cost. SPQ instead prunes only MLP layers, ranks neurons by activation statistics, and derives pruning ratios via log-scale normalization. This yields a lightweight, stable, and hardware-friendly pruning strategy. Quantization in SPQ vs. Prior Quantization. Recent quantization approaches like GPTQ and AWQ achieve 4‚Äì8 bit precision using layer-wise calibration or activation-aware adjustments, but requires calibration data and solver optimization. SPQ instead applies post-training 8-bit symmetric linear quantization to all linear layers and supports per-tensor, per-channel, and hybrid scaling without calibration. This keeps quantization robust, efficient, and fully compatible with SVD and pruning. SPQ Ensemble Technique vs. Prior Combinations. Most combined approaches combine only two techniques (e.g., SVDQuant, QLoRA) and omit pruning, which is particularly effective for MLP layers. Deep Compression applied pruning and quantization to CNNs, but ignored LLM layer heterogeneity. SPQ is the first to combine SVD on attention layers, activation-based pruning on MLPs, and linear quantization across all layers in a modular, layer-aware pipeline, achieving higher compression with stable perplexity than any single method or two-way combination. Figure 1 : SPQ Framework. 1) Attention layers are compressed using variance-retained SVD, where the preserved rank is determined by a variance threshold. 2) MLP layers, per-layer activation means are computed and mapped to pruning ratios via a log-scale formula. 3) Finally, 8-bit linear quantization with different scaling strategies is applied to all linear layers. 4. SPQ Framework The following sections provides methodological details regarding our modular compression SPQ framework (Figure 1 ). Each component is independently configurable and applied to pretrained models without modifying the architecture. SPQ is evaluated on multiple LLM families (OPT, LLaMA, Vicuna, Mistral) ranging from 1B to 7B parameters. 4.1. Variance-Retained Low-Rank SVD We performed low-rank approximation of LLM attention layers using SVD. Let W ‚àà ‚Ñù m √ó n W\in\mathbb{R}^{m\times n} be the weight matrix of a attention layer. The SVD decomposition is W = U ‚Äã Œ£ ‚Äã V ‚ä§ , W=U\Sigma V^{\top}, where U ‚àà ‚Ñù m √ó m U\in\mathbb{R}^{m\times m} and V ‚àà ‚Ñù n √ó n V\in\mathbb{R}^{n\times n} are orthogonal matrices, and Œ£ ‚àà ‚Ñù m √ó n \Sigma\in\mathbb{R}^{m\times n} is diagonal with singular values œÉ 1 ‚â• œÉ 2 ‚â• ‚ãØ ‚â• œÉ min ‚Å° ( m , n ) ‚â• 0 \sigma_{1}\geq\sigma_{2}\geq\cdots\geq\sigma_{\min(m,n)}\geq 0 . We retain the top- k retained k_{\text{retained}} singular components that preserve at least an œµ \epsilon fraction of the total variance: ‚àë i = 1 k retained œÉ i 2 ‚àë j = 1 min ‚Å° ( m , n ) œÉ j 2 ‚â• œµ . \frac{\sum_{i=1}^{k_{\text{retained}}}\sigma_{i}^{2}}{\sum_{j=1}^{\min(m,n)}\sigma_{j}^{2}}\geq\epsilon. The normalized rank ratio is then r = k retained min ‚Å° ( m , n ) , r=\frac{k_{\text{retained}}}{\min(m,n)}, indicating the compression level. Lower r r implies more aggressive compression. 4.2. Activation-Based Structured Pruning We apply structured neuron pruning to the feedforward MLP layers, where entire neurons (rows of the weight matrix and corresponding biases) are removed. Let h j ( l ) ‚Äã ( x ) ‚àà ‚Ñù d h_{j}^{(l)}(x)\in\mathbb{R}^{d} denote the activation vector of neuron j j in layer l l for input x ‚àº ùíü x\sim\mathcal{D} (calibration dataset). We compute neuron magnitude as m j ( l ) = ùîº x ‚àº ùíü ‚Äã { 1 d ‚Äã ‚àë k = 1 d | h j , k ( l ) ‚Äã ( x ) | , p = 1 , 1 d ‚Äã ‚àë k = 1 d ( h j , k ( l ) ‚Äã ( x ) ) 2 , p = 2 , m_{j}^{(l)}=\mathbb{E}_{x\sim\mathcal{D}}\begin{cases}\frac{1}{d}\sum_{k=1}^{d}|h_{j,k}^{(l)}(x)|, p=1,\\[5.69054pt] \frac{1}{d}\sum_{k=1}^{d}(h_{j,k}^{(l)}(x))^{2}, p=2,\end{cases} where p ‚àà { 1 , 2 } p\in\{1,2\} : L 1 L_{1} captures mean absolute activation, while L 2 L_{2} emphasizes large activations. To obtain a per-layer magnitude summary, we compute the mean activation a ( l ) = 1 N l ‚Äã ‚àë j = 1 N l m j ( l ) , a^{(l)}=\frac{1}{N_{l}}\sum_{j=1}^{N_{l}}m_{j}^{(l)}, where N l N_{l} is the number of neurons in layer l l . Layer-wise pruning ratios r ( l ) r^{(l)} are then derived from a ( l ) a^{(l)} using one of three normalization strategies: ‚Ä¢ Linear inverse scaling n ( l ) = a ( l ) ‚àí min l ‚Å° a ( l ) max l ‚Å° a ( l ) ‚àí min l ‚Å° a ( l ) + œµ n^{(l)}=\frac{a^{(l)}-\min_{l}a^{(l)}}{\max_{l}a^{(l)}-\min_{l}a^{(l)}+\epsilon} r ( l ) = r min + ( 1 ‚àí n ( l ) ) ‚Äã ( r max ‚àí r min ) r^{(l)}=r_{\min}+(1-n^{(l)})(r_{\max}-r_{\min}) ‚Ä¢ Log-inverse normalization ‚Ñì ( l ) = log ‚Å° ( a ( l ) + Œ¥ ) \ell^{(l)}=\log(a^{(l)}+\delta) n ( l ) = max l ‚Å° ‚Ñì ( l ) ‚àí ‚Ñì ( l ) max l ‚Å° ‚Ñì ( l ) ‚àí min l ‚Å° ‚Ñì ( l ) + œµ n^{(l)}=\frac{\max_{l}\ell^{(l)}-\ell^{(l)}}{\max_{l}\ell^{(l)}-\min_{l}\ell^{(l)}+\epsilon} r ( l ) = r min + n ( l ) ‚Äã ( r max ‚àí r min ) r^{(l)}=r_{\min}+n^{(l)}(r_{\max}-r_{\min}) ‚Ä¢ Sigmoid decay s ( l ) = 1 1 + exp ‚Å° ( k ‚Äã ( n ( l ) ‚àí 0.5 ) ) s^{(l)}=\frac{1}{1+\exp(k(n^{(l)}-0.5))} r ( l ) = r min + s ( l ) ‚Äã ( r max ‚àí r min ) r^{(l)}=r_{\min}+s^{(l)}(r_{\max}-r_{\min}) Here, œµ = 10 ‚àí 8 \epsilon=10^{-8} prevents division by zero, Œ¥ = 10 ‚àí 6 \delta=10^{-6} avoids log ‚Å° ( 0 ) \log(0) , k = 10 k=10 controls sigmoid steepness, and r min , r max r_{\min},r_{\max} define the pruning ratio range. 4.3. Post-Training Linear Quantization We apply 8-bit symmetric linear quantization to all linear layers. This post-training strategy preserves structural modifications (e.g., pruning and SVD), remains compatible with LoRA fine-tuning, and consistently reduces weight memory by about 25% while maintaining perplexity. For a given weight