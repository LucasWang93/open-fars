Title: Layer-wise LoRA fine-tuning: a similarity metric approach

Abstract: Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA

Body: Layer-wise LoRA fine-tuning: a similarity metric approach 1 Introduction 2 Related Work 3 Preliminaries and Proposed Method 4 Experiments 5 Conclusions 6 Acknowledgments A The first layer of encoder-only models Layer-wise LoRA fine-tuning: a similarity metric approach Keith Ando Ogawa Bruno Lopes Yamamoto Lucas Lauton de Alcantara Lucas Pellicer Rosimeire Pereira Costa Edson Bollis Anna Helena Reali Costa Artur Jordao Abstract Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at GitHub Machine Learning, PEFT, LoRA 1 Introduction Pre-trained Large Language Models (LLMs) exhibit remarkable performance on natural language processing and now shape a new wave of progress in solving complex reasoning tasks (Bengio and et al., 2025 ; Maslej, 2025 ) . However, this effectiveness comes with a substantial computational cost during the training and fine-tuning phases of LLMs. For example, a regular 16-bit fine-tuning of LLaMA with 65B parameters requires over 780GB of GPU memory (Dettmers et al. , 2023 ) . Such computational and infrastructural demands drive the industry to lead the most significant advances in frontier AI research, as academia and small research centers lack resources to develop large and high-capacity models (Maslej, 2025 ; Morrison and et al., 2025 ) . To address the previous issues, parameter-efficient fine-tuning (PEFT) methods emerge as alternatives to enable fine-tuning in low-resource environments (Ding et al. , 2023 ) . These techniques reduce the number of trainable parameters by freezing the majority of them and keeping, or introducing, a small amount to optimize (Ding et al. , 2023 ) . In particular, Low-Rank Adaptation (LoRA) gains prominence due to its simplicity and effectiveness when fine-tuning and no additional inference latency (Hu et al. , 2022 ) . Building upon the hypothesis that the change in weights during model adaptation has low intrinsic rank, LoRA operates by freezing the original weights of the model and adapting low-rank decomposition matrices, which leads to a substantial reduction of the adjustable parameters (Hu et al. , 2022 ) . Formally, instead of optimizing the entire pre-trained weight matrix W 0 ‚àà ùêë d √ó k W_{0}\in\mathbf{R}^{d\times k} as in full fine-tuning, LoRA adapts two matrices B ‚àà ùêë d √ó r B\in\mathbf{R}^{d\times r} and A ‚àà ùêë r √ó k A\in\mathbf{R}^{r\times k} , with r ‚â™ min ‚Å° ( d , k ) r\ll\min(d,k) . Their product is the update Œî ‚Äã W ‚àà ùêë d √ó k \Delta W\in\mathbf{R}^{d\times k} , further scaled by Œ± r \frac{\alpha}{r} and added to W 0 W_{0} to generate the fine-tuned matrix W ‚àà ùêë d √ó k W\in\mathbf{R}^{d\times k} as follows: W = W 0 + Œî ‚Äã W = W 0 + Œ± r ‚Äã B ‚Äã A . W=W_{0}+\Delta W=W_{0}+\frac{\alpha}{r}BA. (1) Figure 1: Trade-offs between LoRA with and without our method. We fine-tune LLaMA 2-7B, Mistral-7B and Gemma-7B on MetaMathQA and evaluate on GSM8K. Notably, our method reduces the number of trainable parameters while preserving predictive performance (in this case, increasing accuracy) compared to fine-tuning all layers with LoRA modules (standard practice). We observe the same behavior across different architectures, including multimodal models. Despite significantly reducing the number of adjustable parameters, the remaining ones can still pose a prohibitive computational burden. Regardless of the existing challenges, LoRA and most of its variants approach the problem at the matrix level, focusing on aspects such as initialization strategies (Meng et al. , 2025 ) and decomposition formats (Zhang et al. , 2023 ) , while more recent methods explore high-rank updates (Huang et al. , 2025 ) . Higher-level choices are typically not part of these methods, despite their potential to further reduce the computational cost. In particular, determining a selection of transformer layers to fine-tune is a promising approach that is orthogonal to these methods. Unless otherwise stated, in this work, fine-tune refers to LoRA fine-tune. In this direction, recent literature indicates that layers contribute unequally to changes in internal representations. More concretely, Skean et al. 2025 refute the common assumption that final-layer representations are the most useful for downstream tasks, implying that some low and mid-depth layers play crucial roles in various tasks. Jin et al. 2025 suggest that LLMs process tasks of different complexities in different layers, from which it follows that the contribution of a layer is task-specific. This context gives rise to a natural question: Is it possible to systematically choose the most important layers for a specific task adaptation? To answer this question, we propose a simple but effective method that explores the similarities between internal representations to identify the most important transformer layers of a model for a specific task. By measuring the importance of an entire transformer layer, our technique becomes orthogonal to LoRA-like methods, enabling easy integration to push the computational efficiency even further. Figure 1 depicts this efficiency gain: we achieve results comparable to fine-tuning with all LoRA modules (standard practice) while using half of them. Research statement and contributions. In general, our work has the following research statement. Given a LLM and a task, we can effectively choose a subset of transformer layers to fine-tune by measuring their participation in changes on internal representations. We quantify this relevance to the task by assessing the similarity between the input and output representations of a layer. The greater the contribution of a layer to the representation, the lower the similarity between its inputs and outputs. This subset comprises a configuration that, when fine-tuned, leads to an effective predictive ability while notably reducing the number of trainable parameters. Among our contributions, we highlight the following. We introduce a novel systematic method for choosing transformer layers to fine-tune that relies on layer contribution to changes in internal representations. From a practical perspective, we contribute to the progress of computational efficiency involving the fine-tuning of large language and multimodal models. Featuring a layer-level granularity, our technique enables a further reduction in the number of trainable parameters in constrained scenarios. From a theoretical perspective, our results suggest not only that certain layers are more influential on the fine-tuning process for a specific task, but also that similarity metrics are adequate tools to find them, overperforming naive strategies and existing methods. Extensive experiments confirm our statement and contributions. Specifically, for encoder-only models, we achieve a 50% reduction in backbone trainable parameters compared to fine-tuning all layers with LoRA, with a maximum drop in predictive performance of 2.5 percentage points in the average score of the GLUE benchmark. Moreover, on decoder-only models, we obtain the same reduction, but with improvements in predictive performance on most tasks for LLaMA 2-7B and Mistral-7B-v0.1, and minor drops for Gemma-7B. Figure 1 illustrates these results and highlights the competitive predictive performance even with half the parameter count. Finally, for this computational budget, we observe similar results with the multimodal model LLaVA-1.5-7B . 2 Related Work LoRA and its variants. As LLMs continue to grow in scale, PEFT techniques arise as promising solutions to enhance performance-to-parameter ratio in the fine-tuning process (Ding et al. , 2023 ) . Recent efforts in this research area concentrate on LoRA-based methods, as LoRA leverages a simple but effective approach: introducing low-rank matrix decompositions to adapt large models with significantly fewer trainable parameters. (Hu et al. , 2022 ) . Extending LoRA, AdaLoRA employs an SVD-like decomposition and dynamically distributes the parameter budget among the matrices according to their importance score (Zhang et al. , 2023 ) . From a different perspective, PiSSA preserves the low-rank decomposition structure of LoRA, but initializes it with the principal singular values and vectors of the pre-trained matrix ( W 0 W_{0} ) (Meng et al. , 2025 ) . Typically, LoRA and most of its variants limit the rank of the adjustment matrix to the r r hyperparameter, constraining its representation expressiveness. On the other hand, HiRA guarantees high-rank updates by using the Hadamard product to build its adjustments (Huang et al. , 2025 ) . In contrast to these and other LoRA-based methods, our approach operates in an orthogonal direction, offering an additional option to further reduce the computational cost by selecting a set of transformer layers to fine-tune through a computationally efficient process. Our method works before starting the fine-tuning phase and relies on inference, avoiding expensive additional training steps or any operation that interferes with fine-tuning latency. We confirm that our technique works well not only with LoRA but also with its modern variants, such as PiSSA (Meng et al. , 2025 ) . Internal Representations and Similarity Metrics. The analysis of internal representations is a popular paradigm to better understand deep learning models (Jin et al. , 2025 ; Skean et al. , 2025 ) . Specifically, representation similarity metrics become powerful tools to explore complex phenomena (Klabunde et al. , 2025 ; Kornblith et al. , 2019 ) . As a concrete example, Jiang et al. 2025 use similarity metrics to improve inference latency by exploring the phenomenon of saturation of events , where model predictions are fully constructed at a specific layer and remain unchanged through the subsequent transformations. In the context of model compression, Pons et al. 2024 employ similarity metrics as an estimation of layer importance in a layer pruning procedure. Rather than promoting architecture changes, in our work, we adopt a similarity metric as an importance measure for selecting layers to fine-tune. Our method quantifies the relevance of a transformer layer to changes in internal representations by assessing the dissimilarity between input and output representations. We select layers with high dissimilarity to fine-tune, as we associate this with impact on internal representations, see Figure 2 . Layer Selection. In the vision domain, Lee et al. 2023 introduce the term surgical fine-tuning : the practice of fine-tuning only a small contiguous subset of layers. Additionally, they propose a gradient-based metric to assign an importance value to a block of layers. Applying this idea to natural language processing, Lodha et al. 2023 establish Fisher Information Matrix (FIM) score as a measure of parameter importance and assess its efficacy on encoder-only models. Our work also addresses the selection of layers to fine-tune, however, we approach the problem through the lens of internal representations. Although our technique shares properties with previous methods, in the sense of being task-specific, we contribute to the domain by proposing a novel perspective, focusing on interactions with other PEFT methods and investigating its effectiveness on modern architectures. Furthermore, our method not only exhibits better predictive performance in comparison to the FIM approach, but also leverages a computationally cheaper process as it relies on inferences instead of expensive training steps. 3 Preliminaries and Proposed Method Problem statement. Recent literature suggests that while some transformer layers play crucial roles in the construction of representations, others may be less important for specific models and data (Jiang et al. , 2025 ; Jin et al. , 2025 ; Skean et al. , 2025 ) . This evidence indicates that fine-tuning only the most important layers for a specific task could further improve the parameter efficiency of existing PEFT methods. Building upon this idea, we pose the following question: how to select a subset of transformer layers to finetune that matches the predictive performance of fine-tuning all layers? Here, the term layer refers to the entire transformer block, composed of normalizations, skip connections, attention modules, and multi-layer perceptron modules. Definitions. Let ‚Ñ± \mathcal{F} be a pre-trained large language model composed of M M transformer layers, written as L 1 , L 2 , ‚Ä¶ , L M L_{1},L_{2},\dots,L_{M} , where the embedding output is sequentially transformed from L 1 L_{1} to L M L_{M} . Define D = { ( x j , y j ) } j = 1 t D=\{(x_{j},y_{j})\}_{j=1}^{t} as a downstream dataset of t t samples for which we want to fine-tune ‚Ñ± \mathcal{F} . For each layer L i L_{i} , we define an internal representation R i R_{i} as the set of token representations at a specific position in the output of L i L_{i} across inputs x j x_{j} . Similarly, we define R 0 R_{0} as the embedding layer output representation. Given ‚Ñ± \mathcal{F} and D D , our layer importance metric assigns a value to each L i L_{i} using R 0 , ‚Ä¶ , R M R_{0},\dots,R_{M} . From this importance, we can select the N N most important ones, where N N is a parameter, enabling us to control the computational cost. In our work, we explore the Centered Kernel Alignment (CKA) similarity me