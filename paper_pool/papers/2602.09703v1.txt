Title: Maastricht University at AMIYA: Adapting LLMs for Dialectal Arabic using Fine-tuning and MBR Decoding

Abstract: Large Language Models (LLMs) are becoming increasingly multilingual, supporting hundreds of languages, especially high resource ones. Unfortunately, Dialect variations are still underrepresented due to limited data and linguistic variation. In this work, we adapt a pre-trained LLM to improve dialectal performance. Specifically, we use Low Rank Adaptation (LoRA) fine-tuning on monolingual and English Dialect parallel data, adapter merging and dialect-aware MBR decoding to improve dialectal fidelity generation and translation. Experiments on Syrian, Moroccan, and Saudi Arabic show that merging and MBR improve dialectal fidelity while preserving semantic accuracy. This combination provides a compact and effective framework for robust dialectal Arabic generation.

Body: Maastricht University at AMIYA: Adapting LLMs for Dialectal Arabic using Fine-tuning and MBR Decoding 1 Introduction 2 AMIYA Shared Task 2.1 Task Description 2.2 Datasets 2.3 Evaluation Metrics Monolingual Dialect Evaluation. Cross-Lingual Evaluation. 3 System Description 3.1 LoRA Fine-tuning 3.1.1 Monolingual Dialect Fine-tuning 3.1.2 Translation-Based Fine-tuning 3.1.3 Adapter Merging 3.2 MBR Decoding with Dialect-Aware Scoring 3.3 Adapter Merging and MBR 4 Results 4.1 JAIS-2 vs. LLaMA 3.2 4.2 Effect of Fine-tuning Data and Adapter Merging 4.3 MBR Decoding with Dialect-Aware Objectives 4.4 Final Submission 5 AMIYA Shared Task Official Results 6 Conclusion Maastricht University at AMIYA: Adapting LLMs for Dialectal Arabic using Fine-tuning and MBR Decoding Abdulhai Alali Abderrahmane Issam Department of Advanced Computing Sciences Maastricht University {abdulhai.alali@student., abderrahmane.issam@}maastrichtuniversity.nl Abstract Large Language Models (LLMs) are becoming increasingly multilingual, supporting hundreds of languages, especially high resource ones. Unfortunately, Dialect variations are still underrepresented due to limited data and linguistic variation. In this work, we adapt a pre-trained LLM to improve dialectal performance. Specifically, we use Low Rank Adaptation (LoRA) fine-tuning on monolingual and English–Dialect parallel data, adapter merging and dialect-aware MBR decoding to improve dialectal fidelity generation and translation. Experiments on Syrian, Moroccan, and Saudi Arabic show that merging and MBR improve dialectal fidelity while preserving semantic accuracy. This combination provides a compact and effective framework for robust dialectal Arabic generation. Maastricht University at AMIYA: Adapting LLMs for Dialectal Arabic using Fine-tuning and MBR Decoding Abdulhai Alali Abderrahmane Issam Department of Advanced Computing Sciences Maastricht University {abdulhai.alali@student., abderrahmane.issam@}maastrichtuniversity.nl 1 Introduction Arabic dialects exhibit substantial variation in vocabulary, morphology, and syntax, making automated generation and translation challenging. Unlike Modern Standard Arabic (MSA), Dialectal Arabic (DA) is underrepresented in NLP resources, leading to difficulties in building models that produce fluent, semantically faithful, and dialectally authentic outputs Alabdullah et al. ( 2025 ) . The AMIYA Shared Task Robinson et al. ( 2026 ) targets these challenges by evaluating LLMs on monolingual dialect generation and cross-lingual translation, emphasizing both dialect fidelity and instruction following. To address these issues, we adapt a Large Language Model (LLM) using parameter-efficient fine-tuning on monolingual and English–Dialect parallel data. We train separate Low-Rank Adaptation (LoRA) adapters Houlsby et al. ( 2019 ); Bapna and Firat ( 2019 ); Hu et al. ( 2021 ) for each task (i.e. self supervised training on monolingual data and translation on parallel data), capturing dialectal surface forms and semantic grounding, and combine them using TIES-Merging Yadav et al. ( 2023 ) . Additionally, we apply Minimum Bayes Risk (MBR) Bickel and Doksum ( 2007 ); Kumar and Byrne ( 2004 ); Deguchi et al. ( 2024 ) decoding with dialect-aware scoring to select outputs that maximize dialect authenticity during generation. Our experiments show that merging monolingual and translation-based adapters improves the balance between dialectal fidelity and semantic accuracy. MBR decoding further enhances dialectal authenticity, leading to consistent gains over single-source fine-tuning and standard decoding. While our approach is effective, the following limitations persist: the training data is relatively small, dialect identification metrics may not capture subtle or informal usage, and MBR decoding increases inference time. 2 AMIYA Shared Task 2.1 Task Description The AMIYA Shared Task focuses on improving LLMs for Dialectal Arabic (DA), which remains significantly underrepresented compared to Modern Standard Arabic (MSA) Bergman and Diab ( 2022 ) . Participants are asked to develop or adapt LLMs that can generate fluent, semantically faithful, and dialectally authentic Arabic across multiple regional varieties. Systems are evaluated using the AL-QASIDA benchmark Robinson et al. ( 2025 ) , which measures dialectal fidelity, generation quality, and robustness to MSA–DA diglossia. Evaluation includes both monolingual dialect generation and cross-lingual settings, such as English–Dialect and MSA–Dialect translation. Performance is assessed using automatic metrics—primarily Arabic Dialect Identification And DIalectnes (ADI2) Robinson et al. ( 2025 ) for dialect fidelity and character-level F-score (chrF++) Popović ( 2015a , b ) for translation quality—as well as human judgments of fluency and instruction adherence. 2.2 Datasets We participated in the closed data track of the shared task focusing on 3 out of 5 Arabic dialects provided by the task, namely: Syrian, Moroccan and Saudi Arabic. For each dialect, we combine two types of training data: Monolingual dialectal text which consists of unstructured sentences in the target dialect, and Machine Translation (MT) data with English and DA parallel text. Dialect (Type) Dataset # Samples Syrian (Mono.) Shami Corpus 25,136 Syrian (MT) UFAL 120,600 Moroccan (Mono.) DoDa 10,000 Moroccan (MT) DoDa 10,000 Saudi (Mono.) SDC 14,891 Saudi (MT) SauDial 1,000 Table 1: Datasets used per dialect and supervision type. Shami Corpus Abu Kwaik et al. ( 2018 ) , UFAL Krubiński et al. ( 2023 ) , DoDa Darija Open Dataset Contributors ( 2023 ) , SDC TaghreedT/SDC Contributors ( 2020 ) , SauDial Alanazi et al. ( 2025 ) . Table 1 summarizes the datasets and sample sizes used for fine-tuning across each dialect and supervision type. While more extensive data is available for most categories, we sub-sample the datasets to maintain computational efficiency and accelerate the experimental process. 2.3 Evaluation Metrics Evaluation is performed using different metrics depending on the task setting: Monolingual Dialect Evaluation. For monolingual generation, ADI2 metric is used. ADI2 score was proposed in Robinson et al. ( 2025 ) to measure whether LLMs generate outputs that are dialectal, and whether they are faithful to the specific requested dialect. The level of dialectness is measured using Arabic Level of Dialectness of text (ALDI) Keleg et al. ( 2023 ) , and the dialect class C C is predicted using a dialect identification baseline model from Nuanced Arabic Dialect Identification (NADI) 2024 shared task Abdul-Mageed et al. ( 2024 ) . More formally, ADI2 score Robinson et al. ( 2025 ) is defined as: score ADI2 ​ ( y ) = score ALDi ​ ( y ) ∗ score NADI ​ ( y ) C \text{score}_{\text{ADI2}}(y)=\text{score}_{\text{ALDi}}(y)*\text{score}_{\text{NADI}}(y)_{C} (1) Cross-Lingual Evaluation. For translation-based and cross-lingual generation tasks, chrF++ is used for evaluation. chrF++ is well suited for morphologically rich languages such as Arabic, where it captures fine-grained character overlap and is robust to spelling variation, making it appropriate for dialectal evaluation where orthographic inconsistency is common. 3 System Description 3.1 LoRA Fine-tuning To incorporate dialectal knowledge into the base model, we use parameter-efficient fine-tuning with LoRA. Fine-tuning is performed separately for each dialect and task (i.e. self-supervised and translation), allowing the model to learn different types of information without intervention between them. Table 2 reports our training hyperparamters. These hyperparameters were chosen to ensure stable training under memory constraints while maintaining sufficient capacity for effective dialect adaptation. Hyperparameter Value Max. sequence length 512 Epochs 5 Learning rate 3e-5 Batch size (per device) 2 Effective batch size 32 Precision BF16 Table 2: Key training hyperparameters. 3.1.1 Monolingual Dialect Fine-tuning For monolingual adaptation, we fine-tune the model on raw dialectal text without any task-specific prompts. The data consists of standalone sentences written in each dialect, encouraging the model to naturally learn dialect-specific vocabulary, morphology, and sentence structure. All sentences are tokenized using the JAIS Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) ( 2025 ) tokenizer with a fixed maximum sequence length. We train the model using a standard causal language modeling objective. To make fine-tuning efficient, we apply LoRA adapters Bapna and Firat ( 2019 ); Houlsby et al. ( 2019 ); Hu et al. ( 2021 ) to the attention layers of the model and update only these additional parameters during training. Furthermore, we rely on memory-optimization techniques such as gradient accumulation and gradient checkpointing, enabling larger effective batch sizes. The model is trained for multiple epochs using a standard optimization setup. This approach allows the model to adapt strongly to dialectal surface forms while preserving the general knowledge of the base model. 3.1.2 Translation-Based Fine-tuning In addition to monolingual data, we fine-tune the model on an English–Dialect parallel dataset. This data exposes the model to aligned semantic content across languages, helping it associate dialectal expressions with their meanings and improving controllability during generation. We frame translation as an instruction-following task in both directions: English → \rightarrow Dialect and Dialect → \rightarrow English. Each training example includes a natural language instruction specifying the translation direction, followed by the target output. During training, the loss is computed only on the output tokens, while the instruction tokens are masked. This encourages the model to follow instructions without learning to reproduce them. Tokenization is performed with a fixed maximum sequence length. The same LoRA setup is used as in monolingual fine-tuning to ensure compatibility across training stages. Training is carried out with a more conservative optimization setup than monolingual fine-tuning, focusing on stable learning and semantic alignment rather than aggressive adaptation. 3.1.3 Adapter Merging The monolingual and translation-based fine-tuning strategies provide complementary supervision. Monolingual fine-tuning emphasizes dialectal fluency and authenticity, while translation fine-tuning reinforces semantic faithfulness and cross-lingual grounding. By training separate LoRA adapters for each dataset, we preserve these distinct signals and later combine them using TIES-Merging. This separation enables fine-grained control over how different sources of supervision contribute to the final dialect-aware model and minimizes intervention between them. 3.2 MBR Decoding with Dialect-Aware Scoring While fine-tuning and merging improve the model’s internal dialect representations, decoding decisions still play a crucial role in output quality. We therefore apply MBR decoding using the mbrs 1 1 1 https://github.com/naist-nlp/mbrs library to explicitly optimize for dialectness at inference time. For each input prompt, the model generates a set of N = 20 N=20 candidate responses via stochastic sampling, then each candidate is scored independently using the ADI2 metric. Finally, the candidate with the highest score is selected as the final output. 3.3 Adapter Merging and MBR TIES-based adapter merging integrates complementary dataset supervision at the parameter level, producing a compact yet expressive dialect-aware model. MBR decoding complements this by enforcing dialectal fidelity at generation time, explicitly selecting outputs that maximize dialectness. Together, fine-tuning, TIES-Merging, and MBR decoding form a unified framework that yields more consistent and authentic dialectal generation than any single technique in isolation. 4 Results The experiments were conducted exclusively on Syrian and Moroccan DA and subsequently applied to the remaining dialects for the final submission, which was trained separately per dialect. This section presents a detailed evaluation of our dialect-aware generation framework. We report results across model variants, data configurations, and decoding strategies, with the goal of understanding (1) the impact of model choice, (2) the role of different supervision signals, and (3) the effectiveness of adapter merging and MBR decoding. The evaluation datasets used are the default datasets provided by AL-QASIDA 2 2 2 https://github.com/JHU-CLSP/al-qasida . 4.1 JAIS-2 vs. LLaMA 3.2 We begin by comparing two LLMs, JAIS-2 3 3 3 https://huggingface.co/inceptionai/Jais-2-8B-Chat Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) ( 2025 ) and LLaMA 3.2 4 4 4 https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct Grattafiori et al. ( 2024 ) , to determine the most suitable backbone for Arabic dialect generation. For a fair comparison, both models are fine-tuned using the same data configuration: a merged setup that combines monolingual dialect data with English–Dialect parallel (MT) supervision. In addition, decoding is performed using MBR decoding with ADI2 score. Models are evaluated using ADI2 for monolingual dialect generation and chrF++ for translation. Table 3 presents the results. On monolingual dialect generation, LLaMA 3.2 achieves a substantially higher ADI2 score (i.e. 0.78), indicating strong dialectal surface realization and fluency. However, its performance drops sharply in translation, with a significantly low chrF++ score (i.e. 0.14), suggesting weak semantic alignment when translating from English into dialectal Arabic. In contrast, JAIS-2 exhibits a more balanced performance. While its ADI2 score (0.33) is considerably lower than that of LLaMA 3.2 for monolingual generation, JAIS-2 achieves a much higher chrF++ score (0.43) on MT-based generation. This indicates stronger semantic fidelity and better handling of translation supervision. Model ADI2 chrF++ LLaMA 3.2 0.78 0.14 JAIS-2 0.33 0.43 Table 3: Comparison between JAIS-2 and LLaMA 3.2 after fine-tuning, TIES-Merging and generation with MBR decoding on Syrian DA . On overall, although LLaMA 3.2 excels in dialectal surface form generation, its poor cross-lingual performance limits its usefulness for translation-driven dialect generation. Given our goal of building a dialect-aware system that remains reliable across both monolingual and cross-lingual scenarios, we select JAIS-2 as the backbone for all subsequent experiments. 4.2 Effect of Fine-tuning Data and Adapter Merging In this section, we analyze the impact of different fine-tuning strategies on JAIS-2. We report the results of JAIS-2 base model, JAIS-2 fine-tuned on either monolingual or translation data, and JAIS-2 with TIES-Merging. The results in Table 4 show that monolingual fine-tuning substantially improves ADI2 scores, indicating stronger dialectal identity and linguistic conformity. Furthermore, fine-