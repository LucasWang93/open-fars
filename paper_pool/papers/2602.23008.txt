Title: Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization

Abstract: Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO^2), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO^2 achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO^2 demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO^2 as a promising framework for building more exploratory and generalizable LLM-based agents.

Body: Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization 1 Introduction 2 Preliminaries 3 The Exploration Problem of LLM Agents 4 Method 4.1 Advancing Exploration with Self-Generated Memory 4.2 Parameterize non-parametric updates via hybrid policy optimization 5 Related Work 6 Experiments 6.1 ScienceWorld 6.2 WebShop 6.3 Ablation study on Mode Combinations 7 Conclusion A Pseudo Code B Prompts C Detailed Explanation of Importance Sampling Ratios in Policy Updates D Experiments Details D.1 Retrospex D.2 Online RL: ScienceWorld D.3 Online RL: WebShop E qualitative analysis on tips E.1 More Examples of Generated Tips E.2 Effects of Tips on Exploration Behavior F More Ablation Study F.1 Mode Selection Probability F.2 Role of Intrinsic Reward G Analysis of Computational Cost G.1 Cost Analysis of Memory-Augmented Rollouts G.2 Cost Analysis of Total Training Time H The Use of Large Language Models ‚Ä† ‚Ä† footnotetext: ‚àó \ast Equal contribution; work done during an internship at Microsoft Research. ‚Ä† ‚Ä† footnotetext: ‚Ä† \dagger Corresponding author. Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization Zeyuan Liu 1‚àó , Jeonghye Kim 1,2‚àó , Xufang Luo 1 ‚Ä† \dagger , Dongsheng Li 1 , Yuqing Yang 1 1 Microsoft Research 2 KAIST gritmaybe@gmail.com, jeonghye.kim@kaist.ac.kr, {xufluo, dongsli, yuqyang}@microsoft.com project page agent-lightning/empo2 Abstract Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO 2 ), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO 2 achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO 2 demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO 2 as a promising framework for building more exploratory and generalizable LLM-based agents. 1 Introduction (a) (b) Figure 1: (a) Comparison of the learning curves of GRPO and EMPO 2 (ours) on the ScienceWorld power-component task. While GRPO converges to suboptimal performance, EMPO 2 continues to improve and accomplish the task. (b) Comparison of EMPO 2 and other baselines in in-distribution (ID) and out-of-distribution (OOD) settings on and WebShop. In ID experiments, it adapts well to familiar environments, achieving 128.6% on ScienceWorld and 11.3% on Webshop improvements over GRPO. In OOD experiments, it also shows strong performance with few trials and no weight updates, indicating effective use of memory to explore unfamiliar environments. Full results are in Tables 6.1 , 2 , and Figure 8 . Large Language Models (LLMs) have recently emerged as powerful agents capable of reasoning, planning, and interacting with external environments (Achiam et al. , 2023 ; Park et al. , 2023 ; Yao et al. , 2023 ; Kim et al. , 2025 ) . When combined with reinforcement learning (RL), such agents can adapt their behavior based on experience and feedback, enabling them to go beyond static prompting or supervised fine-tuning (Guo et al. , 2025 ; Tan et al. , 2024 ) . This paradigm has driven recent progress in areas such as interactive decision-making, tool use, and embodied AI (Feng et al. , 2025b ; Lu et al. , 2025b ; Feng et al. , 2025a ; Dong et al. , 2025 ; Luo et al. , 2025 ) . However, a key limitation of current LLM-based agents lies in their reliance on exploiting prior knowledge rather than engaging in systematic exploration. While RL frameworks emphasize balancing exploration and exploitation, many LLM-agent systems primarily leverage pretrained knowledge and conduct only limited search within familiar distributions. As a result, these agents often struggle in environments where progress depends on discovering novel states or actively acquiring new information, rather than reusing what is already known. To address this challenge, recent research has incorporated external memory modules into LLMs as a form of long-term memory. This enables models to leverage past experiences to correct failed attempts, thereby improving decision-making in subsequent trials without requiring parameter updates (Shinn et al. , 2023 ; Zhang et al. , 2023 ) . However, as noted in Zhang et al. ( 2023 ) , the performance of such methods tends to saturate quickly, since collecting experiences with static parameters cannot fully capture the diversity needed for continuous improvement. Figure 2: Non-parametric updates can encourage exploration, bootstrapping parametric updates. In this work, we present a unified framework that enables LLM agents to learn more effectively through broader exploration by jointly updating their parametric policy parameters with RL and their non-parametric memory module through interaction. Crucially, the non-parametric updates not only complement but also enhance the efficiency of parametric learning, thereby enabling more effective exploration and adaptation. This dual-update paradigm serves as a bridge between parameter-level optimization and memory-augmented reasoning. While memory is utilized during learning, moving toward more generalizable intelligence requires reducing dependence on external memory and instead embedding its benefits directly into the model‚Äôs parameters. To this end, we propose E xploratory M emory-Augmented On- and Off-P olicy O ptimization (EMPO 2 ), a new hybrid RL algorithm that incorporates two modes in the rollout phase‚Äîdepending on whether memory is used‚Äîand two modes in the update phase‚Äîon-policy and off-policy learning‚Äîthereby enabling agents to leverage memory when available while remaining robust in its absence. In our experiments, we evaluate EMPO 2 on two widely used multi-step embodied reasoning environments that require exploration to solve complex tasks: ScienceWorld (Wang et al. , 2022 ) and WebShop (Yao et al. , 2022 ) . We compare its performance against a range of non-parametric and parametric (offline and online) RL approaches. As summarized in Figure 1 , EMPO 2 substantially outperforms prior algorithms, achieving a 128.6% improvement on ScienceWorld and an 11.3% improvement on WebShop over the strong online RL baseline GRPO. The training curve in Figure 1 (a) further shows that, unlike GRPO, which converges prematurely to a suboptimal solution, EMPO¬≤ leverages continuous exploration and successfully solves the task. Moreover, for the OOD experiments (Figure 1 , rightmost), the model also achieves good scores with only a few trials and no weight updates, indicating that the updated model has acquired the ability to use memory to explore unseen or unfamiliar environments. These results highlight EMPO 2 as a promising direction for building more adaptive and generalizable embodied agents. 2 Preliminaries Online RL consists of alternating between a rollout phase, in which trajectories are generated using the current policy œÄ \pi parameterized by Œ∏ \theta , and an update phase, in which the policy is optimized based on those rollouts. Policy Rollout. We consider a setting where, given a sampled task u ‚àº p ‚Äã ( ùí∞ ) u\sim p(\mathcal{U}) , an LLM agent solves the task through multi-step interactions with the environment. Starting from task u u , the LLM œÄ Œ∏ \pi_{\theta} generates the first natural-language action a 1 ‚àº œÄ Œ∏ ( ‚ãÖ ‚à£ u ) ‚àà ùíú a_{1}\sim\pi_{\theta}(\cdot\mid u)\in\mathcal{A} . Executing this action, the environment returns a reward r 1 r_{1} and the next state s 1 s_{1} . At a general timestep t t , conditioned on the current state s t s_{t} and the task u u , the policy produces the next action a t + 1 ‚àº œÄ Œ∏ ( ‚ãÖ ‚à£ s t , u ) a_{t+1}\sim\pi_{\theta}(\cdot\mid s_{t},u) . This interaction loop continues until the task is completed or a maximum number of steps is reached. A rollout trajectory is thus defined as the sequence of states, actions, and rewards, œÑ = ( u , a 1 , r 1 , s 1 , a 2 , r 2 , ‚Ä¶ , s T ) . \tau=\big(u,a_{1},r_{1},s_{1},a_{2},r_{2},\ldots,s_{T}\big). Group Relative Policy Optimization. Group Relative Policy Optimization (GRPO) (Shao et al. , 2024 ) updates the policy by comparing multiple rollouts of the same task u u , removing the need for the value function in PPO (Schulman et al. , 2017 ) . Given a task u u , the policy œÄ Œ∏ \pi_{\theta} generates N N rollout trajectories { œÑ ( 1 ) , ‚Ä¶ , œÑ ( N ) } \{\tau^{(1)},\ldots,\tau^{(N)}\} . Each trajectory receives a return { R ( 1 ) , ‚Ä¶ , R ( N ) } \{R^{(1)},\ldots,R^{(N)}\} , defined as the sum of rewards along the trajectory: R ( i ) = ‚àë t = 1 T r t ( i ) . R^{(i)}=\sum_{t=1}^{T}r_{t}^{(i)}. . For each action a t ( i ) a_{t}^{(i)} taken in trajectory œÑ ( i ) \tau^{(i)} , we define its relative advantage as: A ‚Äã ( a t ( i ) ) = R ( i ) ‚àí 1 N ‚Äã ‚àë j = 1 N R ( j ) œÉ ‚Äã ( R ) , A(a_{t}^{(i)})=\frac{R^{(i)}-\frac{1}{N}\sum_{j=1}^{N}R^{(j)}}{\sigma(R)}, where actions from trajectories with higher-than-average reward obtain positive advantage, while those from lower-performing ones obtain negative advantage. The GRPO loss is then: ùîº u ‚àº p ‚Äã ( ùí∞ ) { œÑ ( i ) } i = 1 N ‚àº œÄ Œ∏ old \displaystyle\mathbb{E}_{\begin{subarray}{c}u\sim p(\mathcal{U})\\ \{\tau^{(i)}\}_{i=1}^{N}\sim\pi_{\theta_{\text{old}}}\end{subarray}} [ 1 N ‚Äã T ‚Äã ‚àë i = 1 N ‚àë t = 1 T min ‚Å° ( œÅ Œ∏ ‚Äã ( a t ( i ) ) ‚Äã A ‚Äã ( a t ( i ) ) , clip ‚Äã ( œÅ Œ∏ ‚Äã ( a t ( i ) ) , 1 ‚àí œµ , 1 + œµ ) ‚Äã A ‚Äã ( a t ( i ) ) ) ] \displaystyle\Bigg[\frac{1}{NT}\sum_{i=1}^{N}\sum_{t=1}^{T}\min\Big(\rho_{\theta}(a_{t}^{(i)})A(a_{t}^{(i)}),\text{clip}\big(\rho_{\theta}(a_{t}^{(i)}),1-\epsilon,1+\epsilon\big)A(a_{t}^{(i)})\Big)\Bigg] ‚àí Œ≤ D KL ( œÄ Œ∏ ( ‚ãÖ | u ) ‚à• œÄ ref ( ‚ãÖ | u ) ) , \displaystyle\quad-\beta\,D_{\text{KL}}\!\big(\pi_{\theta}(\cdot|u)\,\|\;\pi_{\text{ref}}(\cdot|u)\big), (1) where œÅ Œ∏ ‚Äã ( a t ( i ) ) = œÄ Œ∏ ‚Äã ( a t ( i ) | s t ( i ) , u ) œÄ Œ∏ old ‚Äã ( a t ( i ) | s t ( i ) , u ) , \rho_{\theta}(a_{t}^{(i)})=\frac{\pi_{\theta}(a_{t}^{(i)}|s_{t}^{(i)},u)}{\pi_{\theta_{\text{old}}}(a_{t}^{(i)}|s_{t}^{(i)},u)}, with Œ≤ ‚â• 0 \beta\geq 0 controlling the regularization strength toward a reference policy œÄ ref \pi_{\text{ref}} . 3 The Exploration Problem of LLM Agents LLMs encode rich prior knowledge, but such priors often fail to reflect the actual rules or dynamics of a given environment. Blind reliance on these priors can lead to erroneous behaviors, making it necessary for agents to adapt through direct interaction and trial-and-error. A key requirement for such adaptation is exploration , which involves seeking information beyond pre-training, sometimes by taking atypical or counterintuitive actions. However, current LLM-based agents struggle with this (Qiao et al. , 2024 ; Zhou et al. , 2024 ) , as it demands stepping outside the distribution of behaviors where the model feels most confident. Consequently, many prior studies have sought to align agents with new environments through warm-start supervised fine-tuning (SFT) using numerous golden trajectories (Song et al. , 2024 ; Qiao et al. , 2024 ; Xiang et al. , 2024 ) , leveraging large-scale models such as GPT-4 (Tang et al. , 2024 ; Lin et al. , 2023 ) , or employing human engineering or well-established simulation information (Choudhury and Sodhi, 2025 ) . While these methods achieve strong results in constrained settings, their effectiveness is limited to cases where such external support is available, and they generalize poorly to unseen scenarios without it. Figure 3: When training LLM with GRPO in ScienceWorld, the agent struggles because of insufficient exploration. For instance, in the task ‚Äúturn on the red light bulb,‚Äù the agent must first find the red light bulb before activating it. However, the agent fails to locate it and, as a result, cannot complete the task. Rather than analyzing the cause of failure and exploring alternative actions, the agent proceeds unchanged, so its score stagnates even as additional training steps are taken. Therefore, we focus on how to efficiently train agents in online RL through trial and error, without any prior embedding of the environment‚Äôs rules. The key challenge is that, without intrinsic exploration capability, online RL struggles to optimize effectively. As illustrated in Figure 3 , in ScienceWorld (Wang et al. , 2022 ) environment the agent is given the mission ‚Äúturn on the red light bulb.‚Äù The instructions specify that the agent should first focus on the light bulb and then build a circuit to activate it, based on the current room observation. However, since no red light bulb is present in the observation, the agent must search the environment to locate it. Instead, the agent follows the instruction literally, attempts to focus on the red light bulb, and fails because it does not exist in the room. Ideally, when an agent fails to reach its goal, it should analyze the reasons for failure and broaden its action space to discover successful strategies. Yet in representative online RL algorithms GRPO (Shao et al. , 2024 ) , prior trajectory rollouts provide no continuity beyond a scalar reward signal, thereby restricting exploration and ultimately limiting learning. 4 Method In this section, we present Exploratory Memory-augmented On- and Off-Policy Optimization (EMPO 2 ), a novel algorithm aimed at tackling the exploration challenges in online RL. EMPO 2 operates in two modes for both rollout phase and update phase. During rollout, actions can be generated either through (1) prompting without memory , where no retrieved information is used, or (2) memory-augmented prompting , conditioned on tips retrieved from memory. In the update phase, rollouts with memory-augmented prompting are used in two ways: (a) on-policy , where tips are retained and the update is performed with the original prompt, and (b) off-policy , where tips are removed during update. Notably, tips are generated not by a separate model but by the policy œÄ Œ∏ \pi_{\theta} itself, which is continually updated during training. The full algorithm is provided in Appendix A . 4.1 Advancing Exploration with Self-Generated Memory A key component of EMPO 2 is its use of memory to maintain continuity across rollouts. Information obtained from an agent‚Äôs interactions can be encoded into parameters through policy optimization, but it can also be recorded in an external memory that the agent continuously consults. Since our policy is initialized from a pretrained LLM with inherent summarization and reflection abilities, these abilities can be leveraged as auxiliary signals in addition to scalar rewards, thereby guiding exploration more effectively. To realize this, EMPO 2 integrates both parametric (parameter updates within the LLM) and non-parametric (external memory) updates, str