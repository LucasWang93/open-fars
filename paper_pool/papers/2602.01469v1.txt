Title: P-EAGLE: Parallel-Drafting EAGLE with Scalable Training

Abstract: Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.

Body: P-EAGLE: Parallel-Drafting EAGLE with Scalable Training 1 Introduction 2 Architecture 3 Scalable Training Framework for Long Contexts 3.1 Amortized Mask Construction 3.2 Sequence Partitioning 4 Training Recipe 4.1 Hidden State Design 4.2 Increasing Model Capacity 4.3 Unfreezing the Embedding Layer 4.4 Training vs. Inference Speculation Depth 4.5 Extended Training Duration 4.6 Longer Training Sequences 5 Experiments 5.1 Experimental Setup 5.2 Acceptance Length Comparison 5.3 Experimental results from vLLM 6 Related Work 7 Conclusion A Training configuration of ParallelSpec and PARD B Theoretical Justification for Redundancy of Hidden State Augmentation B.1 Application to Hidden State Augmentation B.2 Hidden State Ablation Details C 2-Layer vs 4-Layer P-EAGLE Comparison P-EAGLE: Parallel-Drafting EAGLE with Scalable Training Mude Hui Xin Huang Jaime Campos Salas Yue Sun Nathan Pemberton Xiang Song Ashish Khetan George Karypis Abstract Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting—predicting multiple tokens per forward pass—offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10×–1.36× over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B. Speculative Decoding, Large Language Models, Parallel Drafting, Inference Optimization 1 Introduction Autoregressive decoding in large language models (LLMs) presents a fundamental efficiency challenge: each token requires a complete forward pass through billions of parameters, rendering inference memory-bandwidth bound. Speculative decoding (Chen et al. , 2023 ) addresses this limitation by having a lightweight draft model propose multiple candidates autoregressively, which are then verified by the target model in a single forward pass. Among various approaches in speculative decoding, EAGLE (Li et al. , 2024 , 2025 ) has achieved widespread adoption in production inference systems, including vLLM (Kwon et al. , 2023 ) , SGLang (Zheng et al. , 2024 ) , and TensorRT-LLM (NVIDIA, 2023 ) , delivering 2–3 × \times speedups over standard autoregressive decoding. EAGLE conditions token predictions on hidden states from the target model, leveraging contextual representations that standalone drafters must learn independently through multiple transformer layers. This enables a compact single-layer architecture comprising only 2–5% of target model parameters. However, EAGLE generates draft tokens autoregressively: producing K K tokens requires K K sequential forward passes, creating significant drafting overhead. Parallel drafting presents a promising approach to eliminate the overhead of autoregressive decoding. Multiple prior works have explored parallel drafting strategies for speculative decoding (Gloeckle et al. , 2024 ; Xiao et al. , 2024b ; Cai et al. , 2024 ; Monea et al. , 2023 ; Lin et al. , 2025 ) . ParallelSpec (Xiao et al. , 2024b ) proposed parallel drafting with a single transformer layer, but omits critical implementation details—notably whether and how target model hidden states are utilized—and does not address the memory scaling challenges that arise from extended training sequences with multiple parallel prediction positions. PARD (An et al. , 2025 ) addresses this complexity through Conditional Drop-token (COD) training, which retains progressively fewer sequence positions at later parallel-prediction depths to reduce effective sequence length. However, both methods face scalability challenges when training on long sequences. The scalability limitations of existing parallel drafting methods become consequential in modern inference workloads. Reasoning-capable models produce substantially longer outputs: for example, on UltraChat dataset (Ding et al. , 2023 ) , GPT-OSS 120B (OpenAI, 2025 ) exhibits median sequence lengths of 3,891 tokens, with P90 reaching 10,800 (Figure 1 ). Figure 1 : Sequence length (prompt + generation) distribution on UltraChat dataset with GPT-OSS 120B. Reasoning level: Medium. Median: 3,891 tokens; P90: 10,800 tokens; P99: 20,000 tokens. Draft models trained on shorter sequences encounter a distribution mismatch when deployed on such workloads, exhibiting up to 25% reduction in acceptance rate on the extended reasoning traces (Table 1 ). Effective parallel drafting therefore requires scalability to long training contexts that match inference distributions. Unlike standard training where gradient accumulation operates across examples, parallel multi-token prediction amplifies memory pressure—the effective sequence length grows linearly with the number of parallel prediction positions, posing optimization challenges absent from autoregressive training. To quantify this scalability gap, we compare ParallelSpec (Xiao et al. , 2024b ) and PARD (An et al. , 2025 ) with our method under identical training conditions 1 1 1 ParallelSpec does not release code or sufficient training details; we implemented their method following the paper. PARD supports only standalone drafters; we adapted it to EAGLE’s training framework. . The results are shown in Table 1 . ParallelSpec encounters extremely low acceptance length at 1K and 4K training contexts, and out-of-memory failures at 8K+ due to quadratic attention scaling. PARD’s per-batch mask construction becomes computationally prohibitive beyond 4K contexts (see Section 3 ). Our method scales to 20K tokens while maintaining competitive acceptance length. For hyperparameters and hardware, see Appendix A Table 1 : Acceptance length (AL) comparison on the MT-Bench dataset. Target model: GPT-OSS 120B. Speculation length: 5 5 . “Infeas.” denotes computational infeasibility with 10+h per epoch. Method Layers Training context length 1K 4K 8K 20K ParallelSpec + EAGLE 3 1 1.5 1.6 OOM OOM PARD + EAGLE 3 4 2.4 Infeas. OOM OOM Ours (P-EAGLE) 4 2.4 2.8 2.9 3.0 We present P(arallel-drafting) EAGLE , which transforms EAGLE from autoregressive generation to parallel multi-token prediction with scalable training. Our contributions are as follows. 1. Scalable training framework for long contexts (Section 3 ): We develop amortized mask construction and sequence partitioning to address a unique challenge in parallel-prediction training: attention memory scales quadratically with the product of sequence length and prediction depth. Our sequence partitioning technique splits a single sequence into segments for gradient accumulation while preserving attention dependencies. 2. EAGLE-based parallel drafting architecture (Section 2 ): We introduce a learnable shared hidden state that enables EAGLE to generate multiple draft tokens in a single forward pass. Theoretical analysis shows attention alone encodes sufficient positional information, eliminating the need for position-specific hidden states. Ablations demonstrate this simple design outperforms four position-aware alternatives by 7–15%. 3. Optimzed training recipe (Section 4 ): Through systematic ablations, we establish P-EAGLE training best practices including architecture depth, train-inference prediction depth alignment, and embedding strategies. 4. Production deployment (Section 5 ): We implement P-EAGLE in vLLM. Through comprehensive benchmarking, P-EAGLE demonstrates consistent speedups of 1.10×–1.36× over autoregressive EAGLE-3 across GPT-OSS 120B, 20B (OpenAI, 2025 ) and Qwen3-Coder 30B (Team, 2025 ) . 2 Architecture Figure 2 : P-EAGLE architecture. The target model (top) processes prompt tokens and produces hidden states from layer indexes 2, L / 2 L/2 , and L − 1 L-1 (concatenated to 3 ​ d 3d dimensions), where L L is the number of decoder layers. The P-EAGLE drafter (bottom) takes these hidden states for the Next-Token Prediction (NTP) position (Pos 1), which operates like standard autoregressive prediction with actual context. Multi-Token Prediction (MTP) positions (Pos 2-4) use a learnable shared hidden state h shared h_{\text{shared}} since they lack preceding hidden states. Token embeddings are combined with projected hidden states and processed through N N transformer layers. We present the P-EAGLE architecture in Figure 2 . The drafter follows the LLaMA 3 architecture with rotary positional embeddings (RoPE) (Su et al. , 2024 ) . Background. We first overview autoregressive EAGLE using Figure 2 as reference. To predict token t 1 t_{1} , the hidden state from the target model is concatenated with the token embedding, processed through transformer layers to produce a hidden vector, and passed through the LM head. This corresponds to the Next-Token Prediction (NTP) position (Pos 1) in the figure. To generate token t 2 t_{2} , the drafter takes the predicted token t 1 t_{1} and the hidden vector used to predict t 1 t_{1} (before passing through the LM head) as input for the next forward pass. Similarly, generating t 3 t_{3} uses the predicted token t 2 t_{2} and the hidden vector used to predict t 2 t_{2} . Producing K K draft tokens thus requires K K sequential forward passes. Challenge for parallel drafting. Generating K K tokens in parallel eliminates sequential forward passes but introduces a new problem: positions predicting t 2 , t 3 , … t_{2},t_{3},\ldots (which we call MTP positions) lack the predicted tokens and hidden vectors from previous steps. P-EAGLE addresses this with two learnable parameters: a shared hidden state h shared h_{\text{shared}} that substitutes for the missing hidden vectors, and a mask token embedding that substitutes for the unknown previous tokens. This enables all K K tokens to be generated in a single forward pass. We compare four alternative hidden state designs in Section 4.1 , finding this simple shared approach outperforms position-aware variants by 7–15%. Additional design choices. P-EAGLE unfreezes the token embeddings inherited from the target model, as the mask token embedding must be learned to encode meaningful input for MTP positions (Section 4.3 ). We also use a deeper architecture, with four layers achieving 46% higher acceptance length than one layer (Section 4.2 ). 3 Scalable Training Framework for Long Contexts Training a parallel token prediction model requires extending each sequence of length n n to accommodate K K parallel prediction depths, where depth k k predicts the token k + 1 k+1 positions ahead. Without optimization, this creates n × K n\times K total positions with O ​ ( ( n ​ K ) 2 ) O((nK)^{2}) attention complexity, causing out-of-memory failures at long sequences. PARD (An et al. , 2025 ) addresses this with Conditional Drop-token (COD) sampling, which reduces the number of positions at each prediction depth (also referred to as “groups” in PARD). Specifically, COD applies geometric decay: depth 0 retains all n n positions, depth 1 randomly retains n × r n\times r positions, depth 2 retains n × r 2 n\times r^{2} , and so on, where r ∈ ( 0 , 1 ) r\in(0,1) is the retention rate. The total positions across all depths becomes n × ( 1 + r + r 2 + ⋯ + r K − 1 ) n\times(1+r+r^{2}+\cdots+r^{K-1}) rather than n × K n\times K , significantly reducing attention memory. However, because COD samples different positions randomly for each training example, PARD must construct a custom attention mask per example. This mask enforces causal constraints across prediction depths: positions at depth d d can only attend to positions from earlier depths, not to depths d + 1 d+1 or beyond (which do not exist at inference time). Constructing these masks requires O ​ ( ( n ​ K ) 2 ) O((nK)^{2}) operations per example, becoming prohibitively expensive when training on long sequences (Table 1 ). We address this bottleneck with two techniques: amortized mask construction (Section 3.1 ) and sequence partitioning (Section 3.2 ). 3.1 Amortized Mask Construction The key insight enabling our approach is that the causal structure across prediction depths is position-invariant : the attention pattern for positions 0 through n n is identical regardless of total sequence length. This means a mask for any sequence can be obtained by extracting the top-left ( n × K ) × ( n × K ) (n\times K)\times(n\times K) submatrix from a pre-computed maximum-length mask, as illustrated in Figure 3 . Figure 3 : Position-invariance of causal attention across prediction depths. G0, G1, G2 in the figure denote prediction depths 0, 1, 2, where depth d d predicts the token d + 1 d+1 positions ahead. The mask for a shorter sequence (right) is exactly the top-left submatrix of a longer sequence’s mask (left), enabling constant-time retrieval. We exploit this property by constructing the attention mask once at training initialization for the maximum sequence length. During training, per-example masks are obtained via tensor slicing—a constant-time view operation requiring no additional memory allocation. The one-time initialization cost is amortized across millions of training steps, with a fixed memory footprint independent of dataset size. The practical impact is substantial. Table 2 shows that at 2048-token sequences, PARD’s per-example mask construction causes 48 × \times data loading slowdown and 5 × \times epoch time increase. Our pre-computed approach eliminates this bottleneck entirely. Table 2 : Training overhead (2048 tokens, K = 8 K=8 ). Data loading measured on 128 examples. Epoch measured on UltraChat (200K examples), 8 × \times H200 GPUs. Method Load (128 ex.) Epoch EAGLE-3 2 2 2 EAGLE-3 uses Training-Time Test (Li et al. , 2025 ) , requiring multiple forward passes per example. 14.8s 2.5h PARD 718.5s (48 × \times ) 12+h (5 × \times ) Ours 17.5s 1.8h 3.2 Sequence Partitioning Pre-computed masks eliminate construction overhead, but memory remains a bottleneck as sequences grow. Consider an 8192-token sequence with K = 8 K=8 prediction depths and retention rate r = 0.8 r=0.8 . The total positions across all depths follows n × ( 1 − r K ) / ( 1 − r ) n\times(1-r^{K})/(1-r) , yielding approximately 34K positions. Attention memory scales as O ​ ( L 2 ) O(L^{2}) with total positions L L , while embeddings and output logits scale as O ​ ( L ) O(L) . Training at longer sequences requires managing this memory growth, which introduces two challenges. Challenge 1: Within-sequence gradient accumulation. Standard gradient accumulation addresses memory constraints by splitting a batch into micro-batches, where each micro-batch contains one or more complete training examples. This assumes indiv