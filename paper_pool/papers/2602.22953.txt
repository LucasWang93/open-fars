Title: General Agent Evaluation

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

Body: General Agent Evaluation 1 Introduction 2 Unified Protocol Methodology 2.1 Agent Benchmark Unified Protocol 2.2 Methodology for Adapting Existing Benchmarks 2.3 Methodology for Adapting Existing Agents 3 Exgentic Framework 3.1 Solving the Integration Problem 4 Experimental Setup 4.1 Benchmarks 4.2 Agents 4.2.1 Agent Components 4.3 Metrics 5 Results 5.1 Key Leaderboard Findings 5.2 No Single Agent Dominates Across Task Domains 5.3 Model Quality Drive Performance 5.4 Model Stability to Agent Architectures 5.5 Agent Components Effect 5.6 Cross-Benchmarks Agent Stability 5.7 Cost-Efficiency Tradeoffs 5.8 Failure Patterns and Agent Behavioral Differences 5.9 The Current State of General-Purpose Agents 6 Related Work 7 Discussion A Detailed Benchmark Agent Interaction Example B Benchmark Adaptation B.1 SweBench Task Definition Example B.2 BrowseComp Task Definition Example B.3 Appworld Task Definition Example B.4 Tau2Bench Task Definition Example C Agent Components D Detailed Results D.1 References to Leaderboards D.2 Steps Counts E Statistical Significance F Limitations General Agent Evaluation Elron Bandel Asaf Yehudai Lilach Eden Yehoshua Sagron Yotam Perlitz Elad Venezian Natalia Razinkov Natan Ergas Shlomit Shachor Ifergan Segev Shlomov Michal Jacovi Leshem Choshen Liat Ein-Dor Yoav Katz Michal Shmueli-Scheuer Abstract The promise of general-purpose agents—systems that perform tasks in unfamiliar environments without domain-specific engineering—remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Solo Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic—a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents: www.exgentic.ai . Machine Learning, ICML IBM Research 1 Introduction # General Agent Model Avg Success Avg Cost App World Browse Comp+ SWE BenchV Tau 2 Airline Tau 2 Retail Tau 2 Telecom 1 OpenAI Solo Claude Opus 4.5 .73 $8.5 .68 .61 .81 .74 .85 .84 2 Claude Code Claude Opus 4.5 .67 $8.0 .66 .53 .74 .66 .83 .76 3 Smolagent Claude Opus 4.5 .66 $4.4 .70 .61 .65 .72 .78 .58 4 ReAct Short Gemini 3 .62 $0.7 .55 .48 .71 .70 .82 .73 5 ReAct Short Claude Opus 4.5 .62 $3.8 .64 .49 .61 .66 .78 .76 6 ReAct Gemini 3 .61 $0.8 .51 .48 .71 .70 .82 .73 7 ReAct Claude Opus 4.5 .61 $5.8 .61 .49 .61 .66 .78 .76 8 OpenAI Solo Gemini 3 .60 $2.8 .58 .33 .72 .62 .73 .89 9 Claude Code Gemini 3 .57 $2.5 .36 .51 .67 .70 .78 .69 10 Smolagent Gemini 3 .56 $1.8 .13 .57 .76 .68 .76 .88 11 ReAct Short GPT 5.2 .46 $0.3 .22 .46 .57 .54 .73 .54 12 ReAct GPT 5.2 .41 $0.2 .00 .46 .57 .54 .73 .54 13 OpenAI Solo GPT 5.2 .39 $0.2 .00 .48 .55 .50 .54 .53 14 Claude Code GPT 5.2 .38 $0.4 .00 .43 .58 .48 .51 .55 15 Smolagent GPT 5.2 .38 $0.4 .07 .26 .53 .60 .68 .71 Table 1 : The Open General Agent Leaderboard comparing emerging general agents across standardized benchmarks. Average Success represents the mean success rate across benchmarks; Average Cost represents the mean cost per task. Performance is strongly influenced by backbone model choice. The field of AI agents has witnessed remarkable progress, with agentic systems demonstrating impressive capabilities across diverse domains—from solving software engineering tasks to navigating web interfaces (Zhang et al. , 2024 ; Deng et al. , 2023 ) . However, current progress largely relies on domain specialization and manual tuning; whereas, heterogeneous real-world settings demand general-purpose agents capable of scalable deployment without such manual customization (c.f., Marreed et al. , 2025 ; Bandel et al. , 2026 ) . Despite their importance, current evaluation practices cannot adequately assess general-purpose agent capabilities. Existing agentic benchmarks like SWE-Bench Verified (Jimenez et al. , 2023 ) and τ 2 \tau^{2} -Bench (Yao et al. , 2024 ) provide valuable assessments of domain-specific agents. Yet, they impose two constraints preventing general-agent evaluation: they use bespoke communication protocols (Anonymous, 2026 ) , and they implicitly assume agents have prior knowledge of benchmark-specific goals and environment semantics. Recent consolidation efforts like BrowserGym (Chezelles et al. , 2025 ) and Harbor (Shaw, 2025 ) have integrated multiple benchmarks within single domains, by exposing to the agent the current goals and environment semantics (Fig. 2 (B)). While a step forward, these frameworks still enforce a single protocol (web-based for BrowserGym, CLI-based for Harbor), preventing agents from using their native integration mechanisms and effectively evaluating a diminished version of the agent (Yehudai et al. , 2025 ) . We set general-purpose AI agents as a research target, propose a concrete method for evaluating them, and present the first systematic analysis of general agents across diverse environments (Fig. 3 ). Specifically, our contributions are threefold. (1) We present the Unified Protocol , a benchmark-agent mediation protocol (Fig. 2 (C)). The Unified Protocol bridges the communication between agent interfaces (e.g., CLI, tool-calling APIs, MCP) and benchmarks through a canonical task representation, decoupling evaluation from domain-specific implementations and communication protocols. (2) Based on the Unified Protocol, we release Exgentic an evaluation harness for general agents that supports modular insights—comparing architectures, analyzing language model impact, and optimizing agent-model pairings. (3) Running Exgentic we present the first public Open General Agent Leaderboard to guide general agent development, totaling in overall cost of $22K (See Table 1 ). Our analysis of the Open General Agent Leaderboard highlights both the capabilities and limitations of contemporary general-purpose agents. While these agents demonstrate notable cross-domain generalization—often performing on par with domain-optimized baselines—their success is primarily dictated by the underlying language model (Fig. 1 ). Conversely, different agentic scaffolds exhibit comparable performance, despite substantial variance in cost. Together, these findings point to the potential of general agents. Ultimately, advancing general-purpose agents requires a collective effort. We hope the Open General Agent Leaderboard serves as a catalyst for approaches that transcend individual tasks and invite the research community to expand this ecosystem by contributing benchmarks that challenge generalization and novel evaluation protocols. Figure 1 : Cost-performance tradeoffs across agent-model configurations. The Pareto frontier (red dashed line) shows optimal tradeoffs: GPT 5.2 configurations offer the best cost-efficiency while Claude Opus 4.5 achieve the highest performance at 3-33 × \times higher cost. Figure 2 : Evolution of Agentic Evaluation. (A) Collection of separate benchmarks, each requiring a custom agent or an agent with specific adaptation per benchmark (HAL) (B) Multiple benchmarks consolidated through a single protocol, such as CLI, or Web (C) Multiple benchmarks consolidated through a common protocol that can be adapted to any agent’s protocol (Exgentic). 2 Unified Protocol Methodology This work provides an evaluation solution for any general agent on any agentic benchmark, overcoming the common case of incompatibility between agent and benchmark protocols that either prevent evaluation (Fig. 2 (B)), or require costly pairwise adaptation for each agent and benchmark (Fig. 2 (A)). To address these limitations, we introduce a Unified Protocol that serves as a mediation layer between agents and benchmarks. The Unified Protocol serves as a “narrow waist”, adding a new agent (or benchmark) only needs adhering to it rather than to all benchmarks (agents). Thus, it significantly reduces integration complexity, development effort and learning curve. The Unified Protocol is not an imposed standard, but rather one derived from existing agent and benchmark communication patterns. As such, it naturally accommodates and unifies prevalent interaction protocols—enabling faithful translation between any agent-benchmark pair that employs these paradigms. 2.1 Agent Benchmark Unified Protocol The protocol defines instances that are passed between the benchmark and the agent. Each instance has three fields: task, context, and actions. Here we demonstrate them with τ 2 \tau^{2} -Bench as our running example (see other benchmark examples in Appendix B ). 1. Task : What the agent should do? A textual description of the task. In τ 2 \tau^{2} -Bench, it is ”You are a customer service agent that helps the user according to the policy provided below. Try to be helpful and always follow the policy.” . In addition, the first user utterance, such as ”Cancel my flight reservation AH3BDS” , is passed to the agent separately as the first observation from the environment. 2. Context : What the agent should know? Additional information provided to the agent to accomplish the task. In τ 2 \tau^{2} -Bench, the context contains the policy . We note that the agent can use the context in different ways. For example, the agent can naively append it to the task or store it in a dedicated agent memory or document store for conditional retrieval. 3. Actions : What can the agent do? A set of environment actions. These actions constitute the complete set of operations the environment makes available for performing the task. Each action specifies a typed set of parameters and may return one or more observations of arbitrary types. In τ 2 \tau^{2} -Bench for the airline domain, some actions are cancel ​ _ ​ reservation ⁡ ( r ​ e ​ s ​ e ​ r ​ v ​ a ​ t ​ i ​ o ​ n ​ _ ​ i ​ d ) \operatorname{cancel\_reservation}(reservation\_id) and search ​ _ ​ direct ​ _ ​ flight ⁡ ( o ​ r ​ i ​ g ​ i ​ n , d ​ e ​ s ​ t ​ i ​ n ​ a ​ t ​ i ​ o ​ n , d ​ a ​ t ​ e ) . \operatorname{search\_direct\_flight}(origin,destination,date). Reviewing existing protocols and agents, we observed that many introduce special handling for two specific types of interactions with the environment: (1) sending a message to a user, and (2) submitting a final answer to the benchmark, signaling that the agent has completed the task. To support these common interaction patterns, the Unified Protocol allows implementers to optionally designate one action as the message action and one as the final‑answer action. 2.2 Methodology for Adapting Existing Benchmarks Existing agent benchmarks are typically coupled with specific interaction protocols, and often implicitly assume that agents possess prior knowledge of the benchmark’s semantics, or that a human will manually perform the integration. A representative example is SWE-Bench Verified 1 1 1 SWE-Bench Verified . Each task specifies a GitHub repo, a base commit, and a free‑text bug description, with the expected output being a patch. The benchmark does not define how agents should access the repo or submit fixes—those details are left to the integrator. For general‑purpose agents without human intervention, this interface must be explicit. However, we cannot arbitrarily decide on a setup; instead, we derive the interface from a reference agent implementation. For SWE-Bench Verified , we examined mini-SWE agent 2 2 2 mini-SWE agent as the reference implementation. There, the agent is placed in a bash environment where the repository has already been cloned. When the agent outputs COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT , the system automatically generates a patch and submits it for evaluation. This design fully specifies how the agent interacts with the benchmark, what actions it may take, and how it submits solutions—implictly indicating that repository cloning and patch creation are not evaluation targets. Accordingly, in the Exgentic protocol for SWE-Bench Verified , we introduce two explicit actions: one for executing bash commands and another for submitting a patch constructed from the agent’s code modifications. To define the protocol’s task and context fields, we review both the benchmark tasks and the reference implementation prompts. Many benchmark tasks include irrelevant implementation details, while key instructions appear only in the reference agent’s internal prompts. For instance, in τ 2 \tau^{2} -Bench, the reference prompt states: “You are a customer service agent that helps the user according to the policy below.” Such essential information belongs in the benchmark task itself and is included in the Exgentic task definition. In contrast, instructions like “Each turn you may either message the user or make a tool call, but not both” are excluded because they assume a particular tool‑calling protocol. In summary, we decouple each benchmark from its original protocol by making all agent‑visible assumptions explicit. First, we inspect the reference agent to see how it interacts with the environment and what actions and observations it uses. Then we build task descriptions that include only the information needed for the agent to solve the task, omitting implementation‑specific details and redundant signals. This yields tasks that preserve the benchmark’s intended semantics while remaining independent of any particular agent architecture or communication protocol, making them suitable for evaluating any general agent implementation. Figure 3 : Open General Agent Leaderboard is the first benchmark to consistently test general-agent architectures across key skills in diverse environments. 2.3 Methodology for Adapting Existing Agents Existing agents interface with existing environments through specific protocols such as MCP, python functions, or tool calls. They also receive the task description through some command line or programmatic API. Adapting agents to the Unified Protocol involves deciding how to map the task, context and actions of the protocol to the agents’ specific API. It is important to note that the agent adaptor is benchmark agnostic. The textual task descriptions are typically concatenated with the context fields to textual instructions passed to the model. While not implemented today, the context may be used in different ways. For example, an MCP-based agent may opt to store the