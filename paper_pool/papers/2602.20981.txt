Title: Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models

Abstract: Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.

Body: Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models 1 Introduction 2 Related work 3 Pilot Study 4 Proposed Method 4.1 Preliminaries 4.2 Base Architecture Multimodal conditioning inputs. 4.3 Core Network 4.4 Hierarchical Framework 5 Experiments 5.1 Comparison with the state-of-the-arts 5.2 Analysis and Ablation Study 6 Conclusions 7 Datasets and Settings 8 The Details of MMHNet 9 Additional Experiments Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models Christian Simon ‚Ä† Masato Ishii ‚ô£ Wei-Yao Wang ‚Ä† Koichi Saito ‚ô£ Akio Hayakawa ‚ô£ Dongseok Shim ‚Ä† Zhi Zhong ‚Ä† Shuyang Cui ‚Ä† Shusuke Takahashi ‚Ä† Takashi Shibuya ‚ô£ Yuki Mitsufuji ‚Ä†,‚ô£ ‚Ä† Sony Group Corporation ‚ô£ Sony AI {first_name.last_name}@sony.com Abstract Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations. Our project page: https://echoesovertime.github.io . 1 Introduction Figure 1 : Long-Video to Audio (LV2A) task overview. The challenge is framed as training models on fixed-length segments while requiring them to generalize to variable-length (long-form) audio outputs during inference. Video-to-Audio (V2A) is a generative task that aims to produce realistic and contextually aligned audio from silent video inputs. This capability holds substantial promise for enhancing sound design workflows, particularly in domains such as film and gaming [ 27 , 53 ] . Despite its potential, existing V2A methods [ 4 , 19 , 41 , 30 , 55 ] are primarily tailored for short-form audio generation, typically spanning 8‚Äì10 seconds. Among these, diffusion-based approaches [ 4 , 30 , 55 ] have shown superior performance over transformer-based autoregressive models [ 41 ] , largely by denoising fixed-length noise segments, which is a strategy well-suited for brief clips. However, extending these models to long-form video inputs is challenging due to limited training data and the substantial memory requirements for modeling extended audio sequences. For instance, on some publicly available long audio-video datasets [ 11 , 10 ] , the distributions mostly cover only up-to 1 minute video. When applied to long-video-to-audio (LV2A) tasks, existing models trained on fixed-length segments struggle to accommodate longer sequence generation in testing, thereby constraining their effectiveness in real-world applications. We are interested in train-short and test-long problems where the longer video duration (up to 5 minutes) could be generated properly using only short clips in our training data as shown in Figure 1 . Generating short clips for each short duration could be an alternative for LV2A [ 54 ] . Despite its practicality, this method often results in fragmented audio experiences, marked by disjointed transitions, unaligned sound events, and degraded audio quality stemming from its limited grasp of long-form video context. Please see Sec. 3 to see our early observations. In particular, we identify that existing V2A models [ 4 , 19 , 41 , 30 , 55 , 49 ] expose structural constraints that reduce the generalizability in terms of various length generation and performance. The core base architecture of these models relies on transformer models [ 48 ] . Thus, these existing models depend on explicit positional encodings that are difficult to tame when dealing with longer sequence generation. Explicit positional encodings often hurt generalization to longer sequences [ 21 ] .Fortunately, Mamba [ 13 , 6 ] is introduced as an alternative to transformer modules, showing strong performance on various tasks and modalities [ 15 , 16 , 13 , 6 , 39 ] . Thus, there is an alternative to avoid using explicit positional encodings, which is deteriorating in generating long outputs. To tackle the challenges in LV2A generation, we introduce MMHNet, a novel framework that reconceptualizes the task as one of multimodal alignment across modalities with varying token lengths. Our proposed method could effectively align between modalities and handle long video and audio without further adjustment in the model during inference.MMHNet combines a multimodal video-to-audio (V2A) model with the HNet architecture [ 18 ] , enabling audio synthesis conditioned on diverse multimodal inputs while effectively aligning visual and textual modalities. HNet enhances token processing through a hierarchical structure, moving beyond conventional attention mechanisms. By replacing standard attention blocks with HNet and incorporating dynamic chunking, routing, and smoothing modules, MMHNet achieves effective and coherent audio generation over long durations. Unlike causal models, MMHNet leverages video conditions, which are non-causal, maintaining a global receptive field that supports high-quality audio synthesis for long videos. Our method operates in a compressed space during early layers, where multimodal alignment occurs to effectively integrate tokens from different sources and reduce redundancy. This approach leverages inherent overlaps in visual and audio data ( e.g . , similar frames and sound events within the same timeframe) [ 50 , 36 ] . We introduce multimodal based routing to bridge distinct modalities and apply time-based token routing to reduce temporal complexity and enhance cross-modal alignment. To evaluate MMHNet‚Äôs capabilities, we introduce a long-form V2A evaluation benchmark built upon the UnAV100 [ 10 ] and LongVale [ 11 ] datasets. Our experimental results show that MMHNet not only sets a new standard in long-form audio generation but also consistently delivers high-quality outputs across different durations. The contributions of our work are threefolds: ‚Ä¢ We introduce the length generalization challenge by training on short, fixed-length audio-visual data and evaluating on long-form video-to-audio (V2A) generation tasks using the UnAV100 and LongVale datasets. ‚Ä¢ We propose MMHNet, a multimodal hierarchical network that integrates MMAudio and hierarchical networks for efficient and consistent long-form audio generation. ‚Ä¢ We conduct extensive experiments across long-form benchmarks, validating MMHNet‚Äôs superior performance and ability to scale with video duration. 2 Related work Video-to-audio generation. Video-to-audio synthesis aims to generate sound that is both semantically and temporally aligned with visual content. Existing methods typically fall into two categories: 1) those that inject visual features into pre-trained text-to-audio (TTA) models, and 2) those that train video-to-audio (V2A) models from scratch. Approaches like T2AV [ 32 ] and FoleyCrafter [ 55 ] enhance visual consistency and alignment by integrating visual and textual embeddings into audio generation pipelines. Meanwhile, models e.g . , Diff-Foley [ 30 ] and Frieren [ 51 ] leverage contrastive pre-training and flow matching to improve multimodal coherence. MMAudio [ 4 ] further advances this field with a hybrid architecture combining multimodal and single-modality diffusion transformer (DiT) blocks [ 33 ] , incorporating synchronization features validated by Synchformer [ 20 ] and visual semantic features from CLIP [ 37 ] . V-AURA [ 49 ] is proposed as an autoregressive method to generate audio from given video frames. However, all of these methods are only well-suited for short-form video-to-audio generation, which limits the capability in generating audio beyond the duration covered during training. HunyuanVideo-Foley [ 40 ] was recently introduced, showcasing strong audio generation capabilities from diverse inputs such as text, SigLIP visual embeddings [ 47 ] , and Synchformer [ 20 ] . Nevertheless, previous approaches have yet to fully unlock the potential for generating audio beyond the scope of training data. Figure 2 : We analyze the role of positional embeddings in V2A models such as MMAudio [ 4 ] , built on MMDiT [ 24 ] . Without positional embeddings (a), MMAudio fails to capture temporal structure, producing redundant audio dominated by prominent visual objects ( e.g . , car crashing). With adjusted positional embeddings (b), alignment improves but sound quality degrades over long sequences (see scene C). (c) On UnAV100 [ 10 ] , both configurations show performance drops across durations, with MMAudio without positional embeddings performing worst in distribution matching (FD ‚Üì P ‚Äã A ‚Äã N ‚Äã N {}_{PANN}\downarrow ) and multimodal alignment (IB-Score ‚Üë \uparrow ). Multimodal models. Multimodal conditioning ( e.g . , video and text) is vital to current generative models [ 4 , 30 , 49 , 38 , 43 ] , with many V2A systems relying on Transformer architectures for multimodal processing. While Transformers are effective in multimodal tasks [ 33 ] , their dependence on positional embeddings limits generalization to durations beyond training. Position scaling, e.g . , NTK or interpolation, is often required to extend their temporal range [ 46 , 34 , 3 ] . In contrast, Mamba [ 13 , 6 ] processes sequences without positional embeddings, enabling efficient long-duration generation without modifications. Long video-to-audio generation. LoVA [ 5 ] represents the current state-of-the-art in long-video-to-audio generation, leveraging DiT-based architectures [ 33 ] to produce coherent and temporally aligned audio tracks from extended video inputs. It significantly outperforms earlier models in generating synchronized and contextually appropriate audio for long-form video content. Despite its strengths, LoVA exhibits limitations when tasked with generating audio beyond the one-minute mark, often resulting in noticeable degradation in audio quality and coherence. Autoregressive models [ 49 , 41 ] offer an alternative approach, showing promise in long-form generation due to their step-by-step prediction capabilities. However, they are prone to error accumulation over time, which can lead to drift and loss of fidelity in extended sequences. Another promising direction involves agent-based methods [ 54 ] , which divide long videos into shorter, manageable segments and generate audio for each clip independently. While this segmentation strategy can improve scalability and maintain quality, it introduces additional complexity by requiring accurate text descriptions for each segment and precise control over clip transitions to ensure seamless audio continuity. 3 Pilot Study Why do Transformer-based V2A models fail to generalize to long sequences? We observe that certain aspects of the current Transformer architecture in V2A, specifically positional embeddings [ 3 ] and attention logit exploding [ 14 ] , pose challenges to length generalization unless substantial modifications are made in the inference mode. Problems with positional embeddings. Positional embeddings like RoPE [ 44 ] are essential for Transformer-based models, as they provide positional awareness for tokens. Without them, the model loses this capability. Training without positional embeddings is only viable when training and testing use identical sequence lengths. To explore this, we conduct a pilot study analyzing a pretrained Transformer-based model ( e.g . , MMAudio [ 4 ] ) trained on 8-second audio-visual data and tested on longer sequences ( e.g . , 40 seconds), As illustrated in Figure 2 , pretrained video-to-audio models without positional embeddings would perform poorly. Also, the generated sound becomes homogeneous for the model without positional embeddings because attention modules are orderless and the semantic meaning becomes less on point relative to the positions as shown in Figure 2 (a). Figure 2 (c) shows that increasing durations degrade the Transformer based V2A model significantly, 3-4 points drop for distribution matching (FD PANNs ) and multimodal alignment (IB) scores. Designing a network without positional embeddings is preferable in this case to avoid unnecessary adjustments when generating longer sequences during testing. 4 Proposed Method Let ùíü \mathcal{D} be a dataset where each sample ( ùíô , ùíÑ ) ‚àà ùíü ({\bm{x}},{\bm{c}})\in\mathcal{D} comprises an audio ùíô {\bm{x}} and associated conditions ùíÑ {\bm{c}} ( e.g . , video frames and a text caption). The objective is to train a model on ùíü \mathcal{D} to learn a conditional distribution p model ‚Äã ( ùíô ‚à£ ùíÑ ) p_{\text{model}}({\bm{x}}\mid{\bm{c}}) that closely approximates the true data distribution p data ‚Äã ( ùíô ‚à£ ùíÑ ) p_{\text{data}}({\bm{x}}\mid{\bm{c}}) via flow matching [ 29 , 28 ] . Our focus lies particularly on scenarios where ùíô {\bm{x}} represents a long-form audio, significantly exceeding the lengths typically handled by existing methods, which often operate on short clips of approximately 10 seconds during both training and inference. To effectively model long-form audio distributions, we design the core architecture using Mamba-2 variants [ 6 , 42 ] , which enable token processing without relying on positional embeddings. This choice is motivated by our observation in Sec. 3 that positional embeddings tend to degrade performance in long audio generation scenarios. Additionally, to enhance cross-modal alignment, we incorporate routing strategies that reduce token redundancy by compressing repetitive information, thereby improving efficiency and coherence across modalities. 4.1 Preliminaries Flow matching. We employ the conditional flow matching objective [ 28 , 29 ] for generative modeling. For detailed methodology, we refer readers to [ 28 ] . Briefly, during inference, a sample is generated by first drawing noise ùíô 0 {\bm{x}}_{0} from a standard normal distribution. An ODE solver is then used to numerically integrate from time t = 0 t=0 to t = 1 t=1 , guided by a learned, time-dependent conditional velocity vector field: v Œ∏ ‚Äã ( t , ùíÑ , ùíô ) : [ 0 , 1 ] √ó ‚Ñù C √ó ‚Ñù d ‚Üí ‚Ñù d , v_{{\theta}}(t,{\bm{c}},{\bm{x}}):[0,1]\times\mathbb{R}^{C}\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}, where ùíÑ {\bm{c}} denotes the conditioning input ( e.g ., video and 