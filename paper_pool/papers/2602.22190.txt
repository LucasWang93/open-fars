Title: GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL

Abstract: Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.

Body: GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL 1 Introduction 2 Related Work 2.1 Datasets for Training GUI Agents 2.2 VLM Post-training for GUI Agents 3 Preliminaries VLM-based GUI agents. High-level vs. low-level GUI tasks. Post-training for GUI Models. 4 Reasoning Data Curation for GUI Agents 4.1 Data Curation and Filtering Pipeline 4.1.1 Data Sources 4.1.2 Unified Structured Format 4.1.3 Action-aligned Reasoning Augmentation 4.1.4 Data Filtering for SFT SFT Dataset Statistics. 4.1.5 Data Filtering for RL 5 GUI-Libra 5.1 SFT with Long CoT Hurts GUI Grounding 5.2 Action-Aware Supervised Fine-Tuning Mixed reasoning and direct-action supervision. Action-aware reweighting. 5.3 Reinforcement Learning from Partial Verifiable Rewards Setup. Offline vs. online metrics. Two quantities controlling predictability. Takeaway. 5.3.1 Why Standard RLVR is Easier to Predict? 5.3.2 KL Regularization Improves Predictability KL-induced bounds for occupancy mismatch and off-demo validity mass (informal). 5.4 Success-adaptive Negative Gradient Scaling 5.5 Reward Function Implementation 5.6 Overall Training Framework for GUI-Libra 6 Experiments 6.1 Experimental Setups GUI-Libra Details. Evaluation Benchmarks. Baselines. 6.2 Performance on Offline and Online GUI Navigation Benchmarks 6.2.1 Offline Benchmarks 6.2.2 Online Benchmarks 6.3 Action-aware SFT and RL Mitigate Grounding Performance Degradation 6.4 On the Effectiveness of KL Regularization for RL 6.5 Ablations Ablation of Data Filtering for SFT and RL. Ablations on ASFT and RL for Navigation Tasks. Ablations of KL Regularization Coefficient. Ablations of Reasoning in Model Training and Inference. Ablation of SNGS. 6.6 RL with Mixed Navigation and Grounding Data 7 Conclusion A Additional Related Works Reinforcement Learning from Verifiable Rewards (RLVR) Post-training for VLM-based Agents. B Implementation Details Action Space. SFT. RL. Evaluation. C Benchmark Details C.1 Grounding Benchmarks C.2 Offline GUI Navigation Benchmarks Multimodal-Mind2Web-v2 AndroidControl-v2 C.3 Online GUI Navigation Benchmarks AndroidWorld WebArena-Lite-v2 Online-Mind2Web D Additional Results D.1 Grounding as a Single-step Verifiable Setting D.2 Comparing Different Models for Reasoning Augmentation D.3 Comparing with Uniform Negative Gradient Scaling Strategy D.4 Additional Metrics on Offline Benchmarks AndroidControl-v2. MM-Mind2Web-v2. E Proofs for Theoretical Analysis E.0.1 When does offline step-wise matching predict online success? Discussion. E.0.2 KL regularization improves predictability Takeaway. F Prompt Templates F.1 Prompt for Reasoning Augmentation F.2 SFT Data Example F.3 RL Data Example G Long-Horizon Trajectory Case Studies GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL Rui Yang 1† Qianhui Wu 2∗ Zhaoyang Wang 3† Hanyang Chen 1 Ke Yang 1† Hao Cheng 2 Huaxiu Yao 3 Baolin Peng 2 Huan Zhang 1 Jianfeng Gao 2 Tong Zhang 1 1 UIUC, 2 Microsoft, 3 UNC-Chapel Hill https://gui-libra.github.io Abstract Open-source native GUI agents have made rapid progress in visual grounding and low-level action execution, yet they still lag behind closed-source systems on long-horizon navigation tasks that demand both high-level reasoning and precise actions. This gap stems from two limitations in the open-source ecosystem: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard supervised fine-tuning (SFT) with long chain-of-thought (CoT) reasoning often hurts grounding accuracy, and (ii) step-wise RLVR-tyle training faces partial verifiability , where multiple actions can be correct at a given state but only a single demonstrated action is used for verification. This causes reward ambiguity and makes offline step-wise metrics weak predictors of online task success during RL training. In this work, we present GUI-Libra , a systematic study and tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware supervised fine-tuning that mixes reasoning-then-action and direct-action supervision, and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization and show, both theoretically and empirically, that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion, while strengthening the alignment between offline metrics and online performance. In particular, GUI-Libra-4B and GUI-Libra-8B improve their base models by +15.6% and +12.2% on AndroidWorld, +4.0% and +8.7% on Online-Mind2Web, and +12.5% and +11.3% on WebArena-Lite-v2, respectively. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents. Figure 1 : Overview of GUI-Libra. Using only a subset of existing open-source GUI trajectories, we tackle key limitations of prior training pipelines through action-aligned reasoning data curation, action-aware SFT, and conservative RL, yielding consistent gains on online benchmarks. 1 Introduction Large vision–language models (VLMs) have become a central building block for graphical user interface (GUI) agents (Qin et al., 2025b ; Xu et al., 2025c ; Gou et al., 2025 ; Wang et al., 2025a ; Bai et al., 2025a ) , enabling autonomous systems to interpret visual interfaces and output executable actions to complete complex tasks across digital platforms. Among these approaches, native GUI agents (Qin et al., 2025b ) refer to a single end-to-end model that directly maps user instructions and observations to executable actions, without relying on external planners or separate grounding modules. Recent open-source native agents have achieved substantial progress in visual grounding and low-level action execution (Xu et al., 2025c ; Wang et al., 2025a ; Liu et al., 2025c ; Wang et al., 2025d ) , significantly narrowing the gap with proprietary systems. Despite these advances, native GUI agents remain less effective at long-horizon decision making, where agents must reason over extended observation–action sequences and adapt their behavior reliably to achieve user-specified goals. Advancing native GUI agents increasingly depends on effective post-training of GUI-centric VLMs, yet current approaches face two intertwined bottlenecks. The first is the scarcity of high-quality, action-aligned reasoning data. Existing GUI navigation datasets (Li et al., 2024 ; Zheng et al., 2024 ; Xu et al., 2025c ) often lack explicit rationales, contain only short or weakly grounded reasoning traces, or include noisy action labels, providing limited supervision for learning robust and interpretable policies. The second bottleneck is the widespread use of generic post-training recipes that do not fully account for the unique properties of GUI agents. Most existing open-source pipelines rely on either supervised fine-tuning (SFT) on brief rationales (Wu et al., 2025b ; Xu et al., 2025c ) or reinforcement learning (RL) primarily targeting grounding accuracy (Luo et al., 2025 ; Lu et al., 2025 ; Zhou et al., 2025b ; Yang et al., 2025b ) . In practice, these approaches expose a persistent tension between reasoning and grounding: incorporating chain-of-thought (CoT) often degrades grounding performance, leading many methods to suppress explicit reasoning rather than addressing the underlying trade-off. Meanwhile, motivated by the success of RL from verifiable rewards (RLVR) in domains such as mathematical reasoning (Shao et al., 2024 ; Yu et al., 2025 ) , recent work (Hong et al., 2025 ; Yang et al., 2025c ) has explored step-wise RL for GUI agents. However, these methods overlook a fundamental characteristic of GUI interaction, partial verifiability: at each step, multiple actions may correctly advance the task, yet offline supervision verifies only a single demonstrated action . As a result, alternative valid actions are ambiguously treated as failures, introducing biased gradients, destabilizing training, and weakening the connection between offline evaluation metrics and online task success. To address these challenges, we propose GUI-Libra, a unified post-training framework designed to strengthen decision making in native GUI agents. GUI-Libra is driven by three insights. (i) High-quality rationales and careful data filtering are essential for data-efficient learning, especially because open-source GUI trajectories are often noisy and weakly annotated. (ii) During SFT, CoT tokens can dominate the training loss and interfere with grounding; effective learning therefore requires explicitly prioritizing action and grounding tokens, which directly determine execution. (iii) Under partially verifiable rewards, RL can become unstable without conservative constraints: unlike standard RLVR settings where dropping KL regularization often helps (Yu et al., 2025 ; Liu et al., 2025d ; Zhou et al., 2025b ; Yang et al., 2025b ) , GUI agents benefit from moderate, KL-regularized RL that mitigates reward ambiguity and distribution shift, improving robustness and offline–online alignment. Guided by these insights, GUI-Libra integrates action-aware supervised fine-tuning (ASFT) with conservative reinforcement learning. To alleviate the scarcity of high-quality reasoning data, we develop a scalable construction and filtering pipeline and release a curated 81K GUI reasoning dataset with improved alignment between reasoning traces and executable actions. In ASFT, GUI-Libra trains on a mixture of reasoning-then-action and direct-action supervision, and applies action-aware token reweighting to emphasize action and grounding tokens, reducing the grounding degradation caused by long CoT traces. In RL, GUI-Libra optimizes policies with GRPO (Shao et al., 2024 ) under moderate KL regularization, and further introduces a success-adaptive negative gradient scaling strategy to reduce bias from ambiguously “negative” outcomes. Our pipeline has two practical advantages : (1) it derives all training data from existing open-source resources, showing that careful augmentation, filtering, and training method design can make modest open data competitive with closed-data systems ; and (2) it avoids costly online environment interaction, making training scalable and accessible while strengthening the connection between offline metrics and online task success. Extensive experiments across web and mobile benchmarks show that the GUI-Libra series (3B–8B) consistently improves offline step-wise accuracy on standard offline benchmarks and boosts online task completion on AndroidWorld (Rawles et al., 2025 ) , WebArena-Lite-v2 (Liu et al., 2025c ) , and Online-Mind2Web (Xue et al., 2025 ) . Notably, GUI-Libra-4B and GUI-Libra-8B improve their base models by +15.6% and +12.2% on AndroidWorld, and +4.0% and +8.7% on Online-Mind2Web. Detailed ablations further confirm the roles of action-aware supervision and conservative regularization in mitigating grounding degradation, strengthening action prediction, and stabilizing learning under partially verifiable feedback. We also analyze the impact of data filtering and explicit reasoning, and study the trade-off between reasoning and grounding during RL training. We hope these findings and open-source resources will encourage future work on data-efficient and reliable post-training for native GUI agents. Our main contributions are summarized as follows: • We present GUI-Libra , a unified post-training framework for native GUI agents that tackles two key challenges: reasoning–grounding interference in SFT, addressed by action-aware SFT that emphasizes action and grounding tokens; and weak offline-to-online predictability in RL, addressed by conservative optimization that constrains policy drift and improves offline–online alignment. • We develop a scalable data construction and filtering pipeline and release a high-quality open-source 81K GUI reasoning dataset with improved action alignment. • We achieve consistent gains across representative offline and online web and mobile benchmarks, showing that smaller native VLMs trained on modest open-source data can match or even outperform much larger systems. 2 Related Work Table 1: Comparison of existing training recipes from different perspectives. The task type column indicates the training target of the released models: denotes GUI grounding, denotes GUI navigation (both online and offline), and denotes offline step-wise action prediction. Name Task Type Reasoning SFT RL Open Weights Open Data Open Code OS-Atlas ( Wu et al. , 2025b ) ✗ ✓ ✗ ✓ ✓ ✗ AGUVIS ( Xu et al. , 2025c ) short ✓ ✗ ✓ ✓ ✓ UGround ( Gou et al. , 2025 ) ✗ ✓ ✗ ✓ ✓ ✓ ScaleCUA ( Liu et al. , 2025c ) long ✓ ✗ ✓ ✓ ✓ OpenCUA ( Wang et al. , 2025d ) long ✓ ✗ ✓ ✓ ✓ UI-TARS ( Qin et al. , 2025b ) short ✓ ✓ ✓ ✗ ✗ GLM-4.1-V ( Hong et al. , 2025 ) long ✓ ✓ ✓ ✗ ✗ Ferret-UI Lite ( Yang et al. , 2025c ) long ✓ ✓ ✗ ✗ ✗ UI-R1 ( Lu et al. , 2025 ) short ✗ ✓ ✓ ✓ ✓ GUI-R1 ( Luo et al. , 2025 ) short ✗ ✓ ✓ ✓ ✓ GTA1 ( Yang et al. , 2025b ) ✗ ✗ ✓ ✓ ✓ ✓ GUI-Libra (Ours) long ✓ ✓ ✓ ✓ ✓ 2.1 Datasets for Training GUI Agents Recent progress in GUI agents has been propelled by a diverse ecosystem of datasets that target both visual perception and task execution. For robust visual grounding and screen parsing, datasets such as SeeClick (Cheng et al., 2024b ) , UGround (Gou et al., 2025 ) , GUIAct (Chen et al., 2025c ) , ScaleCUA (Liu et al., 2025c ) , and GUI-360 (Mu et al., 2025 ) provide large corpora of annotated screenshots and UI element supervision (Deka et al., 2017 ; Li et al., 2020b , a ; Bai et al., 2021 ; Wu et al., 2023 ; Yang et al., 2025a ; Zheng et al., 2025b ; Wu et al., 2025b ; Nayak et al., 2025 ; Luo et al., 2025 ) . Moving beyond single-step grounding, several large-scale context-aware and trajectory-based datasets capture multi-step interactions in realistic environments, enabling models to learn how UI state evolves over time. Examples include AITW (Rawles et al., 2023 ) , MM-Mind2Web (Zheng et al., 2024 ; Deng et al., 2023 ) , AMEX (Chai et al., 2025 ) , GUI Odyssey (Lu et al., 2024 ) , and Aria-UI (Yang et al., 2