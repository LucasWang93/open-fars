Title: From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors

Abstract: Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models.

Body: From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors 1 Introduction 2 Related Works 3 Method 3.1 Problem Formulation 3.2 Physics-Driven Data Construction Hierarchical Physics Categories. Structured Generation. Camera Movement and Principle-driven Verification. Constraint-Aware Reasoning Generation. 3.3 PhysicEdit Framework Physically-Grounded Reasoning. Implicit Visual Thinking. Timestep-Aware Dynamic Modulation. From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors Liangbing Zhao Le Zhuo Sayak Paul Hongsheng Li Mohamed Elhoseiny Abstract Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K , a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit , an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models. All code, checkpoints, and datasets are available at https://liangbingzhao.github.io/statics2dynamics/. Machine Learning, ICML Figure 1 : Bridging semantic alignment and physical plausibility. (Top) Despite high semantic fidelity, existing editing models frequently violate physical principles. (Bottom) Traditional image editing treats editing as a black box, learning a discrete mapping with underspecified constraints. Our approach reformulates editing as a Physical State Transition , leveraging continuous dynamics to constrain the state transition space from unreal hallucinations to physically valid trajectories . 1 Introduction Instruction-based image editing, which aims to generate a new image following the given instruction, enables users to do visual creation much more easily. As user demands evolve from simple style transfer or object replacement to more complex scenarios involving hypothetical or counterfactual changes, the focus has shifted toward reasoning-based editing ( huang2024smartedit ; he2025reasoning ; zhuo2025reflection ) . This demand has contributed to the emergence of unified multi-modal models (UMMs), such as Bagel ( deng2025emerging ) and Nano Banana ( nanobanana ) , which leverage the inherent textual reasoning capabilities of Multimodal Large Language Model (MLLMs) to bridge the gap between abstract user intent and visual execution. While these unified frameworks have established a new backbone for image editing, their reasoning modules operate primarily at the semantic level rather than physical causality. Consequently, despite high semantic fidelity, state-of-the-art models frequently hallucinate artifacts that violate fundamental physical principles, as they prioritize object matching over physical plausibility. This drawback becomes particularly apparent in scenarios governed by strict physical interactions. Consider a common scenario: inserting a straw into a transparent glass of water. As shown in Figure 1 , while existing models can correctly identify the object (‚Äústraw‚Äù) and the location (‚Äúin the glass of water‚Äù), they frequently fail to render the optical refraction phenomenon, where the straw should appear disjointed or bent at the water surface. Instead, they tend to generate a naive straw‚Äôs position, maintaining geometric rigidity while violating optical physical laws. This discrepancy reveals a fundamental limitation: current models maximize semantic alignment at the cost of physical plausibility. To bridge this gap, we argue that the next frontier in visual creation lies in physics-aware image editing: a paradigm where generated content must rigidly adhere to the causal rules of the physical world. To achieve this goal, we propose a fundamental shift in problem formulation: the editing process should not be viewed as a static mapping between independent images, but rather as a predictive physical state transition. Under this formulation, the source image represents an initial state, and the edit instruction specifies an external interaction or trigger that drives the scene toward a subsequent state under physical laws. A central challenge is that standard paired images supervision in image editing provides only boundary conditions, leaving the transition itself underspecified. By contrast, temporal sequences show intermediate evidence of how states evolve, making video a natural source of supervision for learning transition priors. Therefore, we construct PhysicTran38K, a large-scale video-based dataset tailored to physical state transitions. Unlike existing benchmarks that predominantly emphasize semantic operations (e.g., add/remove/replace), PhysicTran38K is organized around interaction-driven triggers and law-governed transitions. We design hierarchical physics categories covering five major physical domains, 16 intermediate sub-domains, and spanning 46 distinct transition types. By employing a two-stage filtering pipeline, we finally get and annotate 38,000 high-quality transition data, providing explicit supervision for how physical states evolve over time. However, leveraging video data introduces a practical mismatch: while videos supervise training, intermediate states are unavailable during inference. We address this train-test discrepancy by introducing PhysicEdit, a physics-aware editing framework built on Qwen-Image-Edit ( wu2025qwen ) that learns from video trajectories while remaining compatible with the single-image inference workflow. We propose a textual-visual dual-thinking mechanism that decouples physical understanding into two branches. First, a physically-grounded reasoning branch uses a frozen Qwen2.5-VL-7B ( Qwen2.5-VL ) to produce structured physical constraints as textual context. Second, an implicit visual thinking branch introduces learnable transition queries that are trained to learn transition priors from video. Concretely, intermediate keyframes provide supervision through two complementary encoders (DINOv2 ( oquab2023dinov2 ) for structural semantics and a VAE ( wu2025qwen ) for fine-grained appearance), and the transition queries are aligned to these structure- and texture-level targets via dual projection heads. Finally, we align this guidance with diffusion‚Äôs coarse-to-fine generation through a timestep-aware modulation strategy that emphasizes structure at high noise and texture details at low noise. In summary, our contributions are as follows: ‚Ä¢ We propose PhysicEdit, an end-to-end physics-aware editing framework with a textual-visual dual-thinking mechanism. It combines physically-grounded reasoning with implicit visual thinking to leverage transition priors learned from videos, enabling physically faithful edits. ‚Ä¢ We construct PhysicTran38K, a large-scale video-based dataset of approximately 38k video-instruction pairs organized by hierarchical physics categories. ‚Ä¢ Extensive experiments demonstrate that PhysicEdit achieves state-of-the-art performance among evaluated open-source models, while performing comparably to leading proprietary models, establishing a strong baseline for physics-aware image editing. 2 Related Works Benefit from powerful diffusion models ( ho2020denoising ; songscore ; bakrtoddlerdiffusion ) in generating high-fidelity images, instruction-based image editing has made rapid progress. Early diffusion-based editors ( hertz2022prompt ; zhao2023cross ) typically manipulate cross-attention or invert latents to trade off content preservation and edit strength, but often lack fine-grained controllability under complex structural changes. This motivates instruction-tuned approaches ( brooks2023instructpix2pix ; flux2024 ; wei2024omniedit ; zhuo2025factuality ; zhuo2025reflection ) and, more recently, unified multimodal models for generalized instruction alignment. A representative example is Qwen-Image-Edit ( wu2025qwen ) , which conditions an MMDiT ( esser2024scaling ) on frozen Qwen2.5-VL ( Qwen2.5-VL ) multimodal representations to replace standard text embeddings. In parallel, video data has been explored as a prior for improving editing. Earlier works ( deng2025emerging ; xiao2024omnigen ; chen2025unireal ) construct training pairs from video keyframes to enhance consistency, whereas recent efforts ( wu2025chronoedit ; rotstein2025pathways ) shift toward generative reasoning. Notably, the concurrent work ChronoEdit ( wu2025chronoedit ) explicitly synthesizes intermediate frames as reasoning steps but incurs substantial computation and potential error accumulation. In contrast, we adopt an implicit paradigm that distills physical state transition priors into compact latent queries, enabling efficient feature-space dynamics simulation while inheriting physical fidelity from video data. A more detailed discussion of related work is provided in Appendix LABEL:sec:app_related . Figure 2 : Overview of the PhysicTran38K construction pipeline. Starting from hierarchical physics categories, we synthesize videos using Wan2.2-T2V-A14B, filtered by ViPE with an adaptive strategy to preserve high-dynamic transitions. Candidate videos conduct principle-driven verification by GPT-5-mini, adhering to a rigorous retention rule. Finally, Qwen2.5-VL-7B performs constraint-aware annotation, generating instructions and structured reasoning while incorporating verification results to prevent hallucinations. 3 Method 3.1 Problem Formulation To bridge the gap between semantic alignment and physical fidelity, we first formalize the task of physics-aware image editing. Conventionally, instruction-based image editing is modeled as a discrete conditional mapping. Given a source image I s ‚Äã r ‚Äã c I_{src} and an editing instruction T e ‚Äã d ‚Äã i ‚Äã t T_{edit} , the model approximates a function ‚Ñ± \mathcal{F} : I t ‚Äã g ‚Äã t = ‚Ñ± ‚Äã ( I s ‚Äã r ‚Äã c , T e ‚Äã d ‚Äã i ‚Äã t ) , I_{tgt}=\mathcal{F}(I_{src},T_{edit}), (1) where the goal is to generate a target image I t ‚Äã g ‚Äã t I_{tgt} that follows T e ‚Äã d ‚Äã i ‚Äã t T_{edit} while preserving relevant content from I s ‚Äã r ‚Äã c I_{src} . While effective for semantic edits (e.g., changing a dog to a cat), this formulation treats the transformation as a black-box pixel update and does not explicitly model the underlying dynamics that govern how the scene should evolve. In this work, we treat physics-aware editing not as a static mapping, but as a Physical State Transition . Let I s ‚Äã r ‚Äã c I_{src} represent the initial physical state S 0 S_{0} of a scene, and T e ‚Äã d ‚Äã i ‚Äã t T_{edit} represent an external force or interaction trigger ( e.g. , ‚Äúdrop the glass‚Äù). The editing process is effectively a simulation of the time-evolution of the system under physical laws Œ© \Omega ( e.g. , gravity, fluid dynamics): S f ‚Äã i ‚Äã n ‚Äã a ‚Äã l = S 0 + ‚à´ 0 œÑ Œ¶ ‚Äã ( S t , T e ‚Äã d ‚Äã i ‚Äã t ; Œ© ) ‚Äã ùëë t , S_{final}=S_{0}+\int_{0}^{\tau}\Phi(S_{t},T_{edit};\Omega)\,dt, (2) where Œ¶ \Phi denotes the state transition dynamics and œÑ \tau is the duration of the interaction. The desired target image I t ‚Äã g ‚Äã t I_{tgt} is the visual outcome of the accumulated state S f ‚Äã i ‚Äã n ‚Äã a ‚Äã l S_{final} . The fundamental challenge arises because standard image editing dataset provide only the boundary conditions, i.e. , ( I s ‚Äã r ‚Äã c , I t ‚Äã g ‚Äã t ) (I_{src},I_{tgt}) , leaving the transition dynamics Œ¶ \Phi completely underspecified. As illustrated in Figure 1 , for a given pair ( I s ‚Äã r ‚Äã c , T e ‚Äã d ‚Äã i ‚Äã t ) (I_{src},T_{edit}) , there may exist multiple visually plausible endpoints, yet only a subset results from a valid physical trajectory under Œ© \Omega . Without constraints on the intermediate integral, models tend to violate specific physical laws that satisfy the instruction. We therefore leverage videos as supervision, since they expose intermediate states and directly constrain how states evolve over time, motivating our construction of PhysicTran38K (Section 3.2 ). During inference, however, intermediate states are unavailable, so we propose PhysicEdit (Section 3.3 ) to distill these transition priors from videos into a latent representation, effectively guiding the generation along a physically valid trajectory. Figure 3 : Overview of the PhysicEdit framework. (a) Training: We distill physical transition priors from video data into learnable transition queries. These queries are supervised by complementary visual features extracted from intermediate keyframes. (b) Inference: PhysicEdit follows a sequential workflow. The frozen MLLM first generates physically-grounded reasoning, which is then concatenated with the learned transition queries to serve as the condition for the diffusion backbone. 3.2 Physics-Driven Data Construction We construct PhysicTran38K, a video-based dataset for physics-aware image editing, by casting editing as physical state transitions . As illustrated in Figure 2 , our data construction pipeline is designed to transform hierarchical physics categories into high-quality video data through three stages: structured generation with a video generation model, camera movement and principle-driven verification, and constraint-aware reasoning generation. We provide all the system prompts in Appendix LABEL:sec:sys_prompts . Hierarchical Physics Categories. We begin by establishing a comprehensive physics taxonomy to ensure broad coverage of physical dynamics. As shown in Figure 2 (a), we organize physical laws into five primary state domains: Mechanical , Thermal , Material , Optical , and Biological . Under these domains, we define 16 intermediate sub-domains and 46 transition types (e.g., refraction , melting , germination ), which serve as the specific physical laws Œ© \Omega governing our dataset. For each transition, we curate specialized object pools containing entities that naturally exhibit the corresponding physical changes. Structured Generation. Leveraging the transition categories, we synthesize videos utilizing a structured generation pipeline. We utilize GPT-5-mini to sample objects from the object pool and to instantiate a fixed Wan Prompt template used for video generation: [Start State] + [Trigger Event] + [Transition 