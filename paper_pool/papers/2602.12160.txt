Title: DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation

Abstract: Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.

Body: DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation 1 Introduction 2 Related Work 2.1 Joint Audio-Video Generation 2.2 Controllable Video Generation Model 3 Methodology 3.1 Problem Formulation 3.2 Framework 3.2.1 Symmetric Conditional DiT 3.2.2 Dual-Level Disentanglement 3.3 Multi-Task Progressive Training 3.4 Inference Pipeline 4 Experiments 4.1 Setup 4.2 Comparison 4.3 Ablation Studies 5 Conclusion A Appendix A.1 Comparison Results on RV2AV and RA2V A.2 Data Construction Details A.3 MLLM-Based Judge A.4 User Study A.5 More Visual Results 1]Tsinghua University 2]Intelligent Creation Lab, ByteDance \contribution [‚ãÜ]Equal contribution \contribution [‚Ä†]Project Lead \contribution [¬ß]Corresponding author DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation Xu Guo Fulong Ye Qichao Sun Liyang Chen Bingchuan Li Pengze Zhang Jiawei Liu Songtao Zhao Qian He Xiangwang Hou [ [ ( February 13, 2026 ) Abstract Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni , a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications. \checkdata [Project Page (Demo, Codes, Models)] https://guoxu1233.github.io/DreamID-Omni/ \undefine@key newfloatplacement \undefine@key newfloatname \undefine@key newfloatfileext \undefine@key newfloatwithin 1 Introduction Figure 1 : Showcase of DreamID-Omni . DreamID-Omni seamlessly unifies reference-based audio-video generation (R2AV), video editing (RV2AV), and audio-driven video animation (RA2V). Recently, joint audio-video generation has seen rapid progress, with many breakthrough works emerging. For example, commercial models such as Veo3, Sora2, Wan 2.6 [ wan2025wan ] and Seedance 1.5 Pro [ chen2025seedance ] have achieved impressive results. In the open-source community, models like Ovi [ low2025ovi ] and LTX-2 [ hacohen2026ltx ] have also demonstrated promising performance. These advances have greatly promoted the development of joint audio-video generation. However, in real-world applications, supporting more controllable generation particularly within human-centric scenarios is crucial. Controllable human-centric generation has advanced in several directions. Works such as Phantom [ liu2025phantom ] and Wan2.6 [ wan2025wan ] utilize reference images or voice timbres for video (R2V) or audio-video (R2AV) generation, which rely solely on text prompts as a weakly-constrained guidance. To achieve higher controllability, other approaches introduce stronger supervision, such as source videos or driving audio, for strongly-constrained generation. For instance, Humo [ chen2025humo ] animates videos (RA2V) based on reference identities and driving audio, while works like HunyuanCustom [ hu2025hunyuancustom ] and VACE [ jiang2025vace ] perform video editing given a reference identity and source video, which can be further extended to replace the corresponding audio (RV2AV). Despite these advancements, these capabilities are largely treated as isolated tasks. Researchers in the video-only domain have begun to shift toward unified architectures [ jiang2025vace , ye2025unic , liang2025omniv2v , yang2025many , qu2025vincie , he2025fulldit2 ] to enhance task flexibility and reduce the operational overhead of deploying multiple models. However, the joint audio-video domain still lacks a unified perspective. Fundamentally, we observe that R2AV, RV2AV, and RA2V all share an identical objective: mapping a static identity anchor (image and audio) onto a dynamic spatio-temporal canvas (text, source video, or driving audio). Based on this insight, these tasks are inherently amenable to a unified framework trained on a consistent data source, transcending the limitations of task-specific silos. Nevertheless, developing this unified framework presents several challenges: (1) How to build a unified model framework that supports generation, editing and animation; (2) How to address identity-timbre binding and speaker confusion in multi-person generation; (3) How to design effective training strategies to prevent conflicts among multiple tasks. To address these challenges, we introduce DreamID-Omni , which integrates reference-based generation, editing, and animation into a single paradigm. DreamID-Omni builds upon a dual-stream Diffusion Transformer [ Peebles2022DiT ] (DiT) architecture, where video and audio streams interact via bidirectional cross-attention for fine-grained synchronization. We propose a Symmetric Conditional DiT design that unifies heterogeneous conditioning signals‚Äîreference images, voice timbres, source videos, and driving audio‚Äîinto a shared latent space, enabling seamless task switching without architectural changes. To resolve multi-person confusion, we propose a Dual-Level Disentanglement strategy. At the signal level, Synchronized Rotary Positional Embeddings (Syn-RoPE) is introduced to bind reference identities with their corresponding voice timbres within the attention space. At the semantic level, Structured Captions utilize anchor tokens paired with fine-grained descriptions to establish explicit mappings between specific subjects and their respective attributes or speech content. Finally, we devise a Multi-Task Progressive Training strategy to harmonize the three tasks. In the initial two stages, we focus exclusively on the weakly-constrained R2AV task, employing in-pair reconstruction and cross-pair disentanglement to enhance identity and timbre fidelity while encouraging the model to learn robust reference representations. In the final stage, strongly-constrained tasks (RV2AV and RA2V) are introduced for joint training with R2AV. This approach prevents the model from overfitting to strongly-constrained tasks, thereby maintaining superior performance on the weakly-constrained generation task. In summary, our contributions are as follows: (1) We propose DreamID-Omni , a novel human-centric controllable generation framework based on a Symmetric Conditional DiT, which seamlessly integrates R2AV, RV2AV, and RA2V tasks. (2) We introduce Dual-Level Disentanglement, which addresses identity-timbre binding and speaker confusion in multi-person generation via Syn-RoPE and Structured Captions. (3) We present a Multi-Task Progressive Training strategy that effectively harmonizes diverse tasks with varying constraint strengths. (4) Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even when compared to leading proprietary commercial models. 2 Related Work 2.1 Joint Audio-Video Generation Recent advancements in diffusion-based foundation models in video generation [ wan2025wan , kong2024hunyuanvideo , gao2025seedance ] and audio generation [ pmlr-v202-liu23f , gong2025ace ] have significantly expanded the frontier of joint audio-video synthesis. While pioneering works [ ruan2023mm ] use coupled U-Net backbones, current DiT-based approaches dominate the field. These methods typically employ either dual-stream architectures [ liu2024syncflow , hayakawa2024mmdisco , wang2025universe , liu2025javisdit , low2025ovi , hacohen2026ltx ] with specialized fusion layers (e.g., cross-attention) or unified DiT structures [ wang2024av , zhao2025uniform , huang2025JoVA , wang2026klear ] with joint self-attention to achieve synchronized multi-modal alignment. Despite their impressive generative fidelity, these models are primarily designed for vanilla text-to-audio-video or first-frame-conditioned synthesis. They lack the capability to condition the generative process on external identity or voice timbre references. This limitation restricts their utility in scenarios requiring persistent identity and timbre consistency. 2.2 Controllable Video Generation Model Reference-based Generation. To enhance controllability, reference-based video generation has emerged as a prominent research direction, focusing on maintaining identity consistency by integrating reference features into the diffusion process. While initial efforts [ he2024id , yuan2025identity , polyak2024movie ] were primarily tailored for single-identity scenarios, subsequent research has extended these capabilities to multi-subject settings [ zhong2025concat , huang2025conceptmaster , chen2025skyreels , liu2025phantom , hu2025hunyuancustom , li2025bindweave , deng2025magref ] . However, these works are typically video-centric and do not support audio generation. Video Editing and Animation. In terms of temporal control, tasks can be categorized into video editing and audio-driven video animation. Editing frameworks [ chen2024hifivfs , guo2026dreamidv , luo2025canonswap , wang2025dynamicface , shao2025vividface , jiang2025vace , xu2026end , cheng2025wan ] allow for the modification of identity attributes within the source video. Audio-driven video animation [ wei2024aniportrait , xu2024hallo , chen2025humo , wang2025fantasytalking , lin2025omnihuman ] aims to generate videos from reference images to produce lip movements matching input speech signals. Despite their success, these models are all task-specific, and no existing model attempts to unify reference-based generation, editing, and animation. 3 Methodology 3.1 Problem Formulation We unify the landscape of controllable human-centric generation into a single probabilistic framework. Given a text prompt ùíØ \mathcal{T} , a set of reference identities ‚Ñê = { I 1 , ‚Ä¶ , I N } \mathcal{I}=\{I_{1},\dots,I_{N}\} , and corresponding reference voice timbres ùíú = { A 1 , ‚Ä¶ , A N } \mathcal{A}=\{A_{1},\dots,A_{N}\} , the goal is to synthesize a synchronized video-audio stream Y = { Y video , Y audio } Y=\{Y_{\text{video}},Y_{\text{audio}}\} . To support reference-based editing and animation tasks, we introduce two optional structural conditions: a source video context V src V_{\text{src}} and a driving audio stream A dri A_{\text{dri}} . The framework models the conditional distribution: P ‚Äã ( Y ‚à£ ùíØ , ‚Ñê , ùíú , V src , A dri ) P(Y\mid\mathcal{T},\mathcal{I},\mathcal{A},V_{\text{src}},A_{\text{dri}}) (1) By selectively providing these conditions, our framework seamlessly transitions between three distinct tasks, as summarized in Table 1 . Table 1 : Task Unification in DreamID-Omni . Our framework unifies R2AV, RV2AV and RA2V by toggling input conditions. Task Input Output Goal Human-Reference Audio-Video Generation (R2AV) ùíØ , ‚Ñê , ùíú \mathcal{T},\mathcal{I},\mathcal{A} Generate with references ‚Ñê , ùíú \mathcal{I},\mathcal{A} . Human-Reference Video Editing (RV2AV) ùíØ , ‚Ñê , ùíú , V src \mathcal{T},\mathcal{I},\mathcal{A},V_{\text{src}} Edit identity and audio in V src V_{\text{src}} . Human-Reference Audio-Driven Video Animation (RA2V) ùíØ , ‚Ñê , A dri \mathcal{T},\mathcal{I},A_{\text{dri}} Animate identity ‚Ñê \mathcal{I} using A dri A_{\text{dri}} . Figure 2 : Overview of DreamID-Omni framework. We integrate reference-based generation (R2AV), editing (RV2AV), and animation (RA2V) using a Symmetric Conditional DiT trained via a multi-task progressive training strategy. Structured Caption and Syn-RoPE ensure robust dual-level disentanglement in multi-person scenarios. 3.2 Framework To address the diverse tasks defined in Section 3.1 , we propose DreamID-Omni , a unified framework built upon a dual-stream DiT, as illustrated in Figure 2 . The architecture consists of two parallel backbones: a video stream for visual synthesis and an audio stream for acoustic synthesis. These streams interact via bidirectional cross-attention layers, enabling fine-grained temporal synchronization and semantic alignment between the visual and auditory modalities. 3.2.1 Symmetric Conditional DiT A core architectural contribution of DreamID-Omni is the Symmetric Conditional DiT, designed to seamlessly integrate reference-based generation, editing, and animation within a unified framework. This is achieved through a symmetric dual-stream conditioning strategy that composes heterogeneous control signals in the latent space with structural parity. Let z v z_{v} and z a z_{a} represent the noisy target video and target audio latents, respectively. To guide the denoising process, we construct two comprehensive conditional sequences, X v X_{v} and X a X_{a} , which integrate both identity-specific and structural guidance: X v \displaystyle X_{v} = [ z v ; ‚Ñ∞ v ‚Äã ( ‚Ñê ) ] + [ ‚Ñ∞ v ‚Äã ( V src ) ; ùüé ‚Ñ∞ v ‚Äã ( ‚Ñê ) ] \displaystyle=[z_{v};\mathcal{E}_{v}(\mathcal{I})]+[\mathcal{E}_{v}(V_{\text{src}});\mathbf{0}_{\mathcal{E}_{v}(\mathcal{I})}] (2) X a \displaystyle X_{a} = [ z a ; ‚Ñ∞ a ‚Äã ( ùíú ) ] + [ ‚Ñ∞ a ‚Äã ( A dri ) ; ùüé ‚Ñ∞ a ‚Äã ( ùíú ) ] \displaystyle=[z_{a};\mathcal{E}_{a}(\mathcal{A})]+[\mathcal{E}_{a}(A_{\text{dri}});\mathbf{0}_{\mathcal{E}_{a}(\mathcal{A})}] (3) where [ ‚ãÖ ; ‚ãÖ ] [\cdot;\cdot] denotes concatenation along the sequence dimension, ùüé T \mathbf{0}_{T} represents a zero tensor with the same shape as T T , and ‚Ñ∞ v , ‚Ñ∞ a \mathcal{E}_{v},\mathcal{E}_{a} are the respective VAE encoders. In this symmetric formulation, the reference features ( ‚Ñ∞ v ‚Äã ( ‚Ñê ) , ‚Ñ∞ a ‚Äã ( ùíú ) \mathcal{E}_{v}(\mathcal{I}),\mathcal{E}_{a}(\mathcal{A}) ) are concatenated to the noisy latents, allowing the DiT blocks to extract and disentangle high-level identity and timbre priors. Simultaneously, the structural conditions ( V src , A dri V_{\text{src}},A_{\text{dri}} ) are injected via element-wise addition, serving as a structural canvas that enforces spatial and temporal consistency. This dual-injection strategy effectively decouples the conditioning into identity