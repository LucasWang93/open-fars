Title: FewMMBench: A Benchmark for Multimodal Few-Shot Learning

Abstract: As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench

Body: FewMMBench: A Benchmark for Multimodal Few-Shot Learning 1 Introduction 2 Related Work 2.1 Few-Shot Learning in Multimodal Models 2.2 Existing Vision-Language Benchmarks 3 Curating FewMMBench 3.1 Tasks 3.2 Example Selection 3.3 Chain-of-Thought Description Generation 4 Experimental Setup 4.1 Models 4.2 Evaluation Metrics 5 Results and Findings 6 Conclusion A Models A.1 Evaluated MLLMs Qwen Family. InternVL Family. Idefics Family. Phi Family. LLaVA Family. xGen Family. A.2 Implementation Details B Data Statistics B.1 Data Summary B.2 Demonstration Example Similarity B.3 Reasoning Chain Validation C Additional Analysis C.1 How Hard FewMMBench ? C.2 Uncertainity Quantification C.3 Attribute Recognition C.3.1 Example Selection C.3.2 In-Depth Results C.4 Visual Object Recognition C.4.1 Example Selection C.4.2 In-Depth Results C.5 Plurality Recognition C.5.1 Example Selection C.5.2 In-Depth Results C.6 Object Counting C.6.1 Example Selection C.6.2 In-Depth Results C.7 Spatial Relations Understanding C.7.1 Example Selection C.7.2 In-Depth Results C.8 Action Recognition C.8.1 Example Selection C.8.2 In-Depth Results C.9 Commonsense Reasoning C.9.1 Example Selection C.9.2 In-Depth Results C.10 Temporal Reasoning C.10.1 Example Selection C.10.2 In-Depth Results C.11 Coreference Resolution C.11.1 Example Selection C.11.2 In-Depth Results D How CoT Fails? E Qualitative Examples F Limitations FewMMBench : A Benchmark for Multimodal Few-Shot Learning Mustafa Dogan 1 Ilker Kesen 2 Iacer Calixto 3,4 Aykut Erdem 5,6 Erkut Erdem 6,7 1 Aselsan Research 2 Department of Computer Science, University of Copenhagen 3 Department of Medical Informatics, Amsterdam UMC, University of Amsterdam 4 Amsterdam Public Health, Methodology, Amsterdam, The Netherlands 5 Koç University, Department of Computer Engineering 6 Koç University, KUIS AI Center 7 Hacettepe University, Department of Computer Engineering mustafadogan@aselsan.com Abstract As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench , a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: this URL. FewMMBench : A Benchmark for Multimodal Few-Shot Learning Mustafa Dogan 1 Ilker Kesen 2 Iacer Calixto 3,4 Aykut Erdem 5,6 Erkut Erdem 6,7 1 Aselsan Research 2 Department of Computer Science, University of Copenhagen 3 Department of Medical Informatics, Amsterdam UMC, University of Amsterdam 4 Amsterdam Public Health, Methodology, Amsterdam, The Netherlands 5 Koç University, Department of Computer Engineering 6 Koç University, KUIS AI Center 7 Hacettepe University, Department of Computer Engineering mustafadogan@aselsan.com 1 Introduction Multimodal large language models (MLLMs), models that jointly process images and text, have advanced significantly over the past two years. Beginning with GPT-4V (OpenAI et al. , 2024 ) , these models have evolved along two major axes. First, they have become more general-purpose, exhibiting stronger multimodal reasoning and instruction-following abilities; and second, they now support interleaved image–text inputs within a single prompt, enabling sequential and contextualized few-shot learning. Figure 1: Performance of selected MLLMs on FewMMBench across different evaluation settings. We compare instruction-tuned and non-instruction-tuned models under zero-shot, few-shot (random and similarity-based), and CoT-augmented few-shot configurations. Results show that few-shot prompting does not consistently improve performance for instruction-tuned models, even when demonstrations are semantically similar to the query or when the number of examples increases. Notably, CoT prompting often leads to a performance drop, suggesting modality-specific limitations in current CoT strategies. Early benchmarks such as MMBench (Liu et al. , 2024b ) and SeedBench (Li et al. , 2024 ) offered fine-grained evaluations across a wide spectrum of vision–language skills, ranging from basic object recognition to complex social reasoning. Similarly, MME (Fu et al. , 2024 ) assessed both low-level perception (e.g., object existence, color identification) and high-level cognition (e.g., commonsense inference) using binary question formats. More recent efforts, such as VisualCoT (Shao et al. , 2024 ) and VL-ICL Bench (Zong et al. , 2025 ) , have begun to probe specific reasoning capabilities, such as multi-step visual inference and in-context learning with a few visual exemplars. However, these benchmarks primarily focus on zero-shot evaluation or isolated abilities, and do not systematically test MLLMs in a few-shot , in-context setting with interleaved image–text streams and chain-of-thought prompting. To address this gap, we introduce FewMMBench , a comprehensive benchmark for evaluating multimodal few-shot learning. FewMMBench is designed to assess two emerging capabilities: In-Context Learning , where models are guided by a small set of image–text demonstration examples, and Chain-of-Thought prompting , where models are encouraged to generate intermediate reasoning steps. The benchmark spans nine diverse tasks, ranging from low-level perception (e.g., attribute recognition, object counting) to higher-level reasoning (e.g., spatial relations, grounded coreference, commonsense, and temporal inference). For each task, we construct fixed support sets of 4-8 examples, selected either randomly or via a retrieval-based strategy that identifies semantically similar exemplars. We evaluate 26 open-weight MLLMs from six model families, Qwen (Bai et al. , 2025 ; Wang et al. , 2024a ) , InternVL (Wang et al. , 2024b ; Chen et al. , 2024c , d ; Zhu et al. , 2025 ) , Idefics (Laurençon et al. , 2024a , b ) , Phi (Microsoft et al. , 2025 ; Abdin et al. , 2024 ) , LLaVA (Li et al. , 2025a ) , and xGen-MM (Xue et al. , 2024 ) , ranging from 1B to 9B parameters, under four evaluation settings: zero-shot , random few-shot , retrieved few-shot , and few-shot with CoT prompting . Figure 1 summarizes the overall empirical trends observed across models, including the varying effects of demonstrations, retrieval strategies, and CoT prompting. Beyond these empirical observations, the primary contributions of FewMMBench lie in its benchmark design and evaluation methodology. Specifically, FewMMBench (i) introduces a controlled framework that jointly supports random and semantically similar demonstration examples, where similar exemplars are selected using a graph-cut-based algorithm to balance relevance and diversity; (ii) provides detailed chain-of-thought rationales within demonstrations, enabling systematic analysis of reasoning-guided multimodal inference; and (iii) supports an exhaustive evaluation protocol that extends beyond response accuracy to include perplexity-based pairwise comparisons and uncertainty quantification. 2 Related Work 2.1 Few-Shot Learning in Multimodal Models Early multimodal large language models (MLLMs) were generally limited in their ability to process multiple image–text pairs within a single prompt, with only a few constrained exceptions (Tsimpoukelli et al. , 2021 ; Koh et al. , 2023 ; Alayrac et al. , 2022 ) . The release of GPT-4V marked a turning point, demonstrating robust reasoning over interleaved image–text sequences (OpenAI et al. , 2024 ) and motivating the creation of large-scale, interleaved multimodal corpora (Zhu et al. , 2023 ; Laurençon et al. , 2023 ; Li et al. , 2023 ; Zhao et al. , 2024 ) . As a result, newer models natively support mixed-modality inputs (Laurençon et al. , 2024a ; Xue et al. , 2024 ; Bai et al. , 2025 ; Wang et al. , 2024b ) . Despite this progress, the mechanisms and limitations of few-shot multimodal reasoning remain underexplored, particularly in light of recent findings on CoT and ICL. Prior studies have revealed that CoT exemplars may offer limited benefit relative to zero-shot CoT (Cheng et al. , 2025 ) , that multimodal models struggle with ICL despite mixed-modality pretraining (Doveh et al. , 2024 ) , that demonstrations often help by providing structure rather than ground-truth labels (Min et al. , 2022 ) , and that fine-tuning can degrade or alter CoT reasoning behaviors (Lobo et al. , 2025 ) . These works highlight shortcomings in exemplar dependence, instruction dominance, ICL fidelity, and the stability of reasoning traces. Our benchmark directly targets these open questions by providing controlled few-shot settings with structured demonstrations and explicit CoT rationales, enabling systematic analysis of how multimodal models acquire and apply reasoning patterns in context. Figure 2: Dataset Curation Pipeline for FewMMBench . (a) Task instances are collected and organized based on linguistically meaningful phenomena. (b) We extract visual and textual features for each instance and construct query sets using a Graph Cut-based submodular selection strategy, ensuring both diversity and representativeness. (c) CoT rationales are generated using the Qwen2.5-VL-7B-Instruct model. If the initial prediction is incorrect, the correct answer is injected and a new rationale is generated. An automated filter retains only high-quality examples. 2.2 Existing Vision-Language Benchmarks Recent advances in MLLM evaluation have introduced numerous benchmarks targeting different aspects of vision–language understanding. Broad frameworks such as VHELM (Lee et al. , 2024 ) offer standardized evaluation across perception, reasoning, fairness, multilinguality, and safety, while more focused efforts like MM-Vet v2 (Yu et al. , 2024 ) and CURE (Chen et al. , 2024b ) assess specific abilities such as compositional reasoning and chain-of-thought consistency. Few-shot prompting has also been explored, but limitations remain. VL-ICL Bench (Zong et al. , 2025 ) lacks CoT exemplars and relies on zero-shot prompting. M 3 CoT (Chen et al. , 2024a ) offers CoT rationales but pairs them with randomly selected demonstrations, reducing contextual alignment. VisualCoT (Shao et al. , 2024 ) instead provides explicit procedural instructions rather than reasoning trajectories. These issues hinder systematic assessment of multimodal reasoning, whereas FewMMBench uses semantically aligned demonstrations with explicit CoT rationales. Other work emphasizes dynamic or context-rich evaluation. Mementos (Wang et al. , 2024c ) studies sequential image reasoning, VisDiaHalBench (Cao et al. , 2024 ) examines hallucination in multi-turn visual dialogue, and multi-image benchmarks like MMIU (Meng et al. , 2025 ) and MIBench (Liu et al. , 2024a ) probe spatial reasoning and multi-image coordination. These benchmarks, however, remain orthogonal to few-shot reasoning, as they do not provide structured demonstrations or analyze how models learn reasoning patterns. In contrast, FewMMBench offers a unified, linguistically grounded few-shot framework. Beyond curated demonstrations and CoT rationales, it introduces two methodological contributions absent from prior work: a graph-cut–based construction of representative, non-redundant support sets, and perplexity-based accuracy with uncertainty quantification to mitigate order-sensitivity in ICL. This yields a more stable and diagnostically robust evaluation of multimodal reasoning. 3 Curating FewMMBench FewMMBench is a probing benchmark specifically designed to evaluate the linguistic and reasoning capabilities of MLLMs in few-shot settings. It spans a broad spectrum of tasks, including recognition, numerical reasoning, spatial and temporal inference, commonsense understanding, and linguistic grounding. As shown in Figure 2 , we adopt a systematic dataset construction pipeline to ensure diversity, representativeness, and quality across all evaluation scenarios. This pipeline involves two core steps: (i) selection of representative and challenging samples guided by visual-textual similarity, and (ii) generation of explicit chain-of-thought explanations to guide model inference. We begin by outlining the benchmark tasks in Section 3.1 , with illustrative examples and dataset statistics shown in Figure 3 . Further statistics are presented in Appendix B . We then describe two key factors critical to our few-shot evaluation setup: example selection (Section 3.2 ) using submodular optimization, and CoT rationales generation (Section 3.3 ) to support step-by-step reasoning. Figure 3: Task distributions and representative examples in FewMMBench . The pie chart illustrates the distribution of samples across the nine tasks in the benchmark. Each surrounding example depicts a sample question corresponding to a specific task.These examples highlight the diversity of visual-linguistic reasoning skills covered by FewMMBench , spanning low-level perception, numerical reasoning, and high-level cognitive inference. 3.1 Tasks Attribute Recognition evaluates a model’s ability to associate visual properties with objects in images. We employ two formats: MCQA and caption/foil classification. The MCQA task is derived from the Attribute Recognition dimension of MMBench (Liu et al. , 2024b ) and the Instance Attributes dimension of SeedBench (Li et al. , 2024 ) , assessing recognition of attributes such as color, shape, and material. Foiling task, adapted from the Visual Genome Attribution subset of ARO (Yuksekgonul et al. , 2023 ) , tests the ability to distinguish between correct and incorrect attribute associations in descriptive captions. The sample pool for MCQA consists of ∼ \sim 5K instances, and the caption/foil classification pool contains ∼ \sim 30K instances. Object Recognition evaluates a model’s ability to detect and identify objects within images, including specific instances, general categories, and object presence. We assess this through both MCQA and caption/foil classification. For MCQA, we adopt the Instance Identity dimension from SeedBench (Li et al. , 2024 ) , which examines a model’s ability to recognize specific objects, their categories, or their presence in a given scene. Additionally, we include the Multi-Class Identification task from GVTBench (W