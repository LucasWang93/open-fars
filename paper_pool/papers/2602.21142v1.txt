Title: LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis

Abstract: Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN, that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data.

Body: LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis 1 Introduction 2 Methods 2.1 Data 2.2 Instruction Enhancement 2.3 Expert Model Predictions 2.4 Longitudinal Instructions Design 2.5 Experiments 3 Results and Discussion 3.1 Prognostic (Temporal) Question Answering 3.2 Diagnostic Question Answering 4 Conclusion 5 Compliance with ethical standards 6 Acknowledgments LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis Abstract Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data. Index Terms — Chest X-ray, Diagnosis, MIMIC, Prognosis, Vision-language model 1 Introduction The workload in radiology departments has been steadily increasing [ bruls2020-s ] . Recently, large vision-language models (VLMs) have advanced rapidly, extending from general-purpose applications to clinical domains. VLMs present a promising tool for tackling complex and demanding medical tasks, including radiologic interpretation and decision support. LLaVA [ llava2023-s ] introduced the novel learning paradigm of instruction fine-tuning for pre-trained large models in the multi-modal domain, achieving strong zero-shot performance on vision-language tasks. Since the introduction of LLaVA, multiple open-source multi-modal models have emerged, including VILA [ vila2024cvpr-s ] and its updated version NVILA [ liu2024nvila ] . DeepSeek-VL2 [ deepseek-vl2 ] , Qwen2-VL [ qwen2-vl ] , MiniGPT-V2 [ minigpt-v2 ] are additional examples of open-access VLMs. LLaVA and VILA models stand out as the most accessible for user-driven fine-tuning, allowing broader customization. Both have been adapted towards biomedical applications, leading to the development of specialized models such as LLaVA-Med [ li2023llava-med-s ] and VILA-M3 [ nath2024vilam3 ] , designed for medical vision-language tasks. In the domain of artificial intelligence (AI)-based radiographic understanding, the publicly available MIMIC-CXR dataset [ mimic3 , mimic1 , mimic2 ] and its derived Medical-Diff-VQA dataset [ mimicvqa1 , mimicvqa2 ] serve as valuable resources for evaluating diagnostic vision question answering (VQA). These datasets have become benchmarks for many related studies, providing structured QA pairs that facilitate the development and evaluation of VLMs in radiology. Among these, the D-Rax model [ nisar2025drax-s ] introduced a novel approach by incorporating expert model predictions from state-of-the-art (SOTA) classifiers into instruction-tuning data. This enhancement proposed by D-Rax improved the model’s capabilities in abnormality detection. VILA-M3 further extended the role of medical expert knowledge and also incorporated segmentation data alongside diagnostic labels. However, current VLMs in radiology are typically limited to single-image understanding and cannot perform the temporal reasoning that radiologists routinely do. In practice, radiologists compare current and prior studies to assess disease progression or treatment response. Yet, most published medical VLMs do not incorporate longitudinal information. The emerging differential VQA task highlights this gap. Only recently have a few works attempted such sequential image reasoning [ mimicvqa2 , cho2024pretraining , lu-miccai2024-diff-s , yung-miccai2024-diffvlm ] . These models have focused on comparing a main image with a reference image extracted from two radiological studies at different time points, aiming to enhance the model’s capabilities in detecting differences and assessing disease progression. Beyond diagnosis and description, radiologists often prognosticate – predicting patient outcomes based on imaging findings. Traditional deep learning approaches address these questions using purely image-based models or simple combinations of image features with clinical data [ jiao2021covid , miccai2024covid ] . However, no current VLMs have been developed specifically for prognosis. Developing a prognostic VLM requires the ability to learn from longitudinal data, reason over temporal changes, and handle outcome-related questions, many of which lack a definitive ground-truth answer. In this work, we present LUMEN, a unified VLM designed to address these limitations. Built on NVILA, our model efficiently processes single or sequential input images, enabling end-to-end VQA for both diagnostic and prognostic tasks. We integrate expert model predictions and time intervals from DICOM. A key challenge in prognosis is the lack of reference instruction-following data. To overcome this, we generated an in-house prognostic instruction dataset based on Medical-Diff-VQA. By leveraging known temporal changes between studies extracted from the difference QA pairs, we formulated prognosis-related questions and corresponding outcome predictions. Our main contributions are: (1) Enhancing Medical-Diff-VQA with longer, large language model (LLM)-generated responses for more informative interactions beyond short factual answers; (2) Creating prognostic instruction-following data, incorporating diverse outcome-based questions and predicted answers; (3) Developing a unified VLM capable of processing multiple images, supporting both diagnostic interpretation and prognostic forecasting. 2 Methods 2.1 Data We utilized the MIMIC-CXR dataset [ mimic3 , mimic1 , mimic2 ] and an updated version of the Medical-Diff-VQA dataset [ mimicvqa1 , mimicvqa2 ] . MIMIC-CXR is a large publicly available dataset of 377,110 CXRs from 227,827 studies, with structured labels extracted from free-text radiology reports. The Medical-Diff-VQA dataset is derived from MIMIC-CXR and comprises 700,703 QA pairs. A single image is extracted from a study. The questions span seven categories: abnormality, presence, view, location, level, type, and difference. A difference question involves comparisons between a pair of main and reference images to assess changes over time. QA pairs have two types: 1) open-ended questions ( e.g. , ”what abnormalities are seen in this image?”), which expect descriptive, variable-length natural language responses; 2) close-ended questions ( e.g. , ”is there evidence of any abnormalities in this image?”), typically answered in a binary ”yes/no”. We considered the first six question categories (abnormality, etc.) as diagnostic questions requiring only a single input image. We followed the official Medical-Diff-VQA dataset partitioning and extracted one study per subject to form the test set. This resulted in 1) a training set: 129,231 images, 428,995 diagnostic questions (51% open-ended and 49% close-ended), and 43,381 difference questions; 2) a test set: 4,190 images, 13,688 diagnostic questions (49% open-ended and 51% close-ended), and 1,369 difference questions. 2.2 Instruction Enhancement To enhance the naturalness of the instructions and improve the readability, we refined the answers within the Medical-Diff-VQA dataset. While the original factual instructions were easy for evaluation, it lacked the fluency and readability needed for real-world clinical applications. We used the Llama-3.2-11B-Vision-Instruct model [ llama3 ] to expand each answer into a complete sentence, while maintaining its original meaning with an engineered prompt. 2.3 Expert Model Predictions We followed the strategy outlined in [ nisar2025drax-s ] to improve the instruction-following data from Medical-Diff-VQA by adding expert model predictions generated using TorchXRayVision [ torchxray ] . These SOTA expert models provided predictions for disease diagnosis (18 conditions), patient age, race (Asian, Black, White), and image view (frontal, lateral) for each image in the dataset. To enhance interpretability for the LLM, disease probabilities were mapped to a text-based format with four confidence levels: no, maybe no, maybe yes, and yes, based on calibrated probability thresholds at 0.25, 0.5, 0.75. 2.4 Longitudinal Instructions Design To extend the capabilities of the dataset to prognosis beyond retrospective comparisons, we generated future prediction questions using factual information extracted from difference questions within Medical-Diff-VQA. The original set included questions comparing two images of the same patient taken at different time points. To formulate prognostic questions, we used Llama-3.2-11B-Vision-Instruct [ llama3 ] with a structured prompt designed to predict future changes in the reference image. For each QA pair, we generated an additional answer to a randomly selected question from a predefined set of 100 questions, e.g. , “What changes can be expected in this patient’s chest X-ray in placeholder days?” The selected question, along with the difference answers, was then processed using an engineered prompt. The constructed instruction-tuning dataset will be made publicly available for reproducible research, pending PhysioNet approval. 2.5 Experiments Training We fine-tuned the pretrained NVILA-8B on the MIMIC-CXR dataset using the training and test sets defined in Section 2.1 for both diagnostic and prognostic tasks. Models were trained using the enriched dataset with longer, LLM-generated responses. Specifically, we fine-tuned NVILA-8B using diagnostic instructions with single image input and using both diagnostic and enhanced prognostic instructions with two-image input (LUMEN). The projection layer, the language model, and the vision encoder are updated. Training was conducted for one epoch, with a learning rate of 1.5 ​ e − 5 1.5e^{-5} and a global batch size of 128 128 , utilizing four NVIDIA H100 GPUs (80GB memory each). Evaluation Metrics Performance was first evaluated using BLEU-4 and ROUGE-L scores to access lexical similarity. Additionally, following [ li2023llava-med-s , nisar2025drax-s ] , we used short answers in the original Medical-Diff-VQA to compute the token recall, measuring the ratio of correctly generated tokens relative to the reference tokens for open-ended questions. For close-ended questions, accuracy was used as the evaluation metric. Instruction fine-tuning on the more comprehensive dataset, generated by an LLM to include longer and more descriptive responses, can help the model learn both to provide factual answers and to elaborate when required. However, these conventional scores may not be robust in this setting. BLEU and ROUGE often rated outputs highly even when they contained clinically incorrect information due to surface-level lexical overlap. Relying solely on n-gram metrics can be inadequate for medical text evaluation [ liu-miccai2024-mrscore-s ] . To address this limitation, we incorporated Llama score, an LLM-based metric computed by prompting Llama-3.1-405B to directly compare generated responses with the reference on a scale of 1 to 10. This approach provides a more nuanced assessment of clinical correctness and helpfulness. Notably, cases where traditional scores were high ( e.g. , ROUGE-L=0.7) but the generated response was incorrect ( e.g. , a model responding ”No evidence of any abnormalities is present in this image” when the reference was ”Yes, there is evidence of abnormalities present in this image”) were appropriately penalized by Llama score ( e.g. , scoring only 1/10). This demonstrates that the Llama Score is a more reliable metric in radiological VQA tasks, where factual correctness outweighs lexical similarity. 3 Results and Discussion Fig. 1 : Qualitative evaluation: conversations provided by NVILA-8B, NVILA-8B-FT (finetuned), and LUMEN. 3.1 Prognostic (Temporal) Question Answering As shown in Table 1 , the models showed significantly greater difficulty when answering difference-based and prediction-oriented questions, which required temporal reasoning. NVILA-8B, fine-tuned only on diagnostic tasks, performed poorly when prompted with difference-based questions comparing two time-separated images. However, LUMEN explicitly trained with temporal information improved significantly, suggesting that explicit exposure to temporal image pairs helped the model capture disease progression patterns. Despite these gains, overall performance remained weaker than in diagnostic tasks, as difference-based reasoning requires both precise lesion localization and an understanding of disease evolution over time. Existing studies [ cho2024pretraining , lu-miccai2024-diff-s ] solely focus on the difference-based reasoning can be incorporated to enhance our model’s ability to analyze longitudinal changes. Qualitative results were presented in Figure 1 . Table 1 : Prognostic Performance: evaluated on open-ended difference- and prediction-based questions. † {\dagger} = fine-tuned on Medical-Diff-VQA. The asterisks show statistical significance (p 0.001 0.001 ) across paired comparisons with LUMEN, using the Wilcoxon signed rank test. Model BLEU-4 ROUGE-L Llama Score Test on 1 , 369 1,369 images and 1 , 369 1,369 difference questions NVILA-8B 0.001 0.001 * 0.071 0.071 * 2.486 2.486 * NVILA-8B † 0.020 0.020 * 0.206 0.206 * 3.275 3.275 * LUMEN † 0.375 \mathbf{0.375} 0.656 \mathbf{0.656} 4.611 \mathbf{4.611} Test on 2 , 451 2,451 images and 3 , 936 3,936 prediction questions NVILA-8B 0.003 0.003 * 0.052 0.052 * 2.242 2.242 * NVILA-8B † 0.019 0.019 * 0.167 0.167 * 3.177 3.177 * LUMEN † 0.095 \mathbf{0.095} 0.303 \mathbf{0.303} 4.866 \mathbf{4.866} 3.2 Diagnostic Question Answering As shown in Table 2 , instruction fine-tuning the NVILA-8B model on a specific clinical QA dataset dramatically improved the performance on diagnostic questions. BLEU, ROUGE, and Llama scores were not computed fo