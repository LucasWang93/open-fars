Title: MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data

Abstract: Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.

Body: MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data 1 Introduction 2 Related Work 3 Methods 3.1 Datasets 3.2 Model Architecture 3.3 Experimental Protocol 4 Results 4.1 In-Task Transfer Learning 4.2 Cross-Task Decoding Baseline 4.3 Cross-Task with Transfer Learning 5 Discussion and Conclusion de Zuazo Verbeni Navas Saratxaga Bourguignon Molinaro MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data Xabier Vincenzo Eva Ibon Mathieu Nicola 1 HiTZ Center, University of the Basque Country – UPV/EHU, Spain 2 Basque Center on Cognition, Brain and Language – BCBL, Spain 3 Ikerbasque, Basque Foundation for Science, Spain 4 Laboratory of Functional Anatomy, Faculty of Human Motor Sciences, Université libre de Bruxelles (ULB), Belgium 5 Laboratoire de Neuroanatomie et Neuroimagerie translationnelles (LN2T), ULB Neuroscience Institute, Université libre de Bruxelles (ULB), Belgium 6 WEL Research Institute, Belgium xabier.dezuazo@ehu.eus, v.verbeni@bcbl.eu, eva.navas@ehu.eus, ibon.saratxaga@ehu.eus, mabourgu@ulb.ac.be, n.molinaro@bcbl.eu Abstract Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity. keywords: transfer learning, MEG, speech decoding, cross-task decoding, brain-computer interface, neuroscience 1 Introduction Brain-computer interfaces (BCIs) for speech restoration require robust neural decoders [ king2020 , dezuazo2024_2 ] , but collecting sufficient training data per subject remains a fundamental challenge [ anumanchipalli2019 , moses2019 , silva2024 ] . Nevertheless, while large-scale datasets have enabled substantial progress in non-invasive speech decoding tasks (including speech detection and phoneme decoding) using magnetoencephalography (MEG) [ elvers2025 , ozdogan2025 , landau2025 ] and electroencephalography (EEG) [ sato2024 , dascoli2024 ] , practical BCI deployment is constrained by the limited data available per individual: typically minutes rather than hours. Current approaches train separate models from scratch for each subject and task [ dash2018_1 , dash2018_2 , dezuazo2024_1 , dezuazo2025_2 ] , ignoring the plausible benefits of transfer learning that have revolutionized computer vision [ lecun2015 ] and natural language processing [ vaswani2017 ] . Notwithstanding successful demonstrations of transfer learning in EEG [ banville2025 ] and fMRI [ benchetrit2024 ] , and recent advances in self-supervised neural decoding [ defossez2023 , jayalath2025 ] , transfer learning has only very recently been explored for MEG imagined speech using ImageNet-pretrained vision models [ jhilal2026 ] , and remains untested for MEG-to-MEG pre-training and cross-task transfer between perception and production. This study presents the first successful application of transfer learning to a MEG speech decoding task (speech detection). We pre-train a Conformer-based model [ gulati2020 , dezuazo2025_1 ] on 50 hours of single-subject listening data (LibriBrain [ ozdogan2025 ] ) and fine-tune on just 5 minutes per subject across 18 participants performing speech perception (listening and playback) and speech production tasks [ bourguignon2020 ] . To our knowledge, we also provide the first cross-task MEG speech detection results, extending prior cross-task decoding work in non-speech EEG/MEG [ magnabosco2024 , aristimunha2025 ] . We show that transfer learning improves both in-task performance and cross-task generalization, with gains of 1-6% across metrics. Importantly, models trained on production successfully decode passive listening above chance, confirming that decoding relies on shared neural speech representations rather than task-specific motor activity alone [ philip2022 , philip2023 ] . Together, these findings suggest that transfer learning may support more data-efficient MEG speech detection and improved cross-task generalization. 2 Related Work MEG-based speech decoding has progressed from closed-vocabulary word classification [ dash2018_1 , dash2018_2 , dash2019 ] to phone-level analysis [ dezuazo2024_1 , dezuazo2025_2 , suzuki2025 ] and recent large-scale competitions [ landau2025 , elvers2025 ] . Complementary work has explored Transformer-based encoding models [ klimovichgray2023 ] , compact end-to-end architectures [ sarma2024 ] , and speech synthesis from MEG [ kwon2024 ] . In parallel, EEG research has demonstrated open-vocabulary decoding [ sato2024 , accou2023 , xu2024 ] and large-scale word and phonetic decoding [ dascoli2024 ] , emphasizing the importance of dataset size and model capacity. Transfer learning has been successfully applied to other neuroimaging modalities. EEG studies have leveraged self-supervised pre-training [ banville2025 ] and contrastive learning [ defossez2023 ] to improve cross-subject generalization. Recent work has combined discriminative decoders with language-model rescoring [ jayalath2025 ] and foundation-model guidance [ benchetrit2024 ] to improve performance. In contrast, despite these advances in EEG and fMRI, transfer learning has not been demonstrated for MEG-based speech decoding, leaving a critical gap in understanding whether large-scale pre-training can address the limited per-subject data available in clinical BCI settings. Cross-task speech decoding between perception and production remains underexplored. At the same time, previous work established differences in neural representations between passive listening and overt speech [ bourguignon2020 , schoffelen2019 , gwilliams2023 ] , and investigation of cross-task transfer and the role of motor-related activity versus shared speech representations has been limited [ philip2023 , levy2025 ] . Our work addresses this gap by demonstrating bidirectional cross-task decoding and quantifying the benefits of transfer learning across speech modalities. 3 Methods 3.1 Datasets We evaluate transfer learning across two MEG speech datasets with contrasting characteristics: a large-scale single-subject dataset for pre-training and a multi-subject dataset with limited per-subject data for fine-tuning and cross-task evaluation. LibriBrain (pre-training). We pre-train a single model on the LibriBrain dataset [ ozdogan2025 ] , which provides over 50 hours of within-subject MEG recordings from a single participant during naturalistic English speech listening (Sherlock Holmes audiobooks). Recordings were acquired from a single right-handed male native English speaker using a 306-channel Elekta/MEGIN system (102 magnetometers, 204 planar gradiometers) and processed following the LibriBrain Competition pipeline [ landau2025 ] , with model inputs downsampled to 250 Hz 250\text{\,}\mathrm{H}\mathrm{z} . We use the binary Speech Detection task, which distinguishes speech from silence based on voice-activity labels. Across the dataset, speech accounts for approximately 76.7% of the labeled time. Speech listening and production data [ bourguignon2020 ] (fine-tuning and evaluation). We fine-tune and evaluate on the dataset described by Bourguignon et al., comprising 18 healthy adult participants (9 female, 8 male, 1 unreported; mean age: 23.9 years) who were native Spanish speakers. Each participant performed three speech-related tasks for approximately 5 minutes each: listening to pre-recorded speech, listening to playback of their own voice, and reading aloud (speech production). MEG was recorded using another 306-channel Elekta/MEGIN system (same model as LibriBrain, but in a different lab), downsampled to 250 Hz 250\text{\,}\mathrm{H}\mathrm{z} for consistency with LibriBrain. We formulate a similar speech detection task using voice-activity annotations aligned with the MEG signal. Across subjects, speech occupied on average 78.6 ± 2.3 % 78.6\pm 2.3\% of frames during listening and 74.8 ± 3.9 % 74.8\pm 3.9\% during playback (small differences reflect minor variations in usable recording durations), and 75.6 ± 3.7 % 75.6\pm 3.7\% during speech production, with the remaining frames corresponding to silence. This dataset is not publicly available due to ethical constraints. Full experimental details are provided in [ bourguignon2020 ] . Crucially, the key distinction between datasets (50 hours of single-subject data versus 5 minutes per subject across 18 participants) enables us to investigate whether large-scale pre-training on one subject can improve decoding performance when fine-tuned on limited data from new subjects performing different speech tasks. 3.2 Model Architecture We pretrained the MEGConformer [ dezuazo2025_1 ] , a compact Conformer-based encoder [ gulati2020 ] adapted for MEG time-series that operates directly on windowed raw sensor segments (306 channels) after preprocessing and downsampling to 250 Hz 250\text{\,}\mathrm{H}\mathrm{z} . Accordingly, to match the limited amount of data available per subject and task in Bourguignon2020, we use 0.5 s 0.5\text{\,}\mathrm{s} windows throughout and disable output smoothing, while keeping the remaining architectural choices and training recipe as close as possible to [ dezuazo2025_1 ] . For fine-tuning, we retain the original optimizer and most hyperparameters, but introduce three lightweight task-specific modifications: (i) we select checkpoints using validation loss rather than F1-macro to avoid overfitting to a specific metric; (ii) we introduce RollAugment , a fast roll-based temporal augmentation that circularly shifts each training frame by fixed fractions of the window (25%, 50%, and 75%) and concatenates the shifted copies; and (iii) we use soft targets given by the fraction of speech within each window (instead of hard 0/1 labels) to smooth the labels slightly. Moreover, we use decimation by 4 to obtain 250 Hz 250\text{\,}\mathrm{H}\mathrm{z} (via anti-aliased resampling) and reduce early-stopping patience to 10 epochs for efficiency. 3.3 Experimental Protocol Subsequently, we pre-trained MEGConformer on LibriBrain's 50-hour listening dataset, then fine-tuned on Bourguignon2020 using two experimental paradigms: (i) in-task , where models were trained and evaluated on the same task (Listen, Playback, Production), and (ii) cross-task , where models were trained on one task and evaluated on a different task without retraining, covering all six possible train-test task pairings among Listen, Playback, and Production. For each subject and condition, we compared transfer learning (pre-trained then fine-tuned) against training from scratch (no pre-training). Performance was assessed using F1-macro, balanced accuracy, and AUC-macro. For LibriBrain pre-training, we used the official splits provided with the dataset [ landau2025 , ozdogan2025 ] . For Bourguignon2020, given the short duration of each recording and the absence of natural session boundaries, data were split independently for each subject and task into training (70%), validation (15%), and test (15%) sets using random shuffling at the frame level. Accordingly, to normalize the signal, input windows were z-scored for each subject and task (separately for each sensor and time point). Training and validation data used training-set statistics, while test data were normalized using their own statistics to support fair in-task and cross-task evaluation. All training runs were performed on individual NVIDIA H100 GPUs. Statistical significance was evaluated using the Wilcoxon signed-rank test [ Wilcoxon1945 ] , a non-parametric method for paired comparisons when normality cannot be assumed [ Santafe2015 ] . For each comparison, we tested whether the median improvement (transfer learning minus baseline) differed from zero ( p 0.05 p~ ~0.05 ), using subjects as the unit of replication. All p-values were corrected for multiple comparisons using the Holm-Bonferroni method [ holm1979 ] . Overall transfer learning effects across metrics and task conditions were assessed with subject-level omnibus tests based on aggregated improvements, using a permutation-based sign-flip test (10,000 iterations) [ pitman1937 , good2005 ] . Code availability. To ensure reproducibility, all code, preprocessing scripts, and model configurations will be made publicly available; to preserve author anonymity, the link will be added after the review process. 4 Results 4.1 In-Task Transfer Learning Table 1: In-task evaluation results (mean ± \pm std across subjects, in %). Improvements over baseline are marked in bold. Task Accu. (%) F1 (%) AUC (%) Scratch (baseline) listen 76.2 ± \pm 4.8 85.5 ± \pm 3.2 64.0 ± \pm 9.4 playback 75.1 ± \pm 6.1 84.0 ± \pm 4.7 67.7 ± \pm 4.2 production 83.6 ± \pm 5.4 89.7 ± \pm 3.5 81.1 ± \pm 8.6 Transfer Learning listen 79.0 ± \pm 4.8 87.7 ± \pm 3.2 68.7 ± \pm 6.2 playback 76.0 ± \pm 6.0 85.4 ± \pm 4.4 67.2 ± \pm 8.6 production 84.2 ± \pm 4.9 90.3 ± \pm 3.4 82.0 ± \pm 8.0 Table 1 presents in-task performance comparing models trained from scratch on Bourguignon2020 against models pre-trained on LibriBrain and fine-tuned on the same data. Transfer learning improved performance across nearly all metrics and tasks. For the listening task, transfer learning yielded significant gains of +3.7% accuracy, +2.6% F1, and +7.3% AUC ( W = 17.0 W=17.0 , p = 0.005 p=0.005 ). The playback task showed more modest, non-significant improvements of +1.2% accuracy, +1.7% F1, and -0.7% AUC ( W = 45.0 W=45.0 , p = 0.163 p=0.163 ). Interestingly, despite being pre-trained exclusively on a listening task, transfer learning improved all metrics for speech production, including +0.7% accuracy, +0.7% F1, and +1.1% AUC, though these differences did not reach statistical significance ( W = 61.0 W=61.0 , p = 0.304 p=0.304 ). Across all tasks and metrics, a sign-flip permutation test confirmed a significant overall difference of transfer learning ( p 0.001 p~ ~0.001 ). These results demonstrate that representations learned from large-scale single-subject listening data generalize to new subjects and tasks, even with only 5 minutes of data per subject. 4.2 Cross-Task Decoding Baseline Table 2: Cross-task decoding results for the model trained from scratch (mean ± \pm std across subjects, in %). All reported cross-task results are significantly above chance. Train → \rightarrow Test Accu. (%) F