Title: Causal Motion Diffusion Models for Autoregressive Motion Generation

Abstract: Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.

Body: Causal Motion Diffusion Models for Autoregressive Motion Generation 1 Introduction 2 Related Works 2.1 Motion-Language Alignment 2.2 Diffusion-based Motion Generation 2.3 Autoregressive Motion Generation 3 Method 3.1 Motion-Language-Aligned Causal VAE Motion Alignment Loss. 3.2 Causal Diffusion Forcing 3.3 Causal Diffusion Transformer 3.4 Inference and Streaming Generation Frame-wise Sampling Schedule (FSS). 4 Experiments 4.1 Experimental Setup Datasets. Evaluation Metrics. Implementation Details. 4.2 Quantitative Results Results on HumanML3D. Results on SnapMoGen. Long-Horizon Motion Generation 4.3 Qualitative Results 4.4 Analysis Computational Efficiency Ablation Studies 5 Limitations 6 Conclusion A Implementation Details A.1 MAC-VAE A.2 Causal-DiT A.3 Causal Diffusion Forcing B Additional Quantitative Results B.1 Experiments on BABEL B.2 Evaluation on Other Motion Features B.3 Compositional Motion Generation B.4 Latency analysis B.5 Ablation Studies Architecture of MAC-VAE. Motion-Language Models Model Size of Causal-DiT. Text Encoder C Additional Qualitative results D Sample Code Causal Motion Diffusion Models for Autoregressive Motion Generation Qing Yu 1 Akihisa Watanabe 2 Kent Fujiwara 1 1 LY Corporation 2 Waseda University {yu.qing, kent.fujiwara}@lycorp.co.jp, akihisa@ruri.waseda.jp Abstract Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency. 1 Introduction Synthesizing realistic human motion conditioned on natural language remains a fundamental problem in computer vision and graphics. A successful text-to-motion generation model should not only synthesize spatially accurate body movements but also maintain temporal coherence across long sequences. Recent progress in motion diffusion models [ mdm2022human , zhang2022motiondiffuse , chen2023executing , dai2024motionlcm ] has led to significant improvements in motion quality and diversity, benefiting from the strong generative capacity of diffusion-based frameworks [ ho2020denoising , dhariwal2021diffusion ] . However, most existing diffusion models rely on bidirectional denoising over the entire sequence, which inherently breaks temporal causality and prevents online generation. Figure 1 : Overview of the existing methods and the proposed method. Existing diffusion-based methods (left) perform full-sequence denoising using the same noise level across all frames. In contrast, our proposed CMDM (right) introduces a causal diffusion forcing mechanism that operates on semantic causal latent features with frame-wise noise levels. Autoregressive models [ zhang2023t2m , meng2024rethinking , zhao2024dartcontrol , xiao2025motionstreamer ] offer an alternative by predicting future frames from past ones, ensuring causal consistency and supporting online motion generation. Yet, their sequential dependency often leads to error accumulation, making long-horizon synthesis unstable and inefficient. The key challenge lies in achieving temporally ordered, high-quality motion generation with both the fidelity of diffusion models and the causal structure of autoregressive transformers. To address these challenges, we propose C ausal M otion D iffusion M odels ( CMDM ), a unified framework that integrates causal diffusion and autoregressive modeling within a semantically aligned latent space as shown in Fig. 1 . CMDM is built upon our Motion-Language-Models-Aligned Causal Variational Autoencoder (MAC-VAE), which encodes human motion into temporally causal latent representations guided by motion-language pretraining [ radford2021learning , petrovich2023tmr , yu2024exploring ] . This foundation enables CMDM to operate in a compact and semantically meaningful latent space, preserving alignment between linguistic semantics and motion dynamics. On top of MAC-VAE, we design a Causal Diffusion Transformer (Causal-DiT) that performs diffusion denoising in an autoregressive manner. Unlike conventional diffusion models that jointly process all frames, Causal-DiT applies causal self-attention to ensure each frame depends only on preceding frames. This design enforces strict temporal ordering, allowing streaming motion generation. To accelerate inference, we introduce a frame-wise sampling schedule with causal uncertainty, which allows each frame to be progressively refined from partially denoised preceding frames rather than requiring a fully autoregressive denoising step. Inspired by Diffusion Forcing [ chen2024diffusion ] , originally designed for next-token prediction, during training we perturb each frame with independent noise levels while maintaining causal dependencies across time, enabling the model to learn temporally consistent denoising transitions. During sampling, the model iteratively predicts the subsequent frames based on previously denoised frames with varying noise levels, gradually reducing uncertainty in a causal order. This hierarchical denoising process significantly reduces inference steps, achieving efficient and temporally coherent motion generation. CMDM unifies the stability and realism of diffusion models with the causality and efficiency of autoregressive architectures. The framework enables high-fidelity text-to-motion generation, fast inference, and long-horizon synthesis within a unified causal formulation. Extensive evaluations on HumanML3D and SnapMoGen demonstrate that CMDM consistently outperforms state-of-the-art diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while reducing inference latency by an order of magnitude. Our main contributions are summarized as follows: ‚Ä¢ Causal motion diffusion framework. We propose CMDM, the first motion diffusion framework that unifies causal autoregression and diffusion denoising within a motion‚Äìlanguage‚Äìaligned latent space. ‚Ä¢ Semantically aligned causal latent modeling. We introduce MAC-VAE, a motion‚Äìlanguage‚Äìaligned causal VAE that learns temporally causal and semantically meaningful latent representations for text-to-motion generation. ‚Ä¢ Frame-wise sampling with causal uncertainty. We design a novel frame-wise sampling schedule that models causal uncertainty, allowing each frame to be predicted from partially denoised preceding frames for efficient, low-latency, and temporally consistent motion synthesis. ‚Ä¢ Comprehensive empirical validation. CMDM achieves state-of-the-art performance on HumanML3D and SnapMoGen, surpassing existing diffusion and autoregressive methods on text-to-motion generation and long-horizon motion generation. 2 Related Works 2.1 Motion-Language Alignment Recent advances in vision‚Äìlanguage models [ radford2021learning , oquab2023dinov2 ] have shown that large-scale training can robustly align text and visual semantics. This has led to a surge of interest in exploring motion‚Äìlanguage alignment to enable practical control of motion using natural language. MotionCLIP [ tevet2022motionclip ] maps a single frame to CLIP space but fails to capture temporal dynamics. Subsequent methods, including TMR [ petrovich2023tmr ] and MotionPatches [ yu2024exploring ] , learn joint motion‚Äìtext embeddings via contrastive or generative objectives, while PartTMR [ yu2025remogpt ] introduces body-part-level features for finer alignment. However, most motion‚Äìlanguage models focus on retrieval tasks. Methods such as ReMoGPT [ yu2025remogpt ] and ReMoMask [ li2025remomask ] extend to text-to-motion generation but rely on retrieval-augmented generation rather than integrating motion‚Äìlanguage alignment directly into the generation. 2.2 Diffusion-based Motion Generation Text-conditioned motion generation has been explored through both non-diffusion and diffusion [ ho2020denoising , ho2022classifier , dhariwal2021diffusion ] paradigms. Early works used CNN- or RNN-based architectures [ yan2019convolutional , zhao2020bayesian ] and action-conditioned frameworks [ guo2020action2motion , petrovich2021action ] to synthesize motion from predefined semantics. More recently, diffusion-based methods [ dhariwal2021diffusion , rombach2022high ] have set new benchmarks for motion realism and diversity [ zhang2022motiondiffuse , chen2023executing , mdm2022human ] . MDM [ mdm2022human ] and MotionDiffuse [ zhang2022motiondiffuse ] operate directly in motion space, while MLD [ chen2023executing ] , MotionLCM [ dai2024motionlcm ] , EnergyMoGen [ zhang2025energymogen ] and SALAD [ hong2025salad ] perform diffusion in a latent space for greater stability and efficiency. However, these diffusion models rely on bidirectional attention over entire sequences, breaking temporal causality and limiting real-time or streaming generation. 2.3 Autoregressive Motion Generation Autoregressive (AR) modeling enforces temporal causality by predicting future frames from past context. Discrete-token methods such as T2M-GPT [ zhang2023t2m ] and MotionGPT [ jiang2023motiongpt ] treat motion as ‚Äúlanguage,‚Äù enabling powerful sequence modeling but suffering from exposure bias and cumulative errors. VQ-VAE-based approaches, including MoMask [ guo2024momask ] , MMM [ pinyoanuntapong2024mmm ] , and ParCo [ zou2024parco ] , quantize motion into discrete tokens and predict them autoregressively. Recent works explore causal paradigms for streaming generation: Dart [ zhao2024dartcontrol ] predicts short future segments from limited two historical frames, while MARDM [ meng2024rethinking ] and MotionStreamer [ xiao2025motionstreamer ] employ masked autoregressive transformers [ li2024autoregressive ] with diffusion heads. However, their reliance on teacher forcing [ williams1989learning ] and large diffusion heads causes instability in long-horizon inference and high computational cost, limiting real-time deployment. Our work differs from existing methods in two key aspects: (1) we introduce a causal diffusion process within a motion‚Äìlanguage‚Äìaligned latent space, preserving semantic consistency while enforcing temporal causality; and (2) we design a frame-wise sampling schedule that enables high-quality, streaming motion generation. 3 Method Our proposed framework, Causal Motion Diffusion Models (CMDM), enables temporally ordered, text-conditioned motion generation by integrating causal latent encoding, causal diffusion forcing, and efficient frame-wise sampling. As illustrated in Fig. 2 , CMDM consists of three core components: (1) a Motion-Language-Aligned Causal VAE (MAC-VAE) that encodes motion sequences into semantically aligned and temporally causal latent spaces, (2) a Causal Diffusion Transformer (Causal-DiT) that performs frame-wise diffusion with causal self-attention to ensure autoregressive temporal dependencies, and (3) a Frame-Wise Sampling Scheduler (FSS) that models causal uncertainty by assigning higher noise to future frames and lower noise to past frames, allowing each new frame to be predicted from partially denoised preceding frames. Figure 2 : Overview of the proposed CMDM framework. CMDM consists of three key components: (a) MAC-VAE, which encodes motion sequences into motion‚Äìlanguage‚Äìaligned and temporally causal latent features using a causal encoder‚Äìdecoder structure supervised by motion-language model alignment; (b) Causal-DiT, which performs diffusion denoising with causal self-attention and cross-attention to text embeddings, ensuring temporally ordered and semantically consistent frame refinement; and (c) Causal Diffusion Forcing, which applies independent frame-level noise during training and a causal uncertainty schedule during inference, where the redness intensity represents the noise level. This design enables CMDM to achieve temporally consistent, semantically aligned, and efficient text-to-motion generation suitable for streaming and long-horizon synthesis. 3.1 Motion-Language-Aligned Causal VAE To obtain a temporally structured and semantically consistent latent representation, CMDM employs a causal variational autoencoder aligned with a motion-language foundation model. Given a motion sequence ùê± 1 : T ‚àà ‚Ñù T √ó D \mathbf{x}_{1:T}\in\mathbb{R}^{T\times D} , where T T is the number of frames and D D is the dimension of the joint representation, the proposed MAC-VAE encoder E œï E_{\phi} and decoder D œà D_{\psi} operate causally such that each frame depends only on its past: ùê≥ t = E œï ‚Äã ( ùê± ‚â§ t ) , ùê± ^ t = D œà ‚Äã ( ùê≥ ‚â§ t ) , \mathbf{z}_{t}=E_{\phi}(\mathbf{x}_{\leq t}),\quad\hat{\mathbf{x}}_{t}=D_{\psi}(\mathbf{z}_{\leq t}), (1) where ùê≥ t ‚àà ‚Ñù d z \mathbf{z}_{t}\in\mathbb{R}^{d_{z}} denotes the latent representation at timestep t t . For a motion sequence with T T frames, we encode it into T / 4 T/4 temporal steps in the latent space, effectively achieving a 4 √ó 4\times temporal downsampling ratio. This compression balances representation compactness and temporal resolution, reducing redundancy while preserving the underlying motion dynamics. The encoder and decoder are adapted from [ xiao2025motionstreamer ] and are composed of 1D causal convolution and 1D causal ResNet blocks, ensuring temporal causality during both encoding and reconstruction. In this design, each frame only depends on preceding frames, while future frames are excluded from computation, explicitly modeling temporal causality in the latent space. During inference, reconstructed motions can be decoded sequentially in real time, enabling streaming generation without requiring access to future frames. To enhance semantic alignment, motion features are projected through a pretrained motion‚Äìlanguage encoder (Part-TMR [ yu2025remogpt ] ), which provides part-level semantic supervision. The MAC-VAE objective combines three terms: the standard VAE reconstruction loss, th