Title: Cut Less, Fold More: Model Compression through the Lens of Projection Geometry

Abstract: Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate &gt;1000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate-high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.

Body: Cut Less, Fold More: Model Compression through the Lens of Projection Geometry 1 Introduction 2 Unified Framework for Pruning and Folding 2.1 Preliminaries and Definitions Orthogonal Projection. 2.2 Compression as Orthogonal Projection Structured pruning. Model folding. 2.3 Folding Dominates Pruning 3 Experimental Results 4 Model Compression Ablation Studies 5 Conclusion, Limitations, and Outlook Reproducibility Statement. Acknowledgments. A Code, Data, and Resources B Proofs of Theoretical Claims C Related Work D Training Details D.1 ResNet18 on CIFAR-10 Training Setup with Adam and SGD D.2 PreActResNet18 on CIFAR-10 D.3 ViT-B/32 on CIFAR-10 D.4 CLIP ViT-B/32 on ImageNet-1K D.5 LLaMA-60M on Colossal Clean Crawled Corpus (C4) E Extended Empirical Comparison of Folding and Pruning F Additional Analyses: Sharpness, Runtime, and LLMs F.1 Sharpness and Barrier Analysis F.2 Runtime overhead and equivalence of compressed models F.3 Impact of one rank slack and singleton folding G Use of Large Language Models Cut Less, Fold More: Model Compression through the Lens of Projection Geometry Olga Saukh âˆ˜ ,Â§ {}^{\circ\text{,Â§}} Dong Wang âˆ˜ Haris Å ikiÄ‡ âˆ˜ Yun Cheng âˆ— Lothar Thiele â‹† Abstract Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate 1â€™000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderateâ€“high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory. 1 Introduction Neural network compression is critical for deploying models in resource-constrained environments. Common approaches include quantization, which reduces the precision of weights and activations, and knowledge distillation, which transfers information from a large teacher model to a smaller student model. In this work, we focus on the class of calibration-free post-training structured compression methods that optimize the model architecture itself without access to training data. Among these, the most widely used is magnitude-based pruning , which prunes tensor elements according to their magnitudes, using them as a proxy for their contribution to model accuracy (Han et al. , 2015 ; Mishra et al. , 2021 ; Lu et al. , 2023 ; Ding et al. , 2024 ; Bambhaniya et al. , 2024 ) . When combined with fine-tuning or a lightweight BatchNorm reset (Saikumar and Varghese, 2025 ) , this approach achieves significant compression rates with negligible accuracy loss (Kurtic et al. , 2022 ; Sanh et al. , 2020 ) . In contrast, the recently introduced model folding clusters similar weights and ties them together, providing an approximation of the original network (Wang et al. , 2025 ) . Both pruning and folding reduce parameter count but differ fundamentally: pruning removes weights entirely, while folding preserves them in merged representations. In this work, we develop a unified theoretical and empirical framework to compare pruning and folding through the lens of orthogonal projections in parameter space. We show that both compression methods can be viewed as projections onto lower-dimensional subspaces, but with crucial differences in geometry: pruning corresponds to axis-aligned coordinate projections, while folding projects onto cluster-structured subspaces that retain directional information. At a high level, both pruning and folding compress the weights of a model. We show that for any pruned solution there exists a folded alternative that is almost as smallâ€”using one extra component in the compressed representationâ€”yet is strictly closer to the original weights (smaller Frobenius norm), which in turn bounds the change in the network function. Intuitively, folding merges weight vectors with similar directions rather than zeroing coordinates, so the compressed model stays closer in behavior to the initial network. Empirically, we perform a comprehensive calibration-free study over 1â€™000 checkpoints spanning CNNs and ViTs on CIFAR-10 and ImageNet-1K, trained under diverse hyperparameter choices (optimizers, learning rates, augmentation, regularization, sharpness-aware training). We also train and process 18 LLaMA-family models with 60M and 130M parameters on C4, by varying learning rates, warmup lengths, and weight decay strength. After compression and also followed by lightweight and full fine-tuning, folding typically attains higher post-compression accuracy, with the largest gains at moderate to high compression. The margin narrows, and can occasionally reverse, at very low compression or under specific training setups, but the overall trend is consistent with our theoretical analysis. Our projection-based perspective opens new directions for designing compression methods that explicitly optimize for functional closeness. This paper makes the following contributions: â€¢ We introduce a unified projection framework that casts pruning and folding as orthogonal projections onto, respectively, axis-aligned and cluster-structured subspaces. We prove that at a compression rank difference of one, folding achieves smaller parameter reconstruction error and tighter function-perturbation bounds under mild smoothness assumptions. â€¢ A large-scale evaluation across 1â€™000 checkpoints and diverse hyperparameters, covering CNNs and ViTs on CIFAR-10 and ImageNet-1K, as well as LLaMA-60M and LLaMA-130M on C4. In addition, we use post-compression lightweight LayerNorm reset for ViTs, or full-fine-tuning to show that the strong performance of folding is preserved in these settings. â€¢ We show that folding is a geometry-aware alternative that is often superior in practice, with clearly identified regimes ( e.g. , moderateâ€“high compression) where its advantage is most pronounced, and corner cases where the gap narrows. We discuss related work in Appendix C , however, the main text already positions pruning and folding within our projection framework and clarifies the novelty of our approach. 2 Unified Framework for Pruning and Folding 2.1 Preliminaries and Definitions We consider a neural network with input x âˆˆ â„ d x\in\mathbb{R}^{d} . We assume ReLU activations and normalization layers ( e.g. , BatchNorm or LayerNorm) are present. To develop the theoretical framework, we focus on compressing a single layer at a time. This layer has p p inputs and m m outputs with its parameters collected in matrix ğ– âˆˆ â„ m Ã— p \mathbf{W}\in\mathbb{R}^{m\times p} . A row w â€‹ ( i ) w(i) of ğ– \mathbf{W} is denoted as the i i th parameter vector with individual weights w â€‹ ( i , j ) w(i,j) . Since all other network parameters are treated as fixed, the network function can be expressed as f â€‹ ( x ; ğ‘¾ ) f(x;\bm{W}) , which is trained to minimize a loss function L â€‹ ( ğ– ) L(\mathbf{W}) . We assume that the loss function L L is Lipschitz continuous, i.e. , there exists a constant Îº 0 \kappa 0 such that | L â€‹ ( ğ– 1 ) âˆ’ L â€‹ ( ğ– 2 ) | â‰¤ Îº â€‹ âˆ¥ ğ– 1 âˆ’ ğ– 2 âˆ¥ F |L(\mathbf{W}_{1})-L(\mathbf{W}_{2})|\leq\kappa\,\lVert\mathbf{W}_{1}-\mathbf{W}_{2}\rVert_{F} (1) for all admissible parameter matrices ğ– 1 \mathbf{W}_{1} and ğ– 2 \mathbf{W}_{2} . The Frobenius norm of a matrix is defined as âˆ¥ A âˆ¥ F = âˆ‘ i , j | a i â€‹ j | 2 \lVert A\rVert_{F}=\sqrt{\sum_{i,j}|a_{ij}|^{2}} , that is, the square root of the sum of the squares of its entries, or equivalently, the â„“ 2 \ell_{2} -norm of the vectorized matrix. This Lipschitz condition controls the change in loss with respect to parameter perturbations. Orthogonal Projection. We formalize structured pruning and model folding as orthogonal projections in parameter space. A matrix ğ‚ âˆˆ â„ m Ã— m \mathbf{C}\in\mathbb{R}^{m\times m} is an orthogonal projection if ğ‚ = ğ‚ âŠ¤ = ğ‚ 2 \mathbf{C}=\mathbf{C}^{\top}=\mathbf{C}^{2} , i.e. , it is symmetric and idempotent. Such projections map any parameter vector to its closest point (in the Euclidean norm) within a lower-dimensional subspace. If the columns of ğ” âˆˆ â„ m Ã— k \mathbf{U}\in\mathbb{R}^{m\times k} form a basis of a k k -dimensional subspace, the corresponding orthogonal projection is ğ‚ = ğ” â€‹ ( ğ” âŠ¤ â€‹ ğ” ) âˆ’ 1 â€‹ ğ” âŠ¤ . \mathbf{C}=\mathbf{U}(\mathbf{U}^{\top}\mathbf{U})^{-1}\mathbf{U}^{\top}. (2) Equivalently, ğ‚ â€‹ y = arg â¡ min z âˆˆ Range â€‹ ( ğ” ) â€‹ âˆ¥ y âˆ’ z âˆ¥ 2 \mathbf{C}y=\underset{z\in\mathrm{Range}(\mathbf{U})}{\arg\min}\;\lVert y-z\rVert_{2} meaning ğ‚ â€‹ y \mathbf{C}y is the orthogonal projection of y y onto the subspace spanned by ğ” \mathbf{U} . 2.2 Compression as Orthogonal Projection Structured pruning. Pruning can be viewed as a projection onto a coordinate-aligned subspace at the level of neurons, filters, or channels. Assume the layer outputs are ordered so that the last m âˆ’ k m-k are pruned. The corresponding basis ğ” p \mathbf{U}_{p} spans the k k -dimensional subspace, with projection matrix ğ‚ p \mathbf{C}_{p} and transformed weight matrix ğ– p \mathbf{W}_{p} : ğ” p = ( I 0 ) , ğ‚ p = ( I 0 0 0 ) , ğ– p = ğ‚ p â€‹ ğ– . \mathbf{U}_{p}=\begin{pmatrix}I\\ 0\end{pmatrix},\quad\mathbf{C}_{p}=\begin{pmatrix}I 0\\ 0 0\end{pmatrix},\quad\mathbf{W}_{p}=\mathbf{C}_{p}\mathbf{W}. (3) Consequently, the last m âˆ’ k m-k rows of ğ– p \mathbf{W}_{p} are zero, and the corresponding neurons, filters, or channels can be simply removed. Model folding. Folding groups the parameters into k k clusters and replaces each cluster with its mean. Depending on the choice of clusters, a different folding results. Folding can be represented as an orthogonal projection onto the k k -dimensional subspace spanned by ğ” f âˆˆ { 0 , 1 } m Ã— k \mathbf{U}_{f}\in\{0,1\}^{m\times k} , where each row contains exactly one nonzero entry indicating the cluster assignment. A cluster S j S_{j} comprises all indices of parameter vectors belonging to it; thus, u f â€‹ ( i , j ) = 1 u_{f}(i,j)=1 if and only if i âˆˆ S j i\in S_{j} . The projection ğ‚ f \mathbf{C}_{f} defined in Eq. 2 maps each cluster to its mean (Wang et al. , 2025 ) . Specifically, ğ– f = ğ‚ f â€‹ ğ– , âˆ€ i âˆˆ S j : w f â€‹ ( i ) = Î¼ j , Î¼ j = 1 | S j | â€‹ âˆ‘ i âˆˆ S j w â€‹ ( i ) , \mathbf{W}_{f}=\mathbf{C}_{f}\mathbf{W},\quad\forall i\in S_{j}:\;w_{f}(i)=\mu_{j},\quad\mu_{j}=\frac{1}{|S_{j}|}\sum_{i\in S_{j}}w(i), (4) where Î¼ j \mu_{j} is the mean of cluster j j . After projection, all parameter vectors within a cluster are replaced by their mean, making them identical. As a result, the corresponding layer outputs are also identical, leaving a total of k k distinct neurons, filters, or channels. Practically, the identical layer outputs can be joined while adapting the next layer appropriately, see (Wang et al. , 2025 ) . 2.3 Folding Dominates Pruning To compare pruning and folding, we first show that for any choice of pruning, there exists a folding that yields a more accurate approximation of the parameter matrix ğ– \mathbf{W} . Theorem 2.1 . Given any pruning with basis ğ” p \mathbf{U}_{p} of rank 0 â‰¤ k p â‰¤ m âˆ’ 1 0\leq k_{p}\leq m-1 ( i.e. , at least one parameter vector is pruned), there exists a folding with basis ğ” f â€² \mathbf{U}^{\prime}_{f} and rank k f = k p + 1 k_{f}=k_{p}+1 such that âˆ¥ ğ– âˆ’ ğ– p âˆ¥ F 2 â‰¥ âˆ¥ ğ– âˆ’ ğ– f â€² âˆ¥ F 2 , \lVert\mathbf{W}-\mathbf{W}_{p}\rVert_{F}^{2}\geq\lVert\mathbf{W}-\mathbf{W}^{\prime}_{f}\rVert_{F}^{2}, where ğ– p = ğ‚ p â€‹ ğ– \mathbf{W}_{p}=\mathbf{C}_{p}\mathbf{W} and ğ– f â€² = ğ‚ f â€² â€‹ ğ– \mathbf{W}^{\prime}_{f}=\mathbf{C}^{\prime}_{f}\mathbf{W} , with ğ‚ p \mathbf{C}_{p} and ğ‚ f â€² \mathbf{C}^{\prime}_{f} denoting the orthogonal projections defined in Eq. 2 . In the above theorem, ğ” f â€² \mathbf{U}^{\prime}_{f} denotes the constructive clustering obtained by merging all pruned rows into a single additional cluster. The proof is in Appendix B . By the Lipschitz continuity of the loss function in Eq. 1 , the superior approximation property of folding implies a tighter bound on the loss difference compared to pruning: | L â€‹ ( ğ– ) âˆ’ L â€‹ ( ğ– f â€² ) | â‰¤ Îº â€‹ âˆ¥ ğ– âˆ’ ğ– f â€² âˆ¥ F , | L â€‹ ( ğ– ) âˆ’ L â€‹ ( ğ– p ) | â‰¤ Îº â€‹ âˆ¥ ğ– âˆ’ ğ– p âˆ¥ F , |L(\mathbf{W})-L(\mathbf{W}^{\prime}_{f})|\leq\kappa\,\lVert\mathbf{W}-\mathbf{W}^{\prime}_{f}\rVert_{F},\quad|L(\mathbf{W})-L(\mathbf{W}_{p})|\leq\kappa\,\lVert\mathbf{W}-\mathbf{W}_{p}\rVert_{F}, with âˆ¥ ğ– âˆ’ ğ– f â€² âˆ¥ F 2 â‰¤ âˆ¥ ğ– âˆ’ ğ– p âˆ¥ F 2 . \lVert\mathbf{W}-\mathbf{W}^{\prime}_{f}\rVert_{F}^{2}\leq\lVert\mathbf{W}-\mathbf{W}_{p}\rVert_{F}^{2}. Furthermore, the rank difference k f = k p + 1 k_{f}=k_{p}+1 between pruning and folding is practically negligible, since in typical scenarios many parameter vectors are pruned. For instance, under a uniform 50 % 50\% per-layer retention, a ResNet-18 stage with 256 channels keeps k p = 128 k_{p}=128 (so folding uses k f = 129 k_{f}=129 ), and a ViT-B/32 block with width 768 keeps k p = 384 k_{p}=384 (so k f = 385 k_{f}=385 ); the relative increase is just 1 / k p â‰ˆ 0.78 % 1/k_{p}\approx 0.78\% and 0.26 % 0.26\% , respectivelyâ€”negligible in practice. Moreover, for all layers and architectures we observe that loss and accuracy vary smoothly as the rank increases from k p k_{p} to k p + 1 k_{p}+1 , no jumps in loss or accuracy, and the error difference between ranks k k and k + 1 k+1 is typically much smaller than the difference between pruning and folding at the same rank (see Appendix F.3 ). Finally, we show that folding using optimal k k -means clustering never yields a less accurate approximation of the parameter matrix ğ– \mathbf{W} than pruning. Theorem 2.2 . Let ğ” f â‹† \mathbf{U}^{\star}_{f} be the basis obtained from an optimal k k -means clustering with k f k_{f} clusters, i.e. , the folding clusters are determined by a k k -means algorithm minimizing the accumulated within-cluster sum of squares. Then, for any pruning with basis ğ” p \mathbf{U}_{p} of rank k p = k f âˆ’ 1 k_{p}=k_{f}-1 , we have âˆ¥ ğ– âˆ’ ğ– p âˆ¥ F 2 â‰¥ âˆ¥ ğ– âˆ’ ğ– f â‹† âˆ¥ F 2 , \lVert\mathbf{W}-\mathbf{W}_{p}\rVert_{F}^{2}\geq\lVert\mathbf{W}-\mathbf{W}^{\star}_{f}\rVert_{F}^{2}, where ğ– p = ğ‚ p â€‹ ğ– \mathbf{W}_{p}=\mathbf{C}_{p}\mathbf{W} and ğ– f â‹† = ğ‚ f â‹† â€‹ ğ– \mathbf{W}^{\star}_{f}=\mathbf{C}^{\star}_{f}\mathbf{W} , with ğ‚ p \mathbf{C}_{p} and ğ‚ f â‹† \mathbf{C}^{\star}_{f} denoting the orthogonal projections defined in Eq. 2 . The proof is given in Appendix B . This result demonstrates that k k -means fo