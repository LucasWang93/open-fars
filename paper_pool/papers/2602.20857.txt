Title: Functional Continuous Decomposition

Abstract: The analysis of non-stationary time-series data requires insight into its local and global patterns with physical interpretability. However, traditional smoothing algorithms, such as B-splines, Savitzky-Golay filtering, and Empirical Mode Decomposition (EMD), lack the ability to perform parametric optimization with guaranteed continuity. In this paper, we propose Functional Continuous Decomposition (FCD), a JAX-accelerated framework that performs parametric, continuous optimization on a wide range of mathematical functions. By using Levenberg-Marquardt optimization to achieve up to C^1 continuous fitting, FCD transforms raw time-series data into M modes that capture different temporal patterns from short-term to long-term trends. Applications of FCD include physics, medicine, financial analysis, and machine learning, where it is commonly used for the analysis of signal temporal patterns, optimized parameters, derivatives, and integrals of decomposition. Furthermore, FCD can be applied for physical analysis and feature extraction with an average SRMSE of 0.735 per segment and a speed of 0.47s on full decomposition of 1,000 points. Finally, we demonstrate that a Convolutional Neural Network (CNN) enhanced with FCD features, such as optimized function values, parameters, and derivatives, achieved 16.8% faster convergence and 2.5% higher accuracy over a standard CNN.

Body: Functional Continuous Decomposition 1 Introduction 1.1 Main Contributions 2 Background 2.1 Mode Decomposition 2.2 Local Smoothing Algorithms 2.3 Summary of Research Gap 3 Methodology 3.1 Normalization 3.2 Uniform Segmentation and Mode Calculation 3.3 Local Translation 3.4 Levenberg-Marquardt Optimization 3.5 Ensuring Continuity 3.6 Forward Fit 3.7 Unscaling Parameters 3.8 Computational Optimization 3.9 Configurability and Presets 4 Results 4.1 Accuracy Tests 4.2 Speed Tests 5 Applications 5.1 Velocity Applications 5.1.1 Velocity and Acceleration Analysis 5.2 Application in EEG signals 5.3 Application for efficient CNN training 6 Limitations and Implementation Details 7 Conclusion Functional Continuous Decomposition Teymur Aghayev Abstract The analysis of non-stationary time-series data requires insight into its local and global patterns with physical interpretability. However, traditional smoothing algorithms, such as B-splines, Savitzky-Golay filtering, and Empirical Mode Decomposition (EMD), lack the ability to perform parametric optimization with guaranteed continuity. In this paper, we propose Functional Continuous Decomposition (FCD), a JAX-accelerated framework that performs parametric, continuous optimization on a wide range of mathematical functions. By using Levenberg-Marquardt optimization to achieve up to C 1 C^{1} continuous fitting, FCD transforms raw time-series data into M M modes that capture different temporal patterns from short-term to long-term trends. Applications of FCD include physics, medicine, financial analysis, and machine learning, where it is commonly used for the analysis of signal temporal patterns, optimized parameters, derivatives, and integrals of decomposition. Furthermore, FCD can be applied for physical analysis and feature extraction with an average SRMSE of 0.735 per segment and a speed of 0.47s on full decomposition of 1,000 points. Finally, we demonstrate that a Convolutional Neural Network (CNN) enhanced with FCD features, such as optimized function values, parameters, and derivatives, achieved 16.8% faster convergence and 2.5% higher accuracy over a standard CNN. 1 Introduction Real-world raw signals are highly non-stationary, and analyzing them requires more than just an optimized curve. Standard signal processing techniques, such as B-splines, Savitzky-Golay filtering, and Empirical Mode Decomposition (EMD), are widely used for smoothing and feature extraction but lack functional optimization and physical plausibility. While traditional optimization algorithms, such as Levenberg-Marquardt (LM), Trust Region Reflective (TRF), and LBFGS-B, can fit data with mathematical functions, they cannot be used for continuous C 1 C^{1} fitting between segments for full decomposition of the raw signal, which is essential for deeper analysis and diverse physical applications. Thus, our Functional Continuous Decomposition algorithm bridges the limitations of current signal processing algorithms by decomposing raw time-series data into M M modes with overall C 1 C^{1} continuity. Specifically, initial modes have a higher number of segments to show local patterns, whereas higher modes reveal general patterns in the data. Each segment is fit with a specified mathematical function used for decomposition. Segments maintain C 1 C^{1} continuity by algebraically fixing two parameters of the function. Finally, FCD can be used to express local and global patterns of data, optimized values, derivatives, and parameters of the fitted function. 1.1 Main Contributions This framework introduces several key contributions to the field of time-series analysis, specifically addressing the limitations of traditional signal processing algorithms. 1. Parametric fitting : Segments are fitted with a specified mathematical function; the output contains an optimized fit, a function derivative, an integral, and parameters. 2. Guaranteed Continuity : We introduce a method to enforce C 0 C^{0} and C 1 C^{1} continuity across segment boundaries by algebraically deriving parameters, ensuring an overall continuous fit. 3. Full Configurability : Users can define custom mathematical functions (via SymPy integration [ undefg ] ), initial guesses, tune segmentation, Levenberg-Marquardt (LM) parameters, and set specific C 0 C^{0} , C 1 C^{1} derivative continuity settings. 4. High-Efficiency Decomposition : We demonstrate high-fidelity reconstruction with an average segment-wise SRMSE of 0.735 and processing speed of 0.47s for 1,000 data points across 6 modes, demonstrating linear computational complexity ( O â€‹ ( n ) O(n) ) with respect to signal length. 5. Efficient CNN training : Integrating FCD-derived features (parameters, optimized fit, and derivatives) into a CNN architecture results in 16.8% faster convergence and a 2.5% increase in predictive accuracy. The full implementation and technical documentation can be accessed here . 2 Background The development of Functional Continuous Decomposition (FCD) is situated at the intersection of non-stationary signal decomposition and smoothing algorithms. This section shows the mechanical foundations of current traditional techniques and identifies the technical gaps that FCD aims to solve. 2.1 Mode Decomposition The decomposition of modes from non-stationary signals is traditionally dominated by Empirical Mode Decomposition (EMD). EMD uses a recursive â€siftingâ€ process, which extracts Intrinsic Mode Functions (IMFs) by interpolating local extrema of the raw signal via cubic splines. Mechanically, the algorithm identifies all local maxima and minima to construct upper and lower envelopes via cubic spline interpolation. The mean of these envelopes is then subtracted from the original signal to isolate IMFs; this is done recursively until all modes are extracted. With this approach, EMD efficiently decomposes the signal into different temporal patterns. However, it lacks an analytical formulation f â€‹ ( t ) f(t) , a continuous derivative, and has error propagation due to its recursive nature. 2.2 Local Smoothing Algorithms FCD shares some similarity with local digital filters and piecewise splines, yet it extends their application for a functional continuous analysis. Cubic and B-Splines are highly efficient smoothing algorithms that perform polynomial interpolation of basis functions. B-Splines use a sequence of knots to define piecewise boundaries, resulting in a smooth fit across the data. However, B-splines lack parametric interpretability and can only show a smoothed signal fit. Savitzky-Golay (SG) is a non-parametric filter that performs a local least-squares polynomial fit on a sliding window of fixed length. SG fits a polynomial and keeps only the center point of each fit. While effective for simple denoising, SG is limited by its static window architecture, with which it is not possible to show continuous decomposition, optimized fit, derivatives, and its parameters. 2.3 Summary of Research Gap While existing algorithms provide robust tools for either signal smoothing (B-Splines, Savitzky-Golay) or mode decomposition (EMD), there remains a critical gap in providing a functional decomposition of the raw signal with C 1 C^{1} continuity. Functional Continuous Decomposition addresses this gap by providing a JAX-accelerated framework [ undef ] for continuous and parametric signal analysis. 3 Methodology The Functional Continuous Decomposition consists of four main stages: dataset normalization, uniform mode segmentation, Levenberg-Marquardt (LM) optimization, and algebraic continuity enforcement. By using the JAX-accelerated LM optimization in batches while processing modes in parallel, FCD performs efficient mode decomposition across complex functions and signals. 3.1 Normalization Original x and y datasets are normalized with adaptive standard scaling using the mean ( Î¼ \mu ) and length-dependent standard deviation ( Ïƒ N \sigma_{N} ) of the datasets. z = x âˆ’ Î¼ Ïƒ N z=\frac{x-\mu}{\sigma_{N}} (1) Here, standard deviation Ïƒ N \sigma_{N} depends on the dataset length ( N N ) to ensure constant density regardless of dataset length, which results in much higher stability of the Levenberg-Marquardt algorithm. Scaling factor s f = 0.01 s_{f}=0.01 is used to control the sample density: Ïƒ N = Ïƒ N â‹… s f \sigma_{N}=\frac{\sigma}{N\cdot s_{f}} (2) 3.2 Uniform Segmentation and Mode Calculation The FCD framework begins by decomposing the original signal X = { x 1 , x 2 , x 3 , â€¦ , x N } X=\{x_{1},x_{2},x_{3},\dots,x_{N}\} into M M hierarchical modes starting from the noisiest to the global trend mode. Each mode m m , except the last, utilizes a uniform segmentation, where the number of segments decreases as the algorithm progresses toward long-term trends. The total number of modes, M M , is determined adaptively based on the signal length N N . To ensure each mode captures a distinct temporal pattern, the framework starts with ( N / Î± N/\alpha ) segments with a minimum number of segments Î² \beta . The number of modes is calculated using a logarithmic function: M = âŒˆ log 2 â¡ ( N / Î± Î² ) âŒ‰ + 1 M=\left\lceil\log_{2}\left(\frac{N/\alpha}{\beta}\right)\right\rceil+1 (3) Where Î± \alpha represents the initial divisor for the number of segments (default Î± = 5 \alpha=5 ) and Î² \beta defines the minimum number of segments (default Î² = 4 \beta=4 ); the addition of 1 accounts for the last trend mode. This logarithmic approach ensures that M M scales efficiently with the dataset length. For instance, generating 4 modes for N = 100 N=100 , 7 modes for N = 1 , 000 N=1,000 , and 10 modes for N = 10 , 000 N=10,000 . To generate segment boundaries for each mode, we calculate the number of segments in each mode starting from k 1 = âŒŠ N / Î± âŒ‹ k_{1}=\lfloor N/\alpha\rfloor . Subsequent modes follow a recursive reduction, where the number of segments is halved, k m = max â¡ ( âŒŠ k m âˆ’ 1 / 2 âŒ‹ , Î² ) k_{m}=\max(\lfloor k_{m-1}/2\rfloor,\beta) , until the last trend mode with ( k M = 1 k_{M}=1 ). Afterwards, segment boundary indices are calculated using linear interpolation of the number of segments in each mode into the dataset range ( [ 0 , N ] [0,N] ). 3.3 Local Translation To ensure high numerical stability and better physical interpretation of optimized parameters, each segmentâ€™s x x -values are translated to local coordinates. x k x_{k} represents the absolute x-value at the current segment boundary k k , local x-values are calculated as: x ^ = x âˆ’ x k \hat{x}=x-x_{k} (4) Local translation improves numerical stability and physical applications of parameters. Polynomial coefficients and linear offsets directly represent the signalâ€™s state in the current segment, being independent of the global x x magnitude. 3.4 Levenberg-Marquardt Optimization To represent the signal within each mode m m , a specified general function y = f â€‹ ( x , ğ© ) y=f(x,\mathbf{p}) is used, where ğ© \mathbf{p} is a vector of parameters. To ensure continuity between segments, two parameters are fixed and excluded from ğ© \mathbf{p} during optimization; instead, they are algebraically derived from the remaining parameters in vector ğ© \mathbf{p} during the calculation of residuals. Unlike traditional non-parametric methods, this approach allows for the definition of custom models (polynomial, sinusoidal, or exponential) to reflect the underlying physics of the data with guaranteed continuity. Optimization is done with JAX-accelerated Levenberg-Marquardt algorithm [ undefd , undefe ] in batches of s s segments; modes are fitted in parallel. General formula of LM optimization: ( ğ‰ T â€‹ ğ‰ + ( Î» + Î± ) â€‹ ğˆ ) â€‹ Î” â€‹ ğ© = âˆ’ ğ‰ T â€‹ ğ« , ğ‰ i â€‹ j = âˆ‚ r i âˆ‚ p j \left(\mathbf{J}^{T}\mathbf{J}+(\lambda+\alpha)\mathbf{I}\right)\Delta\mathbf{p}=-\mathbf{J}^{T}\mathbf{r},\quad\mathbf{J}_{ij}=\frac{\partial r_{i}}{\partial p_{j}} (5) The system is solved for Î” â€‹ ğ© \Delta\mathbf{p} where J is the Jacobian matrix of the residuals ğ« \mathbf{r} with respect to the parameters ğ© \mathbf{p} , Î» \lambda is the dynamic damping factor, Î± \alpha is a static ridge regularization, i âˆˆ { 1 , â€¦ , N } i\in\{1,\dots,N\} denotes the data point index and j âˆˆ { 1 , â€¦ , d } j\in\{1,\dots,d\} denotes the parameter index. Residuals are calculated via the Sum of Squared Residuals loss function â„’ \mathcal{L} : â„’ â€‹ ( ğ© ) = âˆ‘ i = 1 n ( y i âˆ’ f â€‹ ( x i , ğ© ) ) 2 \mathcal{L}(\mathbf{p})=\sum_{i=1}^{n}(y_{i}-f(x_{i},\mathbf{p}))^{2} (6) The quality of the proposed step is defined as the actual reduction Î” â€‹ r \Delta r : the difference between the loss function of past and proposed error at iteration ( n n ) of the optimization. Î” â€‹ r = â„’ â€‹ ( ğ© n âˆ’ 1 ) âˆ’ â„’ â€‹ ( ğ© n ) \Delta r=\mathcal{L}(\mathbf{p}_{n-1})-\mathcal{L}(\mathbf{p}_{n}) (7) Crucially, while the localized segments are fitted with the LM optimizer, the final trend mode (not segmented) is optimized using a Trust Region Reflective (TRF) algorithm [ undefa ] to avoid recompilation, as our proposed LM optimizer is designed for segmental fitting. 3.5 Ensuring Continuity To ensure each mode is smooth across segment boundaries x k x_{k} , we enforce C 0 C^{0} (value) and C 1 C^{1} (derivative) continuity by algebraically fixing two parameters. They are solved analytically based on the previous segmentâ€™s y-value and derivative at the segment boundary. The fixed continuity parameters are calculated from the following equations for segment k 1 k 1 , and the previous segmentâ€™s local x-value at the segment boundary, denoted as x k âˆ’ 1 , l x_{k-1,l} : f â€‹ ( 0 , ğ© ğ¤ ) = f â€‹ ( x k âˆ’ 1 , l , ğ© ğ¤ âˆ’ ğŸ ) f(0,\mathbf{p_{k}})=f(x_{k-1,l},\mathbf{p_{k-1}}) (8) f â€² â€‹ ( 0 , ğ© ğ¤ ) = f â€² â€‹ ( x k âˆ’ 1 , l , ğ© ğ¤ âˆ’ ğŸ ) f^{\prime}(0,\mathbf{p_{k}})=f^{\prime}(x_{k-1,l},\mathbf{p_{k-1}}) (9) The first equation is solved for a fixed value parameter, and the second equation for a fixed derivative parameter. In function with linear offset term ( a â€‹ x + b ax+b ), b b can be used as a fixed value parameter and a a as a fixed derivative parameter for continuity between segments. This constrains the Levenberg-Marquardt (LM) optimizer to only explore solutions that are physically consistent, ensuring exact continuity. Demonstration of the FCD algorithm with 6-parameter sinusoidal model (sin6) given as y = ( A 1 â€‹ x + A 0 ) â€‹ sin â¡ ( B 0 â€‹ x + D ) + C 1 â€‹ x + C 0 y=(A_{1}x+A_{0})\sin(B_{0}x+D)+C_{1}x+C_{0} . Blue points show the original dataset, red is the optimized continuous fit, and gray lines are segment boundaries. Figure 1 : FCD example on Bitcoin 1-minute data Optimized functions for mode 5 are presented as follows: f â€‹ ( x ) = { ( 0.263 â€‹ x + 56.296 ) â€‹ sin â¡ ( 0.064 â€‹ x âˆ’ 1.155 ) + ( 0.475 â€‹ x + 29030 ) 0 â‰¤ x 90 ( 1.198 â€‹ x + 96.122 ) â€‹ sin â¡ ( 0.044 â€‹ x + 2.236 ) + ( 1.545 â€‹ x + 28917 ) 90 â‰¤ x 180 ( âˆ’ 1.681 â€‹ x + 220.586 ) â€‹ sin â¡ ( 0.052 â€‹ x âˆ’ 0.823 ) + ( 1.396 â€‹ x + 29200 ) 180 â‰¤ x 270 ( 0.142 â€‹ x + 29.495 ) â€‹ sin â¡ ( 0.059 â€‹ x + 1.922 ) + ( 0.163 â€‹ x + 29254 ) 270 â‰¤ x 360 ( âˆ’ 1.974 â€‹ x + 90.332 ) â€‹ sin â¡ ( 0.063 â€‹ x âˆ’ 0.870 ) + ( âˆ’ 3.441 â€‹ x + 29372 ) 360