Title: Breaking the Stochasticity Barrier: An Adaptive Variance-Reduced Method for Variational Inequalities

Abstract: Stochastic non-convex non-concave optimization, formally characterized as Stochastic Variational Inequalities (SVIs), presents unique challenges due to rotational dynamics and the absence of a global merit function. While adaptive step-size methods (like Armijo line-search) have revolutionized convex minimization, their application to this setting is hindered by the Stochasticity Barrier: the noise in gradient estimation masks the true operator curvature, triggering erroneously large steps that destabilize convergence. In this work, we propose VR-SDA-A (Variance-Reduced Stochastic Descent-Ascent with Armijo), a novel algorithm that integrates recursive momentum (STORM) with a rigorous Same-Batch Curvature Verification mechanism. We introduce a theoretical framework based on a Lyapunov potential tracking the Operator Norm, proving that VR- SDA-A achieves an oracle complexity of O(epsilon -3) for finding an epsilon-stationary point in general Lipschitz continuous operators. This matches the optimal rate for non-convex minimization while uniquely enabling automated step-size adaptation in the saddle-point setting. We validate our approach on canonical rotational benchmarks and non-convex robust regression tasks, demonstrating that our method effectively suppresses limit cycles and accelerates convergence with reduced dependence on manual learning rate scheduling.

Body: Breaking the Stochasticity Barrier: An Adaptive Variance-Reduced Method for Variational Inequalities 1 Introduction 1.1 The Stochasticity Barrier in Variational Inequalities 1.2 Our Contribution: Coupling Variance Reduction with Operator Dynamics 2 Related Work 2.1 Non-Monotone Variational Inequalities 2.2 Adaptive Methods for VIs 2.3 Variance Reduction and Optimal Rates 2.4 Same-Sample and Curvature-Based Methods 2.5 Our Position in the Landscape 3 Preliminaries 3.1 Notations 3.2 Problem Formulation: Stochastic Variational Inequalities 3.3 Standard Tools: Armijo and STORM 3.4 Performance Metric (The Merit Function) 3.5 Assumptions and Properties 4 Proposed Algorithm: VR-SDA-A 4.1 Mechanism 1: Recursive Variance Reduction (STORM) 4.2 Mechanism 2: Same-Batch Curvature Verification 4.3 The Algorithm 5 Theoretical Analysis 5.1 Key Lemmas 5.2 Global Convergence 6 Computational Complexity and Comparative Analysis 6.1 Oracle Complexity Comparison 6.2 Per-Iteration Cost and Memory 7 Experimental Verification 7.1 Canonical Bilinear System Analysis of the Rotational Limit Case. 7.2 Synthetic Ablation Study 7.3 Non-Convex Robust Optimization 7.4 Limitations 8 Conclusion and Future Work A Detailed Algorithm B Proofs of Technical Lemmas B.1 Proof of Lemma 1 (Smoothness of Merit Function) B.2 Proof of Lemma 2 B.3 Proof of Lemma 3 (Rigorous Conditional Stability) C Proof of Theorem 4 (Global Convergence) Breaking the Stochasticity Barrier: An Adaptive Variance-Reduced Method for Variational Inequalities Yungi Jeong Takumi Otsuka Abstract Stochastic non-convex non-concave optimization, formally characterized as Stochastic Variational Inequalities (SVIs), presents unique challenges due to rotational dynamics and the absence of a global merit function. While adaptive step-size methods (like Armijo line-search) have revolutionized convex minimization, their application to this setting is hindered by the Stochasticity Barrier : the noise in gradient estimation masks the true operator curvature, triggering erroneously large steps that destabilize convergence. In this work, we propose VR-SDA-A (Variance-Reduced Stochastic Descent-Ascent with Armijo), a novel algorithm that integrates recursive momentum (STORM) with a rigorous Same-Batch Curvature Verification mechanism. We introduce a theoretical framework based on a Lyapunov potential tracking the Operator Norm, proving that VR-SDA-A achieves an oracle complexity of ùí™ ‚Äã ( œµ ‚àí 3 ) \mathcal{O}(\epsilon^{-3}) for finding an œµ \epsilon -stationary point in general Lipschitz continuous operators. This matches the optimal rate for non-convex minimization while uniquely enabling automated step-size adaptation in the saddle-point setting. We validate our approach on canonical rotational benchmarks and non-convex robust regression tasks, demonstrating that our method effectively suppresses limit cycles and accelerates convergence with reduced dependence on manual learning rate scheduling. Minimax Optimization, GANs, Variance Reduction, Line Search, Game Theory 1 Introduction The frontier of modern machine learning optimization has expanded beyond simple minimization to encompass complex, coupled dynamics. Problems in robust adversarial training, fair machine learning, and multi-agent reinforcement learning are formally characterized as stochastic minimax optimization problems: min Œ∏ ‚àà ‚Ñù d 1 ‚Å° max œï ‚àà ‚Ñù d 2 ‚Å° f ‚Äã ( Œ∏ , œï ) ‚âú ùîº Œæ ‚àº ùíü ‚Äã [ F ‚Äã ( Œ∏ , œï ; Œæ ) ] \min_{\theta\in\mathbb{R}^{d_{1}}}\max_{\phi\in\mathbb{R}^{d_{2}}}f(\theta,\phi)\triangleq\mathbb{E}_{\xi\sim\mathcal{D}}[F(\theta,\phi;\xi)] (1) It is precise to view these problems through the lens of Stochastic Variational Inequalities (SVI). Let z = [ Œ∏ ‚ä§ , œï ‚ä§ ] ‚ä§ z=[\theta^{\top},\phi^{\top}]^{\top} . We define the problem as finding a zero of the operator (vector field) V ‚Äã ( z ) = [ ‚àá Œ∏ f ‚Äã ( Œ∏ , œï ) ‚ä§ , ‚àí ‚àá œï f ‚Äã ( Œ∏ , œï ) ‚ä§ ] ‚ä§ V(z)=[\nabla_{\theta}f(\theta,\phi)^{\top},-\nabla_{\phi}f(\theta,\phi)^{\top}]^{\top} . Unlike minimization, where the negative gradient field ‚àí ‚àá f -\nabla f constitutes a conservative vector field that naturally guides iterates toward a local optimum, the simultaneous gradient descent-ascent dynamics define a non-conservative vector field V ‚Äã ( z ) V(z) . As noted by Balduzzi and others ( 2018 ) , such fields in non-convex non-concave settings often exhibit rotational components (Jacobian eigenvalues with imaginary parts), causing standard first-order methods to orbit the equilibrium rather than converging to it. To stabilize these dynamics, the community has largely relied on manual heuristics or algorithmic lookahead mechanisms (e.g., Extra-Gradient (Korpelevich, 1976 ) or Optimistic GDA (Daskalakis et al. , 2018 ) ). While theoretically sound, these methods typically depend on fixed, conservative step-sizes to approximate the continuous-time integration of the ODE. This creates a fundamental dilemma: a small step-size is required for stability in rotational fields, but the non-convex landscape of high-dimensional models is replete with flat plateaus. A fixed small step-size results in prohibitively slow traversal, while a fixed large step-size risks divergence. In convex minimization, this dilemma is resolved by Adaptive Line-Search (Vaswani et al. , 2019 ; Vaswani and Babanezhad, 2025 ) , which dynamically estimates the local Lipschitz constant to maximize the step size. However, no such reliable, parameter-free mechanism exists for stochastic non-monotone variational inequalities. 1.1 The Stochasticity Barrier in Variational Inequalities Transferring adaptive line-search to stochastic operators presents a unique challenge we term the Stochasticity Barrier . In minimization, line-search accepts a step if the objective function value decreases sufficiently. In the general operator setting, f ‚Äã ( z ) f(z) is not a valid merit function (as the maximizer seeks to increase it), and the operator norm ‚Äñ V ‚Äã ( z ) ‚Äñ 2 \|V(z)\|^{2} is biased by noise. Critically, standard stochastic line-search relies on a ‚Äùdescent‚Äù condition to bound the error. In the stochastic SVI setting, a ‚Äùlucky‚Äù mini-batch with low variance might erroneously suggest the local operator is smooth (small Lipschitz constant), authorizing a large step Œ∑ t \eta_{t} . When applied to the true population dynamics, this large step can catastrophically overshoot, breaking the delicate coupling of the system dynamics. This failure mode implies that in non-monotone operators, variance is not merely noise; it is a structural destabilizer for adaptive methods. We formalize this intuition: without variance reduction, any adaptive method relying on the current batch‚Äôs geometry will authorize step sizes Œ∑ t 2 / L \eta_{t} 2/L with constant probability, violating stability conditions in rotational fields. Thus, variance reduction is strictly necessary to enable adaptive step-sizes in non-convex non-concave SVIs. 1.2 Our Contribution: Coupling Variance Reduction with Operator Dynamics In this work, we propose VR-SDA-A (Variance-Reduced Stochastic Descent-Ascent with Armijo). Our central thesis is that Variance Reduction (VR) is strictly necessary to enable adaptive step-sizes in non-convex non-concave SVIs. We utilize Recursive Variance Reduction (specifically the STORM estimator (Cutkosky and Orabona, 2019 ) ) to construct a low-variance estimate of the operator V ‚Äã ( z ) V(z) . Crucially, we introduce a Same-Batch Curvature Verification strategy. Instead of checking for ‚Äùdescent‚Äù on the objective, our line-search estimates the local curvature of the operator using the same batch used for the update. This effectively treats the stochastic step as ‚Äùlocally deterministic,‚Äù satisfying the rigorous stability conditions required for VIs. Our specific contributions are: ‚Ä¢ Algorithmic Framework: We introduce VR-SDA-A, integrating recursive variance reduction with an adaptive step-size mechanism. Unlike standard VI line-searches (e.g., Malitsky ( 2020 ) ) which often assume monotonicity or full gradients, our method handles the fully stochastic non-convex non-concave setting without manual tuning. ‚Ä¢ Theoretical Guarantee: We provide a convergence analysis based on a Lyapunov potential function Œ¶ t \Phi_{t} . We prove that VR-SDA-A converges to an œµ \epsilon -stationary point of the operator ( ùîº ‚Äã [ ‚Äñ V ‚Äã ( z ) ‚Äñ 2 ] ‚â§ œµ \mathbb{E}[\|V(z)\|^{2}]\leq\epsilon ) with an oracle complexity of ùí™ ‚Äã ( œµ ‚àí 3 ) \mathcal{O}(\epsilon^{-3}) . This matches the optimal rate for non-convex minimization while uniquely addressing the rotational instability of saddle-point problems. ‚Ä¢ Mechanism Analysis: We rigorously derive the ‚ÄùSame-Batch‚Äù condition, showing that it allows us to locally bound the error between the stochastic operator update and the true operator geometry, effectively overcoming the Stochasticity Barrier without requiring the Strong Growth Condition (SGC). 2 Related Work 2.1 Non-Monotone Variational Inequalities The convergence difficulties of Gradient Descent-Ascent (GDA) in solving Variational Inequalities (VIs) are well-documented, particularly when the operator is non-monotone (corresponding to non-convex non-concave games). Balduzzi and others ( 2018 ) formalized this using the Hamiltonian decomposition of the Jacobian, showing that the non-conservative (rotational) component of the vector field causes standard first-order methods to cycle rather than converge. To mitigate these rotational dynamics, the literature has focused on ‚Äùlookahead‚Äù mechanisms that approximate the implicit proximal point method. These include the Extra-Gradient (EG) method (Korpelevich, 1976 ) and Optimistic GDA (OGDA) (Daskalakis et al. , 2018 ) , unified by Gidel et al. ( 2019 ) under the VI framework. However, theoretical guarantees for these methods typically rely on a fixed step-size Œ∑ 1 / L \eta 1/L . In practice, the Lipschitz constant L L is often unknown or locally variable, rendering fixed step-size strategies fragile and requiring extensive hyperparameter tuning. 2.2 Adaptive Methods for VIs In the deterministic VI setting, adaptive step-size mechanisms are well-established. Tseng ( 2000 ) proposed a modified forward-backward splitting method with a line-search that adapts to the local Lipschitz constant. More recently, Malitsky ( 2020 ) introduced the ‚ÄùGolden Ratio‚Äù algorithm, which adapts the step-size based on curvature observed in previous iterations without requiring a backtracking line-search. However, these methods generally assume access to exact operator evaluations. When applied to the stochastic setting, the noise in the operator evaluation V ^ ‚Äã ( z ; Œæ ) \hat{V}(z;\xi) invalidates the monotonicity checks required by Tseng-type line-searches. While Vaswani et al. ( 2019 ) successfully applied Armijo line-search to stochastic minimization (SLS), their analysis relies on the Strong Growth Condition (SGC), which implies the noise variance œÉ 2 ‚Üí 0 \sigma^{2}\to 0 at the optimum. Crucially, general stochastic VIs (and minimax games) violate SGC because the equilibrium is a saddle point where individual player gradients are non-zero, resulting in persistent variance even at optimality. 2.3 Variance Reduction and Optimal Rates To address persistent variance without resorting to vanishing step-sizes (which degrade convergence rates), Variance Reduction (VR) is essential. For finite-sum games, methods like SVRG-Saddle (Palaniappan and Bach, 2016 ) achieve linear convergence, but do not scale to the online (expectation maximization) setting. For online non-convex optimization, recursive momentum estimators like SPIDER (Fang et al. , 2018 ) and STORM (Cutkosky and Orabona, 2019 ) have achieved the optimal oracle complexity of ùí™ ‚Äã ( œµ ‚àí 3 ) \mathcal{O}(\epsilon^{-3}) . Recent works have begun applying these estimators to VIs. Alacaoglu and Malitsky ( 2021 ) proposed variance-reduced extragradient methods, but their analysis requires the operator to be monotone (convex-concave). Other approaches achieving ùí™ ‚Äã ( œµ ‚àí 3 ) \mathcal{O}(\epsilon^{-3}) in the non-monotone setting typically rely on fixed, pre-scheduled step-sizes or second-order information (Hessian-vector products). 2.4 Same-Sample and Curvature-Based Methods Recent work on same-sample optimistic gradient (SS-OG) methods (Huang et al. , 2024 ) addresses variance coupling by re-evaluating gradients on identical samples. Our Same-Batch Curvature Verification shares this principle but extends it to adaptive step-size selection rather than fixed steps. Bias-corrected SEG+ (Pethick et al. , 2023b ) achieves convergence under weak MVI conditions via a two-step correction scheme; we differ by targeting fully non-monotone operators with an adaptive mechanism. CurvatureEG+ (Pethick et al. , 2023a ) develops curvature-aware backtracking for deterministic non-monotone VIs; our work extends this principle to the stochastic setting with variance reduction. 2.5 Our Position in the Landscape Our work addresses the intersection of these three challenges: ‚Ä¢ VS. Stochastic Line-Search: Unlike Vaswani et al. ( 2019 ) , we do not assume the Strong Growth Condition. We handle the bounded variance setting ( œÉ 2 0 \sigma^{2} 0 ) by coupling the line-search with a variance-reduced estimator. ‚Ä¢ VS. Adaptive VIs: Unlike Malitsky ( 2020 ) or Lin et al. ( 2020 ) , we handle the fully stochastic setting where operator evaluations are noisy. ‚Ä¢ VS. Existing VR Methods: Unlike Alacaoglu and Malitsky ( 2021 ) , we do not assume monotonicity. We target general Lipschitz continuous operators (non-convex non-concave), matching the optimal ùí™ ‚Äã ( œµ ‚àí 3 ) \mathcal{O}(\epsilon^{-3}) rate while uniquely offering a robust, adaptive mechanism. 3 Preliminaries 3.1 Notations We use lower-case bold letters to denote vectors (e.g., ùê≥ ‚àà ‚Ñù d \mathbf{z}\in\mathbb{R}^{d} ) and upper-case letters to denote matrices or mappings. For a vector ùê≥ \mathbf{z} , ‚Äñ ùê≥ ‚Äñ \|\mathbf{z}\| denotes the standard Euclidean norm ( L 2 L_{2} ). For a differentiable operator V : ‚Ñù d ‚Üí ‚Ñù d V:\mathbb{R}^{d}\to\mathbb{R}^{d} , ‚àá V ‚Äã ( ùê≥ ) \nabla V(\mathbf{z}) denotes its Jacobian matrix. We use ùîº ‚Äã [ ‚ãÖ ] \mathbb{E}[\cdot] to denote the expectation over the stochastic source Œæ \xi , and ùîº t [ ‚ãÖ ] ‚âú ùîº [ ‚ãÖ | ‚Ñ± t ‚àí 1 ] \mathbb{E}_{t}[\cdot]\triangleq\mathbb{E}[\cdot|\mathcal{F}_{t-1}] to denote the conditional expectation given the filtration up to time t ‚àí 1 t-1 . We use ùí™ ‚Äã ( ‚ãÖ ) \mathcal{O}(\cdot) to denote standard Big-O notation. The set { 1 , ‚Ä¶ , T } \{1,\dots,T\} is denoted by [ T ] [T] . 3.2 Problem Formulation: Stochastic Variational Inequalities We consider the problem of finding a zero of a stochastic operator, formally defined as a Stochastic Variational Inequality (SVI): Find ‚Äã ùê≥ ‚àó ‚àà ‚Ñù d ‚Äã such that ‚Äã V ‚Äã ( ùê≥ ‚àó ) = ùîº Œæ ‚àº ùíü ‚Äã [ V ‚Äã ( ùê≥ ‚àó ; Œæ ) ] = 0 \text{Find }\mathbf{z}^{*}\in\mathbb{R}^{d}\text{ such that }V(\mathbf{z}^{*})=\mathbb{E}_{\xi\sim\mathcal{D}}[V(\mathbf{z}^{*};\xi)]=0 (2) This formulation encapsulates a broad class of adversarial problems, including Minimax Optim