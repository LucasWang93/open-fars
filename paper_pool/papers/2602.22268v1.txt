Title: AutoQRA: Joint Optimization of Mixed-Precision Quantization and Low-rank Adapters for Efficient LLM Fine-Tuning

Abstract: Quantization followed by parameter-efficient fine-tuning has emerged as a promising paradigm for downstream adaptation under tight GPU memory constraints. However, this sequential pipeline fails to leverage the intricate interaction between quantization bit-width and LoRA rank. Specifically, a carefully optimized quantization allocation with low quantization error does not always translate to strong fine-tuning performance, and different bit-width and rank configurations can lead to significantly varying outcomes under the same memory budget. To address this limitation, we propose AutoQRA, a joint optimization framework that simultaneously optimizes the bit-width and LoRA rank configuration for each layer during the mixed quantized fine-tuning process. To tackle the challenges posed by the large discrete search space and the high evaluation cost associated with frequent fine-tuning iterations, AutoQRA decomposes the optimization process into two stages. First, it first conducts a global multi-fidelity evolutionary search, where the initial population is warm-started by injecting layer-wise importance priors. This stage employs specific operators and a performance model to efficiently screen candidate configurations. Second, trust-region Bayesian optimization is applied to locally refine promising regions of the search space and identify optimal configurations under the given memory budget. This approach enables active compensation for quantization noise in specific layers during training. Experiments show that AutoQRA achieves performance close to full-precision fine-tuning with a memory footprint comparable to uniform 4-bit methods.

Body: AutoQRA: Joint Optimization of Mixed-Precision Quantization and Low-rank Adapters for Efficient LLM Fine-Tuning 1 Introduction 2 Related Work 2.1 Efficient LLM Fine-Tuning 2.2 Automated Search for Model Compression 3 Methodology 3.1 Motivation and Problem Formulation 3.2 Phase I: Global Multi-Fidelity Evolutionary Search 3.3 Phase II: Local Bayesian Refinement 4 Experiments 4.1 Experimental Setup 4.2 Main Results 4.3 Screening surrogate quality. 4.4 Search efficiency and sample complexity. 4.5 Ablation Study 5 Conclusion A Orthogonal Sensitivity and Compensatory Potential. B Additional Details for Phase I Exact memory decomposition. Importance signals for warm start and proposals. C Experimental Details C.1 Phase I multi-fidelity step allocation C.2 Phase I termination C.3 Phase II TuRBO high-fidelity evaluation C.4 Phase II early stopping D Task-wise Accuracy Breakdown and Discussion First takeaway: the uniform 4-bit baselines fail in very specific ways, and AutoQRA mostly fixes those failures. LLaMA-3.1-8B: AutoQRA ( ≤ \leq 4b) is basically “FP16-like” on most tasks, with the biggest win on GSM8K. LLaMA-3.2-3B: the gains are smaller and less uniform, but the “compensation” pattern still shows up where it matters. Qwen-2.5-3B: AutoQRA’s gains look like a re-prioritization toward multi-step reasoning. AutoQRA ( ≤ \leq 4b) vs. AutoQRA (Opt): what the extra precision buys. Bottom line. E Additional Ablations E.1 Feasibility Repair Analysis E.2 Search-Protocol Hyperparameter Sensitivity AutoQRA: Joint Optimization of Mixed-Precision Quantization and Low-rank Adapters for Efficient LLM Fine-Tuning Changhai Zhou Shiyang Zhang Yuhua Zhou Qian Qiao Jun Gao Cheng Jin Kaizhou Qin Weizhong Zhang Abstract Quantization followed by parameter-efficient fine-tuning has emerged as a promising paradigm for downstream adaptation under tight GPU memory constraints. However, this sequential pipeline fails to leverage the intricate interaction between quantization bit-width and LoRA rank. Specifically, a carefully optimized quantization allocation with low quantization error does not always translate to strong fine-tuning performance, and different bit-width and rank configurations can lead to significantly varying outcomes under the same memory budget. To address this limitation, we propose AutoQRA , a joint optimization framework that simultaneously optimizes the bit-width and LoRA rank configuration for each layer during the mixed quantized fine-tuning process. To tackle the challenges posed by the large discrete search space and the high evaluation cost associated with frequent fine-tuning iterations, AutoQRA decomposes the optimization process into two stages. First, it first conducts a global multi-fidelity evolutionary search, where the initial population is warm-started by injecting layer-wise importance priors. This stage employs specific operators and a performance model to efficiently screen candidate configurations. Second, trust-region Bayesian optimization is applied to locally refine promising regions of the search space and identify optimal configurations under the given memory budget. This approach enables active compensation for quantization noise in specific layers during training. Experiments show that AutoQRA achieves performance close to full-precision fine-tuning with a memory footprint comparable to uniform 4-bit methods. Machine Learning, ICML 1 Introduction Deploying large language model (LLMs) for specific downstream tasks can be prohibitively memory intensive, which prevents many users from adapting strong base models in practice (Makridakis et al. , 2023 ; Raiaan et al. , 2024 ; Chang et al. , 2024 ) . A common workaround is a sequential pipeline: first quantize the pretrained backbone to fit a tight GPU memory budget, then perform parameter efficient fine tuning by training lightweight adapters such as LoRA while keeping the quantized backbone frozen (Hu et al. , 2022 ; Li et al. , 2023 ; Xu et al. , 2023 ) . In this setting, the deployment budget is a hard constraint, and the objective is the post fine tuning performance achieved within that budget. Recent work has begun to exploit layerwise heterogeneity in the quantize-then-fine-tune pipeline for LLMs. On the quantization side, mixed-precision methods allocate different bit widths across layers according to estimated sensitivity, aiming to reduce the discrepancy between the quantized model and its full-precision counterpart (Huang et al. , 2025 ; Lee et al. , 2025 ) . On the adaptation side, non-uniform rank allocation concentrates LoRA capacity on layers that are more important for task adaptation (Zhang et al. , 2023 ; Zhou et al. , 2025 ) . However, we find that a bit-width allocation that appears favorable under reconstruction or calibration criteria can still lead to poor performance after fine-tuning. Moreover, under the same memory budget, different combinations of bit width and LoRA rank can yield sharply different outcomes. The fundamental reason is that bit width and LoRA rank interact, yet most methods treat them as independent decisions. For example, a typical performance-oriented pipeline first fixes per-layer precision using static proxies and then adjusts non-uniform ranks. This separation is misaligned with the deployment objective because the two knobs are coupled during training. Lower precision introduces quantization noise, while additional adapter capacity can partially compensate for that noise through learning. Once bit widths are fixed, the system loses the opportunity to trade redundant precision for learnability in the layers where adapters can use it most effectively, leading to systematic resource misallocation. These observations motivate a joint optimization problem that assigns a bit width and a LoRA rank to each layer for fine-tuning. Solving this problem is difficult: the search space is large and fully discrete, making exhaustive enumeration infeasible. More importantly, low-cost proxies are unreliable because they do not model the interaction between quantization noise and adapter updates (Frantar et al. , 2023 ; Zhao et al. , 2025 ) . Reliable evaluation therefore requires at least partial fine-tuning, which turns the search into an expensive black-box optimization problem. In many deployment settings, this search is run offline and its cost can be amortized over repeated deployments, making search time a secondary constraint relative to memory and post-fine-tuning performance. Nevertheless, repeated trial-and-error via fine-tuning remains prohibitively expensive. To this end, we adopt a multi-fidelity search strategy that quickly filters poor configurations using short fine-tuning runs, and allocates longer fine-tuning runs only to a small set of promising candidates. We propose AutoQRA , a coarse-to-fine framework for automated quantization and rank allocation. AutoQRA uses a two-phase design that balances global coverage with local refinement. In Phase I , AutoQRA performs a global multi-fidelity evolutionary search to approximate the Pareto frontier over accuracy and memory. The population is warm-started with layer-wise importance priors, and importance-guided mutations focus edits on influential layers. A learned surrogate model screens candidates and improves promotion decisions (Ru et al. , 2020 ) , while the returned frontier is formed from real measured evaluations rather than surrogate predictions. In Phase II , AutoQRA refines strong Phase I candidates with trust-region Bayesian optimization (Eriksson et al. , 2020 ) . We fit a Gaussian process surrogate on evaluations at the highest-fidelity setting and select configurations using Expected Improvement (EI) (Jones et al. , 1998 ) . Both phases terminate automatically when improvements saturate, using hypervolume progress for Phase I (Zitzler and Thiele, 1999 ) and acquisition saturation for Phase II. Our contributions are: (1) We formulate joint per layer bit width and LoRA rank allocation under a strict memory budget, and explain why decoupled pipelines are misaligned with post fine tuning performance. (2) We introduce AutoQRA, a two phase coarse to fine framework that combines multi fidelity evolutionary screening with trust region Bayesian refinement to search the discrete joint space efficiently. (3) Experiments show that AutoQRA achieves performance close to full precision fine tuning with a memory footprint comparable to uniform 4 bit methods. 2 Related Work Our work is situated at the intersection of parameter-efficient fine-tuning and automated model compression, building upon advances in quantization and black-box optimization. 2.1 Efficient LLM Fine-Tuning Quantization and Mixed-Precision. PTQ serves as a foundation for compressing LLMs, with methods like GPTQ (Frantar et al. , 2023 ) and AWQ (Lin et al. , 2023 ) utilizing second-order information or activation statistics to minimize reconstruction error. While highly effective for inference, standard PTQ often degrades performance when weights are frozen during subsequent fine-tuning. To address layer-wise sensitivity, mixed-precision techniques such as SliM-LLM (Huang et al. , 2025 ) allocate bit-widths based on Hessian spectra or salience metrics. However, these methods typically focus exclusively on weight precision, treating the adaptation capacity as a fixed constant or ignoring it entirely. PEFT. Exemplified by LoRA (Hu et al. , 2022 ) , freezes the backbone and injects trainable low-rank matrices. QLoRA (Dettmers et al. , 2023 ) further democratized access by quantizing the backbone to 4-bit, yet it retains a uniform rank assignment. Acknowledging that uniform capacity is inefficient, adaptive approaches like AdaLoRA (Zhang et al. , 2023 ) and RankAdaptor (Zhou et al. , 2025 ) dynamically prune or allocate ranks based on singular value importance. Crucially, these methods optimize the topology of adapters but assume a static, uniform precision for the underlying weights, failing to exploit the memory trade-offs available through variable quantization. Recent efforts have attempted to bridge these two paradigms. LoftQ (Li et al. , 2023 ) and LQ-LoRA (Guo et al. , 2023 ) propose alternating optimization schemes or initialization heuristics to align quantization with low-rank structures. While pioneering, they rely on localized proxies (e.g., reconstruction loss) rather than global task performance, and they typically resort to iterative heuristics rather than a principled global search over the joint discrete design space. (a) Sensitivity Analysis (b) Proxy Failure Figure 1 : Empirical Motivation for Joint Optimization. (a) Impact of Joint Allocation: We visualize the accuracy distribution of feasible mixed-precision configurations across tasks. The substantial performance spread demonstrates that distinct pairings of bit-width ( q q ) and rank ( r r ) yield vastly different outcomes even under the same memory budget. (b) Proxy-Objective Mismatch: Standard calibration metrics (Perplexity, x-axis) fail to predict post-finetuning accuracy (y-axis). The weak correlation ( ρ = 0.46 \rho{=}0.46 ) and frequent rank reversals indicate that static proxies cannot reliably identify configurations where learnable adapters compensate for quantization noise. 2.2 Automated Search for Model Compression Our approach also draws inspiration from Neural Architecture Search and Automated Machine Learning, which frame compression as a discrete optimization problem. Search Strategies for Compression. Early works in NAS utilized Reinforcement Learning (RL) or Evolutionary Algorithms (EAs) to discover efficient architectures (Zoph and Le, 2016 ; Real et al. , 2019 ) . For model compression, similar search strategies have been applied to find per-layer quantization policies (Wang et al. , 2019 ) . However, these methods often incur prohibitive computational costs, making them impractical for LLM fine-tuning loops. Sample-Efficient Black-Box Optimization. To mitigate search costs, multi-fidelity optimization has emerged as a standard. Hyperband (Li et al. , 2018 ) and BOHB (Falkner et al. , 2018 ) leverage cheap, low-fidelity approximations (e.g., partial epochs) to efficiently allocate resources to promising candidates. Techniques for optimizing over mixed categorical and continuous spaces, such as CoCaBO (Ru et al. , 2020 ) , demonstrate that leveraging correlations between variables can significantly accelerate convergence. Figure 2 : Overview of the AutoQRA framework. Phase I (left) approximates the global Pareto frontier via a multi-fidelity evolutionary search, utilizing importance-guided mutations and surrogate screening to navigate the discrete space. Phase II (right) performs a local Bayesian refinement to identify a precise operating point that maximizes user utility under the budget constraint. 3 Methodology We introduce AutoQRA ( Auto mated Q uantization– R ank A llocation), a framework designed to resolve the evaluation dilemma in joint model compression and adaptation. AutoQRA formulates the allocation of bit-width and rank as a constrained black-box optimization problem, replacing unreliable static proxies with dynamic assessment. To navigate the prohibitive search space efficiently, we adopt a coarse-to-fine strategy: Phase I approximates the global Pareto frontier via a multi-fidelity evolutionary search, while Phase II performs local Bayesian refinement to pinpoint configurations where adapter capacity maximally compensates for quantization noise (Figure 2 ). 3.1 Motivation and Problem Formulation Motivation We identify a critical dependency between quantization precision and adaptation rank. Figure 1(a) illustrates the performance distributions of diverse mixed bit-rank configurations across five downstream tasks. Crucially, distinct combinations of bit-width ( q q ) and rank ( r r ) yield vastly different outcomes even under identical memory constraints. We observe substantial performance fluctuations, with accuracy gaps exceeding 25% on tasks such as Winogrande and ARC-Challenge. The consistent relative performance of specific configurations, indicated by color markers, demonstrates that downstream utility is strictly coupled with the joint allocation of q q and r r . Suboptimal combinations lead to severe degradation, whereas optimal pairings approach full-precision performance. This volatility underscores that bit-width and rank cannot be optimized in isolation. Static metrics employed by decoupled methods fail to capture these non-linear interactions. We also provide a detailed discussion on the orthogonality between backbone and adapter contributions in Appendix A . A common strategy is to guide bit allocation using PTQ-style calibration metrics. However, once adapters are introduced, static proxies become unreliable for selecting learnable joint allocations. We randomly sample n = 30 n{=}30 feasible bit–rank configurations and compare a calibration proxy (neg. log perplexity computed wi