Title: pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation

Abstract: Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. However, this approach typically overlooks the potential synergies that could arise from integrating diverse domain knowledge within the same tuning process. In this work, we propose a novel Mixture-of-Experts prompt tuning method called pMoE, which leverages the strengths of multiple expert domains through expert-specialized prompt tokens and the learnable dispatcher, effectively combining their expertise in a unified model framework. Our pMoE introduces expert-specific prompt tokens and utilizes a dynamic token dispatching mechanism at various prompt layers to optimize the contribution of each domain expert during the adaptation phase. By incorporating both domain knowledge from diverse experts, the proposed pMoE significantly enhances the model's versatility and applicability to a broad spectrum of tasks. We conduct extensive experiments across 47 adaptation tasks, including both classification and segmentation in general and medical domains. The results demonstrate that our pMoE not only achieves superior performance with a large margin of improvements but also offers an optimal trade-off between computational efficiency and adaptation effectiveness compared to existing methods.

Body: pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation 1 Introduction 2 Related Work 3 Method 3.1 Visual Prompt Tuning for a Single Model 3.2 Expert Prompt Tokens 3.3 Dynamically Select and Fuse Tokens with the Dispatcher 4 Experiments 4.1 Experimental setup 4.2 Comparison to prior work 4.3 Experimental Analysis 5 Conclusion A Experimental Details B Algorithm for pMoE C Experimental Analysis pMoE : Prompting Diverse Experts Together Wins More in Visual Adaptation Shentong Mo 1 , Xufang Luo 2 , Dongsheng Li 2 1 Carnegie Mellon University 2 Microsoft Research Abstract Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. However, this approach typically overlooks the potential synergies that could arise from integrating diverse domain knowledge within the same tuning process. In this work, we propose a novel Mixture-of-Experts prompt tuning method called pMoE , which leverages the strengths of multiple expert domains through expert-specialized prompt tokens and the learnable dispatcher, effectively combining their expertise in a unified model framework. Our pMoE introduces expert-specific prompt tokens and utilizes a dynamic token dispatching mechanism at various prompt layers to optimize the contribution of each domain expert during the adaptation phase. By incorporating both domain knowledge from diverse experts, the proposed pMoE significantly enhances the model‚Äôs versatility and applicability to a broad spectrum of tasks. We conduct extensive experiments across 47 adaptation tasks, including both classification and segmentation in general and medical domains. The results demonstrate that our pMoE not only achieves superior performance with a large margin of improvements but also offers an optimal trade-off between computational efficiency and adaptation effectiveness compared to existing methods. 0 0 footnotetext: This work was done during Shentong Mo‚Äôs internship at MSRA. Correspondence to: Xufang Luo ( xufluo@microsoft.com ). 1 Introduction The rapid advancement of unsupervised representation learning (He et al. , 2020 ; Chen et al. , 2021 ; Xie et al. , 2021 ; Caron et al. , 2021 ; Oquab et al. , 2023 ) , particularly in visual tasks, has led to an increasing demand for adaptable models that can efficiently transfer knowledge across domains. In recent years, parameter-efficient fine-tuning methods (Jia et al. , 2022 ; Yoo et al. , 2023 ; Mo et al. , 2024b ) have emerged as a powerful tool, achieving strong performance while reducing the computational burden associated with traditional fine-tuning approaches. Among these, prompt tuning, where learnable prompt tokens are added to the input sequences, has gained significant attention for its ability to adjust models pre-trained on large datasets with minimal additional parameters. However, most existing prompt tuning approaches (Jia et al. , 2022 ; Yoo et al. , 2023 ; Mo et al. , 2024b ) focus on adapting a single pre-trained model (Chen et al. , 2021 ; He et al. , 2021 ) , either trained on general visual tasks or specialized datasets, like medical images. While this strategy has yielded encouraging results, it inherently limits the model‚Äôs capacity to benefit from cross-domain knowledge. For example, models pre-trained on general visual datasets might struggle with highly specialized medical tasks. Moreover, the capability provided by a single model is often insufficient to address a real-world downstream task to be adapted to. Solving a complex problem may require a high-level semantic understanding ability from models pre-trained with language supervision, as well as a low-level feature capturing ability from segmentation models. The challenge, therefore, is how to integrate expertise from multiple domains in a way that maximizes both performance and efficiency. A key challenge in this context is the effective coordination of knowledge from multiple, often distinct, domain experts. Traditional approaches to fine-tuning are not designed to handle the potential conflicts or redundancies that arise when incorporating diverse sources of expertise. Furthermore, determining the optimal contribution of each expert dynamically, without inflating computational costs, remains an unsolved problem. This challenge is further compounded by the varying data characteristics and task requirements across general and specialized domains, such as the medical field, making it difficult to strike the right balance between generalization and specialization during adaptation. To address this challenge, we propose a novel framework for prompt tuning, called pMoE for Mixture-of-Experts Prompt Tuning, which explicitly addresses this challenge by leveraging knowledge from multiple expert domains. Our pMoE first introduces expert-specific prompt tokens to each pre-trained model. Then, to facilitate information exchange and determine the contribution of each expert during adaptation, pMoE utilizes a learnable dispatcher module that can dynamically select and fuse tokens from diverse experts. Unlike previous methods that are confined to a single knowledge source, the proposed pMoE effectively integrates various domain expertise, optimizing the adaptation process across diverse tasks. Besides, the dispatcher module is compatible with all existing prompt tuning methods for a single model, and can also be extended to incorporate sophisticated architectures, making pMoE flexible and powerful. We validate the effectiveness of pMoE through extensive experiments on 47 visual adaptation benchmarks, including both classification and segmentation tasks from general and medical domains. Our results demonstrate that pMoE not only achieves state-of-the-art performance, outperforming the previous method by 2.36% in terms of accuracy on ImageNet-21K classification, but also strikes a balance between computational efficiency and adaptation efficacy. We show that pMoE can largely improve multiple existing prompt tuning methods for a single model across all general and medical tasks. By utilizing the strengths of multiple domain experts, our approach sets a new standard for flexible and efficient visual adaptation. Overall, our contributions can be summarized into three main folds: ‚Ä¢ We propose pMoE , a novel Mixture-of-Experts Prompt Tuning framework that extends visual prompt tuning for prompting diverse experts together, allowing for effective and adaptable fine-tuning across both general and medical visual domains. ‚Ä¢ We design a learnable dispatcher module that can flexibly select and fuse expert-specific prompt tokens, enabling dynamically allocating tokens based on the complexity and nature of the visual task. ‚Ä¢ We conduct extensive experiments across diverse datasets, including medical image analysis and general segmentation tasks, demonstrating that pMoE significantly outperforms existing prompt-based adaptation methods. 2 Related Work Visual Adaptation. Visual adaptation seeks to transfer knowledge from pre-trained vision models to new tasks. Early approaches, such as full fine-tuning (Dosovitskiy et al. , 2021 ) , involved updating both the pre-trained backbone and task-specific heads. Recent research has focused on more efficient alternatives, particularly parameter-efficient tuning techniques. For instance, SideTune (Zhang et al. , 2020a ) introduced a side network that linearly interpolates between pre-trained features and side-tuned features. Bias tuning methods, such as TinyTL (Cai et al. , 2020 ) and BitFit (Ben Zaken et al. , 2022 ) , focused on tuning only the bias terms of the backbone to reduce the number of trainable parameters. Adapter-based methods (Houlsby et al. , 2019 ; Pfeiffer et al. , 2020 ) injected lightweight layers into the transformer architecture, introducing task-specific parameters without retraining the full network. These approaches, however, primarily target models pre-trained in supervised settings, with fewer studies exploring parameter-efficient tuning in the context of self-supervised learning, a gap that we address with our proposed method. Our pMoE takes this further by introducing prompt tuning for Mixture-of-Experts to enhance adaptability and scalability across diverse visual domains. Visual Prompt Tuning. Visual Prompt Tuning (VPT) (Jia et al. , 2022 ) has recently gained traction as a parameter-efficient method for visual adaptation. VPT introduces learnable prompt tokens, appended to the input sequence, that modulate information flow through a pre-trained vision transformer. This approach has demonstrated strong performance on a variety of visual tasks, especially when used with supervised ViT backbones. Building on this, GaPT (Yoo et al. , 2023 ) proposed adding gated prompts that control each transformer block‚Äôs influence over the prompt tokens, further improving adaptability. LSPT (Mo et al. , 2024b ) extended this concept by incorporating temporal prompts that retain long-term task-specific information, mitigating catastrophic forgetting. Our method expands upon these advancements by introducing Mixture-of-Experts Prompt Tuning. Unlike previous methods, our pMoE dynamically selects different expert prompts based on the complexity of the task, allowing for fine-grained control over model adaptation. This leads to superior performance across both general and medical domain tasks, as evidenced by our experimental results. Mixture-of-Experts. Mixture-of-Experts (MoE) models, initially proposed to enhance model capacity without increasing computational cost, have shown promise in diverse areas such as natural language processing (Shazeer et al. , 2017 ) . MoE divides tasks among several ‚Äùexperts,‚Äù each specialized in certain aspects of the input, allowing for greater task-specific specialization while maintaining parameter efficiency. Recent works in vision have begun to explore MoE for efficient transfer learning and visual adaptation (Riquelme et al. , 2021 ) . However, these approaches largely focus on architectural improvements and do not directly tackle prompt tuning in visual tasks. To our knowledge, pMoE is the first framework to apply MoE mechanisms to prompt tuning in vision tasks. By enabling task-dependent expert selection, pMoE achieves superior adaptability and performance, particularly in challenging tasks such as fine-grained classification and medical image analysis. 3 Method Given a set of images, our target is to efficiently adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable prompts. We propose a novel mixture-of-experts prompt tuning framework, named pMoE , for capturing multiple expert blocks as prompt sources within pre-trained ViTs, as illustrated in Figure 1 . In this section, we first describe the problem setup and notations, and also revisit the visual prompt tuning technique for a single model in Section 3.1 . Then, we introduce our main method, consisting of added expert prompt tokens in Section 3.2 and the dispatcher module to enable effectively utilizing knowledge from diverse experts in Section 3.3 . 3.1 Visual Prompt Tuning for a Single Model Problem Setup. We consider a ViT model consisting of a patch embedding layer, a stack of L L transformer layers, and a classification head. For an input image ùêó \mathbf{X} with shape of H √ó W √ó 3 H\times W\times 3 , we denote the input patch tokens for the l + 1 l+1 -th layer as ùêô l = [ ùê≥ C l , ùê≥ 1 l , ‚Ä¶ , ùê≥ N l ] ‚àà ‚Ñù ( N + 1 ) √ó D \mathbf{Z}^{l}=[\mathbf{z}^{l}_{C},\mathbf{z}^{l}_{1},...,\mathbf{z}^{l}_{N}]\in\mathbb{R}^{(N+1)\times D} , where N = H ‚Äã W / P 2 N=HW/P^{2} , P P is the patch size, and D D is the dimension of the token, and ùê≥ C l ‚àà ‚Ñù 1 √ó D \mathbf{z}_{C}^{l}\in\mathbb{R}^{1\times D} is an additional learnable classification token concatenated with patch tokens. The transformer layer processes classification and patch tokens as ùêô l + 1 = TransLayer l + 1 ‚Äã ( ùêô l ) \mathbf{Z}^{l+1}=\text{TransLayer}^{l+1}(\mathbf{Z}^{l}) . The token ùê≥ i 0 = embed ‚Äã ( ùê± i ) \mathbf{z}^{0}_{i}=\text{embed}(\mathbf{x}_{i}) , for i ‚àà { 1 , 2 , ‚Ä¶ , N } i\in\{1,2,...,N\} , is obtained by embedding the i i -th patch ùê± i \mathbf{x}_{i} of the input image ùêó \mathbf{X} . Figure 1: Illustration of the proposed pMoE framework. Here, we demonstrate how the dispatcher handles tokens and produces integrated prompt tokens for Expert k k , with the same method applied to other experts as well. The dynamic dispatching method takes expert prompt tokens from all experts and the state of the current expert as inputs, and outputs dispatching weights for controlling portions to integrate prompt tokens for the next layer. Different colors represent distinct weight groups, applied to corresponding expert prompt tokens, yielding different integrated prompt tokens. This dynamic dispatching mechanism ensures communication and interaction among diverse experts, making the model contribute the most relevant knowledge to the final output. Revisit Visual Prompt Tuning. Visual Prompt Tuning (VPT) (Jia et al. , 2022 ) was introduced to adapt pre-trained ViTs for downstream tasks by fine-tuning continuous prompt tokens in the representation space. VPT prepends learnable prompt tokens ùêè = [ ùê© 1 , ‚Ä¶ , ùê© N p ] ‚àà ‚Ñù N p √ó D \mathbf{P}=[\mathbf{p}_{1},...,\mathbf{p}_{N_{p}}]\in\mathbb{R}^{N_{p}\times D} to the input patch tokens, where N p N_{p} is the number of prompt tokens, and D D is the token dimension. VPT fine-tunes these prompt tokens while freezing the ViT‚Äôs pre-trained weights and the classification head. VPT-deep extends this approach by injecting layer-specific prompt tokens ùêè l = [ ùê© 1 l , ‚Ä¶ , ùê© N p l ] ‚àà ‚Ñù N p √ó D \mathbf{P}^{l}=[\mathbf{p}_{1}^{l},...,\mathbf{p}_{N_{p}}^{l}]\in\mathbb{R}^{N_{p}\times D} into each layer. The transformer layer processes tokens as: [ ùêô P l + 1 , ùêô l + 1 ] = TransLayer l + 1 ‚Äã ( [ ùêè l , ùêô l ] ) [\mathbf{Z}_{P}^{l+1},\mathbf{Z}^{l+1}]=\text{TransLayer}^{l+1}([\mathbf{P}^{l},\mathbf{Z}^{l}]) (1) Here, ùêô P l + 1 \mathbf{Z}_{P}^{l+1} is discarded after each block, leading to incomplete usage of accumulated prompts across layers. Our method aims to address these limitations by using these tokens for enabling prompt exchange between domain-specific experts dynamically. Some recent research improves the VPT by introducing more operations on prompt tokens (Yoo et al. , 2023 ; Mo et al. , 2024b ) . Note that our framework is compatible with these modifications, as long as prompt tokens exist. 3.2 Expert Prompt Tokens The core of our pMoE lies in its ability to leverage and effectively integrate multiple domain-specific experts 1 1 1 In this paper, an expert refers to a pre-trained model. through a set of learnable prompt tokens. Thus, we first introduce Expert Prompt Tokens (EPTs), speciali