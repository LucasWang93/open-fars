Title: Hardware-Aware Design of a GNN-Based Hit Filtering Algorithm for the Belle II Level-1 Trigger

Abstract: The Belle~II experiment operates at high luminosity, where an increasing beam-induced background imposes stringent demands on the hardware Level-1 trigger system, which must operate under tight latency and bandwidth constraints. To achieve online data reduction within the Level-1 trigger system, we have developed a hit-filtering algorithm based on the lightweight Interaction Network architecture. In this work, we present a hardware-aware model-compression workflow for this hit-filtering algorithm targeting deployment on FPGA devices within the Belle~II trigger system. The network is adapted to the detector and trigger conditions through model-size and graph-size reduction, low-precision (4 bit) fixed-point arithmetic, and unstructured pruning. We assess the resulting design using the total number of bit operations as a hardware-aware computational complexity metric. Using this metric, we identify a configuration that decreases this cost by more than two orders of magnitude relative to the full-precision reference implementation. This reduction is achieved while preserving performance close to the reference model in terms of hit efficiency and background rejection, as indicated by only a modest decrease in the AUC score from 97.4 to 96.8, evaluated on Belle~II collision data.

Body: Hardware-Aware Design of a GNN-Based Hit Filtering Algorithm for the Belle II Level-1 Trigger 1 Introduction 2 Compression pipeline: from full-precision to online 2.1 Full-precision baseline model Graph inputs: Training strategy: 2.2 Compressed quantized model Model and graph size reduction: 4bit quantization: Pruning: 3 Performance evaluation 3.1 Hit filtering performance 3.2 Number of bit operations 4 Conclusion Hardware-Aware Design of a GNN-Based Hit Filtering Algorithm for the Belle II Level-1 Trigger G. Heine, 1 1 footnotetext: Corresponding author. F. Mayer M. Neu J. Becker T. Ferber Abstract The Belle II experiment operates at high luminosity, where an increasing beam-induced background imposes stringent demands on the hardware Level-1 trigger system, which must operate under tight latency and bandwidth constraints. To achieve online data reduction within the Level-1 trigger system, we have developed a hit-filtering algorithm based on the lightweight Interaction Network architecture. In this work, we present a hardware-aware model-compression workflow for this hit-filtering algorithm targeting deployment on FPGA devices within the Belle II trigger system. The network is adapted to the detector and trigger conditions through model-size and graph-size reduction, low-precision (4 bit) fixed-point arithmetic, and unstructured pruning. We assess the resulting design using the total number of bit operations as a hardware-aware computational complexity metric. Using this metric, we identify a configuration that decreases this cost by more than two orders of magnitude relative to the full-precision reference implementation. This reduction is achieved while preserving performance close to the reference model in terms of hit efficiency and background rejection, as indicated by only a modest decrease in the AUC score from 97.4 97.4 to 96.8 96.8 , evaluated on Belle II collision data. 1 Introduction The Belle II experiment [ 1 ] at the SuperKEKB e + ​ e − e^{+}e^{-} collider is designed to search for physics beyond the Standard Model via precision measurements of rare decays. To handle substantial beam-induced background at high instantaneous luminosity, the Data Acquisition (DAQ) system employs a hardware Level-1 (L1) trigger that selects physics-relevant events in real-time, significantly reducing the data throughput for detector readout. The L1 trigger must maintain a high efficiency for physics signals while operating within a strict 5 µ ​ s 5\text{\,}\mathrm{\SIUnitSymbolMicro s} latency constraint and staying within the DAQ limits [ 13 , 17 ] . One important sub-trigger of the L1 trigger system is the Central Drift Chamber (CDC) trigger [ 16 ] , providing track information for the final trigger decision logic. The increasing background directly impacts track reconstruction efficiency, purity, and precision [ 2 , 13 , 14 ] , necessitating fast and effective hit filtering prior to track finding. Graph Neural Networks (GNNs) have been studied for particle tracking and related reconstruction tasks in high-energy physics, as they can represent irregular detector geometry and local hit correlations in a graph structure [ 6 , 11 ] . Deploying such GNNs in first-level trigger systems introduces additional constraints: the end-to-end inference latency, including graph construction, must be kept in the sub-microsecond range; the model must fit into limited Field-Programmable Gate Array (FPGA) resources; and physics performance in terms of track or cluster reconstruction accuracy needs to be preserved despite model compression and quantization [ 7 , 15 ] . For the Belle II L1 trigger system, the target hit-filtering application, planned to be distributed across 20 FPGA boards, is required to process up to 978 978 sense wires per CDC sector within a sub-microsecond latency budget, providing hit-filter outputs for every 32 MHz 32\text{\,}\mathrm{MHz} CDC trigger clock cycle. This work describes a software-hardware co-design procedure for a GNN -based hit-filtering algorithm under these constraints. In contrast to previous work that primarily documented the final hardware implementation [ 10 ] , the present work focuses on the iterative design steps in which hardware limitations, fixed-point precision, and resource usage guide the development of the model architecture. The remainder of this paper is organized as follows. Section ˜ 2 describes the compression pipeline, starting from the full-precision GNN hit-filtering model and its graph inputs, and then outlining the model-size reduction, quantization-aware training, and pruning steps used to obtain a deployable design. Section ˜ 3 reports the performance evaluation, considering both hit-filtering efficiency and background rejection, and the corresponding number of bit operations as a hardware-aware cost metric summarized in section ˜ 4 . 2 Compression pipeline: from full-precision to online In this section, we describe our workflow for deploying our GNN hit-filtering model on FPGA , as illustrated in Figure ˜ 1 . Starting from a full-precision PyTorch Geometric [ 8 ] model described in subsection 2.1 , size reduction, quantization-aware training and pruning are applied to obtain a compressed, low-precision network described in subsection 2.2 . From this compressed model, a hardware description for the dataflow GNN accelerator is generated and subsequently synthesized into an Register-Transfer Level (RTL) netlist, which serves as the basis for the final FPGA implementation detailed in [ 10 ] . Figure 1 : Our workflow for deploying a PyTorch Geometric model on FPGA : starting from model configuration and layer replacement with Brevitas quantization layers, followed by model compression including quantization-aware training, hardware generation for the dataflow GNN accelerator, and subsequent RTL netlist creation for FPGA implementation. 2.1 Full-precision baseline model Our baseline model employs a modified Interaction Network [ 4 ] as a lightweight GNN architecture, particularly suited to relationship modelling and pattern recognition. The network consists of three sequential Multi-Layer Perceptron (MLP) blocks: an edge block R 1 R_{1} (edge feature update), a node block O O (node feature update), and a final edge block R 2 R_{2} (final edge classification). For each edge the first edge block R 1 R_{1} processes the edge features and associated incoming and outgoing node features ( x i ​ n , x o ​ u ​ t , e ) (x_{in},x_{out},e) , producing updated edge features e ~ \tilde{e} . Updated edge features e ~ \tilde{e} connected to each node are then aggregated via a max-scatter operation to e ¯ \bar{e} , retaining the largest edge response per node. The subsequent node block O O takes e ¯ \bar{e} together with the node features x x and outputs updated node features x ~ \tilde{x} , which are passed, together with e ~ \tilde{e} , to the final edge block R 2 R_{2} as ( x ~ i ​ n , x ~ o ​ u ​ t , e ~ ) (\tilde{x}_{in},\tilde{x}_{out},\tilde{e}) . The one-dimensional edge score outputs e ~ ~ \tilde{\tilde{e}} of R 2 R_{2} are aggregated per node by a mean-scatter operation, yielding node-level scores. A final sigmoid activation converts these node scores to node-wise probability outputs x ~ ~ \tilde{\tilde{x}} between 0 and 1. In the baseline model configuration, each MLP block comprises three ReLu-activated hidden layers with dimensions [8,8,3], [8,8,3], and [8,8,1] respectively, resulting in 495 trainable parameters in total. Graph inputs: A graph representation is constructed from the detector hits by connecting each sense wire to pattern-based neighbour wires via bidirectional edges [ 15 ] . Node features x i ​ n / o ​ u ​ t x_{in/out} encode the wire positions ( x x and y y at the wire centre [ 1 ] ) as well as the Analog-to-Digital Converter (ADC) count sum per wire, while edge features e e capture spatial ( Δ ​ r \Delta r radial and Δ ​ ϕ \Delta\phi azimuthal) and temporal ( Δ \Delta TDC) differences between connected wires. Training strategy: For training, we designed a dataset covering a wide range of track signatures and background conditions, using both simulated events and Belle II 2024 collision data. Details on the dataset, training strategy, and graph construction are provided in [ 10 ] . 2.2 Compressed quantized model For deployment of the GNN model on the target AMD Ultrascale XCVU190 device, network compression and careful adaptation to hardware constraints are required. The final deployed configuration was found by applying the following compression and optimization steps while monitoring both model prediction accuracy and FPGA system resource utilization. Model and graph size reduction: First, the number of trainable parameters is reduced with respect to the full-precision model by decreasing the number of hidden layers per MLP block from two to one and shrinking the remaining hidden layer size from eight to six neurons each, resulting in a reduction from 495 to 211 trainable parameters. In addition, the graph size is reduced by switching from bidirectional to unidirectional edges, effectively halving the number of edges and thus the computational load. 4bit quantization: To further compress the network and prepare it for fixed-point implementation, quantization-aware training is performed using Brevitas [ 9 ] . We replace the floating-point PyTorch network layers with the differentiable Brevitas quantized equivalents, using custom quantizers applying fixed-point quantization with floor rounding mode to the weights, biases, and activation functions of the model. We add MLP wrapping with quantized identity layers to enforce quantization boundaries for in- and outputs to avoid accumulation overflows. We use a mixed precision scheme to balance resource constraints with numerical stability due to accumulation effects: 4 bit 4\text{\,}\mathrm{bit} inputs and 4 bit 4\text{\,}\mathrm{bit} weights, 6 bit 6\text{\,}\mathrm{bit} activations, 16 bit 16\text{\,}\mathrm{bit} bias, 8 bit 8\text{\,}\mathrm{bit} outputs. Pruning: In addition, we apply an iterative magnitude-based unstructured pruning, linearly increasing the weight sparsity every five training epochs up to a final sparsity of 65 % 65\text{\,}\mathrm{\char 37\relax} . For hardware implementation, the exponential sigmoid output activation is removed post-training, since, during inference, classification decisions rely solely on thresholding the resulting score. 3 Performance evaluation For the main compression steps between the offline full-precision reference and the final online model, as discussed in section ˜ 2 , the Receiver Operating Characteristic (ROC) curves and the number of Bit Operations (BOPs) are shown in Figure ˜ 2 . The following configurations are considered, where each step includes all preceding modifications: 1) Full-precision baseline model; 2) model size reduction (of hidden depth and width) with 211 instead of 495 trainable parameters; 3) graph size reduction by using unidirectional instead of bidirectional edges; 4) 4bit quantization with 4bit weights and inputs including removal of the output sigmoid activation, which does not affect classification performance or BOPs ; and 5) pruning with 65 % 65\text{\,}\mathrm{\char 37\relax} final unstructured weight sparsity. (a) ROC curves (b) Number of bit operations Figure 2 : ( 2(a) ) Hit-level classification Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) values for different network compression steps and ( 2(b) ) corresponding BOPs as a proxy for computational complexity towards FPGA implementation. 3.1 Hit filtering performance The hit filtering performance shown in 2(a) is evaluated in the form of ROC curves and associated Area Under the Curve (AUC) scores through an offline re-analysis of Belle II data. The test sample consists of hits from 1000 1000 High-Level Trigger (HLT) -selected μ ​ μ ​ ( γ ) \mu\mu(\gamma) events 2 2 2 HLT μ ​ μ ​ ( γ ) \mu\mu(\gamma) events are selected by requiring two oppositely charged, back-to-back tracks, each carrying momentum greater than 0.5 GeV / c 0.5\text{\,}\mathrm{G}\mathrm{eV}\mathrm{/}\mathrm{c} in the center-of-mass reference frame and matched to an Electromagnetic Calorimeter (ECL) cluster with energy below 0.5 GeV 0.5\text{\,}\mathrm{GeV} . The total energy of clusters, including possible photons, must be below 2 GeV 2\text{\,}\mathrm{GeV} [ 12 ] . recorded at the end of 2024, corresponding to approximately 2.5 million 2.5\text{\,}\mathrm{m}\mathrm{i}\mathrm{l}\mathrm{l}\mathrm{i}\mathrm{o}\mathrm{n} CDC hits. Due to stochastic variations between training runs (e.g., from random initialization), for each configuration, the reported values are the mean and standard deviation over five independent training runs, where in each run the best-performing model out of three runs is selected. Pre-selection cuts ( ADC ≥ 10 \geq 10 and Time-to-Digital Converter (TDC) within 500 ns 500\text{\,}\mathrm{ns} trigger time window) consistent with the default Belle II L1 trigger setup are applied before network training and inference, limiting the maximum achievable hit efficiency (defined as the fraction of signal hits retained over all available signal hits) to 0.980 0.980 . Signal hits are defined as hits on sense wires connected to the L1 trigger that are associated with tracks found by the Belle II offline reconstruction [ 12 , 5 ] passing a quality selection. 3 3 3 Track selection requires transverse momentum p T 0.2 GeV / c p_{\mathrm{T}} $0.2\text{\,}\mathrm{G}\mathrm{eV}\mathrm{/}\mathrm{c}$ , total momentum p 0.7 GeV / c p $0.7\text{\,}\mathrm{G}\mathrm{eV}\mathrm{/}\mathrm{c}$ , longitudinal distance | z 0 | 15 cm |z_{0}| $15\text{\,}\mathrm{cm}$ and radial | d 0 | 15 cm |d_{0}| $15\text{\,}\mathrm{cm}$ distance from the interaction point, and at least 7 CDC hits. Background hits are defined as hits not associated with found tracks. The background rejection rate is given by the fraction of such hits removed by the classifier. Hits matched to tracks failing quality criteria constitute 5 % $5\text{\,}\mathrm{\char 37\relax}$ of signal and 0.1 % $0.1\text{\,}\mathrm{\char 37\relax}$ of all hits and are excluded from performance metrics. The full-precision model attains an AUC of 0.974 ​ ( 1 ) 0.974(1) and a background rejection of 94.2 ​ ( 4 ) % 94.2(4)\text{\,}\mathrm{\char 37\relax} at a hit efficiency of 95 % 95\text{\,}\mathrm{\char 37\relax} , whereas the final compressed and pruned model achieves an AUC of 0.968 ​ ( 2 ) 0.968(2) with a background rejection of 90.9 ​ ( 15 ) % 90.9(15)\text{\,}\mathrm{\char 37\relax} . Thus, the cumulative degradation in hit-filtering performance introduced by the compression pipeline remains modest. 3.2 Number of bit operations To characterize and compare the computational complexity of the different configurations, we estimate the number of BOPs per network layer, following [ 3 ] . For an MLP layer with N w N_{w} weights, N b N_{b} biases, 