Title: ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition

Abstract: LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.

Body: ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition 1 Introduction 2 Related Work 3 Preliminaries 3.1 Low Rank Adaptation 3.2 Matrix Interpolative Decomposition 4 Method 4.1 Parameter Matrix Row Clustering 4.2 Rank Boosting 4.3 Theoretical Analysis of Clustered Decomposition 5 Experiments 5.1 Experimental Setup 5.2 Experiments on Single-Task 5.3 Experiments on Multi-Task 5.4 Efficiency Analysis 5.5 Ablation Analysis 5.5.1 Parameter-Parity Performance Analysis 5.5.2 Experiments on PMRC and RB 6 Conclusion and Future Work A Theoretical Framework of ID-LoRA A.1 Problem Setup and Assumptions A.2 Core Definitions A.3 Main Theorems B Experimental Details and Additional Results B.1 Hyper Parameter Settings B.2 Experimental Details of Parameter-Parity Performance B.3 Ablation: Impact of Pretrained Parameter Cluster Number k ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition Xidian Ma, Rundong Kong, Peng Zhang, Ruoxiang Huang, Yongyu Jiang Tianjin University, Tianjin, China {xindianma,krd,pzhang,huangruoxiang_0927}@tju.edu.cn Corresponding author. Abstract LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model’s capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA. ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition Xidian Ma, Rundong Kong, Peng Zhang † † thanks: Corresponding author. , Ruoxiang Huang, Yongyu Jiang Tianjin University, Tianjin, China {xindianma,krd,pzhang,huangruoxiang_0927}@tju.edu.cn 1 Introduction Large Language Models (LLMs) (Brown et al. , 2020 ; Dubey et al. , 2024 ; Yang et al. , 2024 ) exhibit strong cross-task transfer capabilities, driving widespread adoption of a unified multi-task adaptation framework (Fang et al. , 2024 ; Ding et al. , 2023 ) . In these frameworks, a single foundational LLM is fine-tuned to meet diverse downstream requirements. However, full-parameter fine-tuning is costly. Low Rank Adaptation (LoRA) (Hu et al. , 2022 ) has become the standard choice because it is efficient. Yet it faces a critical trade-off: higher ranks linearly increase the number of trainable parameters and memory use, while a fixed low rank fails to adapt to multi-task scenarios. To address this limitation, we propose ID-LoRA, a novel low-rank PEFT framework that fundamentally rethinks parameter efficiency. Inspired by Matrix Interpolative Decomposition (MID) (Ho and Greengard, 2012 ) , ID-LoRA first clusters rows of the frozen pretrained weight matrix W ∈ ℝ d × d W\in\mathbb{R}^{d\times d} (See Figure 1 (b)) into task-specific bases { A i } i = 1 k \{A_{i}\}_{i=1}^{k} using k-means constrained with minimum cluster size (Levy-Kramer, 2018 ) with the Euclidean distance. These low-rank matrices { A i } i = 1 k \{A_{i}\}_{i=1}^{k} are then frozen, and a shared trainable matrix B B is learned to compose the PEFT update jointly. During this process, we apply the Rank Boosting (RB) method to the intermediate inputs, further shrinking the size of the trainable matrix B B from ℝ d × r \mathbb{R}^{d\times r} to ℝ d s × r s \mathbb{R}^{\frac{d}{s}\times\frac{r}{s}} . The key innovation of ID-LoRA is to reuse clustered parameters from the pretrained weights as frozen auxiliary computations, instead of introducing a trainable matrix A like LoRA (See Figure 1 (a)). ID-LoRA trains only a shared matrix B B to compose bases via MID, enabling higher effective rank ( r r = 128 128 in Figure 1 (c)) with lower trainable parameters. The superiority of ID-LoRA in multi-task scenarios stems from its theoretical guarantees. We prove in Theorem 2 that when m m tasks exhibit a k k -cluster structure (Assumption 1), ID-LoRA’s clustering-derived frozen basis matrices { A i } i = 1 k \{A_{i}\}_{i=1}^{k} achieve tighter error bounds than the global low-rank adaptation, i.e., LoRA. In experiments, we designed extensive single-task and multi-task scenarios. These experimental results collectively validate the effectiveness and efficiency of our method for multi-task adaptation. In single-task settings, ID-LoRA surpasses all baselines on mathematics, code generation, and safety tasks while updating only 0.56% of the parameters of LLaMA-3-8B. In multi-task experiments across MMLU benchmark, ID-LoRA reduces trainable parameters by 46% versus LoRA and yields a 6.0% absolute average gain over LoRA on LLaMA-3-8B. To broaden the evaluation diversity, we added three further benchmarks, namely Math, CommonsenseQA, and Code Generation, where ID-LoRA outperforms the baselines on two of them. The main contributions of our work are as follows: (1) We propose a PEFT method, ID-LoRA, which reuses frozen pretrained weights as low-rank bases and trains only a single shared matrix, eliminating the need for additional matrix A A parameters. (2) We prove that clustered decomposition yields tighter error bounds and improved pivot robustness, ensuring reliable performance in multi-task settings. (3) ID-LoRA achieves or surpasses LoRA-level accuracy while reducing trainable parameters by up to 46%. Figure 1: Architectural Comparison and Parameter Efficiency: LoRA versus ID-LoRA. (a) LoRA requires training two low-rank matrices: randomly initialized A ∈ ℝ r × d A\in\mathbb{R}^{r\times d} and zero-initialized B ∈ ℝ d × r B\in\mathbb{R}^{d\times r} . (b) ID-LoRA employs the parameter clustering and rank boosting to generate multiple low-rank components while sharing a single B, thereby reducing trainable parameters. (c) Trainable parameters: ID-LoRA achieves ∼ 5 × \sim 5\times compression versus LoRA at rank 32 32 (right) and maintains superior scalability across model sizes (left). 2 Related Work Parameter-Efficient Fine-Tuning (PEFT). PEFT aims to adapt LLMs to downstream tasks with minimal parameter changes. This line of research has led to a wide range of advancements (Houlsby et al. , 2019 ; Hu et al. , 2022 ; Huang et al. , 2023 ; Tian et al. , 2024 ) . The three typical PEFT methods include Adapter (Houlsby et al. , 2019 ) , Prefix-tuning (Li and Liang, 2021 ) and LoRA (Hu et al. , 2022 ) . Adapter achieves adaptation to new tasks by inserting small adapter layers between the pre-trained layers, without retraining the entire model. This method has been used in various domains (Pfeiffer et al. , 2021 ; Qiao et al. , 2023 ) . Prefix-tuning optimizes task-specific prefix vectors. LoRA popularized low-rank decomposition for weight updates, balancing trainable parameters and performance. However, LoRA faces a critical trade-off: higher ranks improve task adaptation but linearly increase parameters and memory, while lower ranks degrade performance (Zhang et al. , 2023 , 2025 ) . Several studies have proposed LoRA variants (Liu et al. , 2024 ; Zhang et al. , 2025 ; Kopiczko et al. , 2024 ) to enhance learning capacity and reduce the number of trainable parameters. Other studies (He et al. , 2022 ) have provided a unified perspective, viewing many PEFT methods as a form of adapter. However, these techniques show constraints in multi-task scenarios. Multi-LoRA Adapters. Researchers have explored the benefits of modeling multiple LoRA adapters for multi-task. LoraHub (Huang et al. , 2023 ) adopts a multi-LoRA approach by training several adapters and selecting domain-specific combinations at inference. MoELoRA (Liu et al. , 2023 ) methods leverages the Mixture-of-Experts (MoE) (Jacobs et al. , 1991 ) to implement multi-domain knowledge modeling. HydraLoRA (Tian et al. , 2024 ) uses an asymmetric structure to manage multiple LoRA adapters, and enhances parameter efficiency compared to existing symmetric approaches. These methods, however, necessitate training multiple adapters and consequently initializing additional trainable parameters. Different from previous methods, the core innovation of ID-LoRA lies in constructing a frozen matrix by clustering the pretrained parameters, rather than initializing multiple independent LoRA adapters. This strategy significantly reduces the number of trainable parameters while maintaining adaptability to multi-task scenarios. 3 Preliminaries 3.1 Low Rank Adaptation Low Rank Adaptation (LoRA) (Hu et al. , 2022 ) introduces the concept of “intrinsic rank” in the parameter update process. This rank represents the true degrees of freedom needed for task adaptation. It is far smaller than the total parameter count, yet sufficient for effective fine-tuning. The insight is that pretrained language models have a minimal intrinsic dimension. Fine-tuning within this small subspace matches the effect of adjusting the entire parameter space (Aghajanyan et al. , 2021 ) . For the weight matrix W W , its parameter update Δ ​ W \Delta W can be represented by low rank decomposition A A and B B . In the fine-tuning phase, the adjustment is confined to low-rank matrices A A and B B , while all other model parameters remain immutable. The update mechanism for these matrices is delineated in the subsequent equation: u t = W ​ h t + Δ ​ W ​ x = W ​ h t + α r ​ B ​ A ​ h t u_{t}=Wh_{t}+\Delta Wx=Wh_{t}+\frac{\alpha}{r}BAh_{t} (1) where α \alpha is the scaling factor, W ∈ ℝ d × d W\in\mathbb{R}^{d\times d} , A ∈ ℝ r × d A\in\mathbb{R}^{r\times d} and B ∈ ℝ d × r B\in\mathbb{R}^{d\times r} , and r d r d . To align with LoRA’s initialization and ensure that Δ ​ W \Delta W is zero at the start of training, matrix B B in ID-LoRA is initialized to all zeros. 3.2 Matrix Interpolative Decomposition Matrix Interpolative Decomposition (MID) (Horn and Johnson, 2012 ) is a structured low-rank matrix decomposition method: it identifies key rows (or columns) as a skeleton and approximates the original matrix via multiplication with a small low-rank matrix. In our work, we approximate Δ ​ W \Delta W by selectively reusing pretrained parameter matrix entries, reducing trainable parameters. For MID, we can write Δ ​ W = B ​ A \Delta W=BA , where A = W [ S , : ] A=W_{[S,:]} is formed by taking the rows of W W indexed by the set S S , and B B is the low-rank coefficient matrix to be learn. In this way, a small number of rows of W W serve as the s ​ k ​ e ​ l ​ e ​ t ​ o ​ n skeleton that approximates the matrix Δ ​ W \Delta W . Specifically, the optimal row subset S S and the matrix B B are obtained by solving the following problem: a ​ r ​ g ​ m ​ i ​ n ​ ‖ B ​ W [ S , : ] − Δ ​ W ‖ F arg~min||BW_{[S,:]}-\Delta W||_{F} (2) where | S | = r |S|=r , and ∥ ⋅ ∥ F \|\cdot\|_{F} is the matrix 2-norm. In MID, selecting the key rows from matrix W W , namely W [ S , : ] W_{[S,:]} , is a non-trivial task. To mitigate this, we adopt a clustering strategy that generates several distinct row subsets, each acting as low-rank skeleton for a weighted a approximation of Δ ​ W \Delta W . 4 Method 4.1 Parameter Matrix Row Clustering Given a parameter matrix W ∈ ℝ d × d W\in\mathbb{R}^{d\times d} , the rows of W W are treated as individual elements within a collection. These row vectors are partitioned into k k distinct clusters using the K-Means constrained with minimum cluster size (Levy-Kramer, 2018 ) algorithm with the Euclidean distance metric. Within each cluster C i ​ ( i = 1 , 2 , … , k ) C_{i}(i=1,2,...,k) , we select the r r row vectors exhibiting the smallest Euclidean distance to the cluster centroid μ i \mu_{i} . The selected rows from each cluster C i C_{i} are then aggregated to form a low-rank matrix A i ∈ ℝ r × d A_{i}\in\mathbb{R}^{r\times d} . This process yields a set of k k structured low-rank matrices A i , A 2 , … , A k {A_{i},A_{2},\ldots,A_{k}} that collectively capture the dominant row-space patterns of the original matrix W W , prioritizing proximity to cluster centers. In our experiments, k k is equal to 4 4 , and r r is set according to the specific experimental requirements. To refine the approximation, we introduce an initialized trainable shared matrix B ∈ ℝ d × r B\in\mathbb{R}^{d\times r} that reconstructs the updated parameter matrix Δ ​ W \Delta{W} jointly with each cluster-specific matrix A i A_{i} . Δ ​ W = ∑ i = 1 k α i ​ ( B ​ A i ) \Delta{W}=\sum_{i=1}^{k}\alpha_{i}(BA_{i}) (3) where α i \alpha_{i} is a scalar. As shown in Figure 2 , α i \alpha_{i} = T ⊙ A i ​ h t T\odot A_{i}h_{t} , where ⊙ \odot is the inner product. This blend retains key row patterns and efficiently tracks small updates to Δ ​ W \Delta W . Figure 2: A diagram of the ID-LoRA architecture. 4.2 Rank Boosting Relying on a single shared weight matrix B B significantly complicates achieving balanced and harmonious interactions among expert modules. To address this issue, we first partition the activation vector x ∈ ℝ r x\in\mathbb{R}^{r} through matrix multiplication with A i A_{i} , then integrate these partitioned activation vectors via this unified shared low-rank matrix B B , as shown in Figure 2 . The formula is as follows: x i = A i ​ h t , u t = W ​ h t + ∑ i = 1 k α i ​ f c ​ ( [ B ​ x i 1 , B ​ x i 2 ] ) x_{i}=A_{i}h_{t},~u_{t}=Wh_{t}+\sum_{i=1}^{k}\alpha_{i}f_{c}([Bx_{i}^{1},Bx_{i}^{2}]) (4) where B ∈ ℝ d 2 × r 2 B\in\mathbb{R}^{\frac{d}{2}\times\frac{r}{2}} , A i ∈ ℝ r × d A_{i}\in\mathbb{R}^{r\times d} , and f c f_{c} is the concate operator. x i 1 x_{i}^{1} and x i 2 x_{i}^{2} are equal divisions from vector x i x_{i} . α i \alpha_{i} is the output of router, i.e., α i = T ⊙ x i \alpha_{i}=T\odot x_{i} , where ⊙ \odot is the inner product. Rank Analysis. Compared with LoRA and its multi-task-oriented variant MoELoRA (Liu et al. , 2023 ) , our method simultaneously reduces the number of trainable parameters and preserves a higher rank. MoELoRA instantiates k k independent adapters A i ∈ ℝ r × d A_{i}\in\mathbb{R}^{r\times d} and B i ∈ ℝ d × r B_{i}\in\mathbb{R}^{d\times r} , giving k × r × ( d + d ) k\times r\times(d+d) trainable parameters and, by the sub-additivi