Title: Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement

Abstract: Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.

Body: Based on Data Balancing and Model Improvement for Multi-Label SentimentClassification Performance Enhancement study I Introduction II Related work III Methodology III-A Dataset preparation III-B Data processing III-C Model III-D Training process III-E Evaluation Metrics III-F Implementation IV Analysis and Discussion IV-A Baseline vs Ablation study IV-B Comparison with Published Models IV-C Single emotion performance evaluation IV-D Evaluation and Discussion V Conclution VI Acknowledgement VII Author Contributions VIII Funding IX Conflict of interest X Data Availability XI Appendix Based on Data Balancing and Model Improvement for Multi-Label SentimentClassification Performance Enhancement study 1 st Zijin Su 1 st Huanzhu Lyu 2 rd Yuren Niu 3 th Yiming Liu University College London Central South University Nanyang City Fifth Complete School Wuhan Guanggu Future School UCL Dept of Physics Dept of Computer Science and Technology High school High school London, UK Changsha, China Nanyang, China Wuhan, China 1176564216@qq.com Corresponding Author lvhuanzhuchat@gmail.com zyb12668@163.com lym20070118@outlook.com Abstract Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach. Keywords: Multi-label Sentiment Classification, Data Balancing, FastText, Bidirectional LSTM, Attention Mechanism I Introduction Sentiment analysis, a core task in natural language processing, systematically identifies and categorizes opinions expressed in text, typically classifying them as positive, negative, or neutral [1]. With the rapid growth of online communication, it has become indispensable for monitoring public opinion, interpreting customer feedback, and informing strategic decisions. In particular, online reviews can significantly influence public perception, and organizations increasingly rely on automated sentiment analysis to track brand reputation and consumer attitudes [2]. Despite its broad application, sentiment analysis faces persistent challenges. One of the most prominent is data imbalance, where certain emotion categories are severely underrepresented, leading to weaker performance for minority classes [3]. Another difficulty lies in detecting implicit sentiment, such as sarcasm, irony, or wordplay, where the literal meaning of words diverges from the intended message [4]. These issues are amplified in multi-label settings, where a single text can express multiple overlapping emotions. Traditional solutions—such as oversampling, undersampling, and cost-sensitive learning—are not universally effective: oversampling can lead to overfitting, undersampling risks losing important context, and cost-sensitive methods require careful parameter tuning to avoid bias. To address these limitations, this study focuses on optimizing the GoEmotions dataset for balanced, fine-grained multi-label emotion classification. We augment the original dataset with two additional sources: tweets from the Sentiment140 dataset re-labeled into 28 GoEmotions categories using the SamLowe/roberta-base-go-emotions classifier [11], and 20k GPT-4 mini–generated samples designed to enhance long-tail emotion categories. Both sources undergo a multi-step quality control process combining automated verification and manual review. The resulting balanced dataset is then used to train a unified deep learning architecture, enabling controlled comparisons between unbalanced and balanced settings. Experimental results finally show notable improvements in recognizing minority emotions. II Related work Previous studies on sentiment analysis have examined diverse datasets, algorithms, and classification strategies. Zuo [5] analyzed 7.7 million Steam game reviews from multiple sources, comparing Gaussian Naïve Bayes and Decision Tree classifiers with TF–IDF features. The Decision Tree achieved 75% accuracy, benefiting from feature selection through Information Gain or Gini Index. Likewise, Devika et al. [6] compared three sentiment analysis approaches—machine learning, rule-based, and lexical-based—detailing seven machine learning classifiers, and found that machine learning generally yielded higher precision, F1-score, and accuracy. Sharmila and Sivajothi [7] introduced the Ecology-Inspired Optimization algorithm (EI-ABC) for IMDb sentiment classification (400 positive and 400 negative reviews), demonstrating competitive results for numerical optimization. Shaheen [8] evaluated seven classifiers, including NB-SVM, Random Forest, Multinomial Naïve Bayes, Gradient Boosting, CNN, LSTM, and SGD, on over 400,000 Amazon mobile phone reviews; Random Forest performed best (85 Bashynska et al. [9] applied a pre-trained BERT model to the GoEmotions dataset, achieving a macro-average F1 score of 0.5070 and demonstrating BERT’s effectiveness in multi-label emotion classification. However, the study did not address dataset imbalance, leading to weaker performance on minority classes. Building on these findings, our work targets improved classification performance while explicitly addressing class imbalance, with the goal of enhancing results for underrepresented emotions and improving applicability in real-world settings. III Methodology III-A Dataset preparation The GoEmotions dataset [10], developed by Google AI and available on Kaggle, contains Reddit comments annotated with 28 emotion categories: 12 positive, 11 negative, four ambiguous, and one neutral (e.g., anger, surprise, disgust, joy, fear, sadness). To supplement low-frequency labels, we added data from two sources. First, tweets from the Sentiment140 dataset [12] were re-labeled into the 28 GoEmotions categories using the multi-label classifier SamLowe/roberta-base-go-emotions [11], which produces a 28-dimensional probability output. In our reproduction, this model performed well on the GoEmotions test set and was used as a weak supervision tool for label expansion. Second, we generated 20k additional samples for long-tail labels with GPT-4 mini, using a fixed prompt template and sampling strategy to ensure varied topics and natural language. All automatically labeled and generated samples went through quality checks: automatic verification with the RoBERTa classifier (label alignment 0.7), manual review by five annotators, and conflict resolution by majority vote. A random sample of 1,000 entries from the final balanced dataset was checked manually, and all were found to be reasonable. These verified samples were added to the original GoEmotions training split to balance label distribution, while the validation and test splits remained unchanged to avoid data leakage. Figures 1 and 2 show the label distribution before and after balancing, as well as the 50 most frequent words in the balanced dataset. Figure 1: Unbalanced ’GoEmotions’ dataset, the figure showing the original dataset witout balanced. Figure 2: balanced ’GoEmotions’ dataset, the figure showing the dataset after balance Figure 3: This figure showing the 50 Most frequent word appeared in the dataset After assembling the final dataset, all text samples were preprocessed to ensure consistency and avoid potential data leakage. We removed @mentions, URLs, and non-alphanumeric characters, followed by whitespace normalization. The official GoEmotions train/validation/test split was preserved, and tokenization was performed using a Tokenizer to fit only on the training set, with a maximum sequence length of 30 tokens. Emotion labels were converted into 28-dimensional binary vectors using MultiLabelBinarizer. This preprocessing ensured that non-informative tokens were removed while maintaining the integrity of the labeled data for multi-label classification. Figures 3 present the 50 most frequent words in the final balanced dataset. III-B Data processing Specifically, the new dataset was split into training, validation, and testing sets in an approximate 8:1:1 ratio, following the GoEmotions partition. The embedding layer was initialized with a pre-trained FastText matrix, mapping word indices to continuous word vectors and converting discrete text into representations suitable for model processing (see Figure 4). Also, we made a cosine similarity heat map to illustrate the relationship between emotion labels, with darker colors indicating higher similarity. As shown on the figure, joy and happiness form a close cluster, whereas anger and sadness are more distant from positive emotions. Figure 4: This figure shows the cosine similarity heat map of emotion labels in the word embedding space. After preprocessing, two datasets were fed into the model architecture , which is described in detail in the following section. In brief, the pipeline processes embedded word vectors through convolution, recurrent, and, when applied, attention layers, followed by pooling operations and a sigmoid output layer to produce independent probability scores for each emotion label. The overall workflow is illustrated in Figure 5. Collecting Data Data Processing Feature Computation Embedding Layer + Convolutional Layer (Conv1D) Batch Normalization + Pooling Layer (MaxPooling1D) Bidirectional LSTM + Attention Mechanism Fully Connected Layer (Dense Layer) Accuracy, Precision, Recall, F-score Figure 5: The step by step workflow of the process of the whole experiment III-C Model The model architecture begins with an embedding layer of dimension 300, drawing on word embedding approaches that can represent common and rare words through subword information, thus improving robustness to unseen vocabulary [13]. Next, a 1D convolutional layer with 64 filters and a kernel size of 5 extracts local semantic features, leveraging the proven effectiveness of convolutional neural networks in capturing spatial patterns [14]. Batch normalization is applied to stabilize learning, followed by max pooling to reduce feature map size while preserving essential information. A bidirectional LSTM with 128 units then models contextual dependencies in both forward and backward directions, using gating mechanisms to mitigate the gradient vanishing or exploding problems common in traditional RNNs [14]. In configurations, an attention mechanism is placed after the LSTM to assign higher weights to the most informative time steps, allowing the model to focus on critical segments of text for sentiment prediction [15]; otherwise, temporal average pooling is applied. The resulting feature vector is passed through a dense layer with 128 units and a dropout rate of 0.5 to reduce overfitting, before reaching a sigmoid-activated output layer that produces independent probability scores for 28 emotion labels in a multi-label classification setting. The detailed of each layer in this model will display in the appendix. III-D Training process The baseline was first established by training the full unified model on the processed and balanced GoEmotions dataset. This configuration serves as the primary reference for subsequent evaluations. All experiments adopted the same optimization setup for consistency. Training employed the Adam optimizer ( lr = 1 × 10 − 3 , β 1 = 0.9 , β 2 = 0.999 , ϵ = 10 − 7 \text{lr}=1\times 10^{-3},\beta_{1}=0.9,\beta_{2}=0.999,\epsilon=10^{-7} ) with a batch size of 256, up to 34 epochs, and early stopping if validation loss did not improve for five consecutive epochs. Mixed-precision training (float16 for non-critical operations, float32 for loss and gradient updates) was used to improve efficiency while preserving numerical stability. Thresholds for classification were tuned exclusively on the validation set through a per-label grid search to maximize the F1-score, producing the threshold vector τ \tau . This vector was then applied to the test set for evaluation, ensuring that no data leakage occurred. After assembling the processed GoEmotions dataset, the unified model architecture was first trained to establish the baseline. As shown in Figures 6 , both training and validation losses decreased steadily across epochs, eventually converging to stable values, indicating that the model maintained good generalization without clear signs of overfitting. Figure 6: Training and validation loss curves over 34 epochs for the full model on the processed GoEmotions dataset. Both losses decrease consistently and converge, suggesting effective learning without overfitting. To further examine the contribution of individual components and data balancing strategies, we conducted ablation experiments under three configurations: (i) unbalanced data with attention and (ii) unbalanced data without attention. Figure 7 shows the results for the unbalanced dataset with the attention mechanism. Training loss decreased steadily, while validation loss revealed early signs of overfitting. The attention layer in this setup allowed the model to assign higher weights to the most informative timesteps, enhancing feature representation for multi-label classification. Figure 7: Training and validation loss curves for the unbalanced GoEmotions dataset with attention, indicating effective learning with mild overfitting tendencies. Finally, Figure 8 shows the unbalanced dataset without attention. Training loss declined steadily, but validation loss reached its minimum earlier and then gradually increased, suggesting a stronger overfitting trend when attention was removed. Without attention, the model relied on temporal average pooling after the BiLSTM, which limited its ability to capture informative features. Figure 8: Training and validation loss curves for the unbalanced GoEmotions dataset without attention with validation loss rising afterwards, indicating a tendency toward overfitting. III-E Evaluation Metrics For multi-label classification, we adopted the following standard definitions: • Subset Accuracy: proportion of samples with all predicted labe