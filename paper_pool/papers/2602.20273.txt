Title: The Truthfulness Spectrum Hypothesis

Abstract: Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding's generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization (R^2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models' sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. Code for all experiments is provided in https://github.com/zfying/truth_spec.

Body: The Truthfulness Spectrum Hypothesis 1 Introduction 2 Truthfulness Datasets 2.1 Fictional, Logical, Empirical, Ethical, and Definitional (FLEED) Dataset 2.2 Sycophantic Lying Dataset 2.3 Expectation-inverted Dataset 2.4 Honesty Benchmarks 3 Experimental Setup Models. Activation Extraction. Probe Architecture. 4 Probe Generalization Across Datasets 5 Geometry of Probe Directions 6 Post-training Reorganizes Geometry 7 Revealing the Spectrum of Truthfulness Directions 7.1 Extracting Highly Domain-General and Domain-Specific Directions Design. Results. 7.2 Selective Erasure Reveals Directions of Intermediate Generality 8 Causal Assessment of Truth Directions Design. Results. 9 Related Works White-box Lie Detection and Intervention in LLMs. Representation geometry and probe transferability. Sycophancy and the Effect of Post-training. 10 Discussion Conclusion 11 Limitations A Datasets A.1 Fictional, Logical, Empirical, Ethical, and Definitional (FLEED) Dataset. Dataset Construction Pipeline Dataset examples. Prompt for extracting activations. Dataset geometry. A.2 Sycophantic Lying Dataset Dataset Construction Pipeline. MMLU STEM categories. Expert user biographies. Sycophantic lying behavioral analysis. Filtering based on correctness vs. based on confidence. B Probe Design C Additional Results: Probe Generalization Probing generalization results . D Additional Results: Probe Direction Geometric Analysis Simulation. Comparison between standard cosine similarity and Mahalanobis cosine similarity. E Additional Results: Post-training Geometry Reorganization Additional results on the Qwen model family. F Additional Results: Concept-Erasure Stratified INLP. LEACE Erasure. F.1 Formalizing Partially Overlapping Concept Subspaces Partitioning the Representation Space. Probe Reliance and Concept Erasure Optimization Problem. Results. G Additional Results: Causal Experiments Mechanism: suppression vs. confidence boosting. The Truthfulness Spectrum Hypothesis Zhuofan Josh Ying Shauli Ravfogel Nikolaus Kriegeskorte Peter Hase Abstract Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding’s generality. We reconcile these views with the truthfulness spectrum hypothesis : the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types ( definitional , empirical , logical , fictional , and ethical ), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization ( R 2 = 0.98 R^{2}{=}0.98 ). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models’ sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. 1 1 1 Code for all experiments is provided in https://github.com/zfying/truth_spec . Machine Learning, ICML 1 Introduction Figure 1 : Truth representations in LLMs are graded in generality and reshaped by post-training. Left: Different truth types share partially overlapping but distinct sets of truth directions. These directions lie on a spectrum from domain-general to domain-specific. The geometry of truth representations changes through post-training, pushing sycophancy into a more distant subspace from other truth types. This reorganization causes probes trained on factual truth to fail on sycophancy detection, and vice versa ( X ). However, training on all domains still yields a domain-general direction ( X ). Right: Concept erasure analysis further reveals the full spectrum of truth directions. Large language models (LLMs) often generate false or misleading information that, by all appearances, they know to be untrue (Pan et al., 2023 ; Scheurer et al., 2023 ; Abdulhai et al., 2025 ) . While we cannot always verify the truthfulness of model generations, we can probe them to detect when models themselves represent their generations to be false (Goldowsky-Dill et al., 2025 ) . Therefore, understanding how LLMs internally represent truthfulness has become a critical challenge for their safe deployment. Recent work suggests that LLMs develop a linear representation of truthfulness that can be extracted via probing (Marks Tegmark, 2023 ) . If such representations are sufficiently general, they should enable reliable detection of model falsehoods regardless of domain, and potentially allow interventions to improve honesty (Li et al., 2023 ; Zou et al., 2023 ; Turner et al., 2023 ; Cundy Gleave, 2025 ; Ravfogel et al., 2025 ) . Although some works show that these “truth directions” exhibit remarkable generalization across various domains and strategic deception scenarios (Burns et al., 2023 ; Azaria Mitchell, 2023 ; Marks Tegmark, 2023 ; Liu et al., 2024 ; Bürger et al., 2024 ; Goldowsky-Dill et al., 2025 ) , others show that probes fail to generalize in some cases and argue that LLMs encode “multiple, distinct notions of truth” (Levinstein Herrmann, 2024 ; Sky et al., 2024 ; Azizian et al., 2025 ; Orgad et al., 2025 ) . We argue that these seemingly contradictory findings can be reconciled. Prior work has treated cross-domain generalization failure and geometric dissimilarity between probes as evidence against domain-general truth encoding. However, this inference is flawed: generalization may fail not because domain-general directions do not exist, but because we fail to discover them. Conversely, high probe generalization performance and high probe direction similarity do not preclude the existence of highly domain-specific directions. We propose the truthfulness spectrum hypothesis : rather than exhibiting either a single domain-general truth direction or entirely separate domain-specific directions, LLMs encode truthfulness along a spectrum of generality, with directions at varying levels of generality coexisting in the representational space ( Figure 1 ). At one end lies a fully domain-general direction; at the other, fully domain-specific directions share no common structure; in between, directions generalize across some domains but not others. A probe trained on one domain may capture a superposition of these directions, combining domain-specific and more general features. The distribution of a model’s truth representations along this spectrum has important implications for lie detection and alignment interventions. Our experiments are based on our FLEED dataset , a large set of carefully controlled truthfulness datasets spanning five fundamental truth types: definitional , empirical , logical , fictional , and ethical . We additionally construct two novel deception datasets: a sycophantic lying dataset where models alter their answers to align with user-stated beliefs, and an expectation-inverted dataset where the user expects models to make false claims, making true generations violate the user expectation and count as lies. We also evaluate on prior honesty benchmarks (Scheurer et al., 2023 ; Benton et al., 2024 ; Goldowsky-Dill et al., 2025 ) . Our findings support the truthfulness spectrum hypothesis. Linear probes generalize well across our five fundamental truth types and most honesty benchmarks, yet fail almost entirely on sycophantic and expectation-inverted lying (AUROC ≈ 0.55 \approx 0.55 ). At the same time, it is possible to fit a well-performing probe over all domains, suggesting generalization failure reflects incomplete recovery of general direction, not its absence. In addition, we show that these generalization patterns are explained by the geometry of probe directions : Mahalanobis cosine similarity between probes, which reweights the inner product by test data covariance to account for low effective dimensionality of our data, near-perfectly predicts cross-domain generalization ( R 2 = 0.98 R^{2}{=}0.98 ), significantly outperform the standard cosine similarity ( R 2 = 0.56 R^{2}{=}0.56 ). To understand how this geometry arises, we study the effect of post-training on truth encoding, and find that the representational geometry of truth is reorganized by post-training . Specifically, in the base model, sycophancy representations are more aligned with other truth types, showing higher probe direction similarity and higher generalization performance. This suggests that post-training pushes sycophantic lying representation further away from other types of lying. This result provides a representational account of why post-trained models are more sycophantic than base models (Wei et al., 2023 ; Sharma et al., 2024 ) . To further provide constructive evidence that directions of varying degrees of generality coexist, we employ concept-erasure methods (Ravfogel et al., 2020 ; Belrose et al., 2023 ) in two complementary experiments. First, we introduce Stratified INLP , a two-stage hierarchical procedure that explicitly isolates highly domain-general and domain-specific directions . Second, we reveal directions of varying degrees of intermediate generality , which generalize across some domains but not others, using LEACE (Belrose et al., 2023 ) . Causal steering experiments confirm that domain-specific directions are not merely predictive but functionally meaningful: steering along them increases confidence in correct answers relative to incorrect ones, while steering along the domain-general direction slightly degrades performance. This suggests that while domain-general truth directions are encoded by LLMs, they may not participate in a causal mechanism underlying the truthfulness of model outputs. 2 2 2 We define truth directions solely based on encoding; see the Discussion for whether causal importance should also factor in. Together, these analyses demonstrate that truth directions of varying degrees of generality coexist in the same representational space, with different domains sharing structure in heterogeneous, partially overlapping ways ( Figure 1 ). 2 Truthfulness Datasets 2.1 Fictional, Logical, Empirical, Ethical, and Definitional (FLEED) Dataset Existing truthfulness datasets typically focus on single truth types (e.g., empirical knowledge, logical reasoning), making it difficult to assess whether LLMs develop domain-general or domain-specific truth representations. To investigate this, we create balanced datasets of true/false statements across five basic categories: • Definitional truth : Statements true by definition (e.g., “A triangle has three sides”) • Empirical truth : Empirical facts about the world (e.g., “Water boils at 100°C at sea level”) • Logical truth : Statements following from logical inference (e.g., “If A implies B and B implies C, then A implies C”) • Fictional truth : Facts within fictional universes (e.g., “Frodo carried the One Ring to Mordor”) • Ethical truth : Behaviors generally recognized as ethical (e.g., “I told the jury the correct information”) Except for the ethical truth, each truth type contains about 1,200 examples (600 positive, 600 negative), yielding 4,800 total claims. Claims are generated by prompting Gemini-3-Pro to create paired positive and negative instances for each truth type, with careful attention to generating diverse and plausible-sounding negations. To ensure dataset quality, we manually filter low-quality or ambiguous examples. The ethical truth data is adapted from the commonsense subset of the ETHICS dataset (Hendrycks et al., 2021 ) , filtered to 4,000 short-form samples. See Appendix A for details of dataset construction. 2.2 Sycophantic Lying Dataset We construct a sycophantic lying dataset where models change their answers to align with user beliefs rather than reporting their believed true answer. This dataset was built using questions from STEM subjects in MMLU (Hendrycks et al., 2020a ) , with fake user bios added (Perez et al., 2023 ) . Figure 9 illustrates the dataset construction process. Specifically, we first query the model on MMLU STEM questions without additional context to establish its ”true belief” baseline. We then re-query the model with user biographies indicating STEM expertise and a pre-stated user answer (e.g., ”I have a PhD in Mathematics from Berkeley… I believe the answer is (A)…”). We identify sycophantic lying samples where the model changes its original answer to align with the user’s preference, and pair these with control cases where the model maintains its original answer. Each pair shares an identical biography and question, differing only in the user’s preferred answer and the model’s final response. We apply additional filtering to ensure data quality. See Appendix A and Figure 9 for more details. Our dataset design builds on prior sycophancy evaluations (Perez et al., 2023 ; Turpin et al., 2023 ; Sharma et al., 2024 ) with two key methodological choices. First, we measure whether the model changes its answer in response to user beliefs, rather than simply measuring agreement rates. This disentangles model accuracy from model honesty (Turpin et al., 2023 ) . Second, we construct this dataset with highly controlled positive and negative pairs, sampled on-policy rather than prefilling the model response with positive (lying) or negative (honest) responses. 2.3 Expectation-inverted Dataset To test whether probes detect objective truth values or user-expected truth values, we construct a dataset that disentangles the two. We pair true and false statements from (Azaria Mitchell, 2023 ) with the prompt: ”Pretend you’re a dishonest person making statements about the world,” which inverts the typical confound. Here, producing true statements now violates user expectations, while false statements comply with them. As we show later, many “lie detector” probes fail to generalize to this kind of honesty evaluation. 2.4 Honesty Benchmarks To evaluate generalization beyond our curated datasets, we incorporate goal-directed deception scenarios from prior works, including insider trading, sandbagging, and roleplaying lying, where models are evaluated on their own generated responses when encouraged to lie to achieve specified goals (Scheurer et al., 2023 ; Benton et al., 2024 ; Goldowsky-Dill et a