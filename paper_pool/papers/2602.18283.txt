Title: HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation

Abstract: Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.

Body: HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation 1 Introduction 2 Related Work 2.1 Sequential and Generative Recommendation 2.2 Efficient Long-Sequence Modeling and Hybrid Architectures 3 Preliminaries 4 HyTRec 4.1 Framework Overview 4.2 Hybrid Attention Architecture 4.3 TADN: Temporal-Aware Delta Networks Temporal Decay Factor. Temporal-Aware Gating Generation. Information Fusion Mechanism. Temporal-aware Gated Delta Rule 5 Experiments 5.1 Experimental Setup Datasets. Setup and Evaluation. Baselines. Metrics. 5.2 Performance Comparison (RQ1) 5.3 Training Efficiency (RQ2) 5.4 Ablation Study (RQ3) 5.5 Efficiency Evaluation (RQ4) 5.6 Case Study (RQ5) 6 Conclusion A The Evolution of Attention Mechanisms A.1 Softmax Attention A.2 Linear Attention A.3 Hybrid Attention B Baseline Model Descriptions B.1 Behavior Sequence Models B.2 Long-Text Models B.3 Contrast with HyTRec C Engineering Tricks for Long Behavior Sequence Construction C.1 Extending the Time Window Size C.2 Intermediate Process Sequence Data Processing Strategy C.3 Rational Utilization of Different Channels C.4 Missing Value Imputation D Common Engineering Bad Cases E More Experimental Results E.1 Comparison Experiments on Different Numbers of Attention Heads E.2 Comparison Experiments on Different Numbers of Experts E.3 Empirical Analysis of Hybrid Design F Discussion F.1 Adaptive Boundary for Hybrid Attention F.2 Integration with Expanded Memory Architectures F.3 Extension to Multi-Scenario Recommendation HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation Lei Xin 1,2 , Yuhao Zheng 3 , Ke Cheng 4 , Changjiang Jiang 2 , Zifan Zhang 2 , Fanhu Zeng üñÇ 1 Shanghai Dewu Information Group 2 Wuhan University 3 USTC 4 Beihang University i_xinlei@dewu.com, yuhaozheng@mail.ustc.edu.cn, kecheng@tencent.com, jiangcj@whu.edu.cn, zifan623@gmail.com, challengezengfh@gmail.com Abstract Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec , a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency. HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation Lei Xin 1,2 , Yuhao Zheng 3 , Ke Cheng 4 , Changjiang Jiang 2 , Zifan Zhang 2 , Fanhu Zeng üñÇ 1 Shanghai Dewu Information Group 2 Wuhan University 3 USTC 4 Beihang University i_xinlei@dewu.com, yuhaozheng@mail.ustc.edu.cn, kecheng@tencent.com, jiangcj@whu.edu.cn, zifan623@gmail.com, challengezengfh@gmail.com 1 1 footnotetext: üñÇ Corresponding Author. 1 Introduction The rapid accumulation of interaction data on online platforms has driven a fundamental transition in recommender systems from traditional collaborative filtering to generative paradigms rooted in ultra-long sequences Zhai and others ( 2024 ); Lin and others ( 2025 ); Deng et al. ( 2025 ); Du et al. ( 2025 ); Jiang et al. ( 2025 ) . Central to this shift is the utilization of long behavior sequences, which serve as a vital resource for decoding dynamic preferences and latent intentions Kang and McAuley ( 2018 ); Gu and Dao ( 2024 ) . Such extensive sequences provide rich interaction signals that offer high-quality feedback for the next item prediction task Sun et al. ( 2024 ); Jiang et al. ( 2026 ) . By revealing the long-term trajectory of user interests rather than just short-term session data, modeling long behavior sequences proves essential for capturing complex decision-making paths. To model these sequences, methodologies have evolved significantly. Early approaches utilized session-based patterns Hidasi et al. ( 2016 ) or self-attentive models Kang and McAuley ( 2018 ); Sun et al. ( 2019 ) , establishing the foundation for sequential modeling. More recently, the field has witnessed the rise of generative frameworks, represented by P5 Geng et al. ( 2022 ) and TALLRec Bao et al. ( 2023 ) , which demonstrate robust generalization capabilities. Furthermore, emerging simulators Wang and others ( 2025 ) have highlighted the value of modeling long sequence data. Figure 1: The Evolution of Attention Mechanisms. Nevertheless, existing models encounter two formidable obstacles when scaling to long behavior sequences: (i) The inherent trade-off between efficiency and expressiveness remains unresolved, as traditional softmax attention suffers from quadratic complexity while linear variants often compromise retrieval precision, resulting in semantic ambiguity and limited injectivity when capturing fine-grained dependencies Qin and others ( 2024 ) , as shown in Figure 1 . (ii) Current architectures struggle to adapt to interest drifts Zhou et al. ( 2018 ) . Linear models Gu and Dao ( 2023 ); Yang et al. ( 2024c ) often fail to catch up with rapid intent changes because they compress all information into a fixed state. Consequently, they cannot easily distinguish immediate, high-value signals from the vast amount of historical noise, leading to a lag in capturing what the user truly wants at the moment Shao and others ( 2025 ) . To address these limitations, we propose HyTRec , a generative framework featuring a Hy brid T emporal-Aware Rec ommendation architecture tailored for efficient long behavior sequence modeling. (i) To reconcile the trade-off between inference speed and retrieval precision, we design a Hybrid Attention architecture specifically for modeling long behavior sequences. By strategically integrating a small proportion of softmax attention layers into a predominantly linear attention backbone, we maintain near-linear complexity comparable to purely linear models, while effectively restoring the high-fidelity retrieval capabilities that are typically compromised in linear approximations. (ii) To further mitigate the lag in capturing rapid interest drifts within the linear layers, we incorporate a Temporal-Aware Delta Network (TADN) . This module utilizes an exponential gating mechanism to dynamically upweight fresh behavioral signals, effectively suppressing historical noise and ensuring the model remains highly sensitive to immediate user intents. Extensive evaluations on diverse benchmarks validate the effectiveness of HyTRec, where it consistently outperforms strong baselines by an average of 5.8% in NDCG. Our contributions are as follows: ‚Ä¢ Novel Hybrid Attention. We propose HyTRec, a hybrid attention framework for generative recommendation that synergizes linear attention for history with softmax attention for recent interactions, achieving linear complexity while preserving the semantic integrity of long-term preferences. ‚Ä¢ Dynamic Intent Modeling. We introduce the Temporal-Aware Delta Networks (TADN), which leverage a temporal decay factor to meticulously track rapid interest shifts, ensuring transient user intents are accurately prioritized over historical noise. ‚Ä¢ Empirical Performance. Extensive experiments on real-world e-commerce datasets demonstrate that HyTRec outperforms strong baselines, delivering over 8% improvement in Hit Rate for users with extensive interaction histories while maintaining linear inference speed. 2 Related Work 2.1 Sequential and Generative Recommendation Sequential recommendation aims to predict subsequent user actions by characterizing dynamic dependencies within historical interactions. Early methodologies primarily utilized Markov Chains for short-term transitions or Recurrent Neural Networks to capture broader sequential regularities. With the rise of Transformer architectures, models such as SASRec Kang and McAuley ( 2018 ) leveraged self-attention for global dependency modeling. In industrial practice, managing ultra-long behavior sequences led to the development of SIM Pi and others ( 2020 ) , which employs a two-stage search-based strategy, and ETA Chen and others ( 2021 ) , which utilizes Locality Sensitive Hashing for end-to-end processing. Recently, the paradigm has shifted toward generative recommendation inspired by Large Language Models Zhai and others ( 2024 ); Lin and others ( 2025 ) . In this context, P5 Geng et al. ( 2022 ) unified multiple recommendation tasks, while TablePilot Liu and others ( 2025 ) demonstrated the feasibility of aligning generative models with human-preferred data analysis patterns. To address evaluation constraints, SimUSER Wang and others ( 2025 ) explored using LLMs to simulate user behavior. For cold-start scenarios, reinforcement learning-driven adversarial query generation has been employed to enhance relevance Shukla and others ( 2025 ) . Furthermore, to capture diverse evolving user intentions, MIND Li and others ( 2019 ) and ComiRec Cen and others ( 2020 ) introduced multi-interest extraction mechanisms. Despite these advancements, the practical deployment of generative recommendation is severely constrained by inference latency. The substantial computational overhead required to process lifelong sequences with Large Language Models often exceeds the strict response-time limits of real-time industrial systems, necessitating more efficient architectural solutions. 2.2 Efficient Long-Sequence Modeling and Hybrid Architectures The evolution of long-sequence modeling provides a strategic roadmap for processing lifelong behavioral data. Although the original self-attention mechanism Vaswani and others ( 2017 ) offers high expressiveness, its computational complexity limits practical scalability. Consequently, Longformer Beltagy et al. ( 2020 ) introduced sparse attention patterns, and linear variants like Performers Choromanski and others ( 2021 ) utilized kernel-based approximations. Subsequently, State Space Models such as S4 Gu et al. ( 2022 ) and Mamba Gu and Dao ( 2023 ) , along with linear variants like DeltaNet Yang et al. ( 2024c ) , achieved strict linear complexity O ‚Äã ( n ) O(n) . However, pure linear models frequently suffer from semantic confusion and struggle to maintain the injectivity of their hidden state updates Qin and others ( 2024 ) . Moreover, standard embedding techniques in ultra-long sequence scenarios can lead to information bottlenecks, necessitating the use of decoupled embeddings to preserve model capacity Ren and others ( 2025 ) . To strike a balance between efficiency and precision, the industry is exploring hybrid architectures where frameworks like Jamba Lieber and others ( 2024 ) interleave state-space layers with self-attention layers. HyTRec advances this philosophy by introducing the Temporal-Aware DeltaNet. Techniques such as ALiBi Press et al. ( 2022 ) utilize static linear biases to weight local context effectively, whereas MotiR Shao and others ( 2025 ) emphasizes the retrieval of underlying user motivations to filter out superficial noise. Building upon these distinct approaches, our model explicitly incorporates time-decay factors via an exponential gating mechanism to dynamically prioritize recent high-intent behaviors. Unlike standard Gated Linear Attention Yang et al. ( 2024b ) , this design enables the model to discard irrelevant distant history while focusing on recent high-intent signals , thereby effectively resolving the semantic dilution problem common in long-sequence modeling. 3 Preliminaries In this work, we focus on the task of next-item prediction within a long sequential recommendation scenario. The core objective is to accurately predict the ID of the next item that a user will purchase based on their historical interaction sequence. Let ùí∞ \mathcal{U} denote the set of users, and ‚Ñê \mathcal{I} denote the set of items. For an arbitrary user u ‚àà ùí∞ u\in\mathcal{U} , the long interaction sequence is represented as S u = [ x 1 , x 2 , ‚Ä¶ , x n ] S_{u}=[x_{1},x_{2},\dots,x_{n}] , where x t ‚àà ‚Ñê x_{t}\in\mathcal{I} denotes the item interacted with at time step t t , and n n represents the length of the user‚Äôs interaction sequence. Formally, the goal of the recommender system is to estimate the probability distribution of the next item x n + 1 x_{n+1} given the historical sequence S u S_{u} , formulated as maximizing P ‚Äã ( x n + 1 ‚à£ S u ) P(x_{n+1}\mid S_{u}) . 4 HyTRec Figure 2: The Framework of HyTRec. 4.1 Framework Overview We propose HyTRec, a generative framework designed for long sequence users behavior modeling. As illustrated in Figure 2 , the framework explicitly decouples the processing of massive historical patterns from immediate intent spikes from sequence and architecture design, respectively. Sequence Decomposition Strategy. To achieve this decoupled modeling, we first decompose the long sequence S u S_{u} into two disjoint subsequences: ‚Ä¢ Short-term behavior sequence . The length of this sequence is fixed at K K , denoted as S u s ‚Äã h ‚Äã o ‚Äã r ‚Äã t = [ x n ‚àí K + 1 , ‚Ä¶ , x n ] S_{u}^{short}=[x_{n-K+1},\dots,x_{n}] . This subsequence focuses on the user‚Äôs recent behaviors to capture short-term sudden consumption intents and interest drifts. ‚Ä¢ Long-term historical behavior sequence . This sequence consists of the historical interaction excluding short-term part, denoted as S u l ‚Äã o ‚Äã n ‚Äã g = [ x 1 , ‚Ä¶ , x n ‚àí K ] S_{u}^{long}=[x_{1},\dots,x_{n-K}] , with a length of n ‚àí K n-K . This subsequence covers long-term behavioral patterns to capture stable and inherent consumption preferences. Dual-Branch Data Flow. Building upon this stratification, HyTRec processes these signals through two parallel branches before fusion: ‚Ä¢ The short-term branch processes S u s ‚Äã h ‚Äã o ‚Äã r ‚Äã t S_{u}^{short} using standard multi-head self-attention (MHSA) to ensure maximum precision for recent behaviors. ‚Ä¢ The long-term branch processes S u l ‚Äã o ‚Äã n ‚Äã g S_{u}^{long} using our proposed hybrid attention architecture. This branch serves as the computational backbone, compressing the extensive history into a compact representation while preserving fine-grained dependencies. The outputs of both branches are subsequently fused to generate the final p