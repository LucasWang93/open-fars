Title: Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement

Abstract: Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.

Body: Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement 1 Introduction 2 Related Work Accelerated Optimizers for LLM pre-training. Training Dynamics and Loss Landscape in Deep Learning. 3 Preliminary Notations. 3.1 Manifold Optimization Tangent and Cotangent Spaces. Levi-Civita Connection. 3.2 ODE Perspective of Momentum-based Algorithms 4 A Unified Riemannian ODE Framework for Understanding Adaptive Algorithms 4.1 A Unified Discrete Formulation Optimizers using Hessian Damping 4.2 The Riemannian ODE Framework Continuous-time Viewpoint. First-order Tangent-Cotangent Formulation. Joint effect of the preconditioner and momentum. Connection with the Discrete Formulation. 5 The LITE Approach 5.1 Acceleration Methods from the ODE Perspective 5.2 Practical Implementation of LITE Efficiently approximating the flat directions Accelerating Muon ( Muon-LITE ) Accelerating SOAP ( SOAP-LITE ) 6 Experiments 6.1 Results on Dense Models Main Results. Ablation Studies. 6.2 Results on MoE Models 7 Theoretical Analysis to Training Dynamics 8 Conclusion A More Related Works Comparison with methods enhancing dynamics along flat directions. Comparison with algorithmic frameworks for adaptive optimizers. B Algorithm Details B.1 Details of Muon-LITE Estimating P k P_{k} via Efficient Composite Newton-Schulz. B.2 Details of SOAP-LITE C Experimental Details Datasets. Sequence Packing/Batching. C.1 Experimental Details in Sections 5.2 Measure Coverage Degree between Two Subspaces. C.2 Experimental Details in Sections 6 Muon Baselines. SOAP Baselines. Learning Rate Schedules. Hyper-parameter tuning for Muon-LITE . Hyper-parameter tuning for SOAP-LITE . D Additional Experimental Results D.1 Dense Models E Discussions on Momentum Formulations and their Continuous-Time ODE Counterparts E.1 Continuous-Time ODE Formulations E.2 Heavy Ball Momentum, Nesterov Momentum, and Hessian Damping Heavy Ball Momentum. Nesterov Momentum. The Gradient Correction (Hessian Damping) Insight. Further Improving Nesterov momentum E.3 Modeling AdEMAMix via a Third-Order ODE Intuitive Interpretation. F Foundations of Riemannian Geometry F.1 Riemannian Metric Smooth Vector Fields and the Tangent Bundle. F.2 The Levi-Civita Connection Definition. Coordinate Representation (Christoffel Symbols). F.3 Riemannian Gradient and Hessian Riemannian Gradient. Riemannian Hessian. G Transforming the Second Order RISHD (the Riemannian ODE Framework) into a First Order System H Theoretical Analysis on Quadratic Objectives: An Illustrative Example Stability Analysis. Regime Classification. Acceleration Mechanism of LITE . I Some Common Adaptive Optimizers for LLM Pre-training I.1 Exact Preconditioner (Riemannian Metrics) Forms for Common Adaptive Optimizers EMA-based Preconditioners. Momentum-based Preconditioners. J Proof Notations J.1 Landscape Analysis J.2 Dynamics Analysis J.3 Useful Lemmas Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement Shuchen Zhu Rizhen Hu Mingze Wang Mou Sun Xue Wang Kun Yuan Zaiwen Wen Abstract Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy—relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE , a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M–1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE . Machine Learning, ICML 1 Introduction Large language models (LLMs) have revolutionized artificial intelligence with exceptional capabilities, yet the pre-training procedure is computationally intensive due to massive model and data scales. Improving pre-training efficiency is essential for continued scaling, and the optimizer choice is crucial. Currently, AdamW (Kingma and Ba, 2014 ; Loshchilov and Hutter, 2017 ) serves as the standard in most LLM pre-training pipelines, favored for its simplicity and efficiency. However, it uses coordinate-wise scaling, which acts as a diagonal preconditioner and ignores parameter correlations within the Hessian, limiting the ability to navigate complex optimization landscapes. To address the limitation of the diagonal preconditioning, algorithms like Shampoo (Gupta et al. , 2018 ; Shi et al. , 2023 ) and KFAC (Martens and Grosse, 2015 ) introduce matrix-based preconditioners to capture richer geometric structures, demonstrating superior convergence over AdamW in traditional deep learning tasks. Advancing this paradigm to the scale of LLM pre-training, a new wave of matrix-based optimizers, such as Muon (Jordan et al. , 2024 ) and SOAP (Vyas et al. , 2025 ) , has emerged, achieving significant performance gains over AdamW. Notably, Muon has already achieved validated success in industrial-scale training scenarios (Liu et al. , 2025a ; Team, 2025 ) . These methods signify a paradigm shift towards exploiting matrix-level curvature information to accelerate convergence. Despite the empirical success of these advanced optimizers, significant limitations remain: 1. Preconditioner-induced isotropic update magnitudes. Although adaptive methods correct descent directions by effective preconditioners, prior studies (Staib et al. , 2019 ; Zhou et al. , 2020 ; Liu et al. , 2025a ; Lu et al. , 2025 ; Wang et al. , 2025b ) suggest that the update magnitudes often tend towards isotropic , i.e., of comparable scale across different Hessian eigen-directions. This behavior is suboptimal in the ill-conditioned landscape, which is too cautious along flat directions while potentially aggressive along sharp ones. 2. Inadequate momentum mechanisms for non-convexity. Constrained by the simple linear nature of Exponential Moving Average (EMA), which inherently imposes an isotropic damping effect, existing momentum schemes inadequately exploit second-order information, leaving them susceptible to ill-conditioned curvature. While techniques like Nesterov acceleration implicitly incorporates Hessian-driven damping (Shi et al. , 2021 ) , the design is largely inherited from convex optimization paradigms, rendering them insufficient for effectively accelerating convergence along non-convex directions. Addressing these limitations, however, presents a fundamental challenge: the lack of a unified theoretical framework that provides a comprehensive understanding of how preconditioning and momentum jointly influence training dynamics in anisotropic loss landscapes . Such a framework remains elusive, as current analyses typically treat these components in isolation, failing to elucidate their synergistic mechanisms in non-convex optimization. Bridging this theoretical gap is essential for designing superior optimizers that can simultaneously rectify update magnitudes and leverage curvature-aware momentum. These issues motivate the following questions: • Can we establish a unified theoretical understanding of the synergistic roles of the momentum and preconditioner in adaptive optimization algorithms? • Leveraging this foundation, how can we rectify the inadequate isotropic update and refine the momentum mechanism to achieve superior efficiency in pre-training LLMs? Our contributions are as follows: • We propose a unified Riemannian ODE framework that encapsulates prevalent adaptive optimizers for LLM pre-training ( e.g. AdamW, Lion, Muon, SOAP) as well as their Nesterov-accelerated counterparts. This framework elucidates the synergistic mechanism between preconditioning and momentum from a continuous time manifold optimization perspective: the preconditioner induces a Riemannian geometry that mitigates the landscape’s ill-conditioning, while momentum functions as a Riemannian damping to foster optimization within this metric. • Building on this framework, we propose LITE , a generalized strategy for acce L erating adapt I ve op T imizers in LLM pr E -training. Leveraging our ODE analysis, LITE applies larger Hessian damping coefficients and learning rates in the flat directions , which enhances momentum accumulation and amplifies update magnitudes therein. This approach is designed to accelerate training dynamics along the flat directions that dominate loss reduction within the ill-conditioned landscape, thereby enhancing the pre-training performance. • We conduct extensive empirical evaluations on the acceleration performance of LITE across diverse LLM pre-training settings, spanning different base optimizers (Muon and SOAP), model architectures (LLaMA and QwenMoE), datasets (C4 and Pile), and learning rate schedules (cosine and warmup-stable-decay), with model sizes ranging from 0.13B to 1.3B parameters. Our results demonstrate that LITE -accelerated optimizers achieve remarkable loss reductions and exhibit more favorable scaling laws over standard baselines, notably attaining a 2 × \times speedup in long-horizon training ( Figure 1 ). This indicates the potential of LITE for superior scalability to larger models and extended token budgets. Figure 1 : Muon-LITE exhibits superior scaling behavior across varying token budgets (left) and model sizes (right) compared to Muon. • We provide a theoretical analysis of the training dynamics of LITE within anisotropic landscapes, showing that it boosts dynamics in flat directions and facilitates optimization, thereby supporting our design intuition. 2 Related Work Accelerated Optimizers for LLM pre-training. Adaptive optimizers serve as the cornerstone of modern LLM pre-training. One stream of research focuses on refining the preconditioner: moving beyond simple element-wise scaling of AdamW (Loshchilov and Hutter, 2017 ) , methods like (Liu et al. , 2024 ; Wang et al. , 2025a ) incorporate explicit curvature estimation, while matrix-based approaches (Jordan et al. , 2024 ; Vyas et al. , 2025 ; Pethick et al. , 2025 ; Liu et al. , 2025b ; Lau et al. , 2026 ) leverage richer geometric structures to better approximate the curvature. Complementing these geometric improvements are refinements to the momentum mechanism, spanning multi-timescale momentum (Pagliardini et al. , 2025 ) , Nesterov-type acceleration (Xie et al. , 2024 ; Yuan et al. , 2025 ) , and various momentum correction schemes (Huang et al. , 2025 ; Liang et al. , 2025 ) . Training Dynamics and Loss Landscape in Deep Learning. Prior studies (Ghorbani et al. , 2019 ; Yao et al. , 2020 ; Zhang et al. , 2024 ; Su, 2025 ) elucidate the ill-conditioned and anisotropic nature of the deep learning loss landscape. Specifically, the Hessian spectrum is dominated by a massive bulk of near-zero and negative eigenvalues (referred to as flat directions ), while the large positive eigenvalues are significantly greater in magnitude but sparse in number (referred to as sharp directions ), as schematically illustrated in Figure 2 (left). Recent investigations (Song et al. , 2025 ; Cohen et al. , 2025 ; Wen et al. , 2025 ; Wang et al. , 2025a ) into the training dynamics of gradient-based algorithms on this landscape have led to the consensus that the anisotropic landscape induces a distinct time-scale separation in the training dynamics ( Figure 2 (right)): 1) Fast dynamics: Along sharp directions, the dynamics exhibit rapid but non-divergent oscillations, which dictate training stability yet contribute minimally to loss reduction. 2) Slow dynamics: Along flat directions, the dynamics evolves steadily but slowly, dominating total loss reduction. Leveraging these insights, our LITE approach is designed to accelerate training by boosting the slow training dynamics along flat directions (see schematic in Figure 2 (right)). More related works are deferred to Appendix A . Figure 2 : (left) Hessian eigenvalue distribution of an up_proj FFN block in a toy LLaMA model; (right) Schematic illustration of the time-scale separation property and the acceleration mechanism of the LITE approach. 3 Preliminary Notations. Let v k v_{k} denote the k k -th component of a vector v v . For a positive semi-definite matrix F F , we define the induced inner product and norm as ⟨ u , v ⟩ F = u ⊤ ​ F ​ v \langle u,v\rangle_{F}=u^{\top}Fv and ‖ u ‖ F = u ⊤ ​ F ​ u \|u\|_{F}=\sqrt{u^{\top}Fu} respectively, omitting the subscript when F = I F=I . Given a matrix-valued function F : ℝ p → ℝ p × p F:\mathbb{R}^{p}\to\mathbb{R}^{p\times p} , its directional derivative is written as ∇ F ​ ( w ) ​ [ v ] = lim ϵ → 0 ( F ​ ( w + ϵ ​ v ) − F ​ ( w ) ) / ϵ \nabla F(w)[v]=\lim_{\epsilon\to 0}(F(w+\epsilon v)-F(w))/\epsilon . We distinguish between Riemannian operators ( grad , Hess \operatorname{grad},\operatorname{Hess} ) and their Euclidean counterparts ( ∇ , ∇ 2 \nabla,\nabla^{2} ). The symbol ⊙ \odot represents the element-wise product, and ⊗ \otimes denotes the Kronecker product. A subscript or superscript F F is occasionally attached to geometric operators ( e.g. ∇ F \nabla^{F} ) to emphasize they are induced by the Riemannian metric F F . 3.1 Manifold Optimization Manifold optimization deals with variables constrained to a Riemannian manifold. In this work, we consider a special case: the variable (parameter) space ℝ p \mathbb{R}^{p} not as a standard Euclidean space, but as a Riemannian manifold ℳ = ( ℝ p , F ) \mathcal{M}=(\mathbb{R}^{p},F) equipped with metric F ​ ( w ) ∈ ℝ p × p F(w)\in\mathbb{R}^{p\times p} at each w ∈ ℝ p w\in\mathbb{R}^{p} . This geometric perspective allows us to analyze the optimization landscape through the lens of local curvature defined by F F . Detailed discussions are provided in Appendix F . Tangent and Cotangent Spaces. At any point w ∈ ℳ w\in\mathcal{M} , the tangent space T w ​ ℳ ≅ ℝ p T_{w}\mathcal{M}\cong\mathbb{R}^{p} represents the vector space of all possible directional velocities (or parameter perturbations). The Riemannian metric induces a norm ∥ ⋅ ∥ F ​ ( w ) \|\cdot\|_{F(w)} on the tangent space. The cotangent space T w ∗ ​ ℳ T