Title: MoR: Mixture Of Representations For Mixed-Precision Training

Abstract: Mixed-precision training is a crucial technique for scaling deep learning models, but successful mixedprecision training requires identifying and applying the right combination of training methods. This paper presents our preliminary study on Mixture-of-Representations (MoR), a novel, per-tensor and sub-tensor level quantization framework that dynamically analyzes a tensor's numerical properties to select between a variety of different representations. Based on the framework, we have proposed and experimented concrete algorithms that choose dynamically between FP8 and BF16 representations for both per-tensor and sub-tensor level granularities. Our universal approach is designed to preserve model quality across various quantization partition strategies and datasets. Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format. This work highlights the potential of dynamic, property-aware quantization while preserving model quality. We believe this approach can generally improve the robustness of low precision training, as demonstrated by achieving FP8 accuracies that are on par with existing approaches without the need for fine-grain partitioning, or can be used in combination with other training methods to improve the leverage of even lower precision number formats such as NVFP4.

Body: MoR: Mixture of Representations for Mixed-Precision Training 1 Introduction 2 Group Amax Mantissa Scaling 3 The MoR Framework 3.1 MoR at Tensor Granularity 3.2 MoR at SubTensor Granularity Algorithm 1: Three-Way Selection (E4M3/E5M2/BF16) Algorithm 2: Two-Way Selection (E4M3/BF16) 4 Experiments 4.1 Experiments for MoR at Tensor Granularity 4.1.1 MoR with Different Partition Strategies 4.1.2 Ablation Study on MoR Settings 4.1.3 Tensor Statistics Analysis of Per-Block Scaling (Configuration 1) Comparing Training Configurations Comparing Partitioning Strategies 4.2 Experiments for MoR at Sub-Tensor Granularity 5 Conclusions MoR: Mixture of Representations for Mixed-Precision Training Bor-Yiing Su Nvidia boryiings@nvidia.com Peter Dykas Nvidia wdykas@nvidia.com Mike Chrzanowski Nvidia mchrzanowski@nvidia.com Jatin Chhugani Meta jatinch@meta.com Work done while at Nvidia. Abstract Mixed-precision training is a crucial technique for scaling deep learning models, but successful mixed-precision training requires identifying and applying the right combination of training methods. This paper presents our preliminary study on Mixture-of-Representations (MoR), a novel, per-tensor and sub-tensor level quantization framework that dynamically analyzes a tensor‚Äôs numerical properties to select between a variety of different representations. Based on the framework, we have proposed and experimented concrete algorithms that choose dynamically between FP8 and BF16 representations for both per-tensor and sub-tensor level granularities. Our universal approach is designed to preserve model quality across various quantization partition strategies and datasets. Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format. This work highlights the potential of dynamic, property-aware quantization while preserving model quality. We believe this approach can generally improve the robustness of low precision training, as demonstrated by achieving FP8 accuracies that are on par with existing approaches without the need for fine-grain partitioning, or can be used in combination with other training methods to improve the leverage of even lower precision number formats such as NVFP4. 1 Introduction The scaling law [ scaling_law ] suggests that model performance increases with the number of parameters and the volume of training data, fueling a race to train ever-larger foundation models to achieve state-of-the-art performance on industry benchmarks. For instance, the Llama 3.1 405B model was trained on 24,576 H100 GPUs for months [ llama3 ] , the Megatron-Turing NLG 530B model on 4,480 GPUs for months [ Megatron530B ] , the GPT-4 model was estimated to have been trained on over 20,000 GPUs for three months [ openai2023gpt4 ] , and the DeepSeek-V3 671B model is trained with 2.788M GPU hours [ DeepSeekV3 ] . Consequently, training large foundation models is time-consuming, power-hungry, and expensive. This has led to significant investment in optimizing the training workflow to accelerate performance and reduce resource consumption. Among the most effective optimizations is the use of low-precision numerical representations. Using low-precision representations offers numerous benefits: (a) From a compute perspective, modern hardware offers significant speedups for lower-precision numerical operations. For example, an NVIDIA H100 GPU delivers 2x the FLOPS for FP8 GEMM operations compared to BF16 [ H100_arch ] , while the newer GB200 and GU300 GPUs provide 4x and 6x FLOPS for FP4 GEMM, respectively [ NVIDIA_GB200_NVL72 , NVIDIA_GB300_Ultra ] . (b) From a bandwidth perspective, loading a vector of FP8 values requires only half the memory bandwidth of loading a BF16 vector of the same length. Therefore, low-precision representations directly reduce the latency of both computation and memory access, significantly accelerating training. (c) Moreover, applying low-precision formats during training enables their direct use for inference, creating consistency between the two phases and obviating the need for post-training quantization (PTQ) or quantization-aware training (QAT). The formal specification of the FP8 format can be found in [ fp8_paulius ] . It defines two FP8 formats: E4M3 and E5M2. The E4M3 format has 4 bits for exponent, and 3 bits for mantissa; while the E5M2 format has 5 bits for exponent, and 2 bits for mantissa. But FP8 training is not only about the FP8 formats. In order to increase the representation accuracy, improve the training stability, and also retain the model quality, many prior arts define comprehensive recipes for FP8 training. [ delayed_scaling ] and [ fp8_paulius ] suggested current and delayed per-tensor scaling. Essentially, defining a scaling factor for the full tensor to shift the numerical range of the tensor to the representable range by the format type. Delayed scaling computes this scaling factor based on historical absolute maximum (amax) values of the tensor; while current scaling computes this scaling factor based on the amax of the current tensor. [ block_scaling ] suggested partitioning the tensor into blocks, and compute one scaling factor per block. Llama3 [ llama3 ] uses per-channel scaling that computes one scaling factor per row or column based on the dot product direction. DeepSeekV3 [ DeepSeekV3 ] combines both sub-channel scaling and per-block scaling into the recipe, and proposes using 1 √ó 128 1\times 128 sub-channel scaling for the activation tensor, and 128 √ó 128 128\times 128 per-block scaling for the weight tensor to achieve a balance between fine grained control and computational overhead. The micro-scaling format [ ocp_microscaling , microscaling , asit_mxfp8 ] suggested using a fine-grained 1 √ó 32 1\times 32 sub-channel block to compute the scaling factor. The NVFP4 format [ nvfp4 ] suggested using a fine-grained 1 √ó 16 1\times 16 sub-channel block to compute the scaling factor. Another common strategy is to selectively apply FP8 quantization. [ sun2019hybrid , fp8_paulius ] used E4M3 format in the forward pass, and uses E5M2 format in the backward pass. [ coat ] used per-group scaling for the optimizer states, and per-tensor scaling for the activation tensors. [ fp4alltheway ] suggested using fp4 format for the majority of the training, but fall back to higher precision (FP32/BF16) at the late stage of training. [ nvidia_nvfp42025 ] applied nvfp4 for most of the layers, but left the first two and last eight layers in BF16, it also studied how reducing the number of BF16 layers affect model quality. All of these prior arts focus on selecting the right methods and determining their appropriate application point (location and time) to create a low-precision training recipe that preserves model quality. What we are trying to accomplish, however, is to start gaining insights into why a specific recipe can achieve model quality on par with the baseline. Therefore, we developed the MoR framework, which attempts to capture invariance through tensor analysis. In this paper, we also showed that relative error can serve as a reasonable invariance for FP8 training. While works like DeepSeekV3 [ DeepSeekV3 ] and MXFP8 [ asit_mxfp8 ] achieve good results because their fine-grained partition ensures that the quantization error is small for all tensors, we illustrated that using relative error as an invariance allows us to obtain on-par model quality with much coarser-grained partitions. The relative error invariance we use is conservative and serves as a sufficient, but not a necessary, condition. We plan to continue working in this direction to explore invariance metrics that are more efficient and can be used for more aggressive quantization formats (such as NVFP4). Our primary contributions are as follows: 1. We propose a new strategy for computing the scaling factor that ensures consistency across the entire tensor, regardless of the chosen scaling granularity. 2. We introduce a novel dynamic quantization framework that makes real-time decisions based on numerical properties at runtime, allowing the model to adapt precision throughout the training process. 3. Using this framework, we develop and evaluate tensor-level and sub-tensor-level Mixture-of-Representations (MoR) recipes that dynamically select FP8 and BF16 types, achieving substantial efficiency without compromising quality. 2 Group Amax Mantissa Scaling A major challenge for low-precision quantization is that it requires mapping a tensor‚Äôs wide dynamic range to the limited representable range of the target format, such as FP8. For example, the E4M3 FP8 format can only represent positive values between 2 ‚àí 9 2^{-9} and 448; and the E5M2 FP8 format can only represent positive values between 2 ‚àí 16 2^{-16} and 57,344. Quantizing values outside this range leads to information loss through either saturation (clipping large values) or underflow (flushing small values to zero). An effective strategy is to compute a scaling factor that shifts the tensor‚Äôs values into the representable range, minimizing these quantization errors and preserving numerical fidelity. The prevailing hypothesis in quantization is that large-magnitude values are more critical to preserve than small ones. Consequently, the standard scaling strategy, used in all prior arts [ fp8_paulius , delayed_scaling , coat , block_scaling , llama3 , DeepSeekV3 , microscaling ] , is to compute a scaling factor that maps the absolute maximum value (amax) of a tensor to the maximum representable value of the quantization format. This prioritizes avoiding saturation at the cost of flushing smaller values to zero. Beyond the strategy, the numerical format of the scaling factor itself introduces a critical trade-off. Using a high-precision format like FP32 preserves the amax value the best but adds significant metadata overhead. To reduce this cost, prior work has explored lower-precision formats. The micro-scaling format [ microscaling ] uses E8M0, which offers a wide dynamic range for the scaling factor, while NVFP4 [ nvfp4 ] suggests E4M3, which represents the amax with higher accuracy but has a more limited range. These approaches present a fundamental conflict: one must choose between the wider range of E8M0 and the higher precision of E4M3, or falling back to FP32 with higher memory cost. To resolve this trade-off, we propose the Group Amax Mantissa (GAM) scaling algorithm . The goal of GAM is to combine the respective strengths of prior approaches: to design a scaling factor representation that achieves the wide dynamic range of an E8M0-like exponent while preserving the tensor‚Äôs maximum value with the high precision characteristic of an FP32 mantissa. The core idea of GAM, detailed in Algorithm 1 , is to decouple the mantissa and exponent of the scaling factors. We partition a tensor ùêó \mathbf{X} into blocks b b , which are organized into groups g g . For each group, we find the group-level amax ( g a ‚Äã m ‚Äã a ‚Äã x g_{amax} ) and compute an ideal FP32 scaling factor s g s_{g} . We then store only the mantissa ( m g m_{g} ) of this group-level scaling factor. Subsequently, for each block b b within that group, we compute its local amax ( b a ‚Äã m ‚Äã a ‚Äã x b_{amax} ) and its corresponding ideal FP32 scaling factor s b s_{b} . From s b s_{b} , we extract only the exponent. A crucial rounding step adjusts this block exponent downward if the block‚Äôs mantissa ( m b m_{b} ) is smaller than the group‚Äôs shared mantissa ( m g m_{g} ). This is to prevent saturation from happening. To apply scaling, the per-block FP32 scaling factor is reconstructed on-the-fly by combining the shared 23-bit group mantissa m g m_{g} with the stored 8-bit per-block exponent. While the group concept is generic, there is a trade-off between overhead and quantization error. Smaller groups will have more overhead but with smaller quantization error, while larger groups will have less overhead but with larger quantization error. In our experiments, we use a single group for the entire tensor, which is sufficient for the FP8 format as shown in our experiments in Section 4.1 . This configuration offers three primary benefits: 1. Negligible Overhead: The storage cost is minimal. Each block requires an 8-bit exponent, and the entire tensor only needs one additional 23-bit mantissa. 2. Maximum Precision: The absolute maximum value of the entire tensor is used to derive the mantissa, preserving it with full FP32 precision. 3. Consistent Mantissa Operations: During scaling or de-scaling steps, the mantissa component of the reconstructed scaling factor is identical for all values. This makes the mantissa operations orthogonal to the block sizes. On the other hand, the exponent value of the scaling factor still depends on the block size and is subject to change based on the block amax value. Input: A tensor ùêó \mathbf{X} ; a partition G = { g i } G=\{g_{i}\} over the elements of ùêó \mathbf{X} ; for each group g ‚àà G g\in G , a partition B g = { b j } B_{g}=\{b_{j}\} of g g into blocks, quantization type amax q a ‚Äã m ‚Äã a ‚Äã x q_{amax} Output: The union of all group mantissas M M and all block scale factors in E8M0 S S foreach group g g in G G do g a ‚Äã m ‚Äã a ‚Äã x ‚Üê m ‚Äã a ‚Äã x ‚Äã ( a ‚Äã b ‚Äã s ‚Äã ( g ) ) g_{amax}\leftarrow max(abs(g)) ; s g ‚Üê q a ‚Äã m ‚Äã a ‚Äã x / g a ‚Äã m ‚Äã a ‚Äã x s_{g}\leftarrow q_{amax}/g_{amax} ; m g ‚Üê m ‚Äã a ‚Äã n ‚Äã t ‚Äã i ‚Äã s ‚Äã s ‚Äã a ‚Äã ( s g ) m_{g}\leftarrow mantissa(s_{g}) ; foreach block b b in B g B_{g} do b a ‚Äã m ‚Äã a ‚Äã x ‚Üê m ‚Äã a ‚Äã x ‚Äã ( a ‚Äã b ‚Äã s ‚Äã ( b ) ) b_{amax}\leftarrow max(abs(b)) ; s b ‚Üê q a ‚Äã m ‚Äã a ‚Äã x / b a ‚Äã m ‚Äã a ‚Äã x s_{b}\leftarrow q_{amax}/b_{amax} ; m b ‚Üê m ‚Äã a ‚Äã n ‚Äã t ‚Äã i ‚Äã s ‚Äã s ‚Äã a ‚Äã ( s b ) m_{b}\leftarrow mantissa(s_{b}) ; /* Preventing saturation from happening by rounding down the exponent when m g m b m_{g} m_{b} */ if m g = m b m_{g} =m_{b} then s b ‚Üê e ‚Äã x ‚Äã p ‚Äã o ‚Äã n ‚Äã e ‚Äã n ‚Äã t ‚Äã ( s b ) s_{b}\leftarrow exponent(s_{b}) ; else s b ‚Üê e ‚Äã x ‚Äã p ‚Äã o ‚Äã n ‚Äã e ‚Äã n ‚Äã t ‚Äã ( s b ) ‚àí 1 s_{b}\leftarrow exponent(s_{b})-1 ; return M ‚Üê ‚ãÉ g ‚àà G { m g } M\leftarrow\bigcup_{g\in G}\{m_{g}\} , S ‚Üê ‚ãÉ b ‚àà B { s b } S\leftarrow\bigcup_{b\in B}\{s_{b}\} Algorithm 1 Group Amax Mantissa Scaling 3 The MoR Framework The Mixture-of-Representations (MoR) framework, outlined in Algorithm 2 , provides a systematic approach for dynamically selecting quantization strategies. Given an input tensor, the framework first partitions it into a set of blocks B B according to a chosen quantization granularity. For per-tensor quantization, B B contains a single block (the entire tensor); for 2D block quantization, B B is the set of all 2D blocks; for per-channel quantization, B B consists of the corresponding rows or columns based on the dot product dimension; for sub-channel quantization, B B is the union of all sub-channel sub-rows or sub-columns. The framework operates by iterating through an ordered list of target quantization types, ranked from most aggressive(e.g. E4M3) 