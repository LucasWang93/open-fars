Title: GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping

Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B.

Body: GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping 1 Introduction 2 Background 2.1 LLM Training Preliminaries 2.2 SSD-Offloaded Training 3 SSD-Offloaded Training Scheduling 3.1 Roofline Model 3.2 Single Forward-Backward Schedule 3.3 Horizontal Gradient Accumulation 3.4 Proposed: Vertical Gradient Accumulation 4 Pipelined Vertical Scheduling 4.1 Problems and Principles 4.2 Forward Pass 4.3 Overlap Backward with Optimizer Step 4.4 Overlap Optimizer Step with Forward 4.5 LP-based Configuration Search 5 System Implementation 6 Evaluation 6.1 Evaluation Setup 6.2 End-to-End Throughput Comparison 6.3 Benefits of Delayed Optimizer Step 6.4 Benefits of Vertical Scheduling 6.5 Training Accuracy 7 Conclusion GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping Yishu Yin Xuehai Qian Tsinghua University Corresponding author: Xuehai Qian. Abstract SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling , which executes all micro-batches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO‑Infinity: 1.96× on 1 GPU and 1.93× on 4 GPUs for GPT‑65B, and 2.53× on 1 GPU for GPT‑175B. 1 Introduction Large Language Models (LLMs) have attracted widespread attention across diverse domains [ 31 , 5 , 7 ] . Scaling laws [ 16 ] show that LLM performance improves with increased model parameters, training data, and compute. Motivated by this trend, recent open-source LLMs such as gpt-oss [ 1 ] , Qwen [ 3 ] , and DeepSeek [ 10 ] often exceed 100 billion parameters. To train such large models cost-effectively, heterogeneous memory training has been proposed, where most of the training footprint resides outside GPU memory. Early systems [ 28 , 32 , 13 , 12 , 23 , 27 , 26 , 11 , 30 , 18 ] leverage CPU memory, and later designs, known as SSD-offloaded training [ 25 , 33 , 20 , 35 ] , go a step further by storing various types of training data across both CPU memory and SSD. Without relying solely on GPU memory to host all training data, this approach offers a promising solution for LLM training under GPU memory-constrained settings. In this paper, we rethink the fundamental challenges of SSD-offloaded training and propose novel techniques that significantly improve its efficiency compared to state-of-the-art systems. SSD-offloaded training typically offloads the large optimizer states to SSD and uses the host CPU to execute the optimizer step and update these states [ 25 , 20 ] . In this setting, the optimizer step becomes the primary bottleneck due to excessive I/O: optimizer states must be loaded from SSD for updates and written back afterward, saturating the limited host-SSD bandwidth (typically a few GB/s). Jang et al. [ 15 ] report that this overhead can reach 80% when training GPT-2. To facilitate data movement and reduce storage access time, prior works [ 20 , 35 ] suggest deploying up to 12 SSDs or using two 4-SSD RAID0 arrays to increase host-storage bandwidth, or leveraging in-storage computing [ 15 ] to reduce storage traffic. However, these approaches require additional or specialized hardware. In practice, conventional servers are rarely provisioned with so many SSDs to scale bandwidth, making these solutions unlikely to see widespread adoption. Figure 1 : Gradient Accumulation: From Horizontal to Vertical. The swapping traffic is only plotted for a subset of layers. To tackle the optimizer step bottleneck, a common practice is to use larger batch sizes so that the optimizer step overhead—independent of batch size—can be amortized over an entire training iteration. Some prior works [ 2 , 11 , 20 ] focus on increasing the batch size within a single forward-backward pass. However, the maximum batch size is ultimately capped by the peak memory usage of the largest operator in the model, e.g., matrix multiplications in feed-forward networks. More importantly, increasing batch size causes faster growth in the data traffic for transferring activation checkpoints required by recomputation [ 6 ] , a widely used memory-saving technique. This technique creates activation checkpoints periodically—typically at layer boundaries—during the forward pass and reconstructs intermediate activations between checkpoints in the backward pass. A larger batch size increases not only the size of each checkpoint, but also the frequency of checkpoints, because the amount of recovered activations between checkpoints is bounded by GPU memory capacity. These larger and more frequent checkpoints lead to superlinear growth in the overhead of checkpoint swapping, quickly wiping out the benefits of increasing batch size. A fundamentally better approach to scaling batch size is gradient accumulation adopted by ZeRO-Infinity [ 25 ] , which computes and aggregates gradients from multiple micro-batches within each iteration, scaling the effective batch size by increasing the number of micro-batches. Specifically, consecutive micro-batches of an iteration are executed one by one, and the gradients produced by each micro-batch are accumulated. However, gradient accumulation comes with its own overhead. We analyze the problem from two aspects that directly affect training performance: (1) the amount of data movement; and (2) the ability of overlapping optimizer steps with GPU computation. Consider an N N -layer LLM with total model size m ​ s ms . Each training iteration processes M M micro-batches under gradient accumulation, where the aggregated activation checkpoint size per micro-batch is c ​ s cs . Current systems perform gradient accumulation with a horizontal schedule that executes micro-batches sequentially. This horizontal schedule incurs data movement between the GPU and the lower memory hierarchy from three sources: (1) loading model parameters with total size 2 × M × m ​ s 2\times M\times ms , i.e., parameters are loaded for both the forward pass and the backward pass with recomputation for each micro-batch; (2) writing activation checkpoints in the forward pass and reading them in the backward pass, with total size 2 × M × c ​ s 2\times M\times cs ; and (3) loading, updating, and writing back gradients of all parameters across ( M − 1 ) (M-1) micro-batches, leading to a total of [ 2 ​ ( M − 1 ) + 1 ] × 2 ​ m ​ s = ( 2 ​ M − 1 ) × 2 ​ m ​ s [2(M-1)+1]\times 2ms=(2M-1)\times 2ms 1 1 1 We assume mixed-precision training [ 21 ] . Gradient accumulation is typically kept in full precision, so the buffer size is twice the parameter size. The first micro-batch only offloads gradients; the remaining ( M − 1 ) (M-1) micro-batches fetch and offload gradients. . In this setting, the optimizer step can be overlapped with the backward pass of the last micro-batch for ( N − 1 ) (N-1) layers. This paper reexamines the computation schedule for SSD-offloaded training and proposes GreedySnake , a system based on the vertical schedule that executes each layer’s forward or backward computations across all micro-batches before advancing to the next layer. GreedySnake explores a key tradeoff between reduced model-parameter loading and increased activation checkpoint reads and writes. This elegant yet simple design naturally reduces overall data movement and substantially increases the amount of GPU computation that can be overlapped with the optimizer step. On one side, the vertical schedule enables better reuse of loaded parameters and reduces parameter loading traffic from 2 × M × m ​ s 2\times M\times ms to 2 ​ m ​ s 2ms . The traffic for moving gradients is similarly reduced to 2 ​ m ​ s 2ms : parameter gradients from all micro-batches are accumulated locally in GPU memory, and the fully accumulated gradients for each layer are transferred to CPU memory only once . On the other hand, GreedySnake increases activation checkpoint traffic. Because execution switches across micro-batches at each layer, the intermediate activations of a given micro-batch are no longer kept in GPU memory when its next layer is executed; instead, they must be saved as checkpoints and later reloaded to continue the forward pass. A similar issue arises in the backward pass, where the inter-layer gradients are likewise unavailable in GPU memory when a micro-batch resumes. The key insight of GreedySnake is that parameter size scales quadratically with the model hidden dimension, whereas checkpoint size scales linearly (see Section 3.4 ). In other words, improving parameter reuse is more critical. Naturally, vertical scheduling substantially increases the opportunity to overlap the optimizer step with the backward pass. Specifically, the overlapped computation becomes approximately M × ( N − 1 ) M\times(N-1) layers of backward passes across all micro-batches , up from only ( N − 1 ) (N-1) layers of the backward pass for the last micro-batch . Taking a step further, GreedySnake also overlaps part of the optimizer step with the forward pass of the next iteration, as long as each layer’s parameters are updated before that layer executes. By combining these two techniques, our system can ideally overlap the optimizer step with almost all GPU computation across all micro-batches, providing unprecedented tolerance to long optimizer step execution. Figure 1 compares the horizontal and vertical schedules and illustrates the benefits of the latter. Following the roofline model of SSD-offloaded training (Figure 3 ), as batch size increases, training throughput initially grows under the I/O roofline and then saturates below the computation roofline. Our goal is to (i) maximize this saturated throughput and (ii) minimize the batch size required to reach it. Doing so improves algorithmic flexibility: users need not inflate batch size solely due to system constraints. In GreedySnake, vertical scheduling increases the highest achievable throughput, while aggressive overlap reduces the batch size required to attain it. However, both the saturation point and the batch size at which it is reached depend on (1) how training data are distributed across GPU memory, CPU memory, and SSD, and (2) the percentage of the optimizer step to be overlapped with the forward pass of the next iteration. We formulate a linear program to automatically search this large configuration space and identify the best data-distribution and overlap ratios. We implement GreedySnake in approximately 5K lines of Python (excluding comments and blank lines) on top of PyTorch [ 22 ] to manage computation and tensor storage, and we reuse the asyncio -based pipeline and the cpu_adam module from ZeRO-Infinity [ 25 ] . Our evaluation compares GreedySnake against ZeRO-Infinity and several other recent SSD-offloaded training systems. We run experiments on two GPU clusters and train GPT-30B, GPT-65B, and GPT-175B. The results show that GreedySnake substantially improves the saturated training throughput over all baselines. 2 Background 2.1 LLM Training Preliminaries Vanilla LLM training. Vanilla LLM training proceeds in three stages: forward pass, backward pass, and optimizer step . During the forward pass, the input is propagated layer by layer, going through each layer’s parameters to generate activations. The activations produced by the last layer are used to compute the loss, while intermediate activations are retained for gradient computation. In the backward pass, the loss signal is propagated reversely through all layers, combining with stored activations produced in the forward stage to compute gradients for each parameter. Finally, in the optimizer step, parameters are updated element‑wise using the parameter gradients together with optimizer states, producing updated parameters and optimizer states for the next iteration. Pipelined optimizer step. A straightforward optimization for vanilla LLM training is to pipeline the backward and optimizer‑step stages. The backward pass processes all layers in reverse order, and a layer’s corresponding optimizer step can be executed immediately after its backward pass. This technique reduces memory footprint because the gradients of those layers can be released right after their updates are applied. In addition, this pipeline is the foundation to achieve the overlapped execution of backward pass and optimizer step. Note that gradient clipping is often required for training stability [ 29 ] , which necessitates computing the global L2-norm over all gradients and therefore forces the optimizer to wait until the entire backward pass completes. Recent work breaks this dependency via speculative optimizer steps [ 18 ] , exploiting the observation that gradient clipping rarely modifies gradients in practice. Mixed precision training. It accelerates forward and backward computation by leveraging lower‑precision arithmetic [ 21 ] , a common practice adopted in industrial training pipelines [ 29 , 10 ] . During the forward pass, inputs and full‑precision parameters (termed master parameters) are cast into lower‑precision formats (commonly FP16 or BF16, with recent exploration of FP8 and other quantized variants) as they propagate through the layers. To preserve numerical stability, selected values such as the loss and certain intermediate activations are retained in full precision. During the backward pass, gradients are computed in reduced precision, while critical accumulations are promoted to full precision. Finally, in the optimizer step, gradients are scaled and combined with higher-precision optimizer states (typically FP32, though BF16 has recently been explored) to update the master parameters and optimizer states. Activation checkpointing. This memory-saving technique trades additional computation for reduced storage of intermediate activations [ 6 ] . Rather than retaining all activations from the forward pass, it saves only a subset as checkpoints and discards the rest. During backpropagation, before computing gradients for layers between two consecutive checkpoints, the system recomputes the missing activations by replaying the forward pass starting from the nearest checkpoint, and then runs the backward pass using the reconstructed activations. This enables training deeper models or larger batch sizes within the same memory budget. Gradient accumulation. Designed to reduce the effective memory fo