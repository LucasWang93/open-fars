Title: MediX-R1: Open Ended Medical Reinforcement Learning

Abstract: We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only sim51K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com

Body: MediX-R1: Open Ended Medical Reinforcement Learning Report GitHub Issue √ó Title: Content selection saved. Describe the issue below: Description: Submit without GitHub Submit in GitHub Back to arXiv Why HTML? Report Issue Back to Abstract Download PDF Abstract 1 Introduction 2 Open Ended Medical RL 2.1 Group-based RL with Composite Rewards 2.2 Reward Design 3 Evaluation Framework 4 Experiments and Results 4.1 State-of-the-art Comparisons 4.2 Ablation Experiments 4.3 Reward Hacking and Mitigation 4.4 Human Expert Evaluation 4.5 Evaluation on Real World Clinical Data 4.6 Qualitative Examples 5 Conclusion References A Appendix A.1 Training Data and Modality Distribution A.2 Training Configuration A.3 Reward Coefficient Selection Details Fixed format budget. Single-signal ablations. Combining semantic rewards ( R llm R_{\text{llm}} + R embed R_{\text{embed}} ). Adding modality grounding. Compute limitations. A.4 Reward Function Source Code A.5 Human Expert Comparative Evaluation Protocol A.6 Human Evaluation: Model Reasoning A.7 Reinforcement Learning Training Prompt A.8 Evaluation BASE Template (Short-Form QA/MCQ) A.9 Evaluation Template for Report Generation License: CC BY-NC-SA 4.0 arXiv:2602.23363v1[cs.CV] 26 Feb 2026 MediX-R1: Open Ended Medical Reinforcement Learning Sahal Shaji Mullappilly Mohammed Irfan Kurpath Omair Mohamed Mohamed Zidan Fahad Khan Salman Khan Rao Anwer Hisham Cholakkal Abstract We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only ‚àº 51 \sim 51 K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models , curated datasets and source code are available at MediX-R1 . Medical RL, Open-ended RL, GRPO, GSPO, Medical VLM \undefine@key newfloatplacement \undefine@key newfloatname \undefine@key newfloatfileext \undefine@key newfloatwithin 1 Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI) 2 Jubilee Mission Medical College and Research Institute, 3 JJM Medical College Figure 1 : Average accuracy across multimodal medical benchmarks vs. training dataset size for recent medical VLMs. Colors denote model families; marker shape/size indicates parameter scale ‚àº \sim (2B, 8B, 30B). √ó denote open-source availability of training data (*as of 25/02/2026) . MediX-R1 8B (68.8%) surpasses MedGemma 27B (68.4%) while using significantly less training data, and MediX-R1 30B achieves the highest overall accuracy (73.6%). All training and evaluation resources are available at MediX-R1 . Model Diverse Medical Single-Stage Interpretable Open-Ended Annotation-Free Composite Modalities RL Reasoning Responses Reasoning RL Reward MedVLM-R1 ( pan2025medvlm ) ‚úó ‚úì ‚úì ‚úó ‚úì ‚úó BiMediX2 ( mullappilly2024bimedix2biomedicalexpertlmm ) ‚úì ‚úó ‚úó ‚úì ‚úó ‚úó HuatuoGPT-V ( chen2024huatuogptvisioninjectingmedicalvisual ) ‚úì ‚úó ‚úó ‚úì ‚úó ‚úó MedGemma ( sellergren2025medgemma ) ‚úì ‚úó ‚úì ‚úì ‚úó ‚úó MedMO ( deria2026medmogroundingunderstandingmultimodal ) ‚úì ‚úó ‚úì ‚úì ‚úó ‚úó MediX-R1 ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì Table 1 : Differences with existing Medical VLMs: MediX-R1 integrates diverse modalities, interpretable reasoning, and composite RL rewards, enabling practical clinical use. ‚Ä† ‚Ä† footnotetext: * Equal contribution. 1 Introduction Large medical language and vision-language models are increasingly deployed for clinical question answering, triage support, report drafting, and education ( chen2024huatuogpto1medicalcomplexreasoning ; sellergren2025medgemma ; pieri-etal-2024-bimedix ) . Many of these tasks are inherently open-ended: clinicians expect concise but free-form answers that can flexibly incorporate context, uncertainty, and multimodal evidence. However, most training and evaluation pipelines remain tailored to Multiple Choice Questions (MCQ) or string-matching regimes, which (i) under-reward valid clinical paraphrases, (ii) fail to measure reasoning quality or modality recognition, and (iii) do not provide reliable signals for reinforcement learning (RL) in open-ended settings. As a result, models trained only with supervised objectives or MCQ-style rewards often struggle to produce faithful, interpretable, and robust clinical responses across diverse modalities. RL has improved reasoning in domains with verifiable rewards (e.g., math and code) as shown by DeepSeek models ( shao2024deepseekmath ; guo2025deepseek ) , but medical tasks rarely admit executable checks. Binary exact match is too brittle for clinical phrasing; BLEU/ROUGE can mis-score semantically correct answers; and free-form VLM outputs complicate visual inference. Moreover, using a single reward signal can induce instability or reward hacking, especially when the signal is noisy or overly permissive. Hence, it is desirable to have a principled approach for training medical MLLMs with open-ended RL that integrates semantic correctness with structural and modality constraints, while remaining data- and compute-efficient. We present MediX-R1, an open-ended medical RL framework that fine-tunes a baseline multimodal backbone with Group Based RL (GRPO/GSPO/DAPO) using a composite reward tailored for clinical reasoning. Our design combines: (1) an LLM-based accuracy reward that enforces a strict YES/NO decision on semantic correctness, (2) a medical embedding-based semantic reward that captures paraphrases and terminology variants, (3) a lightweight format reward that elicits interpretable reasoning traces, and (4) a modality recognition reward that discourages cross-modality hallucinations by requiring explicit modality tags. This multi-signal objective stabilizes optimization and supplies informative feedback where traditional verifiable or MCQ-only rewards fall short, enabling single-stage, open-ended RL directly on clinical tasks. Figure 2 : MediX-R1: Overall Architecture The MediX-R1 reinforcement learning framework for open-ended medical reasoning. An input of a medical image and a natural language question is processed by MediX-R1. The model‚Äôs policy is trained using Group Based RL, which leverages a multi-faceted reward signal. This reward is composed of: a) an LLM-based reward for evaluating the overall quality and correctness of the output; b) an embedding-based reward to ensure semantic alignment; c) a format reward to enforce the desired output structure ( think and answer blocks); and d) a modality reward to ensure the response is grounded in the specified imaging modality. This reward-guided approach encourages the model to generate accurate and interpretable reasoning paths. Differences with existing Medical VLMs: Table 1 contrasts MediX-R1 with strong open models across key clinical capabilities. First, on Diverse Medical Modalities , MediX-R1 supports diverse medical modalities including X-Ray, CT, MRI, Microscopy/Histopathology, Ultrasound, Fluoroscopy, Endoscopy, Angiography, Mammography, Clinical Photography, SPECT (Single Photon Emission Computed Tomography), OCT (Optical Coherence Tomography), and Fundus imaging, whereas MedVLM-R1 ( pan2025medvlm ) is limited to radiology images. Models like MedGemma ( sellergren2025medgemma ) , HuatuoGPT-Vision ( chen2024huatuogptvisioninjectingmedicalvisual ) , MedMO ( deria2026medmogroundingunderstandingmultimodal ) , and BiMediX2 ( mullappilly2024bimedix2biomedicalexpertlmm ) provide coverage on clinical modalities but they require extensive multi-stage training. On Single-Stage RL , most baselines rely on multi-stage pipelines (pretraining ‚Üí \rightarrow SFT ‚Üí \rightarrow RL), whereas MediX-R1 is trained end-to-end with a single RL stage using our composite reward (Sec. 2.2 ). This simplifies training and, importantly, enables open-ended RL directly (unlike MedVLM-R1), because the Reference-based LLM-as-judge accuracy signal and medical embeddings provide reliable feedback beyond MCQ exact match. The composite design (format + LLM judge + embeddings + modality recognition) stabilizes optimization and reduces reward hacking (Fig. 7 ), translating into the best average performance in Table 2 . For Interpretable Reasoning , MediX-R1 emits explicit reasoning traces enclosed in think ‚Ä¶ /think , enforced by a format reward, making the decision path auditable. Several baselines do not reliably produce structured clinical rationales. While multiple models support Open-Ended Responses , MediX-R1 is explicitly optimized for free-form clinical answering with modality recognition, which curbs cross-modality hallucinations and improves VLM robustness. Finally, MediX-R1 achieves Annotation-Free Reasoning : it does not require human-curated rationales or verified chain-of-thought. The RL rewards operate on the final answer only (via Reference based LLM judge and embeddings), significantly lowering data curation cost while still encouraging faithful, interpretable reasoning. Together, these properties explain the consistent gains across both text-only and image+text benchmarks and the practical advantages of MediX-R1 for clinical use. To measure progress, we introduce a unified, 3-stage Reference-based LLM-as-judge evaluation framework that supports both text-only and image+text tasks under a common protocol. By replacing brittle string-overlap metrics with instruction-tuned judges served via vLLM ( kwon2023efficient ) , our evaluation captures semantic correctness, reasoning adequacy, and contextual alignment, and scales from short-form QA to long-form report generation. This reduces evaluation-clinical utility mismatch. Despite using only ‚àº \sim 51K instruction examples, MediX-R1 achieves strong results across diverse medical benchmarks. We find that composite rewards not only improve accuracy but also mitigate reward hacking and reduce volatility, yielding stable training and interpretable outputs. Compared to open-source medical models (e.g., BiMediX2, MedGemma, HuatuoGPT-V, MedVLM-R1, MedMO), MediX-R1 combines broad modality coverage with single-stage RL and structured reasoning. Contributions: (i) We introduce open-ended medical reinforcement learning by extending Group based RL with tailored rewards for clinical reasoning. (ii) We design a composite reward with LLM-based accuracy and medical semantic signals that for the first time enables open-ended responses with RL in the medical domain and stabilizes training. (iii) We propose a three-stage Reference-based LLM-as-judge evaluation framework that unifies benchmarking for both LLM (text-only) and VLM (image+text) tasks in the medical setting. (iv) MediX-R1 achieves excellent LLM and VLM results with a single-stage RL recipe using ‚àº \sim 51K instructions , validated through both Reference-based LLM-as-judge and human expert evaluations. (v) Finally, we demonstrate the effectiveness of the proposed composite reward on Group based RL algorithms, achieving consistent performance gains with GRPO ( shao2024deepseekmath ) , DAPO ( yu2025dapoopensourcellmreinforcement ) and GSPO ( zheng2025groupsequencepolicyoptimization ) . Moreover, we have conducted experiments on different baseline VLMs, including Qwen2.5-VL, Qwen3-VL ( qwen3technicalreport ) , and SmolVLM2 ( marafioti2025smolvlmredefiningsmallefficient ) , and achieved consistent performance gains across backbones. 2 Open Ended Medical RL MediX-R1 fine-tunes a baseline multimodal backbone for open-ended medical reasoning using Group Based RL. Given an image I I and question q q , the vision encoder produces visual tokens that are fused with text tokens and fed to the LLM policy œÄ Œ∏ \pi_{\theta} . The model generates structured outputs of the form: [modality tag] think free-form clinical reasoning /think answer final concise answer /answer \caption@setoptions figure \caption@setposition b \caption@setkeys [floatrow]floatrowcapbesideposition=right,top,capbesidewidth=0.26 \caption@setoptions floatbeside \caption@setoptions capbesidefloat \caption@setoptions figurebeside \caption@setoptions capbesidefigure \caption@setposition b \caption@setkeys [floatrow]floatrowcapbesideposition=right,top,capbesidewidth=0.26 \caption@setoptions floatbeside \caption@setoptions capbesidefloat \caption@setoptions figurebeside \caption@setoptions capbesidefigure \caption@setposition b \caption@setkeys [floatrow]floatrowcapbesideposition=right,top,capbesidewidth=0.26 \caption@setoptions floatbeside \caption@setoptions capbesidefloat \caption@setoptions figurebeside \caption@setoptions capbesidefigure \caption@setposition b Figure 5 : Evaluation Framework Our three-stage evaluation pipeline: (1) Generation via vLLM inference on the model under test, (2) Evaluation using Reference-based LLM-as-judge with BASE and MIMIC templates, and (3) Scoring through aggregation of judgment outputs. The framework supports both binary decisions for QA/MCQ tasks and rubric-based scoring for long-form reports, ensuring robust evaluation across diverse medical benchmarks 2.1 Group-based RL with Composite Rewards Setup: Given an input ùêØ \mathbf{v} (image I I + question q q ) drawn from P ‚Äã ( ùêï ) P(\mathbf{V}) , we sample a group of G G candidate completions { o i } i = 1 G \{o_{i}\}_{i=1}^{G} from the frozen behavior policy œÄ Œ∏ old ( ‚ãÖ ‚à£ ùêØ ) \pi_{\theta_{\text{old}}}(\cdot\mid\mathbf{v}) . Each completion receives a scalar reward r i r_{i} computed by our composite reward (Sec. 2.2 ). We then compute a standardized group-relative advantage: A i = r i ‚àí mean ‚Äã ( { r j } j = 1 G ) std ‚Äã ( { r j } j = 1 G ) . A_{i}\;=\;\frac{r_{i}-\mathrm{mean}(\{r_{j}\}_{j=1}^{G})}{\mathrm{std}(\{r_{j}\}_{j=1}^{G})}. This removes the need for a learned value function while preserving a stable relative learning signal within group. GRPO objective: GRPO ( shao2024deepseekmath ) updates œÄ Œ∏ \pi_{\theta} using PPO-style clipping on an importance ratio and a KL re