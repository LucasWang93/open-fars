Title: ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?

Abstract: We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model.

Body: ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads? 1 Introduction 2 Related Work Correctness-driven benchmarks: Efficiency-driven benchmarks: Coding Agent Architectures: LLM-as-a-Judge for Code Evaluation: 3 ISO-Bench 3.1 Task Formulation 3.2 Benchmark Construction Stage 1: Commit Extraction: Stage 2: Manual Curation: Stage 3: PR Analysis: 3.3 Evaluation Metrics 3.3.1 Hard Metrics 3.3.2 Soft Metrics 3.3.3 Quadrant Framework 3.3.4 Functional Correctness 4 Experimental Setup 4.1 Agent Scaffolding 4.2 Execution Environment 4.3 Evaluation Protocol 5 Results 5.1 Can Agents Optimize GPU Inference Code? 5.2 Do Hard Metrics Tell the Full Story? 5.3 Why Do Agents Fail? 5.4 Does Performance Generalize Across Codebases? 5.5 Does Agent Scaffolding Matter? 5.6 Do Optimizations Preserve Correctness? 6 Discussion 6.1 Why Do Open-Source Models Fail at Optimization? Failure to attempt the task Active interaction without task completion 6.2 Conclusion 6.3 Limitations and Future Work Dataset scope Contamination risk Soft metric reliability Hardware scope A Reproducibility Code. Data. License. B Task Collection B.1 Commit Filtering Pipeline B.2 Manual Curation C Task Setup and Agent Configurations C.1 Task Specification C.2 Execution Environment C.3 Agent Configurations C.4 Task Prompt D Case Studies D.1 Case Study: FlashAttention True Success (Q1) D.2 Case Study: Qwen3 Parser Good Intent (Q2) D.3 Case Study: Bamba-9B Accuracy Regression (Q3) D.4 Case Study: Prefix Caching Complete Failure (Q4) D.5 Case Study: MiniMax-M2.1 Planning Without Execution D.6 Case Study: GPT-OSS-120B Environment Confusion D.7 Case Study: GLM-4.7 Task Completion Failure ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads? Ayush Nangia Shikhar Mishra Aman Gokrani Paras Chopra {ayush.nangia, shikhar.mishra, paras}@lossfunk.com (February 2026) Abstract We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model. 1 Introduction LLM inference engines have become essential for deploying large language models at scale. Systems like vLLM [ 9 ] and SGLang [ 21 ] handle production workloads in industry and research, achieving high throughput through systems-level optimizations. Methods such as PagedAttention [ 9 ] and FlashAttention [ 4 ] required extensive work and in-depth knowledge in memory management, kernel development, and scheduling. The need for optimization is growing as more models are released and model architectures continuously evolve. LLM-based coding agents have become powerful tools for software engineering, capable of finding bugs and generating patches across codebases. Systems like SWE-Agent [ 19 ] and OpenHands [ 18 ] perform well on benchmarks like SWE-bench [ 8 ] . However, recent benchmarks suggest these agents still struggle with optimization tasks. KernelBench [ 14 ] finds that frontier models match GPU kernel baselines in under 20% of cases, GSO [ 15 ] reports success rates below 5% on repository-level tasks, and SWE-Perf [ 7 ] observes large gaps between agent and expert solutions. These benchmarks measure whether agents succeed, but not why they fail. GSO takes a step further by combining execution metrics with LLM-as-a-Judge, but a key question remains: when agents fail, do they misunderstand the problem, or do they understand it but struggle to implement the solution? In this work, we present ISO-Bench, a benchmark of 54 optimization tasks from vLLM and SGLang. Beyond measuring throughput gains, we evaluate whether agents target the correct bottleneck and use appropriate strategies. Our contributions are mentioned as follows: 1. Benchmarking Tasks : 54 optimization tasks extracted from real commits in vLLM and SGLang. Each task includes a repository snapshot, throughput and latency benchmarks, and correctness tests. 2. Dual evaluation framework : Hard and soft metrics are introduced that distinguish true successes from lucky wins, showing that traditional metrics might overestimate agent capabilities. 3. Behavioral insights : Identification of an understanding-execution gap as a primary failure mode and showing that agent performance varies substantially across codebases. 2 Related Work Figure 1 : ISO-Bench evaluation pipeline. Given a codebase and task description, a coding agent produces an optimization patch. We compare this patch against the human commit using hard metrics (TTFT, throughput) and soft metrics (bottleneck targeting, implementation approach). Hard metrics measure performance improvement; soft metrics assess whether the agent targeted the correct code. We review earlier works on evaluating and building LLM systems for code generation, focusing on: (i) benchmarks evolving from correctness to efficiency, (ii) coding agent architectures, and (iii) LLM-based evaluation methods. Correctness-driven benchmarks: Early benchmarks for code generation focused on measuring functional correctness for standalone functions. HumanEval [ 2 ] introduced 164 hand-crafted Python problems with unit tests and popularized the pass@ k k metric, which measures whether at least one of k k sampled solutions passes all tests. However, such function-level benchmarks evaluate code generation separate from the complexities of real software development. Repository-scale benchmarks are more realistic as agents must navigate the full codebase, find the relevant code, and edit multiple files. SWE-bench [ 8 ] constructs tasks from real GitHub issues in popular Python repositories, requiring agents to pass executable tests. SWE-bench Verified [ 3 ] filters for reproducible evaluation and has become a standard target for coding agents. Efficiency-driven benchmarks: Optimizing for performance is a different problem: agents must find the bottleneck, and success is measured by actual speedup rather than just correctness. At the kernel level, KernelBench [ 14 ] evaluates LLMs on generating efficient GPU kernels for 250 PyTorch ML workloads, introducing the fast_p metric to count solutions that are both correct and achieve greater than p × p\times speedup over a PyTorch baseline; even strong models succeed on fewer than 20% of tasks. Complementing this, TritonBench [ 10 ] offers two evaluation suites: TritonBench-G (GitHub-sourced operators) and TritonBench-T (PyTorch-aligned tasks), profiling Triton code against reference implementations and reporting both correctness and GPU efficiency. At the repository level, several recent benchmarks investigate if agents can optimize real-world codebases. SWE-Perf [ 7 ] constructs tasks from pull requests that improve performance, ensuring that patches are applied successfully, pass tests, and produce quantifiable speedups. GSO [ 15 ] evaluates agent patches against human expert commits rather than fixed thresholds, while SWE-fficiency [ 11 ] scales this methodology across a broader set of Python libraries, reporting how close agents come to expert-level improvements. Across all three repository-level benchmarks, agents consistently struggle to identify bottlenecks and understand low-level performance. Coding Agent Architectures: Recent work has shown that agent scaffolding significantly impacts software engineering performance. SWE-Agent [ 19 ] demonstrated that agent-computer interfaces are important for better code manipulation, while OpenHands [ 18 ] complements this by providing an open platform for designing and evaluating coding agents using standardized tools and benchmarks. TRAE-Agent [ 16 ] targets repository-level tasks by generating candidate patches, pruning them, and selecting a final solution. In contrast, commercial systems such as Claude Code [ 1 ] and Codex CLI [ 13 ] couple the model with proprietary scaffolding, which makes it difficult to separate gains from the underlying LLM versus the agent architecture. LLM-as-a-Judge for Code Evaluation: Using LLMs to evaluate code has gained traction as a scalable alternative to test-based evaluation. Zheng et al. [ 20 ] showed that strong LLM judges can match human preferences with over 80% agreement. ICE-Score [ 22 ] applies this to code by guiding LLMs through step-by-step assessment, while CodeJudge [ 17 ] enhances accuracy by asking LLMs to think about error categories before scoring. Despite their promises, LLM judges suffer from biases such as favoring lengthier outputs and preferring their own generations, while code evaluation brings additional biases like susceptibility to misleading remarks [ 12 ] . 3 ISO-Bench Existing benchmarks evaluate either standalone kernel generation (KernelBench, TritonBench) or general repository optimization (GSO, SWE-Perf, SWE-fficiency), but none target the specific challenges of GPU-based inference serving systems. ISO-Bench fills this gap by focusing on execution-based metrics for real-world, GPU-based inference optimization workloads. Figure 1 shows the end-to-end ISO-Bench evaluation pipeline, from task inputs and agent-generated patches to hard/soft metrics and correctness validation. In this section, we describe how tasks are formulated, how the benchmark was constructed, and how we evaluate agent performance. 3.1 Task Formulation Each ISO-Bench task presents an agent with two inputs: (i) the repository at a pre-optimization commit state (ii) a task description explaining which performance bottleneck to address without revealing the solution. The agent must produce an optimization patch that improves performance on the specified benchmark. We evaluate agent patches against human expert solutions from the original pull requests. 3.2 Benchmark Construction We collect optimization tasks from two production ML inference engines: vLLM [ 9 ] and SGLang [ 21 ] . We selected vLLM and SGLang primarily because they are widely used inference engines. This enables a realistic evaluation of agent performance on production-grade inference optimization tasks. Stage 1: Commit Extraction: We use a GSO-inspired pipeline to identify performance-related commits through keyword filtering (terms like optim , speed , latency , memory ) followed by LLM-based classification to only keep commits focused on GPU-based inference optimization (see Appendix B for filtering statistics). Stage 2: Manual Curation: We manually review each candidate commit to verify that (i) the optimization is reproducible (ii) the commit represents a genuine optimization rather than a refactoring or bug fix. This curation step filters out false positives from the automated pipeline. Stage 3: PR Analysis: For each filtered commit, we fetch the associated pull request to extract the benchmarking model, evaluation commands, and performance claims from the PR discussion. This metadata serves two purposes: it provides the benchmark commands we use during evaluation, and it helps manual curation by helping us verify whether a commit represents a legitimate optimization. This pipeline produces 54 benchmark instances : 39 from vLLM and 15 from SGLang, each with verified performance benchmarks, model specifications, and evaluation commands. 3.3 Evaluation Metrics We evaluate agent performance using two types of metrics. Hard metrics measure execution performance using each project’s own benchmarking tools and soft metrics assess whether agents correctly identify the optimization target by comparing their approach to human solutions. Combining both allows us to distinguish genuine optimization capability from accidental improvements. 3.3.1 Hard Metrics We measure agent optimizations using the same benchmarks that developers used in the original pull requests. We track Time to First Token (TTFT) and throughput, comparing agent performance against the human baseline and classifying results according to Table 1 : Δ TTFT = TTFT h − TTFT a TTFT h × 100 \Delta_{\text{TTFT}}=\frac{\text{TTFT}_{h}-\text{TTFT}_{a}}{\text{TTFT}_{h}}\times 100 (1) Δ throughput = Throughput a − Throughput h Throughput h × 100 \Delta_{\text{throughput}}=\frac{\text{Throughput}_{a}-\text{Throughput}_{h}}{\text{Throughput}_{h}}\times 100 (2) Table 1 : Classification of hard metrics based on performance delta. Category Criteria Beats Agent improves on human by 5 % 5\% Similar Agent within ± 5 % \pm 5\% of human Worse Agent degrades by 5 % 5\% Failed Patch causes benchmarking error We use a 5% threshold to account for measurement noise. For serving-based benchmarks, we measure latency using Δ TTFT \Delta_{\text{TTFT}} . When TTFT is not produced, we use Δ throughput \Delta_{\text{throughput}} for standalone benchmarks. 3.3.2 Soft Metrics Hard metrics alone cannot distinguish genuine optimization capability from accidental improvements. Agents may achieve performance gains through changes unrelated to the actual optimization target. To address this, we introduce soft metrics, which uses LLM-as-a-Judge (Gemini-3-Flash-Preview [ 6 ] ) to compare agent patches against human solutions. We consider two dimensions as shown in Table 2 . Table 2 : Soft metric categories. Bottleneck Targeting Same target Identical code locations as human Related target Same module or subsystem Different target Unrelated code areas No optimization No performance-relevant changes Implementation Approach Similar approach Same technique as human Valid alternative Different but sound Partial solution Subset of required changes Ineffective Fails to address bottleneck 3.3.3 Quadrant Framework We combine hard and soft metrics to classify each optimization attempt into one of four quadrants, as shown in Figure 2 . The framework maps hard metrics performance classification (Table 1 ) against soft metrics categories (Table 2 ): Figure 2 : Quadrant framework for evaluating optimization attempts. The horizontal axis shows performance (good: beats or similar; bad: worse or failed). The vertical axis shows whether the agent targeted the correct bottleneck (correct: same or related target; wrong: different target or no optimization). Q1 True Success: correct target, good performance. Q