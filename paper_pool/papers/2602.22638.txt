Title: MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios

Abstract: Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .

Body: MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios 1 Introduction 2 Related Work 2.1 Route Planning in Urban Computing 2.2 Tool-augmented Agent Benchmark 3 MobilityBench 3.1 Benchmark Construction 3.1.1 Episode-centric Formulation. 3.1.2 Data Collection and Task Taxonomy Construction 3.1.3 Ground-Truth Construction. 3.1.4 Deterministic Replay Sandbox. 3.1.5 Dataset Statistics. 3.2 Evaluation Protocol 3.2.1 Instruction Understanding 3.2.2 Planning 3.2.3 Tool Use 3.2.4 Decision Making 3.2.5 Efficiency 4 Experiments 4.2.2 Scenario Study 4.2.3 Model Study 5 Conclusion A Appendix A.1 MobilityBench Task Scenarios A.2 Sandbox Tools MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios Zhiheng Song Computer Network Information Center, Chinese Academy of Sciences AMAP, Alibaba Group Beijing China songzhiheng2004@gmail.com , Jingshuai Zhang AMAP, Alibaba Group Beijing China zhangjingshuai0@gamil.com , Chuan Qin Computer Network Information Center, Chinese Academy of Sciences Beijing China chuanqin0426@gmail.com , Chao Wang University of Science and Technology of China Heifei China wangchaoai@ustc.edu.cn , Chao Chen AMAP, Alibaba Group Beijing China cc201598@alibaba-inc.com , Longfei Xu AMAP, Alibaba Group Beijing China longfei.xl@alibaba-inc.com , Kaikui Liu AMAP, Alibaba Group Beijing China damon@alibaba-inc.com , Xiangxiang Chu AMAP, Alibaba Group Beijing China cxxgtxy@gmail.com and Hengshu Zhu Computer Network Information Center, Chinese Academy of Sciences Beijing China zhuhengshu@gmail.com (2026) Abstract. Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench , a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench . Large language models, route-planning agents, benchmarking † † doi: XXXXXXX.XXXXXXX † † journalyear: 2026 † † copyright: acmlicensed † † conference: Make sure to enter the correct conference title from your rights confirmation email; 1. Introduction Figure 1 . Overview of MobilityBench, a systematic benchmark for evaluating route-planning agents. The advance of large language models (LLMs) has catalyzed the emergence of tool-augmented agents, which integrate natural language reasoning with executable actions via external APIs ( Schick et al . , 2023 ; Patil et al . , 2024 ) . By grounding user intent in programmatic interactions with real-world services, such agents substantially broaden the range of tasks they can support, from simple information retrieval to complex decision-making workflows, such as web navigation ( Ma et al . , 2023 ; Shen et al . , 2025 ) , computer interaction ( Hu et al . , 2024 ; Lu et al . , 2025 ) , and route planning ( Chen et al . , 2024 ; Zhe et al . , 2025 ) . Among these agents, route-planning agents constitute a particularly challenging application domain, operating under diverse and dynamic real-world constraints that shape everyday human mobility ( Xie et al . , 2024 ; Chaudhuri et al . , 2025 ; Cheng et al . , 2025 ) . Real-world mobility requests extend far beyond simple point-to-point navigation ( Yu et al . , 2025 ) , often involving multiple, interacting constraints, such as user preferences (e.g., avoiding highways or minimizing transfers), ordered waypoints, modality-dependent conditions, and time-sensitive requirements. Addressing such demands requires agents to accurately interpret nuanced user instructions, invoke appropriate travel-related APIs, and generate executable itineraries with reliable cost estimates—including travel time, distance, and transfer counts—capabilities that remain difficult to evaluate systematically in realistic mobility settings. Recent benchmarks for evaluating the planning capabilities of LLMs and agents, such as TravelBench ( Cheng et al . , 2025 ) and TravelPlanner ( Xie et al . , 2024 ) , primarily focus on high-level itinerary generation and abstract constraint reasoning. As a result, they fall short of capturing the complexity of route planning for everyday human mobility, which requires fine-grained reasoning over large-scale, map-based environments and dynamically changing conditions. Meanwhile, systematically evaluating route-planning agents in real-world mobility scenarios still faces several fundamental challenges: (1) scalable scenario coverage , as evaluation must span route-planning problems of varying difficulty and combinations of constraints, ranging from simple point-to-point queries to complex multi-constraint requests; (2) non-determinism of live mapping APIs , whose responses vary over time due to traffic dynamics, service availability, and backend updates ( Yao et al . , 2022a ; Liu et al . , 2023 ) , thereby undermining reproducibility and fair comparison; (3) comprehensive and reliable evaluation , as effective assessment requires integrating multiple objective criteria beyond LLM-based subjective judging ( Zheng et al . , 2023 ) to verify API-call validity, constraint satisfaction, and factual grounding; and (4) extensible and reproducible evaluation toolkit , as rapid advances in LLM backbones and agent frameworks demand a lightweight, modular toolkit that supports easy deployment, scalable data expansion, and consistent evaluation across settings. To address these challenges, we introduce MobilityBench , a scalable benchmark for evaluating route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap, one of the largest map and navigation service providers in China, and is designed to reflect the diversity and complexity of everyday mobility needs while removing all personally identifiable information. It covers a broad spectrum of real-world route-planning intents, including point-to-point routing, customized multi-waypoint itineraries, and multimodal route planning that integrates driving, walking, cycling, and public transit. In addition, MobilityBench supports preference-aware navigation, such as avoiding highways or minimizing transfers, as well as mobility-related information access, including bus station details, bus line information, and road congestion status. The benchmark spans queries from over 350 cities worldwide and is designed to be easily extensible, enabling continuous expansion to new regions, scenarios, and intent types. Given the inherent non-determinism and reproducibility challenges of live mapping services, MobilityBench is built around a deterministic API-replay sandbox that enables reproducible, end-to-end evaluation of route-planning agents. During dataset construction, responses fromrouting and points-of-interest APIs are captured and cached through a standardized interface, effectively freezing traffic conditions and service states at the time of collection. During evaluation, all API calls issued by an agent are intercepted and resolved against the cached response store, ensuring that identical inputs consistently yield identical, verifiable outputs. By eliminating uncontrolled environmental variance introduced by live services, this sandbox-based design ensures that measured performance faithfully reflects an agent’s reasoning and tool-use capabilities rather than fluctuations in external systems. We further propose a multi-dimensional evaluation protocol that centers on outcome validity while providing complementary assessments of instruction understanding , planning , tool use , and efficiency . This protocol integrates multiple objective criteria to verify executable correctness, constraint satisfaction, and grounded API usage, enabling fine-grained and reliable assessment beyond surface-level plausibility. To facilitate reproducible research and rapid iteration, we publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench , supporting easy deployment, extensibility to new agent frameworks, and consistent comparison across models and settings. 2. Related Work 2.1. Route Planning in Urban Computing Route planning is a long-standing problem in urban computing, attracting sustained attention from both academia and industry due to its central role in large-scale transportation systems and location-based services. Early studies primarily focused on optimizing physical costs, such as distance or travel time, under a graph-theoretic setting. Classical shortest-path algorithms, including Dijkstra ( DlJKSTRA , 1959 ) and A* ( Hart et al . , 1968 ; Delling et al . , 2009 ) , were widely adopted to guarantee optimality while improving scalability in real-world road networks. These methods established the algorithmic foundations of modern navigation systems, but typically assume homogeneous objectives and well-defined cost functions. As mobility demands became increasingly diverse, subsequent research moved beyond single-objective optimization toward preference-aware route planning. These approaches incorporate user interests and contextual factors by integrating routing with recommendation models, such as INTSR ( Yan et al . , 2025 ) . Nevertheless, most existing methods rely on structured features or predefined preference spaces, which limit their ability to accommodate long-tail, ambiguous, or weakly specified requirements expressed in natural language. Recently, LLMs have been explored as a new interface for route planning, owing to their strong capability in understanding complex semantic instructions. However, prior work has shown that LLMs alone are unreliable for spatial reasoning and constrained optimization in geographic settings ( Huang et al . , 2024 ; Agrawal et al . , 2025 ) . To mitigate these limitations, hybrid frameworks have been proposed that couple LLMs with traditional planners, using LLMs for high-level decision guidance ( Meng et al . , 2024 ; Zeng et al . , 2025 ) or intent and constraint extraction ( Yuan et al . , 2025 ) . Further studies introduce hierarchical planning architectures ( Zhe et al . , 2025 ) and reinforcement learning–based optimization strategies ( Qu et al . , 2025 ; Ning et al . , 2025 ) to improve robustness under multiple objectives and constraints. In parallel, tool-augmented language agents have demonstrated strong capabilities in interacting with real-world systems and coordinating external tools for structured decision-making, making them a promising paradigm for route planning in real-world mobility scenarios. Existing travel planning agents, however, mainly focus on high-level itinerary generation and abstract constraint reasoning, without tightly integrating semantic intent understanding with low-level route optimization over real road networks. As a result, they fall short of capturing the complexity of route planning required for everyday human mobility. In this work, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios, to advance research in this area. 2.2. Tool-augmented Agent Benchmark Building on the emergence of tool-augmented agents enabled by LLMs, recent work has focused on evaluating agents’ ability to follow instructions and interact with external tools. For instance, ToolBench ( Qin et al . , 2023 ) constructs a large-scale benchmark over real-world APIs, requiring agents to perform sequential search and planning to complete complex instructions. τ \tau -bench ( Yao et al . , 2024 ) further emphasizes interactive evaluation by simulating user–agent interactions and measuring behavioral consistency across repeated trials. In contrast to these general-purpose evaluations, recent work in urban computing has proposed domain-specific benchmarks for agent evaluation. TravelPlanner introduces a benchmark for multi-day itinerary construction by integrating domain-specific tools such as flight and restaurant search, and evaluates agents under itinerary-level environmental, commonsense, and hard constraints ( Xie et al . , 2024 ) . TravelBench further extends this task to multi-turn dialogue scenarios, enabling the evaluation of agents’ ability to infer and refine users’ implicit preferences through interaction ( Cheng et al . , 2025 ) . Despite these advances, existing benchmarks primarily focus on high-level itinerary generation and abstract constraint satisfaction, and do not systematically evaluate agents’ ability to perform fine-grained route planning under mobility-specific constraints, such as preference-aware routing (e.g., avoiding highways or minimizing transfers), ordered waypoint requirements, modality-dependent conditions, and time-sensitive constraints. To address this gap, we introduce MobilityBench, a scalable benchmark designed to evaluate LLM-based route-planning agents in real-world mobility scenarios. Table 1 . Overview of task scenarios in MobilityBench, grouped by intent family. Intent Family Task Scenario Example Query Basic Information Retrieval POI Query Where is the gas station? \cellcolor gray!8Geolocation Query \cellcolor gray!8Where am I? Nearby Query Search for restaurants near Beijing Capital International Airport. \cellcolor gray!8Weather Query \cellcolor gray!8What is the weather like in Wuhan tomorrow? Traffic Info Query Is there a traffic jam on Chengdu Avenue right now? Route-Dependent Information Retrieval \cellcolor gray!8Route Property Query \cellcolor gray!8How far is it from Hefei to Huangshan? Arrival/Departure Time Query If I drive from my home to Capital International Airport now, when will I