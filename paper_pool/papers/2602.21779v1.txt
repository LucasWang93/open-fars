Title: Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models

Abstract: Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic Answer-Questioning (FAQ), a large-scale benchmark that formulates temporal deepfake analysis as a multiple-choice task. FAQ introduces a three-level hierarchy to progressively evaluate and equip VLMs with forensic capabilities: (1) Facial Perception, testing the ability to identify static visual artifacts; (2) Temporal Deepfake Grounding, requiring the localization of dynamic forgery artifacts across frames; and (3) Forensic Reasoning, challenging models to synthesize evidence for final authenticity verdicts. We evaluate a range of VLMs on FAQ and generate a corresponding instruction-tuning set, FAQ-IT. Extensive experiments show that models fine-tuned on FAQ-IT achieve advanced performance on both in-domain and cross-dataset detection benchmarks. Ablation studies further validate the impact of our key design choices, confirming that FAQ is the driving force behind the temporal reasoning capabilities of these VLMs.

Body: Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models 1 Introduction 2 Related Work 2.1 Traditional Deepfake Detection Datasets 2.2 VLMs for Multimedia Forensics 2.3 QA-based Deepfake Detection Datasets 3 FAQ Benchmark 3.1 Deepfake Video Curation 3.2 Preprocessing 3.3 Task Hierarchy in FAQ 3.4 QA Generation with Forged Trace 3.5 Human Verification 3.6 FAQ Statistics 4 Experiment 4.1 Results 4.2 Ablations 5 Conclusion 6 Data Filtering 7 Prompt Details 8 Modeling Details 9 Annotation Platform 10 Verification Platform Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models Zheyuan Gu 1,2‚àó Qingsong Zhao 1,3‚àó Yusong Wang 1‚àó Zhaohong Huang 1 Xinqi Li 2 Cheng Yuan 1 Jiaowei Shao 1 Chi Zhang 1 Xuelong Li 1‚Ä† 1 Institute of Artificial Intelligence, China Telecom (TeleAI) 2 Peking University 3 Fudan University Abstract Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic Answer-Questioning (FAQ), a large-scale benchmark that formulates temporal deepfake analysis as a multiple-choice task. FAQ introduces a three-level hierarchy to progressively evaluate and equip VLMs with forensic capabilities: (1) Facial Perception, testing the ability to identify static visual artifacts; (2) Temporal Deepfake Grounding, requiring the localization of dynamic forgery artifacts across frames; and (3) Forensic Reasoning, challenging models to synthesize evidence for final authenticity verdicts. We evaluate a range of VLMs on FAQ and generate a corresponding instruction-tuning set, FAQ-IT. Extensive experiments show that models fine-tuned on FAQ-IT achieve advanced performance on both in-domain and cross-dataset detection benchmarks. Ablation studies further validate the impact of our key design choices, confirming that FAQ is the driving force behind the temporal reasoning capabilities of these VLMs. * * footnotetext: Equal contribution. $\dagger$ $\dagger$ footnotetext: Corresponding author. 1 Introduction With the rapid rise of AIGC [ 18 , 34 , 16 , 17 , 27 ] , the creation of realistic deepfakes has become significantly easier, leading to growing social concerns about their potential risks [ 19 , 9 ] . Consequently, researchers have begun exploring Vision‚ÄìLanguage Models (VLMs) for deepfake detection, leveraging their strong visual understanding acquired through large-scale pre-training [ 1 , 45 ] and achieved a number of encouraging results. FakeRadar and DeepShield [ 23 , 3 ] adapt CLIP [ 33 ] for deepfake detection, with the former using contrastive learning and the latter employing anomaly detection to enhance generalization. To achieve better interpretability, some researchers have also employed larger VLM models for deepfake detection. SIDA [ 14 ] and FakeShield [ 43 ] constructed the SID-Set and MMTD-Set, respectively. Through training on these datasets, VLMs can localize and describe forgeries while demonstrating remarkable detection performance. These early successes demonstrate the potential of using VLMs for deepfake detection, suggesting that larger, more challenging training data is expected to further enhance this capability. Figure 1 : Illustration of our hierarchical benchmark. We therefore ask: Which VLM training paradigm is most effective for deepfake detection tasks? What constitutes the most promising training data? The mainstream approach is supervised fine-tuning (SFT) [ 31 ] with high-quality question answering (QA) datasets. Previous works [ 36 , 46 ] extract images from videos and generate annotations using limited question templates and predefined forged regions. We observed that data obtained from such annotations provides spatial discriminative information only, whereas temporal inconsistencies in deepfake videos have been widely adopted as an important signal in conventional deepfake detection methods [ 42 , 28 , 13 , 10 ] . We therefore speculate that VLMs trained solely on such data are unable to leverage the significant temporal cues. One remaining issue is how to effectively guide VLMs to discover and reason about temporal inconsistencies via constructing QA training data. In this work, we continue the vein of SFT-based learning, aiming to establish a comprehensive and robust baseline for the deepfake detection task. We introduce F orensic A nswer- Q uestioning (FAQ), a multiple-choice question (MCQ) benchmark designed to improve the performance of VLMs in deepfake detection. We extract clips from videos based on human annotations, build QA pairs through a reproducible automated process, and include carefully designed distractors in the available options. We construct hierarchical data, which gradually enhances the model‚Äôs reasoning capabilities for deepfake detection. Comparison with other deepfake benchmark in Table 1 . Specifically, questions in FAQ are formulated in three levels to guide VLMs for accurate deepfake detection. Facial perception (level 1) strengthens the model‚Äôs perception of fine-grained visual artifacts. Temporal deepfake grounding (level 2) improves its grounding ability for dynamic artifacts. Forensic reasoning (level 3) elevates its reasoning capabilities to reach an accurate conclusion. FAQ contains 33K QA pairs across approximately 4,500 manipulated videos. Our main contributions are: ‚Ä¢ To our best knowledge, FAQ is the first QA benchmark focused on temporal inconsistencies in deepfake videos. ‚Ä¢ We construct a comprehensive QA generation pipeline that leverages static human annotations to locate video segments exhibiting dynamic artifacts and then transform them into QA pairs. ‚Ä¢ Through extensive experiments, we demonstrate that converting temporal inconsistencies into QA pairs is a feasible VLM training paradigm for deepfake detection. Models trained on FAQ show performance gains on widely used detection benchmarks. Table 1 : Comparison between FAQ and other forgery detection benchmarks. H and A respectively indicate human and automatic annotation. Benchmark MCQ #Sample Annotation #Task Types Trace Image-based Benchmarks GenImage [ 47 ] - 2,681,167 A 1 - DDVQA [ 46 ] - 14,784 H 1 - VLFFD [ 36 ] - 25,663 A+H 1 - MMTD-Set [ 43 ] - 34,470 A 2 - SID-Set desc [ 14 ] - 3,000 A 2 - Video-based Benchmarks GenVideo [ 4 ] - 2,294,594 A 1 - Forensics-Bench [ 40 ] ‚úì 63,292 A+H 4 - FAQ ‚úì 33,000 A+H 7 ‚úì 2 Related Work 2.1 Traditional Deepfake Detection Datasets Early research on deepfake detection datasets focused on binary classification of manipulated and authentic videos. The most widely adopted benchmark is FaceForensics++ (FF++) [ 35 ] , which contains videos tampered by face-swapping and reenactment methods under different compression levels. To enhance realism and data diversity, Celeb-DF [ 22 ] employed an improved synthesis pipeline to generate visually convincing manipulations common in online media. Similarly, DeeperForensics [ 15 ] emphasized data diversity by applying extensive perturbations such as compression, blurring, and color distortion to simulate real-world degradations. Beyond curated studio-quality samples, WildDeepfake [ 48 ] directly collected videos from the Internet with uncontrolled variations in pose, lighting, and quality, challenging the generalization capability of detection methods. While these traditional datasets are instrumental for developing detection methods, they do not explicitly explain why or where a video is fake. As a result, models trained on such data often lack interpretability and reasoning capability, especially in cross-domain scenarios. 2.2 VLMs for Multimedia Forensics Recent advances in VLMs have inspired a new paradigm for multimedia forensics, where deepfake detection is no longer treated as a simple classification problem but as a multimodal reasoning task combining visual perception and textual interpretation [ 43 , 14 , 12 ] . FakeShield [ 43 ] introduced an explainable framework for image forgery detection and localization, which leverages VLMs to provide interpretable textual reasoning and pixel-level localization rather than binary predictions. Huang et al. [ 14 ] extended deepfake image detection to the social media scenario and proposed a unified model that detects, localizes, and explains potential manipulations in online content. Concurrently, M2F2-Det [ 12 ] examined the role of contrastive language-image pretraining in face forgery detection. This model conducts prompt-driven language reasoning based on CLIP representations to produce both authenticity judgments and detailed explanations. However, these methods [ 43 , 14 , 12 ] treat videos as a collection of static frames and largely neglect the temporal evidence on manipulations. In contrast, our method focuses on dynamic inconsistencies in the time domain and constructs temporal grounding and reasoning QA pairs to equip VLMs with the capabilities to discover this evidence. 2.3 QA-based Deepfake Detection Datasets The adoption of VLMs in deepfake detection has stimulated the development of QA datasets. DD-VQA [ 46 ] is the first QA-based face forgery dataset on human perceptual anomalies such as lighting, texture, and facial inconsistency, with the objective of producing interpretable analysis. VLFFD [ 36 ] further employs an automatic prompt generation pipeline to synthesize large-scale QA pairs of higher quality, enabling coarse-to-fine supervision for face forgery detection. Forensics-Bench [ 40 ] includes diverse tasks such as recognition, localization, and reasoning, with a focus on synthetic videos from generative models. Existing datasets are restricted to static images [ 46 , 36 ] or AI-generated content [ 40 ] . Conversely, our dataset guides VLMs to detect deepfake videos through curated three-layer QA pairs. 3 FAQ Benchmark We develop a benchmark to assess the performance of video-language models on the critical task of detecting video forgeries. Specifically, we focus on the temporal inconsistencies inherent in manipulated footage, a non-trivial yet underexplored dimension. FAQ is a large-scale MCQ benchmark that evaluates and enhances the capabilities of VLMs in perceiving, grounding, and reasoning about temporal artifacts. An overview of our benchmark construction pipeline is shown in Figure 2 . 3.1 Deepfake Video Curation Collection. We collect 5,000 deepfake videos and 1,000 authentic videos from FaceForensics++ (i.e., FF++ [ 35 ] ), a widely-used benchmark for facial forgery detection. FF++ provides diverse forgeries with high visual quality, yielding challenging and well-structured video samples. For each fake video, we augment the dataset with manual annotations, including video-level descriptions and precise spatiotemporal clicks ùíû = { c i | c i := ( x i , y i , t i ) } \mathcal{C}=\{c_{i}|c_{i}:=(x_{i},y_{i},t_{i})\} that pinpoint forgery artifacts, where c i c_{i} is the i i -th click, ( x i , y i ) (x_{i},y_{i}) are its spatial coordinates, and t i t_{i} is the timestamp. The fine-grained annotations allow us to design challenging forensic reasoning tasks that bridge low-level authenticity discrimination to high-level deepfake analysis. Filtering. For quality control, we deploy YOLOv8 [ 39 ] to detect faces in every frame of each fake video and filter out low-quality, easily identifiable samples. Specifically, we calculate the average, minimum, and maximum confidence scores of face detection across all frames for each video. A video is retained only if its average face confidence score exceeds 0.78 and its minimum confidence is above 0.71, ensuring consistent and high-quality facial presence throughout. This rigorous filtering process removes approximately 10% of unsatisfactory samples, yielding a refined set of over 4500 high-quality fake videos for subsequent use. Detailed statistics, including the sample distribution across confidence intervals, are reported in the Appendix. 3.2 Preprocessing For a systematic evaluation of video authenticity, we structure the raw annotations into three hierarchical tags, serving as the foundational ground truth for generating FAQ. Spatio-Temporal Clustering. We obtained a total of 14,392 remarkable forged video segments (average duration 2.1 seconds) utilizing over 50,000 sparse clicks provided by human annotators. Specifically, we aggregate the sparse clicks into manipulated segments that contain salient dynamic artifacts by grouping those that are spatiotemporally proximate. This intuitive clustering approach converts discrete annotations into coherent forgery segments. In view of this, we define a spatio-temporal adjacency function f ‚Äã ( c i , c j ) f(c_{i},c_{j}) that evaluates to true when two clicks c i c_{i} and c j c_{j} are within a spatial threshold œÑ s \tau_{s} and a temporal threshold œÑ t \tau_{t} , indicating that they belong to the same video segment. The function is formally expressed as: f ‚Äã ( c i , c j ) = ( ‚Äñ c i ‚àí c j ‚Äñ 2 ‚â§ œÑ s ) ‚àß ( ‚Äñ c i ‚àí c j ‚Äñ 1 ‚â§ œÑ t ) , f(c_{i},c_{j})=\left(\left\|c_{i}-c_{j}\right\|_{2}\leq\tau_{s}\right)\land\left(\left||c_{i}-c_{j}|\right|_{1}\leq\tau_{t}\right), (1) where ‚à• ‚ãÖ ‚à• 2 \left\|\cdot\right\|_{2} and ‚à• ‚ãÖ ‚à• 1 \left\|\cdot\right\|_{1} correspond to the Euclidean and Manhattan distances, respectively. In practice, we set œÑ s = 4 \tau_{s}=4 and œÑ t = 1 \tau_{t}=1 . Following clustering, one group of clicks defines an individual segment. The segment‚Äôs temporal boundaries ( t i , t j ) (t_{i},t_{j}) are determined directly from the earliest and latest timestamps within its click set as the start and end points of a video segment. For any click that cannot be grouped, we generate a segment with a symmetric temporal padding of Œ¥ \delta seconds (e.g., Œ¥ = 0.5 ‚Äã s \delta=0.5s ), resulting in the window ( t i ‚àí Œ¥ , t i + Œ¥ ) (t_{i}-\delta,t_{i}+\delta) , to capture the complete dynamic artifact. Landmark Extraction. From the 14,392 forged video clips obtained in the previous section, we obtain 71,960 trajectories of the manipulated regions. We extract frame facial landmarks from a video clip using dlib [ 20 ] and track them to obtain the motion trajectory of a specific facial component. The facial landmarks (i.e., ùí´ ‚àà { p 1 , p 2 , ‚ãØ , p 5 } \mathcal{P}\in\{p_{1},p_{2},\cdots,p_{5}\} ) consist of five dominant facial components: eyes, nose, mouth, jaw, and ears. For each video clip spanning the interval ( t i , t j ) (t_{i},t_{j}) , we compute its spatial centroid c ¬Ø ( i , j ) \overline{c}_{(i,j)} by averaging the coordinates of its constituent clicks. We then formulate a spatial relevance function that evaluates facial regions based on their distance to the centroid c ¬Ø ( i , j ) \overline{c}_{(i,j)} . The function computes the total Euclidean distance S n S_{n} between the spatial centroid c ¬Ø ( i , j ) \overline{c}_{(i,j)} and all ùí´ ( k )