Title: Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis

Abstract: As GPU architectures rapidly evolve to meet the growing demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA Blackwell (B200) introduces significant architectural advances, including fifth-generation tensor cores, tensor memory (TMEM), a decompression engine (DE), and a dual-chip design; however, systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that provides practical insights into optimizing workloads to fully utilize the rich feature sets of modern GPU architectures. This work enables application developers to make informed architectural decisions and guides future GPU design directions. We study Blackwell GPUs and compare them to the H200 generation with respect to the memory subsystem, tensor core pipeline, and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense and sparse GEMM, transformer inference, and training workloads shows that B200 tensor core enhancements achieve 1.85x ResNet-50 and 1.55x GPT-1.3B mixed-precision training throughput, with 32 percent better energy efficiency than H200.

Body: Microbenchmarking NVIDIA’s Blackwell Architecture: An in-depth Architectural Analysis I Introduction II Related Work III Blackwell Architecture III-A Blackwell Architecture IV PTX-Microbenchmark Methodology IV-A Novel Benchmark Design for Blackwell-Specific Features IV-A 1 Tensor Memory (TMEM) IV-A 2 Decompression Engine Characterization IV-A 3 Tensor Core Characterization IV-A 4 Extended Precision Characterization IV-A 5 Workflow Benchmarks V Memory Subsystem V-A Tensor Memory (TMEM) V-B Decompression Engine (DE) VI GPU Cores Microarchitecture VI-A 5th-Generation Tensor Cores VI-B Extended Precision Support: FP4 and FP6 VII Performance Analysis Case Studies VII-A Experimental Methodology VII-B Large Language Model Inference VII-B 1 Precision Mode Impact VII-B 2 Batch Size Sensitivity VII-C Scientific Computing Workload VII-C 1 FP64 Performance VII-C 2 Sustained Memory Bandwidth VII-C 3 Sparse Operations VII-D Mixed-Precision Training: End-to-End Training Performance VIII Discussion IX Conclusion Microbenchmarking NVIDIA’s Blackwell Architecture: An in-depth Architectural Analysis Double-Blind Aaron Jarmusch Sunita Chandrasekaran Abstract As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA’s Blackwell (B200) generation introduce significant architectural advances including the 5th-generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions. Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrates that B200’s tensor core enhancements achieve 1.85 × \times ResNet-50 and 1.55 × \times GPT-1.3B mixed-precision training throughput with 32% better energy efficiency than H200. I Introduction Artificial Intelligence (AI) and high‑performance computing (HPC) have evolved into data‑intensive disciplines that continuously challenge hardware efficiency, scalability, and precision. Large language models (LLMs) now exceed hundreds of billions of parameters and process context windows spanning millions of tokens [ 5 , 13 ] , alongside multi‑physics and climate simulations that demand teraflops of sustained performance, thus shifting GPU design to enable both massive parallel and architectural adaptability. At these scales, modern accelerators must balance several demands: maintaining arithmetic throughput for dense tensor workloads, minimizing on‑chip and off‑chip memory latency, while offering hardware primitives that effectively support mixed‑precision computation. The growing demands have exposed several limitations of current GPU architectures, particularly within their memory hierarchies, precision flexibility, and latency‑sensitive task scheduling. As a result, sustained architectural innovation in accelerators has become essential for advancing both throughput‑optimized training and time‑critical inference workloads. One such architecture that is designed to address some of these challenges is NVIDIA’s Blackwell architecture [ 22 ] that showcases a major generational evolution. As the direct successor to the Hopper generation, the Blackwell architecture extends NVIDIA’s GPU design in several modifications across the compute pipeline, memory hierarchy, and tensor processing subsystems. Blackwell introduces 5th‑generation tensor cores capable of FP4 and FP6 precision execution, offering trade‑offs between accuracy and performance for large‑scale training. In addition, the introduction of the Tensor Memory (TMEM) subsystem as a dedicated on‑chip memory for tensor data movement reduces reliance on shared memory (SMEM) and per SM register files (RF) during matrix‑intensive operations. Next, NVIDIA included a hardware decompression engine (DE) and redesigned the instruction pipeline for access to compressed model weights. Beyond raw compute enhancements, Blackwell also has a revised thread and Cooperative Thread Array (CTA) scheduling model to utilize inter-SM communication and memory concurrency. With so many changes introduced, intended to address the escalating demands of AI, gaming and scientific computing, an analysis of the microarchitecture and new instructions is necessary, which will provide application developers and scientists to achieve the highest performance possible for modern and future GPUs. This paper introduces a newly developed open-source microbenchmark suite (unable to share the code at this time due to double-blind), implemented in PTX and CUDA, that enables comprehensive architectural analysis of NVIDIA’s Blackwell GPU. Emphasizing innovations that distinguish it from Hopper, the suite systematically evaluates performance under stress—particularly in compute-bound and memory-bound workloads—revealing implications for parallel computing applications. The key contributions of our work are as follows: • Build targeted microbenchmarks to characterize key components of NVIDIA Blackwell B200, to the best of our knowledge - our work is the first detailed microbenchmark characterization of this next-generation GPU. • Quantify TMEM’s impact on matrix-heavy workloads and its role in reducing memory bottlenecks in tensor computations. • Evaluate the decompression engine’s throughput across formats and identify optimal usage. • Analyze 5th-generation tensor core execution via the new tcgen05 PTX instructions to study performance implications • Assess FP4/FP6 performance and accuracy trade-offs in mixed-precision tensor operations quantifying accuracy-performance trade-offs • Benchmark Blackwell across LLM inference/training, scientific kernels, and mixed-precision workloads to demonstrate real-world impact and performance gains. • Provide actionable performance guidelines for developers leveraging Blackwell’s architecture. The remainder of this paper is organized as follows: Section II details our contributions to the current state of the art GPU microbenchmarks. Following, Section III presents an overview of the Blackwell’s B200 architecture. After which, Section IV details the microbenchmark methodology we employ to systematically characterize the Blackwell microarchitecture. While, Section V details the memory subsystem before Section VI presents the tensor core pipeline. Section VII presents performance analysis across key workloads and to conclude we discuss implications and trade-offs in Section VIII . II Related Work Understanding GPU performance has long been a critical focus in HPC research. Over the years, several studies have used microbenchmarks and other methodologies to dissect architectural layers and analyze GPU microarchitectures in fine-grained detail. Early studies on Tesla and Fermi focused on memory and cache behavior [ 29 , 25 ] , while later work dissecting Kepler, Pascal [ 31 ] , and Maxwell [ 21 ] examined warp scheduling and instruction latency. With Turing through Hopper [ 6 , 11 , 27 , 18 , 19 , 23 , 30 , 17 ] , research shifted to mixed-precision and tensor core performance, introducing benchmarks for mma instructions, tile sizes, and data layouts. Recent efforts also explore instruction-level parallelism [ 26 ] , and pipeline dynamics under high register pressure. Beyond microbenchmarking, researchers built frameworks to characterize GPU performance. Application profiling [ 3 ] gathers runtime metrics but faces overhead and limited architectural visibility. Roofline models [ 16 ] offer throughput vs. intensity plots, yet oversimplify bottlenecks and miss dynamic memory behaviors. Cache stall prediction [ 10 ] estimates pipeline delays from access patterns but fails to capture modern GPU complexities like cache bypassing, warp scheduling, and memory coalescing. Analytical models like Accel-Sim [ 14 ] and GCoM [ 15 ] , built on Hong and Kim’s work [ 9 ] , offer useful GPU performance insights but neither of them model Blackwell-specific features like TMEM or the DE, as the detailed architectural information required for accurate simulation remain unknown. Thus, without a systematic understanding of these components, the research community lacks critical data needed for performance modeling, workload optimization, and accurate simulation of AI reasoning workloads typically needed for datacenter deployments. III Blackwell Architecture In this Section, we introduce the architecture of the data center NVIDIA B200 GPU, based on the NVIDIA Blackwell Architecture, and then detail the divergence from prior designs. III-A Blackwell Architecture The B200 GPU signifies a decisive progression in architectural philosophy. Previously, GPU generations from Tesla to Hopper focused on maximizing floating-point operations per second (FLOPS) for large-scale model training. In contrast, Blackwell emphasizes post-training and inference efficiency, adopting transformational changes in both memory and compute organization. One B200 GPU includes a dual-die configuration [ 22 ] where two GPU dies comprise 208 billion transistors, feature 148 SMs spread across eight GPCs, provide four L2 cache partitions (double those in Hopper), and include eight HBM3e memory stacks. Though physically partitioned, both dies are unified by the NVIDIA High-Bandwidth Interface (NV-HBI) providing a coherent and single device to software, with unified 192 GB HBM3e memory space. Figure 1: NVIDIA Blackwell GPU dual-die design interconnected via NV-HBI. Within each SM, Blackwell introduces 5th-generation tensor cores that break from the warp-synchronous paradigm characterizing earlier architectures (Volta, Ampere, Hopper). Previous generations enforced that all 32 threads within a warp synchronize before executing matrix multiply–accumulate (MMA) operations via the mma.sync or wgmma instructions. This lock-step model reduced scheduling flexibility and created idle cycles, especially for dependency chains of varying lengths. Blackwell replaces warp-synchronous MMA with tcgen05.mma , a single-thread instruction. Now, each thread independently issues MMA operations, removing warp-level synchronization and enabling true per-thread scheduling for tensor operations. Operands are now supplied from shared memory (SMEM) and a new memory pathway: Tensor Memory (TMEM) . Per SM, the TMEM provides memory access to and from tensor cores. Allocation, data movement, and deallocation are explicitly managed in software via the tcgen PTX set of instructions, giving compiler toolchains precise control over tile locality and traffic patterns. Figure 2: Tensor Core instruction pipeline for tcgen05 , wgmma , and Volta/ Ampere architectures. The flexibility of independent MMA dispatch reduces idle cycles and exposes optimization opportunities for the compiler, though it also raises questions on new performance limits: instruction latency under dependency, concurrency of tensor core usage, and pipeline saturation. These remain undocumented in vendor literature and are explored in our systematic characterization. In terms of numerical support, Blackwell’s Tensor Cores introduce native 4-bit and 6-bit floating-point precision (FP4 and FP6) for quantized inference, further improving memory and computational efficiency for AI workloads. Architectural innovations extend to the thread-block level, with CTA pair execution : two Cooperative Thread Arrays (CTAs) with adjacent ranks share operands, reducing redundant data movement. Each CTA pair maps to a TPC and leverages a dedicated intra-TPC communication network for efficient operand sharing. Further broadening the functionality, Blackwell’s Tensor Cores provide native support for convolution operators with weight-stationary dataflows that use a collector buffer to cache and reuse matrix B (weight tensor) operands. Hence, optimizing for convolutional kernels that benefit from operand locality. Blackwell also addresses growing model and data sizes by introducing a hardware-based Decompression Engine (DE) to offload decompression tasks from general-purpose SMs. This subsystem supports various algorithms, more on that in Section V , enabling model weights and large database tables to be stored in compressed form within HBM3e and decompressed transparently during memory access [ 22 ] . While some architectural details are publicly disclosed, critical microarchitectural information, such as instruction latency, pipeline depth, cache interaction, and saturation, remains unknown. Our PTX microbenchmark experiments (Sections V - VI ) provide a systematic investigation to fill these knowledge gaps as they relate to AI and HPC performance. IV PTX-Microbenchmark Methodology We employ a microbenchmarking approach based on NVIDIA’s Parallel Thread Execution (PTX) to characterize Blackwell’s microarchitectural features. While prior GPU characterization studies [ 29 , 21 , 30 ] established foundational PTX-level benchmarking principles, we extend these methodologies with novel techniques specifically designed to dissect Blackwell’s previously characterized components, including the 5th-generation tensor cores’s FP4/FP6 precision modes, the DE, and the revised cache hierarchy. Our approach leverages PTX to provide explicit control over registers and memory operations specific to architectures. PTX code is compiled into Streaming Assembler (SASS) instructions. We design dependency-controlled kernels to isolate the targeted behaviors, and validate the PTX-to-SASS translation. IV-A Novel Benchmark Design for Blackwell-Specific Features IV-A 1 Tensor Memory (TMEM) Unlike previous architectures where MMA operations relied exclusively on SMEM, DSMEM, and RFs, Blackwell introduces TMEM as a dedicated on-chip memory specifically for tensor operations. This necessitates new data movement paradigms and presents unexplored opportunities for performance optimization. Understanding TMEM’s performance characteristics is critical for several reasons. First, traditional data movement instructions (including wmma.load , ldmatrix , ld.shared , and cp.async ) which cannot interface with TMEM. Thus developers are required to adopt entirely new instruction sequences ( tcgen05.ld , tcgen05.st , tcgen05.cp ). Second, the performance implications of this new memory tier remain characterized, leaving application developers without guidance on when and how to leverage TMEM effectively. Our work addresses this gap by provid