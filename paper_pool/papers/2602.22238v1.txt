Title: TT-SEAL: TTD-Aware Selective Encryption for Adversarially-Robust and Low-Latency Edge AI

Abstract: Cloud-edge AI must jointly satisfy model compression and security under tight device budgets. While Tensor-Train Decomposition (TTD) shrinks on-device models, prior selective-encryption studies largely assume dense weights, leaving its practicality under TTD compression unclear. We present TT-SEAL, a selective-encryption framework for TT-decomposed networks. TT-SEAL ranks TT cores with a sensitivity-based importance metric, calibrates a one-time robustness threshold, and uses a value-DP optimizer to encrypt the minimum set of critical cores with AES. Under TTD-aware, transfer-based threat models (and on an FPGA-prototyped edge processor) TT-SEAL matches the robustness of full (black-box) encryption while encrypting as little as 4.89-15.92% of parameters across ResNet-18, MobileNetV2, and VGG-16, and drives the share of AES decryption in end-to-end latency to low single digits (e.g., 58% -&gt; 2.76% on ResNet-18), enabling secure, low-latency edge AI.

Body: TT-SEAL: TTD-Aware Selective Encryption for Adversarially- Robust and Low-Latency Edge AI 1 Introduction 2 System Context and TTD-Aware Threat Model 2.1 TTD Compression for Deployed DNNs 2.2 TTD-Aware Threat Model for Edge AI 2.3 Limits of Selective Encryption in TT Models 3 TT-SEAL: Proposed Method 4 Experimental Work 4.1 Prototype Processor and Evaluation Setup 4.2 Evaluation 4.2.1 Adversarial Robustness 4.2.2 TT-SEAL Performance Analysis via Optimization 5 Conclusion TT-SEAL: TTD-Aware Selective Encryption for Adversarially- Robust and Low-Latency Edge AI Kyeongpil Min, Sangmin Jeon, Jae-Jin Lee, and Woojoo Lee ‚Ä† Department of Intelligent Semiconductor Engineering, Chung-Ang University, Seoul, Korea ‚Ä° {\ddagger} AI Edge SoC Research Section, Electronics and Telecommunications Research Institute, Daejeon, Korea Abstract. Cloud‚Äìedge AI must jointly satisfy model compression and security under tight device budgets. While Tensor-Train Decomposition (TTD) shrinks on-device models, prior selective-encryption studies largely assume dense weights, leaving its practicality under TTD compression unclear. We present TT-SEAL , a selective-encryption framework for TT-decomposed networks. TT-SEAL ranks TT cores with a sensitivity-based importance metric, calibrates a one-time robustness threshold, and uses a value-DP optimizer to encrypt the minimum set of critical cores with AES. Under TTD-aware, transfer-based threat models‚Äîand on an FPGA-prototyped edge processor‚ÄîTT-SEAL matches the robustness of full (black-box) encryption while encrypting as little as 4.89‚Äì15.92% of parameters across ResNet-18, MobileNetV2, and VGG-16, and drives the share of AES decryption in end-to-end latency to low single digits (e.g., 58% ‚Üí \rightarrow 2.76% on ResNet-18), enabling secure, low-latency edge AI. This work was supported in part by Institute of Information communications Technology Planning Evaluation (IITP) grants funded by the Korea government (MSIT) (No. RS-2023-00277060), and in part by the National Research Foundation of Korea (NRF) grant funded by MSIT (No. RS-2024-00345668). Kyeongpil Min and Sangmin Jeon contributed equally to this work. Woojoo Lee is the corresponding author. ‚Ä† ‚Ä† copyright: none ‚Ä† ‚Ä† conference: Design Automation Conference; July 26‚Äì29, 2026; Long Beach, CA 1. Introduction Cloud-edge collaborative AI leverages cloud resources to train large-scale neural networks while enabling low-latency inference on edge devices (Wang et al. , 2023b ; Zhang et al. , 2024 ) . This architecture has accelerated the deployment of intelligent services such as speech recognition, video analytics, and IoT applications by flexibly distributing workloads between cloud and edge. However, frequent model updates require transmitting large parameter sets, and edge devices‚Äîoperating under tight storage, computation, and energy budgets‚Äîface persistent challenges in efficiently storing and synchronizing models (Shuvo et al. , 2022 ; Wang et al. , 2022 ) . To address these challenges, numerous model compression techniques have been studied, including quantization (Shen et al. , 2025 ) , pruning (He and Xiao, 2023 ) , knowledge distillation (Suwannaphong et al. , 2025 ) , and low-rank or tensor factorization (Liu and Parhi, 2023 ) . Among these, Tensor-Train Decomposition (TTD) (Oseledets, 2011 ) has shown particular promise by decomposing a high-dimensional weight tensor into a sequence of low-dimensional cores. TTD preserves the structural properties of layers while reducing redundancy, yielding models with much lower parameter counts and storage complexity. As a result, it reduces model size, communication volume, and memory-access cost during inference, thereby enabling efficient deployment under edge constraints (Kwak et al. , 2025b ; Dong et al. , 2025 ; Kwak et al. , 2025a ) . Beyond efficiency, however, cloud‚Äìedge collaborative AI raises significant security concerns. Unlike conventional cloud-only models where parameters remain protected in datacenters, distributed deployment requires storing parameters in off-chip DRAM and transmitting them over external buses on edge devices, exposing them to physical attacks (Gai et al. , 2021 ; Wang et al. , 2023a ) . Compromised weights can lead to IP theft, reverse engineering (Hua et al. , 2018 ) , model cloning (Kariyappa and Qureshi, 2020 ) , and adversarial attacks (Kurakin et al. , 2018 ; Yuan et al. , 2019 ) , ultimately threatening the reliability of IoT devices (Bao et al. , 2021 ; Khowaja et al. , 2022 ) and autonomous systems (Qayyum et al. , 2020 ) . Notably, when applying adversarial transferability attacks (Papernot et al. , 2017 ; Goodfellow et al. , 2018 ) to TTD-compressed models, we observe that despite reduced representational capacity and constrained decision boundaries, substitute models can still approximate the original model‚Äôs behavior closely. This finding indicates that TTD compression alone does not mitigate adversarial transferability risks, and adversarial threats remain a pressing security concern. A straightforward defense is to encrypt all model parameters before storing or transmitting them. While full encryption provides strong protection, it imposes significant latency and energy overhead because every inference requires heavy decryption at the edge. To reduce this overhead, selective encryption has been explored. Instead of protecting all weights, selective encryption targets only a subset of parameters, approximating the robustness of full encryption at a fraction of the cost (Zuo et al. , 2021 ; Tian et al. , 2021 ; Hu and Li, 2024 ) . Examples include L1-norm‚Äìbased selection of convolutional kernel rows (Zuo et al. , 2021 ) , probabilistic masking (Tian et al. , 2021 ) , encrypting only a portion of gradient updates in federated learning (Hu and Li, 2024 ) , or combining partial functional encryption with adversarial training (Ryffel et al. , 2019 ) . These works collectively show that encrypting only part of a dense model can maintain robustness against inference attacks while greatly reducing cost. However, nearly all prior approaches assume dense weight representations. Directly applying such dense-oriented selective encryption to TTD-compressed models is problematic. In dense networks, redundancy allows partial encryption (e.g., 50% of weights) to achieve robustness close to full encryption. In contrast, TTD removes much of this redundancy, concentrating information into fewer parameters; leaving even a small portion unencrypted can still expose critical structure. This fundamental difference limits the effectiveness of existing methods and calls for a TTD-aware selective encryption scheme. In this work, we present TT-SEAL ‚Äî Selective Encryption for Adversarially-Robust and Low-Latency ‚Äîto our knowledge, the first selective encryption framework explicitly tailored to TTD -compressed neural networks. TT-SEAL leverages the structural properties of TT-format weights to achieve strong protection while minimizing encryption overhead. Concretely, TT-SEAL introduces (i) a core-wise importance metric that evaluates the security impact of each TT-core, (ii) a data-driven threshold calibration that aligns protection strength with a robustness target, and (iii) a minimal-cost selection algorithm that efficiently identifies the smallest set of TT-cores to encrypt using dynamic programming-based optimization. Through extensive experiments, we demonstrate that TT-SEAL significantly reduces decryption overhead while maintaining robustness comparable to full encryption, thereby enabling secure and low-latency inference on edge devices. The contributions of this work are as follows: ‚Ä¢ TTD-tailored selective encryption. We introduce TT-SEAL, the first selective encryption method specifically designed for TTD-compressed models, exploiting TT-core structure for strong protection with reduced overhead. ‚Ä¢ Core-level metric and optimization. We define a sensitivity-based importance metric, calibrate a robustness threshold, and formulate an optimization procedure to identify the minimal set of TT-cores to encrypt. ‚Ä¢ Prototype-based validation. On an FPGA-based edge AI processor, TT-SEAL achieves robustness comparable to full encryption while encrypting as little as 4.89% of parameters (ResNet-18), with similar trends observed on VGG-16 and MobileNetV2. Moreover, the decryption share in end-to-end inference is reduced from 58% to 2.76%, making secure edge inference feasible. Figure 1 . Transfer-based adversarial attack using JBDA. Blue lines: the attacker queries oracle ùë∂ ‚Äã ( ùíô ) \bm{O(x)} with clean inputs, trains a substitute ùë≠ ‚Äã ( ùê± ) \bm{F(\mathbf{x})} , and augments data ùê± ‚Ä≤ \mathbf{x}^{\prime} near decision boundaries. Red lines: ùë≠ \bm{F} generates adversarial examples ùê± ùíÇ ‚Äã ùíÖ ‚Äã ùíó \mathbf{x}_{\bm{adv}} that transfer to ùë∂ \bm{O} . 2. System Context and TTD-Aware Threat Model 2.1. TTD Compression for Deployed DNNs TTD represents a d d -dimensional weight tensor ùí≤ \mathcal{W} as the product of d d low-dimensional core tensors G k G_{k} (Oseledets, 2011 ) : (3) ùí≤ ‚Äã ( i 1 , i 2 , ‚ãØ , i d ) ‚âà G 1 ‚Äã [ i 1 ] ‚Äã G 2 ‚Äã [ i 2 ] ‚Äã ‚ãØ ‚Äã G d ‚Äã [ i d ] , ùí≤ ‚àà ‚Ñù n 1 √ó n 2 √ó ‚ãØ √ó n d , G k ‚àà ‚Ñù r k ‚àí 1 √ó n k √ó r k , \displaystyle\begin{gathered}\mathcal{W}(i_{1},i_{2},\cdots,i_{d})\approx G_{1}[i_{1}]\,G_{2}[i_{2}]\cdots G_{d}[i_{d}],\\ \mathcal{W}\in\mathbb{R}^{n_{1}\times n_{2}\times\cdots\times n_{d}},\quad G_{k}\in\mathbb{R}^{r_{k-1}\times n_{k}\times r_{k}},\end{gathered} where n k n_{k} denotes the size of the k k -th mode and r k r_{k} the TT-rank with r 0 = r d = 1 r_{0}=r_{d}=1 . This TT-format reduces parameter count and storage cost while preserving the layer structure. Applied to neural networks, TTD compresses convolutional, recurrent, and transformer models with minimum accuracy loss. For example, a convolutional layer has a four-dimensional weight tensor ùí≤ c ‚Äã o ‚Äã n ‚Äã v ‚àà ‚Ñù C o ‚Äã u ‚Äã t √ó C i ‚Äã n √ó K h √ó K w \mathcal{W}_{conv}\in\mathbb{R}^{C_{out}\times C_{in}\times K_{h}\times K_{w}} , where C i ‚Äã n C_{in} and C o ‚Äã u ‚Äã t C_{out} denote the number of input and output channels, and K h K_{h} and K w K_{w} represent the kernel height and width, respectively. The storage complexity of ùí≤ c ‚Äã o ‚Äã n ‚Äã v \mathcal{W}_{conv} is O ‚Äã ( C o ‚Äã u ‚Äã t ‚Äã C i ‚Äã n ‚Äã K h ‚Äã K w ) O(C_{out}C_{in}K_{h}K_{w}) , while its TT-format counterpart requires only O ‚Äã ( r 2 ‚Äã ( C o ‚Äã u ‚Äã t + C i ‚Äã n + K h + K w ) ) O(r^{2}(C_{out}+C_{in}+K_{h}+K_{w})) . This reduction alleviates memory footprint, reduces transmission volume, and lowers inference cost, which is particularly beneficial for edge devices (Lee et al. , 2024 ; Wei et al. , 2024 ; Han and Xiang, 2023 ) . 2.2. TTD-Aware Threat Model for Edge AI System setting. An edge SoC integrates compute units (CPUs or accelerator cores), system interconnect, on-chip memory (registers, caches, eSRAM), and off-chip DRAM. Large neural network weights cannot fit on-chip and are stored in compressed form in DRAM. During inference, these parameters are fetched over the DDR bus, decompressed on-chip, and then loaded into compute units for execution. Trust boundary. The on-chip region is physically embedded and thus difficult to probe directly, whereas off-chip DRAM and memory buses are external and exposed to adversaries. We therefore assume model parameters in DRAM are always encrypted as a baseline protection. Nevertheless, outputs (e.g., labels, logits, or scores) remain observable through normal I/O interfaces and can be collected by an attacker. Adversary capability. We assume (i) encrypted parameters in DRAM can be observed by the attacker, and (ii) the attacker can repeatedly query the deployed model (oracle O O ) to collect input-output pairs ( ùê± , O ‚Äã ( ùê± ) ) (\mathbf{x},O(\mathbf{x})) . Even when all parameters remain encrypted (black-box setting), such queries allow the attacker to train a substitute model F F that approximates O O . Attack procedure. Figure 1 illustrates the process: in the blue path , the attacker queries the oracle O ‚Äã ( x ) O(x) with clean inputs, trains a substitute F ‚Äã ( x ) F(x) , and augments data near decision boundaries using Jacobian-based Dataset Augmentation (JBDA) (Papernot et al. , 2017 ) . In the red path , the trained F F generates adversarial examples ùê± a ‚Äã d ‚Äã v \mathbf{x}_{adv} that transfer to O O . A common generator is the Fast Gradient Sign Method (FGSM) (Goodfellow et al. , 2014 ; Kurakin et al. , 2018 ) : (4) ùê± a ‚Äã d ‚Äã v = ùê± + œµ ‚ãÖ sign ‚Äã ( ‚àá ùê± L ‚Äã ( ùê± , y true ) ) , \displaystyle\mathbf{x}_{adv}=\mathbf{x}+\epsilon\cdot\text{sign}\!\left(\nabla_{\mathbf{x}}L(\mathbf{x},y_{\text{true}})\right), where y true y_{\text{true}} is its ground-truth label, L ‚Äã ( ‚ãÖ , ‚ãÖ ) L(\cdot,\cdot) is the loss (e.g., cross-entropy), ‚àá ùê± \nabla_{\mathbf{x}} is the gradient w.r.t. the input, and sign ‚Å° ( ‚ãÖ ) \operatorname{sign}(\cdot) is applied element-wise. Larger œµ \epsilon values typically raise the chance of misclassification but also make perturbations more visible, whereas smaller values keep inputs visually closer to the original but reduce attack success. Both non-targeted (any misclassification) and targeted (forcing a specific label) variants are considered realistic threats in this work. Implication for compressed models. The effectiveness of such transfer-based attacks depends critically on how well F F approximates O O : the closer the approximation, the more likely adversarial examples crafted on F F will transfer to O O . In practice, full secrecy of weights constrains F F , but even partial parameter exposure can substantially improve its accuracy and thus boost attack transferability (Goodfellow et al. , 2018 ) . Our experiments confirm that TTD compression does not eliminate this risk: on CIFAR-10, substitute models trained in a white-box setting achieved accuracies of 92.29% (ResNet-18 (He et al. , 2016 ) ), 89.5% (MobileNetV2 (Sandler et al. , 2018 ) ), and 90.73% (VGG-16 (Simonyan and Zisserman, 2014 ) ). Despite parameter reduction and redundancy removal, TTD models still leak sufficient structural information for substitutes to approximate decision boundaries, leaving them vulnerable to transfer-based adversarial attacks. 2.3. Limits of Selective Encryption in TT Models Figure 2 . Substitute-model accuracy vs. selective encryption ratio in ResNet-18 with dense and TTD-compressed weights. Selective encryption seeks to reduce the overhead of full encryption by protecting only a subset of parameters while preserving robustness. This approach is attractive for edge deployment where full encryption incurs excessive latency and power. Prior work has explored several strategies: kernel-row selection based on L1 norm (showing 50% suffices for dense CNNs) (Zuo et al. , 2021 ) , probabilistic masking (8% encrypted) (Tian et al. , 2021 ) , selective gradient encryption in federated learning (up to 4.15 √ó 4.15\times communication savings) (Hu and Li, 2024 ) , and partial functional encryption combined with adversarial trainin