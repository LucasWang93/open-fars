Title: Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting

Abstract: Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. Project Website: https://sk-fun.fun/DropAnSH-GS

Body: Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting 1 Introduction 2 Related Work 3 Method 3.1 Preliminaries 3.2 Pilot Study 3.3 Anchor-based Dropout 3.4 Spherical Harmonics Dropout 3.5 Loss Function 4 Experiments 4.1 Implementation Details 4.2 Comparison Results 4.3 Compression of Model Size 4.4 Compatibility 4.5 Training Efficiency 4.6 Ablation Study 5 Conclusions 6 Additional Experimental Results 7 Discussion on Future Work Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting Shuangkang Fang 1 , I-Chao Shen 2 , Xuanyang Zhang 3 , Zesheng Wang 1 , Yufeng Wang 1 , Wenrui Ding 1 , Gang Yu 3 , Takeo Igarashi 2 1 Beihang University 2 The University of Tokyo 3 StepFun https://sk-fun.fun/DropAnSH-GS Abstract Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. 1 Introduction Realistic novel view synthesis (NVS) remains a central challenge in computer vision and graphics. 3DGS [ 25 ] , with a remarkable balance between rendering speed and visual fidelity, has recently emerged as the leading method in this domain [ 5 , 51 , 11 , 34 , 23 ] . Although 3DGS excels in dense input views [ 2 , 31 , 6 , 61 , 30 , 21 ] , it faces significant challenges when trained with sparse views [ 37 , 16 , 9 , 63 , 20 , 10 ] . The scarcity of training views often induces severe overfitting, manifesting as artifacts, blurring, or geometric distortions that limit its practical applicability. Figure 1 : Difference between DropGaussian [ 46 ] and our DropAnSH-GS . (a) DropGaussian randomly sets the opacity of individual Gaussians to zero. (b) DropAnSH-GS selects random Gaussians as anchors, discards their neighbors, and applies Dropout to high-degree SH. Anchor-based Dropout eliminates contiguous 3D regions, which effectively inhibits adjacent Gaussians from compensating for the dropped ones, forcing remaining Gaussians to learn more robust scene representations, thereby strengthening the Dropout regularization effect. Dropping SH further reduces overfitting and yields a more compact model. Figure 2 : Compensation effect in neighbor Gaussians . (a) We measure the spatial autocorrelation of opacity and color between Gaussians at varying distances using Moran’s I metric [ 40 , 37 ] . Closer Gaussians exhibit higher similarity in opacity and color. This spatial redundancy implies that dropping only individual Gaussians can be easily compensated for by their surrounding similar neighbors, thereby limiting the effectiveness of dropout. (b) We report the Mean Absolute Error between rendered images before and after applying two different Dropout strategies, S1 and S2. When dropping a similar number of Gaussians, the strategy that drops Gaussians and their neighboring Gaussians (S1) has a larger impact on rendering quality, as it avoids simple compensation by neighboring Gaussians. (c) In regions surrounding the dropped Gaussians, the S1 strategy activates a greater number of remaining Gaussians, resulting in stronger gradient updates. Inspired by regularization techniques in deep learning [ 52 ] , recent works introduce the Dropout mechanism into 3DGS [ 58 , 46 ] . These methods randomly set the opacity of certain Gaussians to zero during training, aiming to prevent over-reliance on specific Gaussians and thereby regularize the model. However, our analysis reveals critical limitations in these approaches: (1) Neighbor compensation effect (Figure 2 ). 3DGS leverages numerous overlapping Gaussians to collaboratively render a scene, which often exhibits highly similar opacity and color attributes in local regions. When a single Gaussian is dropped, its rendering contribution is easily compensated by neighboring ones, thereby weakening the intended regularization effect of Dropout. (2) Limited attribute utilization (Figure 3 ). Existing dropout strategies in 3DGS exclusively target the opacity attribute, while neglecting the distinct roles of other attributes in overfitting. Figure 3 : Overfitting caused by high-degree SH . On the LLFF dataset, (a) under full-view conditions, moderately increasing the degree of spherical harmonics can improve the performance of the 3DGS model. However, (b) under sparse-view settings, using high-degree spherical harmonics leads to performance degradation and a significant increase in model size, indicating that spherical harmonics themselves also constitute a source of overfitting. To address these limitations, we propose a novel Dropout regularization strategy termed DropAnSH-GS. Unlike previous Dropout strategies [ 46 , 58 ] that discard isolated Gaussians, DropAnSH-GS randomly selects a set of anchor Gaussians and drops these anchors and their surrounding neighbours within a local spatial region. This structured Dropout strategy eliminates entire clusters of correlated Gaussians, creating larger-scale “information voids” that actively disrupt spatial coherence and prevent local compensation. Consequently, the optimization process is compelled to utilize more long-range contextual information to reconstruct the dropped regions, thereby fostering more robust and generalizable scene representations. Moreover, existing 3DGS Dropout methods are limited to manipulating opacity without considering the nuanced differences among multiple attributes. We identify that SH, particularly higher-degree terms, also cause overfitting (as shown in Figure 3 ). To this end, we further introduce a Dropout mechanism targeting high-degree SH. This strategy provides twofold benefits: it (1) mitigates overfitting to color variations, and (2) promotes a coarse-to-fine SH representation, prioritizing lower-degree SH for scene representation. Consequently, the trained model permits post-hoc pruning of high-degree SH to obtain a more compact model without requiring retraining, as shown in Figure 1 . The main contributions of our work are as follows: • We identify and analyze the limitations of existing Dropout strategies in 3DGS, showcasing how spatial redundancy and high-degree spherical harmonic coefficients weaken their regularization effect. • We propose DropAnSH-GS, a novel structured spatial Dropout method that discards clusters of Gaussians, achieving stronger regularization against overfitting. • We extend the Dropout strategy to appearance attributes by dropping SH, which simultaneously suppresses overfitting and enables flexible post-hoc model compression. • Comprehensive experiments demonstrate that our method significantly outperforms the state-of-the-art Dropout techniques for 3DGS, while incurring negligible additional computational cost. Moreover, it can be used to enhance the quality of multiple 3DGS variants, highlighting its broad applicability and value. 2 Related Work NeRF and 3DGS. Neural Radiance Fields (NeRF) implicitly model 3D scenes using neural networks to enable high-quality novel view synthesis [ 39 , 41 , 3 , 62 , 1 , 35 , 36 , 44 , 45 , 47 , 48 , 29 , 53 ] . However, the training and rendering processes of NeRF are computationally intensive, limiting its applicability in real-time scenarios [ 49 , 13 , 50 , 18 , 57 , 55 , 33 , 26 , 12 ] . Subsequent efforts, such as Instant-NGP [ 41 ] and Plenoxels [ 15 ] , improve efficiency through network architecture optimization or by introducing explicit scene representations. Recently, 3DGS has gained significant attention as an efficient explicit representation [ 25 , 32 , 31 , 21 , 23 , 17 , 37 , 24 , 59 ] . It models the 3D scene as a set of Gaussian functions, using attributes such as position, covariance, opacity, and spherical harmonic coefficients for fast and high-fidelity rendering. Follow-up works have further enhanced 3DGS in terms of parameter efficiency and rendering quality [ 61 , 6 , 31 , 30 , 7 , 21 , 32 , 5 , 2 , 8 , 14 , 43 , 27 ] . NVS under Sparse Views. Novel view synthesis under sparse views aims to generate high-quality novel views from limited input images. Prior studies [ 54 , 37 , 4 , 22 , 42 , 16 , 60 ] have noted that the performance of both NeRF and 3DGS methods heavily depends on the dense views. To address this, researchers have proposed various methods to enhance NeRF’s generalization under sparse views, such as DietNeRF [ 22 ] , RegNeRF [ 42 ] , and FreeNeRF [ 60 ] , which employ diverse regularization loss strategies to mitigate overfitting. For sparse-view optimization in 3DGS, FSGS [ 66 ] introduces a Proximity-guided Gaussian Unpooling strategy, CoR-GS [ 63 ] uses mutual constraints between two distinct 3DGS models, InstantSplat [ 9 ] jointly optimizes poses, while DNGaussian [ 28 ] and NexusGS [ 65 ] introduce the depth constraint to enhance the performance of the 3DGS model. Dropout Technique in 3DGS. In this work, we focus on leveraging the Dropout technique to improve the performance of 3DGS under sparse-view conditions. The most closely related works are DropoutGS [ 58 ] and DropGaussian [ 46 ] , both of which propose mitigating overfitting in 3DGS by randomly discarding Gaussians during training. However, the high correlation among neighboring Gaussians limits the effectiveness of their Dropout strategies. In contrast, our DropAnSH-GS method significantly strengthens regularization by discarding clusters of neighboring Gaussians and applying Dropout to spherical harmonic coefficients. Our approach achieves a flexible trade-off between performance and model size, offering a novel solution for the sparse-view NVS task. Figure 4 : Overview of DropAnSH-GS . During the training phase, we randomly select a set of anchor Gaussians and drop these anchors and their surrounding neighbours within a local spatial region, thereby reducing the likelihood of neighboring Gaussians compensating for one another and enhancing the regularization effect. Simultaneously, high-degree SH coefficients are randomly dropped to further strengthen regularization and yield a more compact model representation. 3 Method 3.1 Preliminaries 3DGS represents a 3D scene using a large number of explicit Gaussians. Each Gaussian G i G_{i} is parameterized by: position μ i ∈ ℝ 3 \mu_{i}\in\mathbb{R}^{3} , covariance Σ i ∈ ℝ 3 × 3 \Sigma_{i}\in\mathbb{R}^{3\times 3} , color c i c_{i} (represented by a set of spherical harmonics coefficients), and opacity α i ∈ [ 0 , 1 ] \alpha_{i}\in[0,1] . For efficient optimization, the covariance Σ i \Sigma_{i} is typically factorized into a scaling vector s i ∈ ℝ 3 s_{i}\in\mathbb{R}^{3} and a rotation quaternion q i ∈ ℝ 4 q_{i}\in\mathbb{R}^{4} . Given a camera view defined by a view matrix W W , the 3D Gaussians are projected onto the 2D image plane. The color C C of a pixel is computed using α \alpha -blending over all N N depth-sorted Gaussians: C = ∑ i = 1 N c i ​ α i ​ ∏ j = 1 i − 1 ( 1 − α j ) . C=\sum_{i=1}^{N}c_{i}\alpha_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}). (1) 3.2 Pilot Study To mitigate overfitting under sparse view settings, prior works propose randomly setting the opacity α i \alpha_{i} of Gaussians to zero during training. However, the expressive nature of 3DGS exhibits significant local redundancy. A visible surface is typically represented by hundreds or thousands of overlapping Gaussians. When a single Gaussian is removed, as per Eq. ( 1 ), its contribution to pixel color is compensated by the increased contributions of neighboring Gaussians. Consequently, the change in pixel color Δ ​ C \Delta C is negligible, resulting in weak gradient signals during backpropagation that fail to impose effective regularization on the model’s geometry and appearance learning. As illustrated in Figure 2 , spatial correlation among Gaussians is inversely related to their distance. Removing a single Gaussian within the view frustum results in minimal performance degradation and weak gradients. In contrast, discarding a cluster of 10 neighboring Gaussians produces significant performance changes and stronger gradient signals. This suggests that Gaussians exhibit strong spatial complementarity. While this facilitates cooperative rendering, it also diminishes the effectiveness of naive Dropout. Moreover, existing methods focus solely on dropping opacity values, overlooking the regularization potential of other crucial attributes, such as the spherical harmonic coefficients that encode appearance. As illustrated in Figure 3 , while using high-degree SH improves performance under full-view conditions, it leads to overfitting in sparse-view settings. Appropriately reducing the degree of SH not only enhances 3DGS performance in sparse-view scenarios but also significantly reduces the model size. Figure 5 : Qualitative comparison on the LLFF dataset (3 views). Table 1 : Quantitative comparison on the LLFF dataset (3, 6 and 9 views). Methods 3-view 6-view 9-view PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓ NeRF-based Mip-NeRF [ 4 ] 16.11 0.401 0.460 22.91 0.756 0.213 24.88 0.826 0.170 DietNeRF [ 22 ] 14.94 0.370 0.496 21.75 0.717 0.248 24.28 0.801 0.183 RegNeRF [ 42 ] 19.08 0.587 0.336 23.10 0.760 0.206 24.86 0.820 0.161 FreeNeRF [ 60 ] 19.63 0.612 0.308 23.73 0.779 0.195 25.13 0.827 0.160 3DGS-based 3DGS [ 25 ] 19.17 0.646 0.268 23.74 0.807 0.162 25.44 0.860 0.096 DNGaussian [ 28 ] 19.12 0.591 0.294 22.18 0.755 0.198 23.17 0.788 0.180 FSGS [ 66 ] 20.43 0.682 0.248 24.09 0.823 0.145 25.31 0.860 0.122 CoR-GS [ 63 ] 20.36 0.710 0.202 24.34 0.831 0.122 25.94 0.872 0.088 DropoutGS [ 58 ] 19.39 0.632 0.279 24.02 0.816 0.144 25.13 0.869 0.099 DropGaussian [ 46 ] 20.33 0.709 0.201 24.58 0.830 0.125 25.85 0.864 0.093 Ours 20.68 0.724 0.194 24.76 0.837 0.116 26.24 0.875 0.088 3.3 Anchor-based Dropout To overcome the above limitations, we propose DropAnSH-GS method, as illustrated in Figure 4 , which drops groups of spatially related Gaussians rather than individual