Title: Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC

Abstract: Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank updates on top of frozen weights, dramatically reducing trainable parameters and memory. In this work, we address the gap between training with full steps with low-rank projections (SVDLoRA) and LoRA fine-tuning. We propose LoRSum, a memory-efficient subroutine that closes this gap for gradient descent by casting LoRA optimization as a proximal sub-problem and solving it efficiently with alternating least squares updates, which we prove to be an implicit block power method. We recover several recently proposed preconditioning methods for LoRA as special cases, and show that LoRSum can also be used for updating a low-rank momentum. In order to address full steps with preconditioned gradient descent, we propose a scaled variant of LoRSum that uses structured metrics such as K-FAC and Shampoo, and we show that storing the diagonal of these metrics still allows them to perform well while remaining memory-efficient. Experiments on a synthetic task, CIFAR-100, and language-model fine-tuning on GLUE, SQuAD v2, and WikiText-103, show that our method can match or improve LoRA baselines given modest compute overhead, while avoiding full-matrix SVD projections and retaining LoRA-style parameter efficiency.

Body: Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC 1 Introduction Contributions. Setup and notation. 2 Related Work 3 Without SVD: Proximal Subspace Iteration LoRA 3.1 Projected gradient view and the SVDLoRA oracle 3.2 Approximating rank- r r projections via warm-started ALS Gauss-Seidel (alternating) inner iterations. Jacobi (simultaneous) iterations and preconditioned LoRA. 3.3 LoRSum as proximal rank- r r projections. Proximal ALS updates. Low-Rank Sum ( LoRSum ) Subspace iteration. 3.4 Momentum with LoRSum 4 Beyond SGD: F-LoRSum with Kronecker-factored metrics 4.1 LoRSum with Kronecker-factored metrics Diagonal K-FAC for memory-efficiency. Fractional metrics. 5 Experiments 6 Discussion and Limitations A Additional implementation details A.1 LoRSum technical details Caching activations for inner iterations. Mixed precision and loss scaling. Choosing Ï \rho and Î´ \delta . Solving the inner systems. A.2 Gradient clipping A.3 Distributed data-parallel training Synchronizing diagonal Kronecker statistics. B Proofs B.1 Proof of Theorem Ëœ 3.1 Theorem 3.1 (restated). C Interpretations of the proximal projection (i) A stabilized approximation to the rank- r r projection. (ii) Lifted-space proximal step under a non-linear constraint. D Square-root K-FAC yields implicit gradient normalization E Algorithms and PyTorch reference implementation E.1 LoRSum and PSI-LoRA Momentum buffer update. E.2 F-LoRSum and scaled PSI-LoRA Partially preconditioned low-rank sums. Diagonal Kronecker-factored metrics with fractional power F Additional experimental details F.1 Linear-task momentum ablation Linear task tuning notes. GLUE RoBERTa validation curves. Hyperparameters. Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC Abdulla Jasem Almansoori Maria Ivanova Andrey Veprikov Aleksandr Beznosikov Samuel HorvÃ¡th Martin TakÃ¡Ä Abstract Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank updates on top of frozen weights, dramatically reducing trainable parameters and memory (Hu et al., 2022 ) . In this work, we address the gap between training with full steps with low-rank projections ( SVDLoRA ) and LoRA fine-tuning. We propose LoRSum , a memory-efficient subroutine that closes this gap for gradient descent by casting LoRA optimization as a proximal sub-problem and solving it efficiently with alternating least squares updates, which we prove to be an implicit block power method. We recover several recently proposed preconditioning methods for LoRA as special cases, and show that LoRSum can also be used for updating a low-rank momentum. In order to address full steps with preconditioned gradient descent, we propose a scaled variant of LoRSum that uses structured metrics such as K-FAC and Shampoo, and we show that storing the diagonal of these metrics still allows them to perform well while remaining memory-efficient. Experiments on a synthetic task, CIFAR-100, and language-model fine-tuning on GLUE, SQuAD v2, and WikiText-103, show that our method can match or improve LoRA baselines given modest compute overhead, while avoiding full-matrix SVD projections and retaining LoRA-style parameter efficiency. 1 1 1 Code: https://github.com/zeligism/PSI-LoRA Optimization, LoRA, Low-rank, alternating least squares, SVD, K-FAC, Natural Gradient, Memory-efficient 1 Introduction Fine-tuning large pretrained models remains costly as updating all parameters demands substantial GPU memory and state tracking. Parameter-efficient methods like LoRA replace full fine-tuning with low-rank adapters ğ” âˆˆ â„ d out Ã— r \mathbf{U}\in\mathbb{R}^{d_{\text{out}}\times r} , ğ• âˆˆ â„ d in Ã— r \mathbf{V}\in\mathbb{R}^{d_{\text{in}}\times r} added to a frozen base weight ğ– base \mathbf{W}^{\text{base}} so that ğ– tuned = ğ– base + ğ”ğ• âŠ¤ \mathbf{W}^{\text{tuned}}=\mathbf{W}^{\text{base}}+\mathbf{U}\mathbf{V}^{\top} with r â‰ª min â¡ ( d out , d in ) r\ll\min(d_{\text{out}},d_{\text{in}}) (Hu et al., 2022 ) . Although LoRA reduces parameter and memory costs, first-order training of ( ğ” , ğ• ) (\mathbf{U},\mathbf{V}) can be ill-conditioned (Tong et al., 2021 ) and sensitive to hyperparameters. Preconditioning with ğ• âŠ¤ â€‹ ğ• \mathbf{V}^{\top}\mathbf{V} and ğ” âŠ¤ â€‹ ğ” \mathbf{U}^{\top}\mathbf{U} remedies part of this (Zhang Pilanci, 2024 ; Zhang et al., 2023a , 2021 ) , yet each step still acts only locally on the current factor subspaces. As an oracle baseline, consider taking a full step in weight space ğ– â† ğ– + Î” \mathbf{W}\leftarrow\mathbf{W}+\Delta , and then projecting it back to rank r r w.r.t. the Frobenius norm. We refer to this baseline as SVDLoRA because by Eckart-Young-Mirsky theorem, the optimal rank- r r projection is given by the truncated SVD of ğ– + Î” \mathbf{W}+\Delta . While this does not guarantee maximal objective decrease for a non-quadratic loss, it cleanly isolates the optimization gap introduced by enforcing rank r r after a full update. Indeed, this has been recently proposed as an initialization scheme for LoRA-One (Zhang et al., 2025 ) , so one can imagine that reapplying this projection each step would yield the best possible LoRA performance. But this is clearly costly in terms of both compute and memory since it requires running an SVD every step on a d out Ã— d in d_{\text{out}}\times d_{\text{in}} matrix. Nonetheless, the existence of the rank- r r updates themselvesâ€”the difference between the current weights and their rank- r r SVD projectionâ€”suggests that we can do better. In this work, we propose LoRSum (Low-Rank Sum), a stable and memory-efficient subroutine for finding the best rank- r r solution to a sum of low-rank matrices. The utility of LoRSum lies in its ability to approximate the above SVD baseline without forming or operating on the full matrix, which is inspired from the observation that the gradient ğ† \mathbf{G} in LoRA admits a memory-efficient factorization based on the input and output gradients of the layer. The derivation of LoRSum is simple and follows from casting the SVD of an update as a proximal sub-problem in LoRA parameter space. The solution uses alternating least squares (ALS) to approximate the solution iteratively, which we prove to be a block power method implicitly. In addition to approximating the full update, LoRSum can also be used for updating low-rank approximations of the momentum buffers efficiently. We further show that LoRSum can recover many existing methods in the literature (Tong et al., 2021 ; Zhang et al., 2021 , 2023a ; Xu et al., 2023 ; Zhang Pilanci, 2024 ) as special cases. When the step Î” \Delta is not expressible as a sum of low-rank terms, such as in preconditioned gradient descent like Adam (Kingma Ba, 2017 ) or natural gradient descent (Amari, 1998 ) , the standard LoRSum is no longer directly applicable. In order to address this important case, we propose F-LoRSum (Fisher-metric LoRSum ), which is a memory-efficient scaled variant that uses Kronecker-factored metrics like K-FAC (Martens Grosse, 2020 ) and Shampoo (Gupta et al., 2018 ) to scale the updates. In general, Kronecker-factored metrics are not memory-efficient for LoRA training since they require storing full d in Ã— d in d_{\text{in}}\times d_{\text{in}} and d out Ã— d out d_{\text{out}}\times d_{\text{out}} matrices. However, our implementation only requires storing their diagonal approximations, which ultimately makes F-LoRSum â€™s memory storage requirements even less than Adamâ€™s. Indeed, F-LoRSum with rank- 1 1 momentum and diagonal metrics needs an extra memory of 2 â€‹ ( d in + d out ) 2(d_{\text{in}}+d_{\text{out}}) , whereas Adam requires an extra memory of 2 â€‹ r â€‹ ( d in + d out ) 2r(d_{\text{in}}+d_{\text{out}}) for the first and second moments. Compare with the memory-efficient AdaFactor (Shazeer Stern, 2018 ) which requires ( d in + d out ) (d_{\text{in}}+d_{\text{out}}) extra memory for the second moment. Still, we show in the experiments that, not only does F-LoRSum remain memory-efficient, it can even outperform full-tuned Adam baseline on some GLUE tasks (Wang et al., 2019 ) . Contributions. We make the following contributions: 1. LoRSum : a stable and memory-efficient subroutine for approximating the best proximal rank- r r sum of low-rank matrices, which we used to approximate the SVD a full update and full momentum. We show that LoRSum recovers existing preconditioned LoRA methods as special cases, and prove in Theorem Ëœ 3.1 that LoRSum is a subspace iteration method that converges to SVDLoRA as the number of inner iterations K K increases, as shown in Figure Ëœ 1 . 2. F-LoRSum : a LoRSum variant for preconditioned steps with Kronecker-factored metrics. 3. PSI-LoRA and Scaled PSI-LoRA , two practical and memory-efficient optimizers based on the subroutines LoRSum and F-LoRSum , with full reference implementations in Appendix Ëœ E . 4. Show empirical gains and learning rate robustness over LoRA baselines on linear synthetic task, CIFAR-100, GLUE, SQuAD v2, and WikiText-103. Setup and notation. For a single layer with frozen ğ– base âˆˆ â„ d out Ã— d in \mathbf{W}^{\text{base}}\in\mathbb{R}^{d_{\text{out}}\times d_{\text{in}}} , we learn rank- r r adapters ğ” âˆˆ â„ d out Ã— r \mathbf{U}\in\mathbb{R}^{d_{\text{out}}\times r} and ğ• âˆˆ â„ d in Ã— r \mathbf{V}\in\mathbb{R}^{d_{\text{in}}\times r} , so that the effective tuned weight at iteration t t is ğ– t tuned = ğ– base + ğ” t â€‹ ğ• t âŠ¤ \mathbf{W}_{t}^{\text{tuned}}=\mathbf{W}^{\text{base}}+\mathbf{U}_{t}\mathbf{V}_{t}^{\top} . Henceforth, we make the dependence on ğ– base \mathbf{W}^{\text{base}} implicit as it does not affect our analysis. We define the low-rank matrix formed by the adapter as ğ– t := ğ” t â€‹ ğ• t âŠ¤ \mathbf{W}_{t}:=\mathbf{U}_{t}\mathbf{V}_{t}^{\top} . Let f â€‹ ( ğ– t ) f(\mathbf{W}_{t}) be the objective and define the full gradient ğ† t := âˆ‡ ğ– f â€‹ ( ğ– t ) \mathbf{G}_{t}:=\nabla_{\mathbf{W}}f(\mathbf{W}_{t}) , which is also equal to the gradient w.r.t. ğ– t tuned \mathbf{W}_{t}^{\text{tuned}} . By the chain rule, ğ† ğ” , t = âˆ‡ ğ” f â€‹ ( ğ– t ) = ğ† t â€‹ ğ• t \mathbf{G}_{\mathbf{U},t}=\nabla_{\mathbf{U}}f(\mathbf{W}_{t})=\mathbf{G}_{t}\mathbf{V}_{t} and ğ† ğ• , t = âˆ‡ ğ• f â€‹ ( ğ– t ) = ğ† t âŠ¤ â€‹ ğ” t \mathbf{G}_{\mathbf{V},t}=\nabla_{\mathbf{V}}f(\mathbf{W}_{t})=\mathbf{G}_{t}^{\top}\mathbf{U}_{t} . Fix a linear layer, and let ğ— t âˆˆ â„ B Ã— d in \mathbf{X}_{t}\in\mathbb{R}^{B\times d_{\text{in}}} be the inputs and ğ’ t = âˆ‡ ( ğ–ğ— ) âŠ¤ f â€‹ ( ğ– ) âˆˆ â„ B Ã— d out \mathbf{S}_{t}=\nabla_{(\mathbf{W}\mathbf{X})^{\top}}f(\mathbf{W})\in\mathbb{R}^{B\times d_{\text{out}}} be the gradients w.r.t. its outputs, where B B is the effective batch size. Then, ğ† t = ğ’ t âŠ¤ â€‹ ğ— t \mathbf{G}_{t}=\mathbf{S}_{t}^{\top}\mathbf{X}_{t} . We exploit this factorized structure to perform only thin GEMM operations. 2 Related Work Preconditioned optimization for fixed-rank parameterizations. Prior work improves the conditioning of low-rank factor optimization by preconditioning factor gradients, leading to updates that depend on ğ• âŠ¤ â€‹ ğ• \mathbf{V}^{\top}\mathbf{V} and ğ” âŠ¤ â€‹ ğ” \mathbf{U}^{\top}\mathbf{U} (Tong et al., 2021 ; Zhang et al., 2021 , 2023a ; Xu et al., 2023 ) . Riemannian preconditioned LoRA derives analogous updates from fixed-rank manifold geometry (Zhang Pilanci, 2024 ) . We instead view LoRA training as approximately solving the rank- r r projection of a full (possibly preconditioned) step. One simultaneous iteration recovers these methods, while additional alternating iterations refine toward the best rank- r r direction. Projection and SVD-based low-rank updates. Projecting a dense update back to rank r r via truncated SVD (our SVDLoRA oracle) is the optimal Frobenius-norm rank- r r projection, but it is typically too expensive for large layers. This motivates iterative low-rank approximation (e.g., subspace iteration) and incremental SVD updates (Brand, 2006 ) . Our LoRSum can be seen as a warm-started subspace-iteration approximation that uses only thin operations and exploits gradient factorization. Related in spirit, GaLore projects gradients to a low-dimensional subspace (Zhao et al., 2024 ) . We instead target the rank- r r projection of a full step , with K K controlling projection accuracy. LoRA variants, reparameterizations, and initialization. A number of works modify LoRA or its training protocol to improve efficiency or generalization. AdaLoRA adapts rank allocation using SVD-inspired criteria (Zhang et al., 2023b ) , while DoRA reparameterizes weights into magnitude and direction and adapts both components (Liu et al., 2024 ) . Other methods periodically merge adapters into the base weights or change the adapter schedule (Lialin et al., 2023 ) . Several approaches focus on improving optimization through initialization or local geometry. PiSSA initializes LoRA adapters using leading singular directions of pretrained weights (Meng et al., 2024 ) , and LoRA-Pro uses a higher-dimensional tangent space to derive improved updates (Wang et al., 2025 ) . Particularly close in motivation is LoRA-One. It uses an SVD-based initialization associated with projecting a full step ( SVDLoRA ) and then applies preconditioned factor updates (Zhang et al., 2025 ) . Our contribution is complementary because rather than changing the adapter or relying on periodic SVD resets, we provide a memory-efficient subroutine that approximates the rank- r r projected step itself. Optimizer state under rank constraints and Fisher-structured scaling. Maintaining optimizer state (e.g. momentum) in low-rank training is nontrivial because naive momentum accumulates past gradients that generally do not lie in the current factor subspaces. LoFT addresses this by projecting momentum-like quantities onto iteratesâ€™ subspaces, which can restrict momentum to joint subspaces across time (Tastan et al., 2025 ) . Recent work also proposes maintaining explicit low-rank factorizations of momentum matrices (Mahdavinia Mahdavi, 2025 ) . In our approach, the same LoRSum routine used for projected steps can also be used to maintain a low-rank estimate of momentum in Frobenius norm. Moreover, we incorporate curvature information through Fisher/Kronecker-structured metrics. K-FAC approximates the natural-gradient metric using Kronecker-factored Fisher blocks (Heskes, 2000 ; Martens Grosse, 2020 ) , and Shampoo uses Kronecker-structured second moments with fractional matrix powers (Gupta et al., 2018 ) . We endow our projection subproblem with such metrics (and practical diagonal approximations), yielding scaled variants of LoRSum and PSI-LoRA . How our work is positioned. Overall, we provide a reusable, SVD-free projection subroutine for approximating full steps and low-rank state updates within standard LoRA fine-tuning. 3 Without SVD: Proximal Subspace Iteration LoRA In this section, we derive LoRSum , a memory-efficient subroutine for approximating the best rank- r r projection of a full step, and describe PSI-LoRA (Proximal Subspace Iteration LoRA), a LoRA optimizer that use