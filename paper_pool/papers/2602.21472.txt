Title: The Design Space of Tri-Modal Masked Diffusion Models

Abstract: Discrete diffusion models have emerged as strong alternatives to autoregressive language models, with recent work initializing and fine-tuning a base unimodal model for bimodal generation. Diverging from previous approaches, we introduce the first tri-modal masked diffusion model pretrained from scratch on text, image-text, and audio-text data. We systematically analyze multimodal scaling laws, modality mixing ratios, noise schedules, and batch-size effects, and we provide optimized inference sampling defaults. Our batch-size analysis yields a novel stochastic differential equation (SDE)-based reparameterization that eliminates the need for tuning the optimal batch size as reported in recent work. This reparameterization decouples the physical batch size, often chosen based on compute constraints (GPU saturation, FLOP efficiency, wall-clock time), from the logical batch size, chosen to balance gradient variance during stochastic optimization. Finally, we pretrain a preliminary 3B-parameter tri-modal model on 6.4T tokens, demonstrating the capabilities of a unified design and achieving strong results in text generation, text-to-image tasks, and text-to-speech tasks. Our work represents the largest-scale systematic open study of multimodal discrete diffusion models conducted to date, providing insights into scaling behaviors across multiple modalities.

Body: The Design Space of Tri-Modal Masked Diffusion Models 1 Introduction 2 Background and Related Work 2.1 Masked Diffusion Models Text. Image. Audio. 2.2 Multimodal Masked Diffusion Models 3 Method Training. Denoising model. Inference. 3.1 Architecture 4 Hyperparameter Transfer 4.1 Eliminating B opt B_{\text{opt}} with SDE Parametrization 4.2 Isonoise and Isohorizon Scaling Rules 5 Scaling Behavior of MDM under the SDE Transfer Rule 5.1 Scaling Rules for Critical Batch Size Critical batch size without SDE. Critical batch size for SDE. Critical batch size as function of model size. Critical batch-size as a function of the token horizon. 5.2 Optimal Drift‚Äìhorizon Tradeoffs Link with learning rate schedule. 5.3 Scaling Laws for Tri-modal MDM Analysis. Interpretation. 6 Data Text data. Audio-text data. Image-text data. 7 Results 7.1 Unified 3B Tri-modal MDM 7.2 Modality Mixing Ratios 7.3 Best Generation Hyperparameters Text-to-image generation. Text-to-speech generation. 7.4 Anti-Masking 8 Conclusion A General formulation of weighting and the masking process A.1 Connection of Loss Weighting and Cumulative Corruption A.2 Unbiasedness of the Importance Weighting B Tokenizer Ablations B.1 Audio Tokenizer Ablations B.2 Image Tokenizer Ablations C Training details C.1 Optimal Global Hyper-Parameter Search C.2 Runtime as Function of Batch Size C.3 Hyperparameters for the Unified 3B Tri-modal MDM D MDM with Per-module Hyperparameters Training Details. Multiplier analysis. E Extended Scaling Laws Results Computation of FLOPs for experiments. Scaling laws for Uni-Modal Text MDM. F Masking Schedules for Image and Audio Generation G Extended generations H Contributions Code. Experiments. Data. General Infrastructure. Theoretical formulation and situating work. Project Organization and Tech Lead. I Acknowledgments The Design Space of Tri-Modal Masked Diffusion Models Louis Bethune Victor Turrisi Bruno Kacper Mlodozeniec 3‚Ä† Pau Rodriguez Lopez Lokesh Boominathan Nikhil Bhendawade Amitis Shidani Joris Pelemans Theo X. Olausson 4‚Ä† Devon Hjelm Paul Dixon Jo√£o Monteiro Pierre Ablin Vishnu Banna Arno Blaas Nick Henderson Kari Noriy Dan Busbridge Josh Susskind Marco Cuturi Irina Belousova Luca Zappella Russ Webb Jason Ramapuram 2 Apple, 2 Google Deepmind (work done at Apple), 3 University of Cambridge, 4 MIT, ‚Ä† Work done during Apple internship. Abstract Discrete diffusion models have emerged as strong alternatives to autoregressive language models, with recent work initializing and finetuning a base unimodal model for bi-modal generation. Diverging from previous approaches, we introduce the first tri-modal Masked Diffusion Models (MDM) pretrained from scratch on text, image-text, and audio-text data. We systematically analyze multimodal scaling laws, modality mixing ratios, noise schedules, batch-size effects and provide optimized inference sampling defaults. Our batch-size analysis yields a novel stochastic differential equation (SDE) based reparameterization, eliminating the need for tuning the optimal batch size as reported in recent work. This re-parameterization decouples the physical batch size , often chosen based on compute (GPU saturation, FLOP-efficiency, wall-clock time) from the logical batch size , chosen to balance the variance of gradients during stochastic optimization. Finally, we pretrain a preliminary model showcasing the capabilities of a unified design, achieving strong results at 3B model scale (6.4T tokens), in both text generation, T2I tasks, and T2S tasks. Our work represents the largest scale systematic open study of multimodal discrete diffusion models conducted to date, providing valuable insights into scaling behaviors across multiple modalities. \metadata [Correspondence] Louis Bethune: louisbethune@apple.com ; Victor Turrisi: v_turrisi@apple.com ; Jason Ramapuram: jramapuram@google.com . For a detailed breakdown of author contributions see Appendix Àú H . 1 Introduction A recurring theme in sequence modeling is that, whenever the full context is available, bidirectional information tends to perform better. Early work on bidirectional RNNs ( DBLP:journals/tsp/SchusterP97 ) and LSTMs ( DBLP:journals/nn/GravesS05 ) demonstrated clear gains over purely forward recurrent models when both past and future states were accessible during training. This makes the dominance of causal transformers ( DBLP:conf/nips/VaswaniSPUJGKP17 ) in modern language modeling slightly surprising: the strongest transformer-based language models ( DBLP:journals/corr/abs-2412-19437 ; DBLP:journals/corr/abs-2312-11805 ; singh2025openai ; claude3 ) are trained with a strictly left-to-right factorization (auto-regressively). The causal constraint is undeniably convenient (simple likelihood factorization, efficient per-token learning signal and fast streaming decoding via KV cache), but it is not evidently the best fit for conditional generation problems where the observed evidence may be scattered across positions and modalities. Discrete diffusion revisits the bidirectional viewpoint by replacing a fixed generation order with iterative refinement: rather than predicting the next token, the model repeatedly denoises a partially corrupted sequence. Recent progress in diffusion based language modeling ( DBLP:journals/corr/abs-2502-09992 ) has narrowed much of the quality gap to strong causal baselines, including widely used models such as LLaMA 3 ( DBLP:journals/corr/abs-2407-21783 ) , strengthening the case that order agnostic, globally conditioned generation can be competitive at scale. Despite narrowing the performance gap at equivalent pretraining FLOP budgets, naive implementations still exhibit substantial latency compared to autoregressive baselines and further work is required to improve sampling efficiency in MDM ( jazbec2025learning ; DBLP:journals/corr/abs-2505-22618 ) . This refinement perspective is particularly appealing in multimodal settings. MDM s train on a simple corruption process (masking) and learn to reconstruct missing tokens. With multimodal tokenization, text, image, and audio tokens can be concatenated, partially masked, and jointly denoised. This naturally supports infilling and arbitrary conditioning without re-deriving a new factorization for every task. Although image and audio tokens could be independently modeled in continuous domains as in DBLP:journals/corr/abs-2509-16197 , we instead adopt discrete modeling to streamline optimization and substantially reduce complexity by employing a unified embedding space and loss function. While much of the current multimodal MDM literature emphasizes adapting existing pretrained models, either by performing supervised finetuning on discrete diffusion bases like LLaDA ( DBLP:journals/corr/abs-2505-16933 ; DBLP:journals/corr/abs-2505-15809 ) or by distilling and repurposing autoregressive backbones such as Qwen 2.5-Coder or other large AR models ( DBLP:journals/corr/abs-2409-12186 ; DBLP:journals/corr/abs-2506-20639 ; DBLP:journals/corr/abs-2510-01329 ; DBLP:conf/icml/ZhangZ0TOSJ25 ; DBLP:journals/corr/abs-2512-15745 ) , our work targets the pretraining regime, where the dominant compute is spent and where the latent spaces are shaped. (a) (b) (c) (d) Figure 1 : High-fidelity generation. The pretrain-only 3B MDM demonstrates strong prompt adherence alongside high-quality visual rendering of texture , lighting , and composition . Samples show: (a) natural daylight and depth of field ("egg in a field of crocuses"); (b) fine-grained fur texture in B W ("lion‚Äôs face"); (c) soft, warm lighting with vintage color tones ("preparing bread dough"); and (d) complex multi-object arrangement ("noodle soup with toppings"). Extended generations in Appendix Appendix Àú G . Moving from unimodal to multimodal MDM introduces a large and underexplored design space. Choices that may appear secondary in isolation can dominate stability and compute efficiency. Because exhaustive large-scale sweeps are often infeasible, progress depends on reliable transfer rules, i.e. rules on how to transfer hyperparameters from small models to larger models. In this work, we take a step toward sound pretraining recipes for native multimodal MDM s. We extend MDM to a tri-modal setting (text, image, audio) via a unified discrete token space with modality-specific boundary and mask tokens. The single resulting model supports multiple conditional generation queries, including text-to-image generation, image captioning, text-to-speech (TTS) and automatic speech recognition (ASR) . Our main contribution is an empirical study of the pretraining and inference choices that govern scaling behavior and efficiency in this regime, together with controlled inference-time ablations that reveal modality-specific sensitivities: 1. Unified Multimodal MDM. Introduce the first tri-modal MDM capable of generating text, image, and audio from each other in any direction via a single transformer backbone and unified vocabulary, eliminating the need for modality-specific heads, adapters or unimodal conditioning ( DBLP:conf/nips/LiuLWL23a ) . 2. Elimination of Optimal Batch Size ( B opt B_{\text{opt}} ) and per-module transfer. We leverage SDE-based reparameterization to render training loss invariant to batch size up to a critical threshold ( B crit B_{\text{crit}} ), eliminating the need to search for an optimal batch size , B opt B_{\text{opt}} ( DBLP:journals/corr/abs-2505-13738 ) . We also validate the effectiveness of per-module (e.g., MLP, attention weights) hyperparameter scaling using CompleteP + SDE scaling ( mlodozeniec2025completed ) within the multimodal MDM regime ( Appendix D ). 3. Multimodal Scaling Laws. We derive empirical scaling laws for validation loss as a function of model size ( N N ) and token budget ( D D ), providing prescriptive guidance for compute optimal tri-modal MDMs. We find the seminal formula L ‚Äã ( N , D ) = E + ( A ‚Äã N ‚àí a / b + B ‚Äã D ‚àí 1 ) b L(N,D)=E+(AN^{-a/b}+BD^{-1})^{b} from DBLP:journals/corr/abs-2001-08361 to be a better fit than the additive form of DBLP:journals/corr/abs-2203-15556 . In particular, we find these models to be asymptotically more data efficient than their auto-regressive counterpart, with the compute optimal frontier of D ‚ãÜ ‚Äã ( N ) ‚âà 7754 ‚ãÖ N 0.84 D^{\star}(N)\approx 7754\cdot N^{0.84} . 4. Modality Dependent Design Space. We characterize the distinct inference tradeoffs for each modality, identifying that optimal noise schedules and sampling parameters (guidance, temperature) differ significantly between text, image, and audio generation. 2 Background and Related Work 2.1 Masked Diffusion Models Although diffusion models first gained prominence through their success in continuous settings such as image generation ( DBLP:conf/iclr/SongME21 ) , the original formulation by DBLP:conf/icml/Sohl-DicksteinW15 provided a unified framework encompassing both continuous and discrete domains. One form of diffusion on discrete data, MDM s were first proposed by DBLP:conf/nips/AustinJHTB21 , and generalized in earlier discrete diffusion work by DBLP:conf/nips/HoogeboomNJFW21 . In their formulation, the diffusion forward steps, q ‚Äã ( x t | x t ‚àí 1 ) q(x_{t}|x_{t-1}) , progressively noise the data x 0 x_{0} with mask tokens [MASK], turning the data distribution q 0 := p d ‚Äã a ‚Äã t ‚Äã a q_{0}:=p_{data} into a stationary distribution q T := q ‚Äã ( x T ) q_{T}:=q(x_{T}) in which every token is masked. A masked diffusion model p Œ∏ ‚Äã ( x t ‚àí 1 | x t ) p_{\theta}(x_{t-1}|x_{t}) with parameters Œ∏ \theta then learns the reverse process such that starting from the stationary masked distribution q T q_{T} , the reverse process ‚àè t p Œ∏ ‚Äã ( x t ‚àí 1 | x t ) \prod_{t}p_{\theta}(x_{t-1}|x_{t}) reconstructs the original text from such noised sequences. This approach for masked diffusion with fixed timesteps t ‚àà 0 , 1 , ‚Ä¶ , T t\in 0,1,\ldots,T was extended to a continuous-time framework by DBLP:conf/nips/CampbellBBRDD22 , who formulated the forward noising process and corresponding reverse process as Continuous Time Markov Chains (CTMCs), and by DBLP:journals/corr/abs-2211-15089 , who propose embedding discrete inputs into continuous space before learning the diffusion model. Below, we provide an overview of applications of MDMs to the three modalities we tackle in this work. Text. DBLP:conf/nips/AustinJHTB21 first applied MDMs to relatively small-scale text datasets like LM1B ( DBLP:conf/interspeech/ChelbaMSGBKR14 ) . Then, recent adaptations of the continuous-time framework by DBLP:conf/nips/CampbellBBRDD22 have enabled training of larger language diffusion models. LLaDA ( DBLP:journals/corr/abs-2502-09992 ) , for instance, trained an 8B-parameter MDM on a 2.3T token corpus, obtaining strong performance on benchmarks such as MMLU ( DBLP:conf/iclr/HendrycksBBZMSS21 ) and GSM8K ( DBLP:journals/corr/abs-2110-14168 ) . In contrast, Dream ( DBLP:journals/corr/abs-2508-15487 ) finetuned a pretrained autoregressive Qwen2.5-7B model using a 580B token corpus ( DBLP:journals/corr/abs-2412-15115 ) , without accounting for the initial AR model‚Äôs pretraining budget. Both methods employ the same training objective, representing an upper bound on the negative log-likelihood, or variational evidence lower bound (ELBO) , of the continuous-time masked diffusion process: ‚àí ùîº x 0 ‚àº p d ‚Äã a ‚Äã t ‚Äã a , t ‚àº U ‚Äã ( 0 , 1 ) x t ‚àº q ‚Äã ( x t | x 0 ) ‚Äã [ w ‚Äã ( t ) ‚Äã ‚àë ‚Ñì = 1 L ùüè x t ‚Ñì = MASK ‚Äã p Œ∏ ‚Äã ( x 0 ‚Ñì | x t ) ] . -\mathbb{E}_{\begin{subarray}{c}x_{0}\sim p_{data},\penalty 10000\ t\sim U(0,1)\\ x_{t}\sim q(x_{t}|x_{0})\end{subarray}}\left[w(t)\sum_{\ell=1}^{L}{\mathbf{1}_{x^{\ell}_{t}=\text{MASK}}\,p_{\theta}(x_{0}^{\ell}|x_{t})}\right]. (2.1) Here L L is the sequence length and x ‚Ñì x^{\ell} denotes the ‚Ñì ‚àí \ell- th token of x x . The indicator function ùüè x t ‚Ñì = MASK \mathbf{1}_{x^{\ell}_{t}=\text{MASK}} makes sure the loss is computed only over masked tokens. The weights w ‚Äã ( t ) w(t) depend on the form of the forward noise q ‚Äã ( x t | x s ) q(x_{t}|x_{s}) ; for LLaDA, it is w ‚Äã ( t ) = 1 / t w(t)=1/t , while Dream uses a more complex schedule. Follow up works such as DBLP:journals/corr/abs-2505-19223 and DBLP:journals/corr/abs-2509-24389 improve performance and efficiency by introducing variance reduction and mixture-of-experts (MoE) methods to large language diffusion. We provide a principled exposition of weighting in Appendix A . Image. DBLP:conf/nips/AustinJHTB21 and DBLP:conf/nips/ShiHWDT24 apply masked diffusion directly to pixel values by modeling them as categorical variables. However, their experiments are restricted to low resolution image datasets, such as CIFAR10 and downsampled ImageNet 64x64 ( DBLP:conf/cvpr/DengDSLL009 ; DBLP:conf/icml/OordKK16 ) . MaskGIT and VQ-Diffusion ( DBLP:conf/cvpr/ChangZJLF22 ; DBLP:conf/cvpr/GuCBWZCYG22 ) instead use pretrained image tokenizers such as VQ-GAN ( DBLP:conf/cvpr/EsserRO21 ; sber_movqgan ; zheng2022movqmodulatingquantizedvectors ) or VQ-VAE ( DBLP:conf/nips/OordVK17 ) to do