Title: Deploying a Hybrid PVFinder Algorithm for Primary Vertex Reconstruction in LHCb's GPU-Resident HLT1

Abstract: LHCb's Run 3 upgrade introduced a fully software-based trigger system operating at 30~MHz, processing an average of 5.6 proton-proton collision vertices per bunch crossing (event). This work presents the development of an inference engine for PVFinder, a hybrid deep neural network for finding primary vertices, the proton-proton collision points from which all subsequent particle decays originate into Allen, LHCb's High Level Trigger (HLT1) framework. The integration addresses critical real-time constraints including fixed memory pools, single-stream execution, and sub-400~$μ$s per-event processing budgets on NVIDIA GPUs. We introduce a translation layer that bridges Allen's Structure-of-Arrays (SoA) data layout with cuDNN's tensor format while maintaining zero-copy semantics and deterministic behavior. Current performance shows the CNN stage contributes significant throughput overhead. We present a roadmap targeting order-of-magnitude improvements through mixed-precision computing, model compression and other techniques.

Body: 1 Introduction 1 Introduction 2 PVFinder Architecture for Real-Time Inference 3 Allen Integration and Translation Layer 4 Integration Performance Results 5 Optimization Roadmap and Outlook 6 Conclusions Deploying a Hybrid PVFinder Algorithm for Primary Vertex Reconstruction in LHCb’s GPU-Resident HLT1 Simon Akar 1 , Mohamed Elashri 2,† , Conor Henderson 2 , Michael Sokoloff 2 1 LPC, Clermont-Ferrand, France 2 Department of Physics, University of Cincinnati, OH, USA † Corresponding Author ABSTRACT LHCb’s Run 3 upgrade introduced a fully software-based trigger system operating at 30 MHz, processing an average of 5.6 proton-proton collision vertices per bunch crossing (event). This work presents the development of an inference engine for PVFinder, a hybrid deep neural network for finding primary vertices, the proton-proton collision points from which all subsequent particle decays originate into Allen, LHCb’s High Level Trigger (HLT1) framework. The integration addresses critical real-time constraints including fixed memory pools, single-stream execution, and sub-400 μ \mu s per-event processing budgets on NVIDIA GPUs. We introduce a translation layer that bridges Allen’s Structure-of-Arrays (SoA) data layout with cuDNN’s tensor format while maintaining zero-copy semantics and deterministic behavior. Current performance shows the CNN stage contributes significant throughput overhead. We present a roadmap targeting order-of-magnitude improvements through mixed-precision computing, model compression and other techniques. Contribution to the proceedings of the Connecting the Dots Workshop (CTD 2025), November 10–14, 2025 1 Introduction The LHCb experiment at the Large Hadron Collider [ 3 ] studies matter-antimatter asymmetry through precision measurements of beauty and charm hadron decays. Run 3 operations introduced five-fold luminosity increase with respect to the previous period op operations (2010–2018), resulting in 30 MHz collision rate with 5.6 primary vertices (PVs) per event on average [ 6 , 7 ] , with event selection performed by a fully software-based running on GPUs. The Allen framework [ 5 ] implements LHCb’s High-Level Trigger Level 1 (HLT1) on GPUs with strict constraints: sub-400 μ \mu s per-event processing, fixed memory pools, and single-stream execution. PVFinder [ 2 ] is a novel algorithm using Machine Learning to reconstruct primary vertices in LHC collisions. The model is a hybrid deep neural network combining fully-connected (FC) layers with a one-dimensional (1D) convolutional neural networks (CNN), achieving 97% efficiency with 0.03 false positives per event. Achieving optimal trigger performance requires balancing speed and physics sensitivity: improving reconstruction quality while maintaining the throughput demanded by 30 MHz operations. Modern deep learning architectures, including CNN, offer significant gains in physics performance over traditional heuristic approaches for tasks such as vertex finding, track fitting, and particle identification. However, integrating these algorithms into Allen’s GPU-resident framework presents unique challenges, as standard ML inference patterns, dynamic memory allocation, multi-stream execution, and library-managed workspaces, conflict with Allen’s deterministic, fixed-resource execution model. Resolving this tension is essential for instrumenting the trigger with state-of-the-art algorithms without compromising its real-time guarantees. This work provides a first demonstration of how to approach this problem, using primary vertex reconstruction as a concrete case. We present the development of an inference engine for PVFinder inside Allen’s HLT1 framework, introducing a translation layer that bridges Allen’s native CUDA environment with cuDNN’s tensor execution model for CNN inference. We characterize the integrated throughput performance and present an optimization roadmap targeting deployment in regular data-taking operations by 2030. Due to the nature of this work, it should be understood as a proof-of-concept deployment that establishes patterns applicable to future ML integration efforts in Allen. 2 PVFinder Architecture for Real-Time Inference PVFinder transforms reconstructed track parameters into primary vertex positions through a three-stage pipeline: feature extraction via FC layers, spatial pattern recognition using a UNet-style CNN, and heuristic peak finding. The input consists of nine features per track from VELO detector reconstruction. A fixed spatial slicing scheme divides the 400 mm interaction region into 40 intervals of 100 bins each. The FC stage processes tracks through six FC layers implemented in native CUDA, combining each track’s contribution into a 800-bin histogram (8 channels each with 100-bin). CNN stage employs a 5-layer UNet architecture (64 channels deployed): the encoder reduces spatial resolution while increasing channel depth, capturing local and contextual patterns; the decoder upsamples to original resolution producing refined probability distributions with enhanced nearby-vertex separation. Two design decisions enable Allen compatibility: the FC stage outputs in [B, C, L] format required by cuDNN, eliminating data reorganization between stages, all memory requirements are pre-allocated from Allen’s pools at initialization. The peak finding stage applies local maximum detection with learned thresholds. Figure 1: PVFinder physics performance showing efficiency vs. false positive rate for different configurations. The magenta configuration (FP32, 64-channel UNet) selected for deployment achieves 97% efficiency with 0.03 false positives per event, significantly outperforming the LHCb heuristic baseline [ 4 ] . FP16 configurations show minimal performance degradation. Physics performance on simulated events shows 97% efficiency for 3-8 vertices per event with 0.03 false positives per event, as shown in Fig. 1 . The algorithm produces 0.6% spurious vertices per true PV. The complete architecture is shown in Fig. 2 . Track Parameters (9 features) Fully Connected Network (6 layers) UNet CNN (5 layers) PV Histogram (100 bins) Native CUDA cuDNN Figure 2: PVFinder hybrid architecture showing the three-stage pipeline: FC layers process track parameters (9 features/track) into some representation, UNet CNN refines spatial patterns into probability histograms, and peak finding extracts vertex positions. The FC stage is implemented in native CUDA while the CNN stage uses cuDNN, bridged by the translation layer. 3 Allen Integration and Translation Layer Allen’s architecture imposes constraints that diverge from typical ML deployment environments. Allen processes events using stream-ordered execution where each event maps to one GPU thread block with a SoA layout for coalesced memory access. Memory management employs fixed pools, runtime dynamic allocation is forbidden to ensure predictable latency and avoid memory fragmentation that would destabilize the trigger. All operations execute on a single framework-managed stream without global synchronizations that would serialize event processing. These choices enable 30 MHz rates with deterministic performance but create interface mismatch with cuDNN’s (and other standard inference libraries) NCHW tensors, workspace management, and typical multi-stream execution patterns. The translation layer serves as a three-phase adapter implementing zero-copy semantics while maintaining deterministic guarantees. The Prepare phase creates non-owning tensor views reinterpreting Allen’s SoA buffers as cuDNN-compatible tensors. Since FC output is already in [B, C, L] format (B=40 spatial intervals, C=64 channels, L=800 bins per interval), most data requires no reorganization. We allow tracks to contribute to one or two intervals which means that the tracks on boundaries of intervals are part of two intervals and are handled as pre-allocated staging buffers. Shape validation and descriptor creation occur at initialization time; per-event overhead reduces to pointer arithmetic and bounds checking, contributing negligible latency to the critical path. The Execute phase invokes cuDNN convolution operations on Allen’s stream, preserving event-parallel semantics without introducing serialization points. A fixed workspace arena (16 MB) sized for largest intermediate activations is pre-allocated, bound to the cuDNN handle at initialization, and reused across all events. This eliminates per-event allocation overhead that would violate Allen’s memory model constraints. The implementation ensures no default CUDA stream operations leak into the execution path, which would introduce implicit synchronizations violating Allen’s deterministic execution model. The Extract phase maps cuDNN outputs to Allen’s SoA buffers for downstream algorithm consumption. Output data remains in cuDNN-native layout until explicitly accessed by consumers, minimizing unnecessary data movement and cache pollution. The peak finding algorithm reads directly from this layout, avoiding additional format conversion. The API exposes minimal initialization (TensorSpec freezing batch dimensions, channel count, sequence length, and data type; init() method accepting Allen’s stream, workspace allocation, and cuDNN backend handle) and lightweight per-event execution (TensorView creation, execute() invocation). Error handling employs a Status enum enabling graceful degradation: events failing validation bypass ML reconstruction without disrupting data acquisition. 4 Integration Performance Results Performance evaluation focuses on characterizing throughput impact within Allen’s HLT1 budget. All measurements are performed on NVIDIA RTX 2080 Ti GPUs. Standalone measurements characterize the computational cost of each pipeline stage. The FC stage processes events at 300 kHz throughput ( 5% HLT1 budget), while the complete hybrid model reaches 110 kHz, indicating CNN dominance. Integration into Allen’s full HLT1 sequence reveals the overhead of running within the complete reconstruction chain. Baseline HLT1 serves as the reference 100%. Adding only FC reduces to 95%, consistent with standalone measurements. Full hybrid implementation reduces to 25%—a 75% reduction, as summarized in Table 1 . Configuration Relative Throughput Notes Baseline HLT1 100% Reference + FC stage only ∼ \sim 95% Native CUDA + Full hybrid (FC+CNN) ∼ \sim 25% Current Target (2030) ≥ \geq 95% After optimization Table 1: HLT1 throughput impact measured on NVIDIA RTX 2080 Ti. The CNN stage dominates overhead, reducing throughput to 25% of baseline. Optimization roadmap targets return to ≥ \geq 95% performance. Profiling identifies three contributing factors. First, memory bandwidth saturates under the combined load of VELO tracking algorithms, PVFinder inference, and downstream trigger reconstructions. Standalone measurements isolated PVFinder’s bandwidth consumption and could not capture these interference effects. Second, cache interference between PVFinder’s working set (model parameters, intermediate activations) and other algorithms’ data structures degrades memory access latency across the entire trigger chain, creating system-wide performance degradation beyond PVFinder’s direct costs. Third, the unoptimized CNN implementation does not fully utilize available compute resources; streaming multiprocessor occupancy remains below 50% during convolution operations due to suboptimal kernel launch configurations that leave compute units idle while memory operations dominate execution time. While current implementation does not meet 5% regression targets, the detailed characterization validates the integration approach and identifies specific, actionable optimization paths. 5 Optimization Roadmap and Outlook Performance analysis reveals CNN stage improvements are required to reach 5% HLT1 throughput regression. Multiple optimization strategies address the identified bottlenecks with clear technical justification. Mixed-precision inference using FP16 provides 2 × \times throughput improvement through two mechanisms. First, halving precision from 32 to 16 bits doubles arithmetic intensity, allowing the same memory bandwidth to feed twice as many operations. Second, FP16 enables tensor core utilization on modern GPUs, providing dedicated hardware for matrix multiplication with 1.5-2 × \times additional speedup through specialized 4 × \times 4 × \times 4 multiply-accumulate (MMA) units. Physics validation confirms FP16 introduces 0.5% efficiency degradation, well within acceptable tolerances for trigger applications. The combination of bandwidth and compute improvements yields 3-4 × \times [ 1 ] total speedup from precision alone. Model compression via 32-channel UNet (versus current 64 channels) provides 4 × \times speedup due to quadratic scaling of convolution cost with channel count. Convolution operations scale as O ​ ( C i ​ n × C o ​ u ​ t × K ) O(C_{in}\times C_{out}\times K) where C C denotes channels and K K is kernel size. Halving channels therefore reduces compute by 4 × \times while memory footprint (model parameters and activations) also decreases by 4 × \times . This addresses both the compute and memory bandwidth bottlenecks identified in profiling. Physics studies indicate the 32-channel variant maintains 96% efficiency, suggesting the 64-channel model is over parameterized for this task. Knowledge distillation from the 64-channel teacher to 32-channel student model can potentially recover any marginal performance loss. Memory layout optimizations target the observed cache interference and bandwidth saturation. Fusing the FC stage output directly into contiguous [B, C, L] cuDNN-native tensors eliminates intermediate format conversion overhead and improves spatial locality. Explicit workspace reuse minimizes working set size, reducing cache pollution for concurrent algorithms. Launch parameter tuning addresses the 50% on average SM occupancy by optimizing thread block dimensions and register allocation, allowing better hiding of memory latency through increased parallelism. Profiling shows that current convolution kernels achieve only 40-45% theoretical occupancy due to register pressure and shared memory conflicts. Restructuring thread block geometry to reduce per-thread register usage while maintaining sufficient parallelism can increase occupancy to 75-80%, directly translating to improved throughput. Combined memory optimizations contribute an additional 1.5 × \times speedup. The optimization strategy prioritizes changes with highest impact-to-effort ratio. FP16 conversion requires minimal code changes (data type specifications and range validation) but delivers immediate 2 × \times gains. The 32-channel model necessitates retraining but the training infrastructure already exists. Memory optimizations involve systematic kernel restructuring guided by profiler feedback. This phased approach allows incremental validation at each step rather than attempting all optimizati