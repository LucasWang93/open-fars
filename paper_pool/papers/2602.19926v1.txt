Title: Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models

Abstract: Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\textbf{L}ocal \textbf{A}lternating \textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($Œµ= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\% in test accuracy. Code is provided in \repolink.

Body: Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models 1 Introduction 2 Background And Related Work 3 Limitations of LoRA under Privacy-Preserving FL 3.1 Gradient Coupling in Asymmetric Updates 3.2 Structural Amplification of DP Noise 3.3 Sharpness in Global Aggregation 4 Our Method 4.1 Local Alternating Update Strategy 4.2 Smoothing with a Low-Pass Filter 4.3 LA-LoRA Framework Local client update. 5 Theoretical Analysis 6 Experiments 6.1 Experimental setups 6.2 Results 6.3 Ablation Study 7 Discussion and Conclusion 8 Ethics Statement 9 Reproducibility Statement A Additional Details for Image Classification A.1 Datasets and models A.2 Experimental setup A.3 Additional experimental results A.4 Computational and memory overhead A.5 Impact of different ranks on performance A.6 Centralized Experiments A.7 Non-Private Federated Experiments B Additional Details for Language Understanding B.1 Datasets and models B.2 Experimental setup B.3 Additional experimental results B.4 Additional Experiments on Llama-2-7B B.5 Language model results under data heterogeneity Œ≤ = 0.3 \beta=0.3 C Additional Ablation Studies C.1 Ablation Results for Swin-T on CIFAR-100 and Tiny-ImageNet C.2 Ablation on Smoothing Strategies C.3 Ablation on the three challenges C.4 Effect of local steps K D More results for Gaussian low-pass smoothing filter D.1 Smoothing parameter œÉ s \sigma_{s} D.2 Effect of different kernel widths E Detailed Theoretical Analysis E.1 Privacy guarantee Composition (additivity) in RDP. Conversion from RDP to ( œµ , Œ¥ ) (\epsilon,\delta) -DP. Post-processing. E.2 Closed-form projected gradients E.3 Stable feature learning First half-step (updating B B ). Second half-step (updating A A ). E.4 Convergence Analysis E.4.1 Set Up E.4.2 Useful Lemma E.5 Comparison with FFA-LoRA and RoLoRA FFA-LoRA: fixed row-space subspace. RoLoRA: stale-block remainder ‚Ñ∞ k \mathcal{E}_{k} . F TABLE OF NOTATIONS G LLM Usage Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models Jin Liu 1 , Yinbin Miao 1 , Ning Xi 1 , Junkang Liu 2 1 School of Cyber Engineering, Xidian University 2 College of Intelligence and Computing, Tianjin University {jinliu9787, junkangliukk}@gmail.com, {ybmiao, nxi}@xidian.edu.cn Corresponding author. Abstract Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA ( L ocal A lternating LoRA ), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ( œµ = 1 \epsilon=1 ), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83% in test accuracy. Code is provided in https://anonymous.4open.science/status/LA-LORA-50E5/ . 1 Introduction As foundation models such as GPT (Achiam et al. , 2023 ) , BERT (Kenton et al. , 2019 ) , and ViT (Dosovitskiy et al. , 2020 ) scale in size and capacity, adapting them to downstream tasks increasingly relies on vast and heterogeneous datasets. However, centralized collection is becoming impractical due to data silos and rising concerns over user privacy (Liu et al. , 2024b ; a ) . Federated learning (FL, McMahan et al. ( 2017 ) ) offers a privacy-preserving solution by enabling decentralized model training without sharing raw data across clients. Despite this promise, applying large-scale models in FL remains a major challenge. These models typically consist of billions of parameters, making full-model fine-tuning computationally expensive and communication-heavy. To mitigate this, parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA, Hu et al. ( 2021 ) ), have been integrated into FL. Approaches like FedIT (Zhang et al. , 2024a ) freeze the majority of the model weights and only update a lightweight set of LoRA parameters, reducing communication overhead to less than 0.1% of the full model while preserving performance . However, privacy remains a critical concern in FL . Although raw data is not directly shared, transmitting gradients or model updates can still expose sensitive information. Prior work has shown that adversaries may reconstruct private data from such gradients (Liu et al. , 2024c ; Han et al. , 2024 ; Fan et al. , 2025 ; Ye et al. , 2025 ) . To provide formal privacy guarantees, differential privacy (DP, Dwork ( 2006 ) ) is commonly integrated into federated optimization. While differentially private federated learning (DPFL) with PEFT methods (e.g., DP-LoRA, Liu et al. ( 2025e ) ) provides privacy guarantees, it often incurs reduced performance due to federated constraints. Federated training introduces additional complexities, including non-iid data distributions, noisy updates, and difficulties in aggregating LoRA modules. In practice, LoRA-based fine-tuning under privacy constraints often suffers from severe performance degradation. Beyond the prior finding of DP noise amplification in FFA-LoRA (Sun et al. , 2024a ) , we identify two further challenges, leading to three key issues of LoRA in DPFL Feng et al. ( 2022 ); Feng and Patras ( 2023 ); Feng et al. ( 2024b ; a ) : ‚Ä¢ Gradient coupling. Simultaneous updates to LoRA‚Äôs asymmetric matrices cause gradient interference, destabilizing training, especially under DP noise and non-iid distributions. ‚Ä¢ Amplified DP noise. LoRA‚Äôs semi-quadratic update structure amplifies the impact of injected DP noise, severely degrading learning stability. ‚Ä¢ Sharp global solutions. LoRA‚Äôs low-rank constraints reduce client capacity, often resulting in sharp global minima after aggregation, compromising robustness and generalization. To overcome these, we propose LA-LoRA , a novel framework built on a structural rethinking of how LoRA fits into DPFL. Its core is a local alternating update mechanism that decouples gradient interference and suppresses DP noise. To further enhance stability and generalization, LA-LoRA incorporates a simple yet effective low-pass smoothing filter . We provide theoretical evidence that our update scheme ensures stable optimization while preserving the low-rank structure of LoRA. Extensive experiments on both image classification and language understanding tasks validate the effectiveness of LA-LoRA. Results show that LA-LoRA achieves strong performance while offering privacy protection and optimization stability. Our contributions are summarized as follows: ‚Ä¢ We identify two overlooked challenges of applying LoRA in DPFL: gradient coupling and aggregation sharpness. Along with noise amplification, these motivate the design of our algorithm. ‚Ä¢ We propose LA-LoRA, a novel algorithm that alternates local updates of LoRA components to mitigate gradient interference, noise amplification, and aggregation instability. A low-pass smoothing filter further enhances cross-client consistency and stability. ‚Ä¢ Theoretically, we prove that alternating updates yield unique closed-form solutions and ensure stable low-rank optimization, mitigating projection distortion in federated settings. ‚Ä¢ We validate our algorithm on both vision (Swin Transformer) and language (RoBERTa) models, covering image classification and NLP tasks. LA-LoRA outperforms SOTA privacy-preserving federated LoRA methods on various tasks and demonstrates significant performance gains. Figure 1: The illustration of DP-LoRA, FFA-LoRA, RoLoRA, and LA-LORA. DP-LoRA updates both noisy A A and B B simultaneously and sends them to the server for aggregation. FFA-LoRA freezes A A , updates only the noisy B B , and sends it to the server. RoLoRA alternately updates noisy A A and B B across rounds. Our LA-LoRA alternately updates noisy A A and B B within each local round. 2 Background And Related Work Definition 1 ( ( œµ , Œ¥ ) (\epsilon,\delta) -DP, Dwork et al. ( 2014 ) ) . Let œµ 0 \epsilon 0 and 0 Œ¥ 1 0 \delta 1 . A random mechanism ‚Ñ≥ : ùí≥ n ‚Üí ùí™ \mathcal{M}:\mathcal{X}^{n}\!\to\!\mathcal{O} satisfies ( œµ , Œ¥ ) (\epsilon,\delta) -DP if, for any two neighboring datasets ùíü \mathcal{D} and ùíü ‚Ä≤ \mathcal{D}^{\prime} differing in one sample, and for any measurable subset ùí∞ ‚äÜ ùí™ \mathcal{U}\subseteq\mathcal{O} of possible outputs, Pr ‚Å° [ ‚Ñ≥ ‚Äã ( ùíü ) ‚àà ùí∞ ] ‚â§ e œµ ‚ãÖ Pr ‚Å° [ ‚Ñ≥ ‚Äã ( ùíü ‚Ä≤ ) ‚àà ùí∞ ] + Œ¥ . \Pr[\mathcal{M}(\mathcal{D})\in\mathcal{U}]\leq e^{\epsilon}\cdot\Pr[\mathcal{M}(\mathcal{D}^{\prime})\in\mathcal{U}]+\delta. (1) Differentially private federated learning (DPFL). A general FL system with a central server and N N clients. Each client i i holds a local dataset ùíü i \mathcal{D}_{i} and performs local training on it. The server aggregates the model updates from all clients and updates the global model W W . Our privacy goal is sample-level protection (changes to any single data sample should not significantly affect the output). Building on Noble et al. ( 2022 ) , we provide ( œµ , Œ¥ ) (\epsilon,\delta) -DP guarantees. We perform privacy accounting via R√©nyi DP (RDP) (Mironov, 2017 ) , composing per-step privacy loss for the subsampled Gaussian mechanism (Wang et al. , 2019 ) with per-sample clipping, and finally convert to ( œµ , Œ¥ ) (\epsilon,\delta) -DP. To achieve DP in FL, each client i i clips the per-sample gradients to a fixed ‚Ñì 2 \ell_{2} norm to bound sensitivity, then adds Gaussian noise to the aggregated clipped gradients locally, protecting the contribution of individual training examples. For a local mini-batch ‚Ñ¨ i \mathcal{B}_{i} , the privatized update is computed as: g i ‚Äã j = ‚àá ‚Ñí i ‚Äã j / max ‚Å° ( 1 , ‚Äñ ‚àá ‚Ñí i ‚Äã j ‚Äñ 2 / C ) , ‚àÄ j ‚àà ‚Ñ¨ i , g i = ( ‚àë j ‚àà ‚Ñ¨ i g i ‚Äã j + ùí© ‚Äã ( 0 , C 2 ‚Äã œÉ 2 ) ) / | ‚Ñ¨ i | , g_{ij}=\nabla\mathcal{L}_{ij}/\max\left(1,\|\nabla\mathcal{L}_{ij}\|_{2}/C\right),\forall j\in\mathcal{B}_{i},\quad{g}_{i}=\left(\!\sum\nolimits_{j\in\mathcal{B}_{i}}g_{ij}+\mathcal{N}(0,C^{2}\sigma^{2})\right)/|\mathcal{B}_{i}|, (2) where ‚Ñí i ‚Äã j \mathcal{L}_{ij} denotes the loss for sample j j on client i i , C C is the clipping norm bound, and œÉ \sigma is the Gaussian noise multiplier determined by the privacy budget ( œµ , Œ¥ ) (\epsilon,\delta) . Parameter-efficient fine-tuning (PEFT). The rapid scaling of modern pre-trained models has led to an increased demand for fine-tuning in resource-constrained environments. PEFT tackles this by training a small set of parameters while freezing most backbone weights, with representative methods including adapters (Houlsby et al. , 2019 ) , prefix-tuning (Li and Liang, 2021 ) , prompt-tuning (Lester et al. , 2021 ) , LoRA (Hu et al. , 2021 ; Tian et al. , 2024 ) and other methods (Shin et al. , 2023 ; Zaken et al. , 2021 ) . LoRA is particularly popular for its simplicity and strong performance under minimal parameter overhead: it augments a pre-trained weight matrix W 0 ‚àà ‚Ñù m √ó n W_{0}\in\mathbb{R}^{m\times n} with two low-rank matrices, an up-projection B ‚àà ‚Ñù m √ó r B\in\mathbb{R}^{m\times r} and a down-projection A ‚àà ‚Ñù r √ó n A\in\mathbb{R}^{r\times n} with r ‚â™ min ‚Å° { m , n } r\ll\min\{m,n\} , and reparameterizes the weights as W = W 0 + s ‚Äã B ‚Äã A W=W_{0}+sBA while keeping W 0 W_{0} fixed. B B is initialized to an zero matrix suppress early updates, while A A uses a random Gaussian initialization. PEFT in FL. PEFT in FL has been explored to reduce communication overhead, computational cost, and privacy risks. Accordingly, various strategies have been proposed, such as adapter-based (Cai et al. , 2023 ; Ghiasvand et al. , 2024 ) , prompt-based (Zhao et al. , 2023 ; Qiu et al. , 2023 ) , and selective tuning approaches (Yu et al. , 2023 ) . Recently, LoRA-based methods have received increasing attention in FL. FedIT (Zhang et al. , 2024a ) directly incorporates LoRA into the standard FedAvg framework for instruction tuning. RoLoRA (Chen et al. , 2024 ) addresses heterogeneity by alternately optimizing A A and B B across communication rounds. FedSA-LoRA (Guo et al. , 2024 ) uploads only A A to the server for aggregation, keeping B B local to support personalized adaptation. Other works explore heterogeneous configurations (Cho et al. , 2024 ; Wang et al. , 2024 ) or personalized decomposition (Qi et al. , 2024 ) to improve adaptability under system and data heterogeneity. FedBCGD (Liu et al. , 2024b ) proposes an accelerated block coordinate gradient descent framework for FL. FedSWA (Liu et al. , 2024a ) improves generalization under highly heterogeneous data by stochastic weight averaging. FedAdamW (Liu et al. , 2025c ) introduces a communication-efficient AdamW-style optimizer tailored for federated large models. FedNSAM (Liu et al. , 2025a ) studies the consistency relationship between local and global flatness in FL. FedMuon (Liu et al. , 2025b ) accelerates federated optimization via matrix orthogonalization. DP-FedPGN (Liu et al. , 2025d ) develops a penalizes gradient norms to encourage globally flatter minima in DP-FL. Table 1: Comparison of LoRA-based methods in DPFL. ‚úì denotes support, ‚úï denotes not considered. Exp. indicates the effective expression ability under DP. Method DP LVMs Exp. Speed DP-LoRA ‚úì ‚úï mid slow FFA-LoRA ‚úì ‚úï low slow RoLoRA ‚úï ‚úï mid mid LA-LoRA ‚úì ‚úì high fast LoRA in DPFL. In DPFL, LoRA-based methods typically apply DP mechanisms to the low-rank matrices A A and B B , rather than the full model parameters, which significantly reduces computational and communication overhead. DP-LoRA (Liu et al. , 2025e ) computes per-sample gradients for A A and B B locally, clips them, adds Gaussian noise, and sends privatized updates for aggregation. FFA-LoRA (Sun et al. , 2024a ) freezes the down-projection matrix A A and trains only the up-projection B B , injecting calibrated noise into its updates. This design avoids noise amplification, but at the cost of reduced model expressiveness. Despite recent efforts, differentially private federated LoRA methods still suffer from noticeable performance degradation and remain underexplored. Figure 1 shows the differences in the ideas 