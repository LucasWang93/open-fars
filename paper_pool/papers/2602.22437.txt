Title: veScale-FSDP: Flexible and High-Performance FSDP at Scale

Abstract: Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.

Body: veScale-FSDP: Flexible and High-Performance FSDP at Scale 1 Introduction 2 Background and Motivation 2.1 Structure–Aware Training 2.2 DTensor and JaggedTensor 2.3 ZeRO and FSDPs 3 Overview 4 RaggedShard for Flexibility 5 Grouped RaggedShard for Performance 6 Evaluation 6.1 End-to-End Performance 6.2 Scalability and Composability 6.3 8-bit Adam and Muon Optimizer 6.4 Planning Quality 6.5 Performance Breakdown 7 Lessons Learned 8 Conclusion 9 Acknowledgments \contribution [*]Equal Contribution \contribution [†]Corresponding authors veScale-FSDP: Flexible and High-Performance FSDP at Scale Zezhou Wang Youjie Li Zhiqi Lin Jiacheng Yang Cong Xie Guanyu Feng Zheng Zhong Ziyue Huang Hongyu Zhu Zhi Zhang Yanghua Peng Xin Liu ByteDance Seed youjie.li@bytedance.com pengyanghua.yanghua@bytedance.com ( February 25, 2026 ) Abstract Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP’s fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today’s implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5 ∼ \sim 66% higher throughput and 16 ∼ \sim 30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs. Abstract \correspondence and \projectpage https://github.com/volcengine/veScale 1 Introduction Large language models (LLMs) have become a transformative technology in everyday applications. Driven by the scaling law [ kaplan2020scaling ] , LLMs now reach billions of parameters and achieve human-level performance. Training such giant models requires parallelization techniques that distribute the model and optimizer states across thousands of GPUs [ jiang2024megascale ] . Among these, Deepspeed ZeRO [ rajbhandari2020zero ] or Fully Sharded Data Parallel (FSDP) [ zhao2023pytorch , pytorch2024fsdp2 , megatron_fsdp ] is one of the most fundamental techniques. FSDP is often the first choice because of its efficient yet flexible data-parallel programming paradigm and decoupling from model architecture. When additional scaling is needed [ smith2022using , ma2025veomni ] , FSDP can be combined with other parallelisms. However, existing FSDP/ZeRO systems struggle with modern structure-aware training. State-of-the-art models use non-element-wise optimizers such as Shampoo [ gupta2018shampoo ] and Muon [ jordan2024muon ] , and block-wise quantized training like DeepSeek-V3 [ liu2024deepseek ] , all of which require atomic tensor blocks. The core limitation is that existing FSDP frameworks shard parameters, gradients, and optimizer states either element-wise [ rajbhandari2020zero , zhao2023pytorch ] or row-wise [ pytorch2024fsdp2 , megatron_fsdp ] , producing sharding boundaries that often misalign with the required block sizes. Consequently, model developers must intrusively modify their models or optimizers to match tensor boundaries, or system developers must handle complex boundary checks, padding, and additional communication logic. Beyond inflexibility, current FSDP systems fall short of our production throughput and memory targets, where we aim to extract every bit of hardware efficiency. GPU Memory is the tighter constraint: in shared clusters, jobs run out of memory or will operate at the memory limit incurring expensive device-side frees, prompting over-provisioning that leaves GPU resources wasted. These demands become even more critical when scaling training to over 10K GPUs and trillions of parameters. Few existing FSDP systems can scale to this level while maintaining efficiency. Deepspeed ZeRO [ rajbhandari2020zero ] pioneered the FSDP research but suffers from fragmented AllGather operations [ deepspeed_AG ] and inefficient memory management [ fsdp_record_stream ] . PyTorch FSDP1 [ zhao2023pytorch ] addresses some AllGather inefficiency, but incurs slow ReduceScatter [ fsdp1_reduce ] and does not solve memory overhead [ fsdp_record_stream ] . PyTorch FSDP2 [ pytorch2024fsdp2 ] improves memory management [ per_parameter_shard_rfc ] but introduces high tensor copy overhead. Meanwhile, both FSDP1 and FSDP2 suffer from slow collectives due to unaligned communication buffer [ wu2025terabyte , nccl16byte ] . Megatron-FSDP [ megatron_fsdp ] further improves performance but requires extra padding, increasing both communication and memory costs. To this end, we reinvent PyTorch FSDP2 and present veScale-FSDP, combining both flexibility and performance at scale: ⊳ \triangleright For flexibility, veScale-FSDP introduces a novel sharding format, RaggedShard , which supports arbitrary sharding granularity with custom block sizes for structure-aware training, while seamlessly composing with existing PyTorch DTensor sharding formats. ⊳ \triangleright For performance, veScale-FSDP introduces a planning algorithm that rearranges RaggedShard tensors to maximize communication efficiency while respecting their desired sharding granularities. We formulate planning as a NP-hard optimization problem and use practical polynomial-time heuristics that achieve high-quality solutions in practice. ⊳ \triangleright veScale-FSDP further provides a high-performance primitive, Distributed Buffer ( DBuffer ), that backs RaggedShard tensors with slices of a global buffer, not only enabling zero-copy access and minimal communication overhead but also reducing memory fragmentation via batched memory allocations. Our extensive evaluations demonstrate that veScale-FSDP outperforms all existing FSDP systems on both dense and sparse LLMs across different scales, achieving 5 ∼ \sim 66% higher throughput and 16 ∼ \sim 30% lower memory usage, while scaling efficiently to tens of thousands of GPUs. In addition, case studies show that veScale-FSDP is able to natively accommodate both non-element-wise optimizers like Muon [ jordan2024muon ] and block-wise quantization methods like 8-bit Adam [ dettmers20218 ] . veScale-FSDP has been battle-tested in production and is portable without relying on internal infrastructure. RaggedShard code is open sourced at https://github.com/volcengine/veScale . 2 Background and Motivation 2.1 Structure–Aware Training Structure–aware training is the core technique behind the top-tier models such as Gemini [ team2024gemini ] and DeepSeek [ liu2024deepseek ] , and becomes increasingly important, including: Matrix Optimizers. Matrix-based optimizers such as Shampoo [ gupta2018shampoo ] and Muon [ jordan2024muon ] can deliver faster convergence rate. The calculation is conducted on the matrix with the original 2D shape, requiring the full matrix to be present locally on each device and then be computed only on chosen devices. Block-wise Quantization. Training with quantized model [ liu2024deepseek ] and optimizer states [ dettmers20218 ] is widely used to improve system efficiency. Block-wise quantization is one of the prevailing techniques to preserve both quality and efficiency of training, but requires slicing the tensors into 2D blocks for the calculation of the scaling factors. Sharding parameters without care would end up in sharded blocks across devices, incurring high complexity in either model design or system development. 2.2 DTensor and JaggedTensor Figure 1 : The DTensor for flexible communication and computation. Here shows an example of DTensors executing a sharded matrix multiplication on a device. The darken part in each DTensor indicates the materialized local tensor on this device. Distributed Tensor (DTensor) [ torch-dtensor , li2025vescale ] is a promising primitive of PyTorch that provides the opportunity towards structure–aware training. It represents a global tensor distributed across devices, where each device holds a local sharded tensor. DTensor supports three sharding formats (placement): Shard(dim) that evenly shards a global tensor along a tensor dimension, Replicate that replicates the global tensor, and Partial . It also enables users to switch between these placement via redistribute with implicit collective communications. Additionally, DTensors can be computed directly by operators like matmul , as shown in Figure 1 . However, a fundamental limitation prevents structure-aware training: the Shard format cannot represent the block-wise sharding needed for quantization or the uneven sharding required by matrix optimizers. JaggedTensor/NestTensor on a single device [ jaggedtensor , nestedtensor , raggedtensors ] are PyTorch/TensorFlow primitives that represent single-device tensors whose last dimension is “jagged”. For example, a 2D tensor in which each row may have a different length. While these primitives still fail to represent the block-level granularity as the atomic unit, they offer a useful hint for how veScale-FSDP can support structure–awareness in distributed training setting. 2.3 ZeRO and FSDPs DeepSpeed ZeRO [ rajbhandari2020zero ] pioneered this line of FSDP research. Its core idea is to concatenate a layer of tensors (parameters, gradients, and optimizer states) and then shard each concatenated tensors across devices, where tensors can be irregularly sharded across device boundary. ZeRO only unshards a layer using AllGather before forward and backward pass, and reduces the layer gradients using ReduceScatter back to different devices. Such sharding design is limited in element-wise plain tensor and cannot support structure-aware training. FullyShardedDataParallel (FSDP1) [ zhao2023pytorch ] is the first PyTorch-native ZeRO, following the same sharding format and limitation, but it is optimized in performance. Table 1 : Interleaved copy time (ms) compared to their corresponding collectives in FSDP2 for GPT-OSS-120B on 64 GPUs. Shard(0) is the default parameter sharding mode and Shard(1) is used when Shard(0) incurs large padding. Gather side Reduce side AllGather Copy-Out ReduceScatter Copy-In Shard(0) 43.71 ms 5.22 ms 94.24 ms 12.37 ms Shard(1) 44.35 ms 13.72 ms 95.36 ms 23.14 ms fully_shard (FSDP2) is the second PyTorch-native ZeRO, representing the state-of-the-art FSDP in the community. It replaces the concatenated shard design with per-parameter sharding, representing each tensor as a Shard(0) DTensor. This exposes maximal DTensor flexibility for FSDP parameters in communication, computation, and model checkpointing. However, this even sharding format is still far from enabling structure-aware training. Moreover, this per-parameter design introduces performance overhead from copying parameters in interleaved memory addresses, as shown in Figure 2 and Table 1 . Figure 2 : The fundamental overhead in FSDP2. (AllGather is shown; ReduceScatter is a reverse process.) Megatron-FSDP [ megatron_fsdp ] is the most recent FSDP prototype that pursues speed. It forgoes FSDP2’s design and rolls back to FSDP1’s concatenated sharding to avoid the copying overhead, while heavily optimizing performance. However, Megatron-FSDP develops a special mechanism to enforce a concatenation-sharded tensor become a Shard(0) DTensor, such that the model checkpointing can use DTensor. This mechanism inserts padding into the concatenation so that tensors are sharded row-wise along device boundaries rather than element-wise. Without careful padding planning, the concatenation size can grow significantly, increasing both memory usage and communication volume. Moreover, row-wise sharding still falls short of supporting structure-aware training. 3 Overview Figure 3 : veScale-FSDP overview. To address both challenges of flexibility and performance, we present veScale-FSDP, a novel FSDP that combines the best of worlds. Figure 3 provides the overview. Model developers are given the freedom to develop sophisticated large models (e.g., with sparse MoE structures) and structure-aware optimizers (e.g., with non-element-wise operators) for achieving unprecedented model quality. Meanwhile, the model/optimizer can be simply parallelized with PyTorch-native API fully_shard like FSDP2, without intrusively hacking model/optimizer code. During parallelization, complex operators of models/optimizers can still enjoy single-device semantics, thanks to a proposed sharding format, dubbed RaggedShard that offers the flexibility to express arbitrary sharding granularity and arbitrary distribution across devices for each DTensor (§ 4 ). Under the hood, RaggedShard DTensors are grouped for bucketed communication. Toward optimal performance, their layouts are rearranged via a proposed planning algorithm which is derived from a NP-hard optimization problem. The planned layouts are then mapped to a Distributed Buffer ( DBuffer ), a new primitive that achieves zero-copy and minimal overhead (§ 5 ), enabling efficient scaling up to 10K GPUs in real production deployments. 4 RaggedShard for Flexibility This section proposes a novel and general sharding format RaggedShard to enable flexibility of FSDP for complex model and structure-aware optimizers. Figure 4 : Flexibility comparison of different sharding formats. Existing sharding formats. The second format is the Row-wise (Even) Shard , where a tensor is evenly partitioned along a dimension, with equal-sized shards assigned to each device. This design improves flexibility by enabling non-element-wise computations on sharded tensors and allowing dimension redistribution via All2All collectives. However, it still faces challenges with block-wise quantization, as evenly divided shards are not guaranteed to align with block boundaries. This row-wise sharding format serves as the foundation of FSDP2. The RaggedShard format. Inspired by the JaggedTensor/NestedTensor on single device [ jaggedtensor , nestedtensor , raggedtensors ] , we propose RaggedShard format for DTensor to offer the flexibility to express arbitrary sharding granularity in contiguous memory (the atomic non-shardable block consisting of contiguous elements or rows or planes) and arbitrary sharding distribution (the number of blocks per device). For a simple example in Figure 4 , setting RaggedShard ’s granularity as one tensor row gives Row-wise RaggedShard with different number of rows across devices. A similar concept has been prototyped in the model checkpointing mechanism of Megatron-FSDP. The most flexible sharding format is the Block-wise RaggedShard , where the shar