Title: RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format

Abstract: Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning &amp; general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.

Body: RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format 1 Introduction 2 Preliminary and Observations 3 Our RAIN-Merging Method 4 Experiments 4.1 Experimental Setup 4.2 Results 5 Conclusion A Ethics Statement B Reproducibility Statement C LLM Usage Statement D Related Work E Proof E.1 Proof of why orthogonal in parameters ‚â† \neq invariant in outputs E.2 Proof of Prop. 1 F Algorithm G Method Implementation Details G.1 Forward Mechanism in Transformer G.2 Implementation Details in Merging H Calibration Set Construction H.1 Reasoning calibration set H.2 Instruction calibration set I Detailed Experimental Setup I.1 Benchmarks Instruction-following Benchmarks. Reasoning General Benchmarks. I.2 Baselines I.3 Hyperparameters J Additional Experiments J.1 Detailed Math Benchmark Results J.2 Ablation Study of the Global Scalar J.3 Ablation Study of Reasoning Calibration Set Size J.4 Visualization of Merging Coefficients in Stage 2 J.5 Ablation Study of Instruction Calibration Set Generalization J.6 Generalization to Unseen Instruction-Following Benchmarks J.7 Instruction-Type Breakdown on IFEval J.8 Semantic Robustness to Paraphrased Instructions J.9 Case Study on IFEval J.10 Case Study on GPQA K Implications L Limitations and Future Work RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format Zhehao Huang 1 Yuhang Liu 1 Baijiong Lin 2 Yixin Lou 1 Zhengbao He 1 Hanling Tian 1 Tao Li 1 Xiaolin Huang 1,3 1 Institute of Image Processing and Pattern Recoginition, School of Automation and Intelligent Sensing, Shanghai Jiao Tong University 2 The Hong Kong University of Science and Technology (Guangzhou) 3 MoE Key Laboratory of System Control and Information Processing (Shanghai) {kinght_h,yuhangliu,loulou_0727,lstefanie,hanlingtian}@sjtu.edu.cn {li.tao,xiaolinhuang}@sjtu.edu.cn {blin241}@connect.hkust-gz.edu.cn Abstract Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that na√Øve merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM‚Äôs structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agentic scenarios. Code is available at https://github.com/K1nght/RAIN-Merging . 1 Introduction In the current boom of research, Large Reasoning Models (LRMs, like OpenAI-o1 (Jaech et al. , 2024 ) , DeepSeek-R1 (Guo et al. , 2025 ) ) have shown strong potential on tasks that require rigorous multi-step reasoning (Wei et al. , 2022 ) , such as mathematical derivation (Shao et al. , 2024 ) and program synthesis (Guo et al. , 2024 ) . However, a discouraging paradox has emerged: although LRMs perform well in purely reasoning-oriented settings, they lag in instruction following (Fu et al. , 2025a ; Li et al. , 2025a ) . They often generate lengthy logical derivations yet ignore user-specified formats, constraints, or specific operational requirements in the final response. This inconsistency undermines LRM practicality and reliability in real-world applications (Chkirbene et al. , 2024 ) , especially in agent (Qi et al. , 2025 ) and professional tool deployments (Zhao et al. , 2024 ) . A straightforward remedy is to continue training LRMs with supervised fine-tuning (SFT) to strengthen instruction following. However, building high-quality supervision datasets for tasks that require generating long chains of thought entails substantial annotation and computational resources (Qin et al. , 2025 ) . Moreover, these post-training methods often induce capability regressions, with degradation in generality and in responses to unseen instructions (Shenfeld et al. , 2025 ) . In contrast, a training-free and compute-light alternative is model merging, which extracts parameter differences between fine-tuned and pre-trained models (namely the task vector ), then combines these task vectors to create a unified model that preserves pre-trained knowledge while incorporating capabilities from multiple tasks (Ilharco et al. , 2023 ) . This motivates a central question: whether we can merge the LRM and the Instruction-tuned Model (ITM) to enhance the instruction following while preserving its reasoning capability . Figure 1: An overview of RAIN-Merging . In the case, the LRM arrives at the correct solution but ignores the required format and specific code. To preserve the reasoning structure, we perform training-free merging by combining a task vector projected onto the null space of the thinking format with instruction-attention guided coefficients. The merged model remains correct while satisfying the specified constraints. See Sec. 3 for details. We begin with a parameter-space analysis of the task vectors from the LRM and the Instruction-tuned Model (ITM) relative to their shared base. We find that their principal subspaces are nearly orthogonal across key modules, which indicates minimal interference between the two capabilities and suggests that merging is a promising lightweight way to enhance the LRM‚Äôs instruction following (Ortiz-Jim√©nez et al. , 2023 ) . However, direct merging carries risks. LRMs and ITMs differ fundamentally in output structure: the former explicitly separates ‚Äúthinking‚Äù and ‚Äúresponse‚Äù with special markers (e.g., R1-style think ‚Ä¶ \dots /think ), whereas the latter provides only a final answer. Traditional data-free merging (Ilharco et al. , 2023 ; Goddard et al. , 2024 ) prunes or scales the task vector purely from parameter-internal statistics to balance domain performance, thereby ignoring output-distribution mismatches and disrupting the LRM‚Äôs structured reasoning. Recent work (Nobari et al. , 2025 ; Yao et al. , 2025 ; Chopra et al. , 2025 ) has tried to guide merging with forward activations using small calibration sets. Although this introduces data-driven constraints, the lack of an explicit notion of the output mismatch between the two types still makes it difficult to achieve a stable and effective balance between preserving reasoning structure and improving instruction following. To this end, we propose a two-stage merging strategy that enhances instruction-following capability without sacrificing the thinking format and reasoning performance of the LRM. First, leveraging task-vector orthogonality between the LRM and ITM, we preserve reasoning ability and enforce thinking-format invariance by projecting the ITM task vector into the null space derived from forward features at thinking tokens on a small reasoning-calibration set. This keeps the merged model‚Äôs reasoning representations aligned with the original LRM and retains structured outputs. Second, while keeping these invariances fixed, we aim to enhance instruction-following performance as much as possible. We improve instruction adherence by estimating per-module importance based on attention outputs over instruction-related spans from a small set of instruction examples. Attention-guided coefficients are then assigned to strengthen instruction-relevant behaviors.We refer to the overall two-stage approach as Reasoning-Aware Instruction-guided Null-space projection Merging ( RAIN-Merging ) in Fig. 1 , which effectively synergizes reasoning and instruction-following performance. We conduct a systematic evaluation of our proposed method on four instruction-following benchmarks and on nine evaluation benchmarks that cover mathematics, code, STEM, and creative-writing capabilities. The results show that RAIN-Merging not only substantially improves the LRM‚Äôs instruction-following ability but also maintains reasoning and general capability. Moreover, our method exhibits consistent stability across different model sizes and architectures, and demonstrates enhanced performance in agentic scenarios. 2 Preliminary and Observations Task Vector. A task vector (Ilharco et al. , 2023 ) characterizes the parameter delta from a base model to a task-specific one. A straightforward way to combine capabilities is task arithmetic , which linearly adds such deltas to a base model to obtain a multi-skilled model. This simple approach can work when tasks are compatible. However, for distinct abilities such as reasoning and instruction-following that impose different output structures (Yadav et al. , 2023 ) , naive linear addition may cause capability interference and disrupt the representations essential to each domain. Figure 2: Principal subspace cosine similarity between LRM and ITM task vectors for each layer and submodule. The similarities are consistently low ( 0.1 0.1 ). Orthogonality between Reason Instruction Task Vectors. To examine whether capability interference arises when merging ITM Œ∏ I \theta_{I} into LRM Œ∏ R \theta_{R} , we take the shared base model Œ∏ B \theta_{B} as reference and define the LRM task vector Œî R = Œ∏ R ‚àí Œ∏ B \Delta_{R}=\theta_{R}-\theta_{B} and the ITM task vector Œî I = Œ∏ I ‚àí Œ∏ B \Delta_{I}=\theta_{I}-\theta_{B} . We perform singular value decomposition (SVD) within the main forward modules, e.g. attention and FFN, for these two task vectors and evaluate the principal subspace cosine similarity of their principal subspaces. As shown in Fig. 2 , A1 , A2 , the two are nearly orthogonal since their similarities are all 0.1 0.1 . Prior studies (Ortiz-Jim√©nez et al. , 2023 ) indicate that this phenomenon reflects a low degree of coupling between reasoning ability and instruction following in parameter space, which suggests that lightweight task-vector merging strategies can enhance instruction following while preserving the original reasoning performance. More details are in Appendix E.1 . Risks in Thinking Format During Merging. However, orthogonality in parameter space is not sufficient to guarantee that the merged model will retain the LRM‚Äôs structured output behavior, since this behavior is determined by downstream propagation and forward features (see Appendix E.1 for proof). In particular, the LRM relies on special tokens such as think and /think to explicitly separate the reasoning segment from the answer segment, and these tokens are crucial in instruction-following tasks. For example, if the model fails to generate the terminator correctly after merging (as Fig. 3 ), it may conflate the reasoning content with the instruction-compliant response, which can violate constraints such as limits on output length. Therefore, although task-vector orthogonality suggests minimal capability interference, we still need to explicitly constrain the distributional shift of the output structure during merging to preserve the integrity of the reasoning process. 3 Our RAIN-Merging Method Notation. For notational convenience in later derivations, we flatten model submodules by layer and head with index k = 1 , ‚Ä¶ , K k=1,\dots,K as Œ∏ = ‚®Å k = 1 K W k := [ vec ( W 1 ) ‚ä§ , ‚Ä¶ , vec ( W K ) ‚ä§ ] ‚ä§ \theta=\bigoplus_{k=1}^{K}W^{k}:=[\operatorname{vec}(W^{1})^{\top},\ldots,\operatorname{vec}(W^{K})^{\top}]^{\top} , where ‚®Å \bigoplus denotes the block-wise concatenation that assembles disjoint parameter blocks into a single coordinate vector. More details of the forward mechanism in Transformer (Vaswani et al. , 2017 ) are in Appendix G.1 . Let h t k h_{t}^{k} denote the forward input vector at the k k -th submodule and the t t -th sampled token position. The corresponding linear map of this submodule admits the Kronecker-vectorization form (Koning et al. , 1991 ) with Kronecker product ‚äó \otimes , identity matrix diag ‚Äã ( 1 ) \mathrm{diag}(1) , and vectorization operator vec ‚Å° ( ‚ãÖ ) \operatorname{vec}(\cdot) , as W k ‚Äã h t k = ( ( h t k ) ‚ä§ ‚äó diag ‚Äã ( 1 ) ) ‚Äã vec ‚Å° ( W k ) . W^{k}h_{t}^{k}=((h_{t}^{k})^{\top}\otimes\mathrm{diag}(1))\operatorname{vec}(W^{k}). Stacking all sampled positions t t row-wise yields the forward feature operator Œ¶ { t } k \Phi^{k}_{\{t\}} and outputs for the k k -th submodule: Œ¶ { t } k := [ ( h 1 k ) ‚ä§ ‚äó diag ‚Äã ( 1 ) , ‚Ä¶ , ( h T k ) ‚ä§ ‚äó diag ‚Äã ( 1 ) ] , W k ‚Äã h k = Œ¶ { t } k ‚Äã vec ‚Å° ( W k ) . \Phi^{k}_{\{t\}}\;:=\;\begin{bmatrix}(h_{1}^{k})^{\top}\otimes\mathrm{diag}(1),\dots,(h_{T}^{k})^{\top}\otimes\mathrm{diag}(1)\end{bmatrix},\quad W^{k}h^{k}\;=\;\Phi^{k}_{\{t\}}\operatorname{vec}(W^{k}). (1) Optimization Objective. To preserve the original reasoning performance of the LRM as much as possible, we take the reasoning model parameters Œ∏ R \theta_{R} as the anchor. We transform the ITM task vector Œî I \Delta_{I} through a merging function f f to obtain Œî = f ‚Äã ( Œî I ) \Delta=f(\Delta_{I}) , and form the merged model Œ∏ = Œ∏ R + Œî \theta=\theta_{R}+\Delta . Our goal is to enhance instruction following without damaging the LRM‚Äôs thinking format and reasoning performance . We therefore formulate a constrained optimization problem: over the instruction data distribution ùíü I \mathcal{D}_{I} , maximize the surrogate objective for instruction following, ùí• I ‚Äã ( Œ∏ ) ‚âú ùîº x ‚àº ùíü I ‚Äã ùîº y ‚àº œÄ Œ∏ ( ‚ãÖ ‚à£ x ) ‚Äã [ IF ‚Å° ( x , y ) ] \mathcal{J}_{I}(\theta)\triangleq\mathbb{E}_{x\sim\mathcal{D}_{I}}\,\mathbb{E}_{y\sim\pi_{\theta}(\cdot\mid x)}\bigl[\operatorname{IF}(x,y)\bigr] , while, over the reasoning data distribution ùíü R \mathcal{D}_{R} , constraining the deviation between the model‚Äôs output distribution within the segment of thinking special tokens Œ© think \Omega_{\mathrm{think}} and the reference policy of the original reasoning model Œ∏ R \theta_{R} . This constraint is quantified by aggregating the per-step KL divergence within the segment: ‚Ñí think ( Œ∏ ) ‚âú ùîº x ‚àº ùíü R ùîº y ‚àº œÄ Œ∏ R ( ‚ãÖ ‚à£ x ) ‚àë t ‚àà Œ© think ‚Äã ( x ) KL ( œÄ Œ∏ ( ‚ãÖ ‚à£ x , y t ) ‚à• œÄ Œ∏ R ( ‚ãÖ ‚à£ x , y t ) ) . \mathcal{L}_{\text{think}}(\theta)\triangleq\mathbb{E}_{x\sim\mathcal{D}_{R}}