Title: Optimal Learning-Rate Schedules under Functional Scaling Laws: Power Decay and Warmup-Stable-Decay

Abstract: We study optimal learning-rate schedules (LRSs) under the functional scaling law (FSL) framework introduced in Li et al. (2025), which accurately models the loss dynamics of both linear regression and large language model (LLM) pre-training. Within FSL, loss dynamics are governed by two exponents: a source exponent $s&gt;0$ controlling the rate of signal learning, and a capacity exponent $Œ≤&gt;1$ determining the rate of noise forgetting. Focusing on a fixed training horizon $N$, we derive the optimal LRSs and reveal a sharp phase transition. In the easy-task regime $s \ge 1 - 1/Œ≤$, the optimal schedule follows a power decay to zero, $Œ∑^*(z) = Œ∑_{\mathrm{peak}}(1 - z/N)^{2Œ≤- 1}$, where the peak learning rate scales as $Œ∑_{\mathrm{peak}} \eqsim N^{-ŒΩ}$ for an explicit exponent $ŒΩ= ŒΩ(s,Œ≤)$. In contrast, in the hard-task regime $s &lt; 1 - 1/Œ≤$, the optimal LRS exhibits a warmup-stable-decay (WSD) (Hu et al. (2024)) structure: it maintains the largest admissible learning rate for most of training and decays only near the end, with the decay phase occupying a vanishing fraction of the horizon.   We further analyze optimal shape-fixed schedules, where only the peak learning rate is tuned -- a strategy widely adopted in practiceand characterize their strengths and intrinsic limitations. This yields a principled evaluation of commonly used schedules such as cosine and linear decay. Finally, we apply the power-decay LRS to one-pass stochastic gradient descent (SGD) for kernel regression and show the last iterate attains the exact minimax-optimal rate, eliminating the logarithmic suboptimality present in prior analyses. Numerical experiments corroborate our theoretical predictions.

Body: Optimal Learning-Rate Schedules under Functional Scaling Laws: Power Decay and Warmup-Stable-Decay 1 Introduction 2 Related Work Optimal learning-rate schedules for linear regression. Understanding cosine and WSD schedules. One-pass SGD for kernel regression. 3 Preliminaries 3.1 Feature-Space Linear Regression 3.2 Functional Scaling Laws 4 Optimal Learning Rate Schedules 5 Shape-Fixed Learning-Rate Schedules: Optimality and Limitations Practical implications. 6 SGD for Kernel Regression 7 Proof Sketch of Theorems 4.1 and 5.2 7.1 Proof Sketch of Theorem 4.1 Step 1: Deriving the optimal profile under a fixed intrinsic time budget. Step 2: Determining the optimal intrinsic time budget. Step 3: Incorporating the peak learning-rate constraint. 7.2 Proof Sketch of Theorem 5.2 8 Conclusion Appendix A Proof for Theorem 4.1 (Optimal LRS under FSL) A.1 Step 1: Deriving the Optimal Profile under a Fixed Intrinsic Time Budget A.2 Step 2: Determining the Optimal Intrinsic Time Budget A.3 Step 3: Incorporating the Peak Learning Rate Constraint B Proofs for Section 5 (Shape-Fixed Optimality and Capacity Saturation) B.1 The Intrinsic-Time Profile Function Learning rate in intrinsic time. B.2 Proof of Theorem 5.2 (i) Early region. (ii) Tail region and the logarithmic boundary. B.3 Proof of Theorem 5.3 (i) Hard-task regime: s 1 ‚àí 1 Œ± s 1-\frac{1}{\alpha} . (ii) Easy-task regime: s ‚©æ 1 ‚àí 1 Œ± s\geqslant 1-\frac{1}{\alpha} . C Proofs for Section 6 (Discrete-Time SGD and Kernel Regression) C.1 Preliminaries for Kernel Methods Spectral systems and the bridge between L 2 L^{2} and ‚Ñç \mathbb{H} . Capacity and source conditions. C.2 Analysis of One-Pass SGD One-pass SGD in RKHS. Translating the update to ‚Ñç \mathbb{H} . Intrinsic time and excess risk bound C.3 Bounding Signal Learning and Noise Accumulation C.3.1 Signal-Learning Term S N S_{N} C.3.2 Noise-Accumulation Term ùí© N \mathcal{N}_{N} Noise term ùí© N \mathcal{N}_{N} for power decay. (i) Tail part m ‚©Ω m 0 m\leqslant m_{0} . (ii) Head part m m 0 m m_{0} . C.4 Proof of Theorem 6.3 (i) Easy-task regime ( s ‚©æ 1 ‚àí 1 Œ≤ s\geqslant 1-\frac{1}{\beta} ). (ii) Hard-task regime ( s 1 ‚àí 1 Œ≤ s 1-\frac{1}{\beta} ). C.5 Proof of Proposition 6.4 D An Alternative Proof of Optimal Intrinsic-Time LRS Profile (Step 1) D.1 A Quick Introduction to Variational Calculus Euler‚ÄìLagrange equation. Beltrami identity. D.2 Proof of Optimal Intrinsic-Time LRS Profile Optimal Learning-Rate Schedules under Functional Scaling Laws: Power Decay and Warmup-Stable-Decay Binghui Li 1, Zilin Wang 2, 1 1 footnotemark: 1 Fengling Chen 2 Shiyang Zhao 2 Ruiheng Zheng 2 Lei Wu 1,2,3, 1 Center for Machine Learning Research, Peking University 2 School of Mathematical Sciences, Peking University 3 AI for Science Institute, Beijing libinghui@pku.edu.cn , wangzilin@stu.pku.edu.cn flchen_lwycc@stu.pku.edu.cn , 2300010604@stu.pku.edu.cn 2300010742@stu.pku.edu.cn , leiwu@math.pku.edu.cn Equal contribution.Corresponding author. Abstract We study optimal learning-rate schedules (LRSs) under the functional scaling law (FSL) framework introduced in Li et al. ( 2025 ) , which accurately models the loss dynamics of both linear regression and large language model (LLM) pre-training. Within FSL, loss dynamics are governed by two exponents: a source exponent s 0 s 0 controlling the rate of signal learning, and a capacity exponent Œ≤ 1 \beta 1 determining the rate of noise forgetting. Focusing on a fixed training horizon N N , we derive the optimal LRSs and reveal a sharp phase transition. In the easy-task regime s ‚©æ 1 ‚àí 1 / Œ≤ s\geqslant 1-1/\beta , the optimal schedule follows a power decay to zero, Œ∑ ‚àó ‚Äã ( z ) = Œ∑ peak ‚Äã ( 1 ‚àí z / N ) 2 ‚Äã Œ≤ ‚àí 1 \eta^{*}(z)=\eta_{\mathrm{peak}}(1-z/N)^{2\beta-1} , where the peak learning rate scales as Œ∑ peak ‚âÇ N ‚àí ŒΩ \eta_{\mathrm{peak}}\eqsim N^{-\nu} for an explicit exponent ŒΩ = ŒΩ ‚Äã ( s , Œ≤ ) \nu=\nu(s,\beta) . In contrast, in the hard-task regime s 1 ‚àí 1 / Œ≤ s 1-1/\beta , the optimal LRS exhibits a warmup‚Äìstable‚Äìdecay (WSD) (Hu et al. , 2024 ) structure: it maintains the largest admissible learning rate for most of training and decays only near the end, with the decay phase occupying a vanishing fraction of the horizon. We further analyze optimal shape-fixed schedules, where only the peak learning rate is tuned‚Äìa strategy widely adopted in practice‚Äìand characterize their strengths and intrinsic limitations. This yields a principled evaluation of commonly used schedules such as cosine and linear decay. Finally, we apply the power-decay LRS to one-pass stochastic gradient descent (SGD) for kernel regression and show the last iterate attains the exact minimax-optimal rate, eliminating the logarithmic suboptimality present in prior analyses. Numerical experiments corroborate our theoretical predictions. 1 Introduction Learning-rate schedules (LRSs) are a fundamental component of stochastic optimization, governing the trade-off between optimization progress and statistical noise during training, and thereby shaping convergence behavior. They play a central role in both theoretical analyses and practical algorithm design in modern machine learning. The formal study of learning-rate schedules dates back to the seminal work of Robbins and Monro ( 1951 ) , which established sufficient conditions for convergence of stochastic approximation, namely ‚àë k = 1 ‚àû Œ∑ k = ‚àû \sum_{k=1}^{\infty}\eta_{k}=\infty and ‚àë k = 1 ‚àû Œ∑ k 2 ‚àû \sum_{k=1}^{\infty}\eta_{k}^{2} \infty . A canonical realization satisfying these conditions is the polynomial-decay schedule Œ∑ k ‚àù k ‚àí Œ∫ \eta_{k}\propto k^{-\kappa} with Œ∫ ‚àà ( 1 / 2 , 1 ) \kappa\in(1/2,1) , which has been extensively adopted in the analysis of both convex and non-convex optimization (Lacoste-Julien et al. , 2012 ; Bubeck, 2014 ) . In linear regression, such schedules‚Äîwhen combined with iterate averaging (Ruppert, 1988 ) ‚Äîcan achieve minimax-optimal convergence rates in a statistical sense (Bach and Moulines, 2013 ; Dieuleveut and Bach, 2015 ; M√ºcke et al. , 2019 ) . More recently, a line of work has focused on the performance of the last iterate, showing that exponential-decay schedules can attain nearly optimal rates without averaging (Ge et al. , 2019 ; Wu et al. , 2022a ; Zhang et al. , 2024 ) . Despite the rich theoretical literature, two notable gaps remain. First, the learning-rate schedules that are theoretically optimal or near-optimal in classical analyses‚Äîsuch as polynomial or exponential decay‚Äîare rarely used in modern large-scale training. Second, practical training overwhelmingly relies on alternative schedules, most notably cosine decay (Loshchilov and Hutter, 2016 ; Hoffmann et al. , 2022 ; Touvron et al. , 2023 ) and warmup‚Äìstable‚Äìdecay (WSD) (Zhai et al. , 2022 ; Hu et al. , 2024 ; Liu et al. , 2024 ; Team et al. , 2025 ) . WSD schedules typically keep the learning rate constant for the majority of the training horizon (often up to 80%) and defer decay to a short final annealing phase. This stark contrast raises a fundamental theoretical question: why can schedules that delay decay until the very end of training still perform well? A key reason for these gaps is that most existing analyses do not derive learning-rate schedules from a principled approach. Instead, they follow a ‚Äúpropose-and-verify‚Äù paradigm: a specific schedule‚Äîtypically motivated by classical sufficient conditions or heuristics‚Äîis posited a priori and then shown to achieve a desired convergence rate under certain conditions. While this approach provides useful guarantees, it decouples performance analysis from schedule design and offers limited guidance on important questions such as which decay shapes are preferable and how optimal schedules depend on problem characteristics like model capacity and task difficulty. Recent work by Li et al. ( 2025 ) offers a complementary perspective in a controlled yet expressive setting. Under feature-space linear regression with power-law structure (Bahri et al. , 2024 ; Bordelon et al. , 2024 ; Lin et al. , 2024 ; Paquette et al. , 2024 ; Li et al. , 2025 ) , they derive a functional scaling law (FSL) that expresses the loss as an explicit and analytically tractable functional of the learning-rate schedule. Moreover, empirical evidence in Li et al. ( 2025 ) shows that this functional characterization remains accurate in large language model (LLM) pre-training, despite the substantial gap between the theoretical setting and practical LLM training. While the FSL provides a precise description of how a given learning-rate schedule shapes the loss, it does not address the problem of learning-rate schedule design, nor the characterization of optimal schedules. In this paper, we focus on the problem of optimal learning-rate schedules (LRSs) . Specifically, we characterize the LRS that minimizes the final-step loss over a fixed training horizon N N (equivalently, a fixed data budget). Under the FSL framework, this problem can be formulated as a constrained variational optimization problem, in which the loss is governed by a competition between signal learning , controlled by the source exponent s 0 s 0 , and noise forgetting , determined by the capacity exponent Œ≤ 1 \beta 1 . Our main contributions are summarized as follows. Optimal learning-rate schedules (LRSs). We derive that the optimal LRS depends critically on the task difficulty. In the easy-task regime ( s ‚©æ 1 ‚àí 1 / Œ≤ s\geqslant 1-1/\beta ), the optimal LRS follows a power decay to zero: Œ∑ ‚àó ‚Äã ( z ) = Œ∑ peak ‚Äã ( 1 ‚àí z / N ) 2 ‚Äã Œ≤ ‚àí 1 , \eta_{*}(z)=\eta_{\text{peak}}\left(1-z/N\right)^{2\beta-1}, where peak learning rate Œ∑ peak ‚âÇ N ‚àí 1 + s ‚Äã Œ≤ ‚àí Œ≤ 1 + s ‚Äã Œ≤ \eta_{\text{peak}}\eqsim N^{-\frac{1+s\beta-\beta}{1+s\beta}} . In contrast, in the hard-task regime ( s 1 ‚àí 1 / Œ≤ s 1-1/\beta ), the optimal LRS exhibits a WSD-like structure: the decay phase occupies only a o N ‚Äã ( 1 ) o_{N}(1) fraction of the training horizon, while retaining the same power-decay profile as the easy-task regime. Figure 1 (left) provides an illustration of these optimal LRSs. Shape-fixed optimality and capacity saturation. To isolate the essential structure underlying optimal LRSs, we consider a class of fractional schedules of the form Œ∑ ‚Äã ( z ) = Œ∑ 0 ‚Äã Œ∂ ‚Äã ( z / N ) , \eta(z)=\eta_{0}\,\zeta(z/N), which depend on training steps only through the relative progress z / N z/N and exhibit a power-decay tail near the end of training: Œ∂ ‚Äã ( x ) ‚àù ( 1 ‚àí x ) Œ≥ \zeta(x)\propto(1-x)^{\gamma} as x ‚Üí 1 x\to 1 . Within this shape-fixed setting‚Äîwhere the decay shape is fixed and only the peak learning rate is tuned‚Äîwe uncover a capacity saturation phenomenon: such schedules adapt to model capacity only up to Œ≤ ‚©Ω Œ≥ + 1 \beta\leqslant\gamma+1 , beyond which the achievable convergence rate saturates regardless of peak-rate tuning. Figure 1 (right) summarizes this behavior via a phase diagram of convergence rates over the ( Œ≤ , s ) (\beta,s) plane. This characterization clarifies the strengths and limitations of widely used practical schedules; in particular, Figure 1 (middle) shows that cosine LRS ( Œ≥ = 2 \gamma=2 ; see Section 5 ) indeed exhibits the predicted saturation. Improved convergence rates for kernel regression. Finally, to substantiate the predictions of the continuous-time FSL analysis, we provide a rigorous discrete-time analysis showing that one-pass SGD with a power-decay LRS attains the exact optimal convergence rate at the last iterate. To the best of our knowledge, this is the first such result without logarithmic factors, improving upon prior analyses based on exponential-decay LRS (Wu et al. , 2022a ; Lin et al. , 2024 ; Li et al. , 2025 ) . Figure 1 : (left) Illustration of optimal learning-rate schedules (LRSs): power decay in the easy-task regime and WSD with power decay in the hard-task regime. (middle) Performance comparison of cosine ( Œ≥ = 2 \gamma=2 ) and power-decay ( Œ≥ = 4.2 \gamma=4.2 ) LRSs for feature-space linear regression with source exponent s = 0.8 s=0.8 and capacity exponent Œ≤ = 5 \beta=5 . Power decay achieves the minimax-optimal rate N ‚àí Œ≤ ‚Äã s / ( Œ≤ ‚Äã s + 1 ) N^{-\beta s/(\beta s+1)} , whereas cosine decay suffers from capacity saturation and exhibits the suboptimal rate predicted by our theory (corresponding to the green region in the right phase diagram). For each data size, we perform 500 500 independent runs of SGD, and tune the peak learning rate to minimize the average final-step loss. (right) Phase diagram of convergence rates under shape-fixed fractional LRSs (Theorem 5.3 ). Each region in the ( Œ≤ , s ) (\beta,s) plane corresponds to distinct convergence rates. The vertical boundary Œ≤ = Œ≥ + 1 \beta=\gamma+1 marks a capacity-saturation threshold induced by fixing the decay shape; in the green region, this restriction leads to suboptimal convergence rates. 2 Related Work Neural scaling laws. Hestness et al. ( 2017 ) first observed that the performance of deep learning models follows predictable power-law relationships in model and data size, later formalized as neural scaling laws (Kaplan et al. , 2020 ) . These laws have since guided large-scale training and been refined across architectures and training regimes (Henighan et al. , 2020 ; Hoffmann et al. , 2022 ; Kadra et al. , 2023 ; Aghajanyan et al. , 2023 ; Muennighoff et al. , 2023 ; Kumar et al. , 2024 ; Tissue et al. , 2024 ; Luo et al. , 2025 ) , with parallel theoretical efforts explaining their origins and mechanisms (Bordelon et al. , 2024 ; Lin et al. , 2024 ; Bahri et al. , 2024 ; Paquette et al. , 2024 ; Lin et al. , 2025 ; Yan et al. , 2025 ; Kunstner and Bach, 2025 ; Li et al. , 2026 ; Wang et al. , 2026 ) . Our work fits into this line of research by providing a scaling-law analysis of the design of learning-rate schedules. In particular, we study the structure of optimal schedules by leveraging the FSL framework introduced by Li et al. ( 2025 ) . Optimal learning-rate schedules for linear regression. Previous analyses of optimal learning-rate schedules for linear regression often formulate the problem as an optimal control problem. However, such analyses are technically challenging and typically restricted to low-dimensional settings or isotropic Hessians (Li et al. , 2017 ; Fahrbach et al. , 2023 ) . By contrast, leveraging the FSL framework, we develop a principled variational approach to derive and analyze optimal LRSs. Understanding cosine and WSD schedules. For cosine schedules, Li et al. ( 2021 ) attributes their empirical success to a duration-aware property: the cumulative learning rate scales linearly with the total training horizon while the learning rate itself decays to a horizon-independent minimum. Standard polynomial decay schedules fail to satisfy this dual requirement. In this work, we generalize this property through a class of fractional LRS and identify the regimes in which it attains optimal rates and those in which it does not. For WSD schedules, Wen et al. ( 2024 ) provide a river-valley landscape interpretation of their