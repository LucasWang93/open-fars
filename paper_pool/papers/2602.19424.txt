Title: Hepato-LLaVA: An Expert MLLM with Sparse Topo-Pack Attention for Hepatocellular Pathology Analysis on Whole Slide Images

Abstract: Hepatocellular Carcinoma diagnosis relies heavily on the interpretation of gigapixel Whole Slide Images. However, current computational approaches are constrained by fixed-resolution processing mechanisms and inefficient feature aggregation, which inevitably lead to either severe information loss or high feature redundancy. To address these challenges, we propose Hepato-LLaVA, a specialized Multi-modal Large Language Model designed for fine-grained hepatocellular pathology analysis. We introduce a novel Sparse Topo-Pack Attention mechanism that explicitly models 2D tissue topology. This mechanism effectively aggregates local diagnostic evidence into semantic summary tokens while preserving global context. Furthermore, to overcome the lack of multi-scale data, we present HepatoPathoVQA, a clinically grounded dataset comprising 33K hierarchically structured question-answer pairs validated by expert pathologists. Our experiments demonstrate that Hepato-LLaVA achieves state-of-the-art performance on HCC diagnosis and captioning tasks, significantly outperforming existing methods. Our code and implementation details are available at https://pris-cv.github.io/Hepto-LLaVA/.

Body: Hepato-LLaVA: An Expert MLLM with Sparse Topo-Pack Attention for Hepatocellular Pathology Analysis on Whole Slide Images 1 Introduction 2 Methods 2.1 Preliminary 2.2 HepatoPathoVQA Dataset 2.3 Sparse Topo-Pack Attention 2.4 Hepato-LLaVA 3 Experiments 3.1 Experimental Settings 3.2 Main Results 3.3 Ablation Study 4 Conclusion 1 1 institutetext: 1 School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing 100876, China 2 Department of Pathology, National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing 100021, China 1 1 email: zhonghao.yan@bupt.edu.cn, zhangyi@cicams.ac.cn âˆ— Equal Contribution â€  Project Lead â€¡ Corresponding Author Hepato-LLaVA: An Expert MLLM with Sparse Topo-Pack Attention for Hepatocellular Pathology Analysis on Whole Slide Images Yuxuan Yang âˆ— 1 Zhonghao Yan âˆ— â£ â€  1 {}^{*\ \dagger\ 1} Yi Zhang âˆ— 2 Bo Yun 1 Muxi Diao 1 Guowei Zhao 2 Kongming Liang â€¡ 1 Wenbin Li â€¡ 2 Zhanyu Ma 1 Abstract Hepatocellular Carcinoma diagnosis relies heavily on the interpretation of gigapixel Whole Slide Images. However, current computational approaches are constrained by fixed-resolution processing mechanisms and inefficient feature aggregation, which inevitably lead to either severe information loss or high feature redundancy. To address these challenges, we propose Hepato-LLaVA, a specialized Multi-modal Large Language Model designed for fine-grained hepatocellular pathology analysis. We introduce a novel Sparse Topo-Pack Attention mechanism that explicitly models 2D tissue topology. This mechanism effectively aggregates local diagnostic evidence into semantic summary tokens while preserving global context. Furthermore, to overcome the lack of multi-scale data, we present HepatoPathoVQA, a clinically grounded dataset comprising 33K hierarchically structured question-answer pairs validated by expert pathologists. Our experiments demonstrate that Hepato-LLaVA achieves state-of-the-art performance on HCC diagnosis and captioning tasks, significantly outperforming existing methods. Our code and implementation details are available at https://pris-cv.github.io/Hepto-LLaVA/ . 1 Introduction Hepatocellular Carcinoma (HCC), a leading cause of global cancer mortality, relies on histopathological Whole Slide Images (WSIs) examination as the gold standard. However, HCCâ€™s high heterogeneity and complex microenvironment render gigapixel WSIs interpretation labor-intensive. Manual analysis is prone to inter-observer variability, especially for subtle early-stage lesions. Current Multiple Instance Learning (MIL) approaches [ 3 ] [ 10 ] [ 12 ] [ 17 ] are often limited to classification, lacking clinical diagnostic capacity. This has catalyzed WSI-based Multi-modal Large Language Models (MLLMs) [ 9 ] [ 16 ] [ 2 ] [ 7 ] [ 14 ] , which align visual pathology features with language to enable VQA. A key design choice in these MLLMs is how to represent gigapixel WSIs for MLLMs. Existing methods typically fall into two categories: thumbnail-based approaches [ 9 ] [ 16 ] that resize the gigapixel image into a megapixel image, and slide-encoder-based approaches [ 2 ] [ 7 ] that aggregate thousands of patches into global tokens. However, thumbnail-based approaches inevitably lose patch-level details, while both methods ingest only WSIs inputs, consequently less equipped with patch-level capabilities essential for diagnosis. We investigate two primary research questions regarding the slide encoder, which acts as a bottleneck: â€¢ RQ1: How can the slide encoder compress HCC WSIs into representations that preserve critical diagnostic details while minimizing redundancy? â€¢ RQ2: How can the slide encoder accommodate variable-resolution inputs to generate features for multi-scale diagnosis in complex liver tissues? To address RQ1 , we design a Sparse Topo-Pack Attention mechanism that simulates the local aggregation and global juxtaposition of pathological diagnosis. By modeling 2D tissue topology, the slide encoder extracts topology-informed representations while significantly reducing spatial redundancy. A light-weight connector then condenses these features into fixed-length query tokens, effectively bridging the modality gap without losing critical diagnostic details. To address RQ2 , we leverage hierarchical features to develop HepatoPathoVQA , a comprehensive multi-scale VQA dataset specifically for HCC. This dataset contains over 33K expert-validated QA pairs covering the full clinical workflow across three distinct scales: WSIs, Region of Interest (ROI, 5 Ã— \times mag.), and Patch (10 Ã— \times and 20 Ã— \times mag.). Finally, through Low-Rank Adaptation (LoRA) fine-tuning on this unified framework, we present Hepato-LLaVA , a specialized MLLM optimized for fine-grained hepatocellular pathology analysis. In summary, our three main contributions are: 1. We construct HepatoPathoVQA , the first multi-scale WSI dataset for HCC, comprising over 33K QA pairs across three scales, which improves multi-scale modeling and bridges data with real-world clinical practice. 2. We introduce a Sparse Topo-Pack Attention mechanism that models 2D tissue topology, extracting global-context-aware representations for WSIs while mitigating information redundancy. 3. We present Hepato-LLaVA , a specialized MLLM optimized via a three-stage pipeline, achieving an improvement of 20% in average diagnostic accuracy in HCC over existing open-source pathology MLLMs. 2 Methods 2.1 Preliminary Patch Encoding. Let ğˆ r â€‹ a â€‹ w âˆˆ â„ H r â€‹ a â€‹ w Ã— W r â€‹ a â€‹ w Ã— 3 \mathbf{I}_{raw}\in\mathbb{R}^{H_{raw}\times W_{raw}\times 3} denote raw WSIs. We tessellate the tissue region into non-overlapping patches of size P Ã— P P\times P . This forms a grid layout of dimensions H Ã— W H\times W , where H = âŒŠ H r â€‹ a â€‹ w / P âŒ‹ H=\lfloor H_{raw}/P\rfloor and W = âŒŠ W r â€‹ a â€‹ w / P âŒ‹ W=\lfloor W_{raw}/P\rfloor . Let x i , j x_{i,j} represent the patch at the spatial coordinate ( i , j ) (i,j) for 1 â‰¤ i â‰¤ H , 1 â‰¤ j â‰¤ W 1\leq i\leq H,1\leq j\leq W . A frozen feature encoder f e â€‹ n â€‹ c f_{enc} then maps each patch into a D D -dimensional embedding space, yielding a feature grid: ğ‡ g â€‹ r â€‹ i â€‹ d = { ğ¡ i , j âˆˆ â„ D âˆ£ ğ¡ i , j = f e â€‹ n â€‹ c â€‹ ( x i , j ) } 1 â‰¤ i â‰¤ H , 1 â‰¤ j â‰¤ W \mathbf{H}_{grid}=\left\{\mathbf{h}_{i,j}\in\mathbb{R}^{D}\mid\mathbf{h}_{i,j}=f_{enc}(x_{i,j})\right\}_{1\leq i\leq H,\,1\leq j\leq W} (1) Slide Encoding. Since the raw feature ğ‡ g â€‹ r â€‹ i â€‹ d \mathbf{H}_{grid} contains thousands of patches, directly utilizing it for slide-level tasks introduces excessive dimensionality and redundant information. To address this, a slide encoder ğ’¯ â€‹ ( â‹… ) \mathcal{T}(\cdot) is employed to extract semantic features from the flattened patch sequence ğ’ p â€‹ a â€‹ t â€‹ c â€‹ h = Flatten â€‹ ( ğ‡ g â€‹ r â€‹ i â€‹ d ) \mathbf{S}_{patch}=\text{Flatten}(\mathbf{H}_{grid}) : ğ‡ s â€‹ l â€‹ i â€‹ d â€‹ e = ğ’¯ â€‹ ( ğ’ p â€‹ a â€‹ t â€‹ c â€‹ h ) \mathbf{H}_{slide}=\mathcal{T}(\mathbf{S}_{patch}) (2) In conventional MIL, ğ’¯ \mathcal{T} acts as a global aggregation function, where the output ğ‡ s â€‹ l â€‹ i â€‹ d â€‹ e âˆˆ â„ 1 Ã— D \mathbf{H}_{slide}\in\mathbb{R}^{1\times D} is a single compact token. In contrast, for Transformer-based approaches, ğ’¯ \mathcal{T} typically functions as a self-attention network where the output ğ‡ s â€‹ l â€‹ i â€‹ d â€‹ e âˆˆ â„ N Ã— D \mathbf{H}_{slide}\in\mathbb{R}^{N\times D} , often requiring a subsequent pooling to obtain a global token. 2.2 HepatoPathoVQA Dataset Figure 1: Overview of the HepatoPathoVQA construction pipeline: (1) Extracts ROIs and Patches from WSIs using MST-based clustering and triangular seed-point selection. (2) Employs Gemini-3-flash for hierarchical inference by integrating macroscopic descriptions as context for subsequent microscopic analysis. (3) Generates multi-scale QA pairs and captions for instruction tuning and alignment. Dataset Construction. We collected 200 WSIs containing HCC, with all patient identifiers removed to ensure privacy. From these WSIs, we constructed HepatoPathoVQA , a multi-scale dataset featuring 33K QA pairs centered on morphological analysis and diagnosis. As shown in Fig. 1 , following the standard diagnostic workflow of pathologists, we developed a data generation pipeline using Gemini-3-flash [ 4 ] . This pipeline simulates the clinical reasoning process, transitioning from macroscopic observation to microscopic details. Each step in the pipeline is designed as an atomic operation. The data construction pipeline consists of three stages. (1) Hierarchical Multi-Scale Sampling. WSIs serve as the raw input. We identify ROIs using a triangular seed-point algorithm and a Minimum Spanning Tree (MST). The MST aggregates adjacent patches based on cosine similarity to form three candidate regions. These ROIs represent a 5 Ã— \times magnification based on medical priors of cell adjacency. We manually remove uninformative regions. Finally, 10 Ã— \times and 20 Ã— \times local patches are randomly sampled around the ROI centers. (2) Context-Injected Reasoning Generation. Since Gemini cannot process gigapixel images, all regions are resized to 2048 Ã— \times 2048 pixels. We design scale-specific prompts based on clinical focus and diagnostic status. Gemini generates descriptions in a hierarchical manner. Descriptions from higher scales serve as context for the next scale to ensure logical consistency from macroscopic to microscopic levels. (3) Scale-aware Data Synthesis. Leveraging the generated hierarchical descriptions, we generate diverse multi-scale QA pairs for instruction tuning. These pairs cover all three resolutions, enabling the model to perform both granular morphological analysis and holistic clinical diagnosis. Furthermore, these descriptions are utilized to produce high-quality image-text captions, which serve as the foundation for connector pre-training to achieve robust vision-language alignment. Dataset Statistics. HepatoPathoVQA comprises 33,332 QA pairs. At our three proposed spatial scales, we follow the WSI-LLaVA [ 7 ] setup to prompt the model to generate reasoning chains along two axesâ€”morphology and diagnosis. Each dimension is further subdivided into four subcategories, thereby supporting hierarchical diagnosis from overall patterns to cellular details. Beyond the VQA pairs, we provide HepatoPathoCaption (3,288 pairs) for alignment pre-training and HepatoPathoBench (3,056 pairs) split from HepatoPathoVQA for evaluation. The entire dataset construction process was validated by three expert pathologists under a blind evaluation protocol, yielding high inter-rater consistency (Pearson r = 0.96 r=0.96 ) and a low rejection rate ( 2 % 2\% ). 2.3 Sparse Topo-Pack Attention Topological Mismatch in Existing Modeling. Existing pathology-specific MLLM predominantly rely on Longnet to extract features, treating the WSIs as a flattened 1D sequence. This assumption neglects the intrinsic 2D topological properties of pathological tissues. Biologically, semantic dependencies vary significantly across scales: local regions share a common semantic meaning(e.g., tumor margins crossing multiple patches), whereas distant regions are distinct with lower semantic coupling. To address this, we propose a hierarchical sparse attention mechanism that restores topological priors (Fig. 2 , upper panel). Hierarchical Sequence Construction. Unlike standard flattening, we construct a structured input sequence that explicitly encodes the grid hierarchy. We organize this grid into M M Summary Packs in row-major order. Each pack ğ m \mathbf{P}_{m} represents a local k Ã— k k\times k window. We generate the pack summary token ğ¬ m \mathbf{s}_{m} by resizing the local image region of pack m m and passing it through the patch encoder f e â€‹ n â€‹ c f_{enc} for semantic anchors. Similarly, the global token ğ  g â€‹ l â€‹ o â€‹ b â€‹ a â€‹ l \mathbf{g}_{global} is derived by resizing entire WSIs and encoding it. The total sequence ğ’ i â€‹ n \mathbf{S}_{in} is formed as: ğ’ i â€‹ n = [ ğ  g â€‹ l â€‹ o â€‹ b â€‹ a â€‹ l , ğ¡ 1 , 1 1 , â€¦ , ğ¡ k , k 1 , ğ¬ 1 âŸ Pack â€‹ ğ 1 , â€¦ , ğ¡ 1 , 1 M , â€¦ , ğ¡ k , k M , ğ¬ M âŸ Pack â€‹ ğ M ] \mathbf{S}_{in}=[\ \mathbf{g}_{global},\ \underbrace{\mathbf{h}_{1,1}^{1},\dots,\mathbf{h}_{k,k}^{1},\mathbf{s}_{1}}_{\text{Pack }\mathbf{P}_{1}},\ \dots,\ \underbrace{\mathbf{h}_{1,1}^{M},\dots,\mathbf{h}_{k,k}^{M},\mathbf{s}_{M}}_{\text{Pack }\mathbf{P}_{M}}\ ] (3) where ğ¡ i , j m \mathbf{h}_{i,j}^{m} represents the fine-grained patch tokens within the m m -th pack. In our implementation, we set k = 3 k=3 , resulting in 10 tokens per pack. Through the hierarchical attention mechanism, the summary token functions as a dynamic query that aggregates diagnostic evidence from patch tokens ğ¡ i , j m \mathbf{h}_{i,j}^{m} . Hierarchical Sparse Mask. We define a hierarchical mask â„³ \mathcal{M} to enforce the specific interaction rules. Let Î© P \Omega_{P} and Î© S \Omega_{S} denote the sets of all patch tokens and all summary tokens, respectively. Let ğ’« â€‹ ( t ) \mathcal{P}(t) denote the pack index of a token t t . For any query token t i t_{i} and key token t j t_{j} in ğ’ i â€‹ n \mathbf{S}_{in} , the attention mask â„³ i , j \mathcal{M}_{i,j} is: â„³ i , j = { 0 if â€‹ t j = ğ  g â€‹ l â€‹ o â€‹ b â€‹ a â€‹ l ( Global Sink ) 0 if â€‹ t i , t j âˆˆ Î© P âˆ§ ğ’« â€‹ ( t i ) = ğ’« â€‹ ( t j ) ( Intra-Pack Dense ) 0 if â€‹ t i âˆˆ Î© S âˆ§ t j âˆˆ Î© P âˆ§ ğ’« â€‹ ( t i ) = ğ’« â€‹ ( t j ) ( Aggregation ) 0 if â€‹ t i , t j âˆˆ Î© S ( Summary-Level Interaction ) âˆ’ âˆ otherwise \mathcal{M}_{i,j}=\begin{cases}0 \text{if }t_{j}=\mathbf{g}_{global}\quad(\text{Global Sink})\\ 0 \text{if }t_{i},t_{j}\in\Omega_{P}\land\mathcal{P}(t_{i})=\mathcal{P}(t_{j})\quad(\text{Intra-Pack Dense})\\ 0 \text{if }t_{i}\in\Omega_{S}\land t_{j}\in\Omega_{P}\land\mathcal{P}(t_{i})=\mathcal{P}(t_{j})\quad(\text{Aggregation})\\ 0 \text{if }t_{i},t_{j}\in\Omega_{S}\quad(\text{Summary-Level Interaction})\\ -\infty \text{otherwise}\end{cases} (4) This hierarchical architecture effectively models pathological topology by harmonizing dual-scale interactions. The global token provides a macro-level reference, enabling local windows to focus on aggregating dense features into summary tokens. Summary tokens then facilitate long-range modeling, preserving the structural integrity of tissue regions across entire slide. In our implementation ( k = 3 k=3 ), this sparsity reduces the attention overhead to â‰ˆ ğŸ % \approx\mathbf{1\%} of the dense counterpart. 2.4 Hepato-LLaVA Figure 2: Overview of the Hepato-LLaVA framework: (Upper) Incorporates Sparse Topo-Pack Attention into the model architecture. (Lower) Implements a three-stage training pipeline: MAE pre-training, MoCo pre-training, and instruction tuning (via LoRA). The sparse attention mask defines three topological interactions: (1) Global Sink for macro-context broadcasting, (2) Intra-Pack for local dense interactions, and (3) Inter-Pack for summary-level connections across packs. MAE Pretrain. As shown in the lower panel of Fig. 2 , we applied