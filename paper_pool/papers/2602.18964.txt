Title: Yor-Sarc: A gold-standard dataset for sarcasm detection in a low-resource African language

Abstract: Sarcasm detection poses a fundamental challenge in computational semantics, requiring models to resolve disparities between literal and intended meaning. The challenge is amplified in low-resource languages where annotated datasets are scarce or nonexistent. We present Yor-Sarc, the first gold-standard dataset for sarcasm detection in Yor√πb√°, a tonal Niger-Congo language spoken by over 50 million people. The dataset comprises 436 instances annotated by three native speakers from diverse dialectal backgrounds using an annotation protocol specifically designed for Yor√πb√° sarcasm by taking culture into account. This protocol incorporates context-sensitive interpretation and community-informed guidelines and is accompanied by a comprehensive analysis of inter-annotator agreement to support replication in other African languages. Substantial to almost perfect agreement was achieved (Fleiss' Œ∫= 0.7660; pairwise Cohen's Œ∫= 0.6732--0.8743), with 83.3% unanimous consensus. One annotator pair achieved almost perfect agreement (Œ∫= 0.8743; 93.8% raw agreement), exceeding a number of reported benchmarks for English sarcasm research works. The remaining 16.7% majority-agreement cases are preserved as soft labels for uncertainty-aware modelling. Yor-Sarchttps://github.com/toheebadura/yor-sarc is expected to facilitate research on semantic interpretation and culturally informed NLP for low-resource African languages.

Body: Yor-Sarc: A gold-standard dataset for sarcasm detection in a low-resource African language 1 Introduction 2 Related works 3 Dataset, Multi-Annotator Framework and Inter-Annotator Agreement 3.1 Annotation Process 3.2 Annotator Protocol 3.3 Inter-Annotator Agreement Metrics 3.4 Pairwise Agreement (Cohen‚Äôs Kappa): 3.4.1 Multi-Rater Agreement (Fleiss‚Äô Kappa) 3.4.2 Agreement Pattern Analysis 3.4.3 Soft Label Derivation 3.5 Annotator Bias and Disagreement Analysis 3.5.1 Annotator Bias Quantification 3.5.2 Instance-Level Uncertainty 3.5.3 Confusion Matrix Analysis 3.6 Interpretation threshold 4 Inter-Annotator Agreement Analysis 4.1 Overall Agreement Metrics 4.2 Agreement Distribution and Consensus Patterns 4.3 Soft Labels and Uncertainty Preservation 4.4 Annotator Behavior and Pairwise Patterns 4.5 Benchmark Comparison and Quality Assessment 5 Conclusion and Future Directions A Supplementary Agreement Analysis A.1 Label Distribution Across Annotators A.2 Annotator Labelling Bias A.3 Agreement Level Distribution 7 Overall agreement distribution (left) and distribution by class (right) Yor-Sarc: A gold-standard dataset for sarcasm detection in a low-resource African language Toheeb A. Jimoh Corresponding author: toheeb.jimoh@ul.ie Department of Computer Science and Information Systems, University of Limerick, Castletroy, V94 T9PX, Limerick, Ireland Tabea De Wille Department of Computer Science and Information Systems, University of Limerick, Castletroy, V94 T9PX, Limerick, Ireland Nikola S. Nikolov Department of Computer Science and Information Systems, University of Limerick, Castletroy, V94 T9PX, Limerick, Ireland Abstract Sarcasm detection poses a fundamental challenge in computational semantics, requiring models to resolve disparities between literal and intended meaning. The challenge is amplified in low-resource languages where annotated datasets are scarce or nonexistent. We present Yor-Sarc , the first gold-standard dataset for sarcasm detection in Yor√πb√°, a tonal Niger-Congo language spoken by over 50 50 million people. The dataset comprises 436 instances annotated by three native speakers from diverse dialectal backgrounds using an annotation protocol specifically designed for Yor√πb√° sarcasm by taking culture into account. This protocol incorporates context-sensitive interpretation and community-informed guidelines and is accompanied by a comprehensive analysis of inter-annotator agreement to support replication in other African languages. Substantial to almost perfect agreement was achieved (Fleiss‚Äô Œ∫ = 0.7660 \kappa=0.7660 ; pairwise Cohen‚Äôs Œ∫ = 0.6732 \kappa=0.6732 ‚Äì 0.8743 0.8743 ), with 83.3 % 83.3\% unanimous consensus. One annotator pair achieved almost perfect agreement ( Œ∫ = 0.8743 \kappa=0.8743 ; 93.8 % 93.8\% raw agreement), exceeding a number of reported benchmarks for English sarcasm research works. The remaining 16.7 % 16.7\% majority-agreement cases are preserved as soft labels for uncertainty-aware modelling. Yor-Sarc 1 1 1 https://github.com/toheebadura/yor-sarc is expected to facilitate research on semantic interpretation and culturally informed NLP for low-resource African languages. Keywords: Natural Language Processing (NLP), Yor√πb√°, African languages, Sarcasm detection, Low-resource language, Data annotation 1 Introduction Sarcasm detection has become an important sub-task in natural language processing (NLP) as sarcasm frequently masks surface sentiment, thereby degrading the reliability of sentiment and opinion mining systems deployed on social media and other user-generated content. Automatically identifying sarcastic utterances is, however, a difficult problem given that it mostly relies on subtle pragmatic cues, literary knowledge, and cultural context, and is often misconstrued with figurative language phenomena such as irony and metaphor. Contemporary studies have explored sarcasm detection for English (Misra and Arora, 2023 ; Bamman and Smith, 2015 ) and a handful of other high-resource languages using supervised and neural approaches, including transformer-based models, demonstrating that robust performance typically depends on carefully annotated corpora and task-specific benchmarks. In contrast, low-resource languages remain significantly underexplored in sarcasm research. While there are emerging efforts for languages such as Urdu (Khan et al. , 2024 ) , and Arabic (Farha and Magdy, 2020 ) , among others, the global landscape remains heavily skewed toward English and a small number of non-African languages. These low-resource language studies consistently highlight a primary challenge: the scarcity of high-quality, manually annotated sarcasm datasets (Jimoh et al. , 2025 ) . For African languages, the resource gap is even more keen. Despite recent progress in sentiment analysis and general text classification, as exemplified by the AfriSenti benchmark for Twitter sentiment in 14 14 African languages (Muhammad et al. , 2023 ) and Naijasenti (Muhammad et al. , 2022 ) , which explores sentiment analysis among Nigerian languages, there are, to the best of current knowledge, no publicly documented gold-standard corpora dedicated to sarcasm detection for any major West African language, save Nigerian Pidgin (Ladoja and Afape, 2024 ) . Yor√πb√°, a tonal and morphologically rich language of the Niger-Congo family spoken by over 50 50 million people in Nigeria and the diaspora (Fagbolu et al. , 2016 ) , suffers this fate. A recent study of NLP for Yor√πb√° identifies significant advances in tasks such as diacritic restoration, part-of-speech tagging, machine translation, and sentiment analysis, but explicitly notes that figurative language phenomena‚Äîincluding sarcasm‚Äîremain virtually unexplored (Jimoh et al. , 2025 ) . This lack of resources not only limits the development of sarcasm-aware sentiment systems for Yor√πb√° speakers but also excludes Yor√πb√° pragmatics and satirical connotations from the broader scientific understanding of sarcasm across languages. This research addresses this gap by introducing the first, to the best of our knowledge, manually annotated sarcasm dataset for the Yor√πb√° language. The corpus currently consists of 436 436 short texts collected from diverse sources, including X (formerly Twitter), Facebook, Instagram, BBC News Yor√πb√°, YouTube video captions, and crowdsourced examples obtained via an ehtically approved online survey, all written in Standardised Yor√πb√° (SY) orthography with tone and diacritic marks. Each instance is independently labelled as ‚Äúsarcastic‚Äù or ‚Äúnon-sarcastic‚Äù by three native Yor√πb√° speakers, and a gold ‚Äútarget‚Äù label is derived via majority voting while preserving individual annotator decisions. This three-annotator scheme is inspired by recent low-resource sarcasm and sentiment datasets that emphasise multi-annotator truth sets and the importance of modelling agreement for pragmatically complex phenomena. The contributions of this work are threefold. First, it presents the first publicly available gold-standard sarcasm corpus for Yor√πb√°, filling a critical gap in African NLP resources and complementing existing text classification benchmarks such as AfriSenti (Muhammad et al. , 2023 ) . Second, it documents a culturally grounded annotation protocol and three-annotator scheme tailored to Yor√πb√° sarcasm, including a comprehensive inter-annotator agreement analysis such that it would inform similar efforts in other African languages. Ultimately, it provides a corpus-level analysis of the dataset, examining class distribution and source characteristics, among others. The rest of this paper is structured as follows: Section 2 reviews related work on sarcasm detection and dataset curation for this task. Section 3 describes the Yor√πb√° sarcasm dataset, annotation framework, and inter-annotator agreement analysis. Section 4 presents the inter-annotator agreement analysis (IAA). Section 5 presents the conclusion, discusses limitations, and outlines future directions. 2 Related works Early work on sarcasm detection focused primarily on English and treated the task as sentence‚Äëlevel text classification, using rule-based approaches involving lexical, syntactic, and sentiment‚Äëbased incongruity features (Joshi et al. , 2017 ; Riloff et al. , 2013 ) . Over time, the field has moved from rule‚Äëbased and traditional machine‚Äëlearning models, including k-Nearest Neighbor and logistic regression, among others, to deep learning and transformer‚Äëbased approaches that can better capture semantic and pragmatic cues (Davidov et al. , 2010 ; Misra and Arora, 2023 ) . Recent studies have been incorporating large language models and agentic systems (Lee et al. , 2025 ; Zhang et al. , 2025 ) , indicating progress in the sarcasm detection task in NLP. Simultaneously, publicly available datasets for monolingual sarcasm detection in high-resource languages and a few low-resource languages are being released to facilitate continuous research in the domain. Datasets such as SARC (Reddit) (Khodak et al. , 2018 ) and MUStARD (multimodal dialogue) (Castro et al. , 2019 ) have enabled research on sarcasm in multi‚Äëturn dialogue settings, showing the competence of advanced methods of sarcasm detection on these benchmarks. More recently, multimodal sarcasm detection has attracted attention (Kamau and Abbas, 2025 ) , with surveys documenting how visual, involving images and emojis, and acoustic signals complement text in social media and speech‚Äëbased sarcasm recognition (Gao et al. , 2024 ) . These lines of work provide resource and methodological foundations, albeit they are almost exclusively focused on high‚Äëresource, predominantly Western languages. In African NLP, progress has largely been driven by efforts to build foundational resources for core tasks such as language identification (Asubiaro et al. , 2018 ) , part‚Äëof‚Äëspeech tagging (Dione et al. , 2023 ) , named entity recognition (Adelani et al. , 2022 ) , and sentiment analysis (Muhammad et al. , 2022 ) . The AfriSenti benchmark (Muhammad et al. , 2023 ) is a major contribution, providing over 110 , 000 110,000 annotated tweets for sentiment analysis across 14 14 African languages, including Yor√πb√°. It further illustrates how carefully designed, native-annotated corpora can accelerate research on African languages; however, it focuses on coarse-grained sentiment labels rather than fine-grained figurative phenomena such as sarcasm. 3 Dataset, Multi-Annotator Framework and Inter-Annotator Agreement We collected 436 436 Yor√πb√° instances from six diverse sources (Figure 1 ). BBC News Yor√πb√° comprises 65.4 % ‚Äã ( n = 285 ) 65.4\%(n=285) , providing contextually-grounded examples with professional editing. Social media platforms contribute 124 124 instances ( 28.5 % ) (28.5\%) of informal, spontaneous language use: Instagram (21.8%), X/Twitter (3.9%), Facebook ( 2.8 % ) (2.8\%) , and YouTube ( 2.3 % ) (2.3\%) . Crowdsourced contributions ( 3.9 % ) (3.9\%) fill gaps in naturally occurring data, particularly face-to-face contexts. This multi-source approach balances formal and informal records. Figure 1 : Distribution of instances across data sources 3.1 Annotation Process We employed a rigorous multi-annotator framework to ensure the highest quality gold standard for Yor√πb√° sarcasm detection. Three native Yor√πb√° speakers, all with linguistic expertise and fluency in the language, independently annotated 436 436 instances of Yor√πb√° text for sarcasm presence or otherwise. Annotators were provided with comprehensive annotation guidelines developed through an iterative pilot study involving 20 20 training examples with subsequent discussion and refinement. 3.2 Annotator Protocol Given that our annotators are native speakers of the language and understand diverse dialectal backgrounds, such as Standard Yor√πb√°, If·∫πÃÄ, √åj·∫πÃÄb√∫, among others, it ensures our gold standard captures varied interpretations while maintaining consistency through clear guidelines. For effectiveness, each annotator independently labeled all instances without consultation or access to other annotators‚Äô decisions. Binary labels (sarcastic or non-sarcastic) were assigned based on whether the statement conveyed meaning contrary to its literal interpretation. 3.3 Inter-Annotator Agreement Metrics To assess the reliability and quality of our annotations, we employ a comprehensive suite of inter-annotator agreement metrics. These metrics quantify the extent to which our three independent annotators ( A 1 A_{1} , A 2 A_{2} , and A 3 A_{3} ) concur in their sarcasm judgments across all N = 436 N=436 instances. We adopt a multi-metric approach to provide robust annotation quality, following best practices in computational linguistics (Artstein and Poesio, 2008 ) . Our agreement analysis framework comprises three complementary measurement approaches. 3.4 Pairwise Agreement (Cohen‚Äôs Kappa): Cohen‚Äôs kappa coefficient ( Œ∫ \kappa ) is used to measure the agreement between each pair of annotators, correcting for chance agreement. For each annotator pair ( i , j ) (i,j) , Cohen‚Äôs kappa (McHugh, 2012 ) measures agreement while correcting for chance: Œ∫ i ‚Äã j = P o ‚àí P e 1 ‚àí P e \displaystyle\kappa_{ij}=\frac{P_{o}-P_{e}}{1-P_{e}} (1) where P o P_{o} is the observed agreement proportion and P e P_{e} is the expected agreement by chance. For the binary labels l i , k ‚àà { 0 , 1 } l_{i,k}\in\{0,1\} : P o \displaystyle P_{o} = 1 N ‚Äã ‚àë k = 1 N ùüô ‚Äã [ l i , k = l j , k ] \displaystyle=\frac{1}{N}\sum_{k=1}^{N}\mathbbm{1}[l_{i,k}=l_{j,k}] (2) P e \displaystyle P_{e} = ‚àë c ‚àà { 0 , 1 } p i , c ‚ãÖ p j , c , where ‚Äã p i , c = 1 N ‚Äã ‚àë k = 1 N ùüè ‚Äã [ l i , k = c ] \displaystyle=\sum_{c\in\{0,1\}}p_{i,c}\cdot p_{j,c},\quad\text{where }p_{i,c}=\frac{1}{N}\sum_{k=1}^{N}\mathbf{1}[l_{i,k}=c] (3) We report all three pairwise values ( Œ∫ 12 \kappa_{12} , Œ∫ 13 \kappa_{13} , Œ∫ 23 \kappa_{23} ) and their average Œ∫ ¬Ø = 1 3 ‚Äã ‚àë i j Œ∫ i ‚Äã j \bar{\kappa}=\frac{1}{3}\sum_{i j}\kappa_{ij} , with 95% bootstrap confidence intervals (Efron and Tibshirani, 1985 ) . 3.4.1 Multi-Rater Agreement (Fleiss‚Äô Kappa) Also, to assess overall agreement across all three annotators simultaneously, we compute Fleiss‚Äô kappa (Fleiss, 1971 ) : Œ∫ F = P ¬Ø ‚àí P ¬Ø e 1 ‚àí P ¬Ø e \displaystyle\kappa_{F}=\frac{\bar{P}-\bar{P}_{e}}{1-\bar{P}_{e}} (4) For each instance k k , let n k , c n_{k,c} denote the number of raters assigning category c c . The per-instance agreement is: P k = 1 r ‚Äã ( r ‚àí 1 ) ‚Äã ‚àë c = 1 C [ n k , c 2 ‚àí n k , c ] = n k , 0 2 + n k , 1 2 ‚àí 3 6 \displaystyle P_{k}=\frac{1}{r(r-1)}\sum_{c=1}^{C}[n_{k,c}^{2}-n_{k,c}]=\frac{n_{k,0}^{2}+n_{k,1}^{2}-3}{6} (5) where r = 3 r=3 is the number of raters and C = 2 C=2 is the number of categories. The mean observed agreement is: P ¬Ø = 1 N ‚Äã ‚àë k = 1 N P k \displaystyle\bar{P}=\frac{1}{N}\sum_{k=1}^{N}P_{k} (6) Furthermore, the expected agreement is computed from marginal distributions as follows: P ¬Ø e = ‚àë c = 1 C p c 2 , where ‚Äã p c = 1 N ‚ãÖ r ‚Äã ‚àë k = 1 N n k , c \displaystyle\bar{P}_{e}=\sum_