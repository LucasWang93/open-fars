Title: Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning

Abstract: While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.

Body: Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning 1 Introduction 2 Related Work 2.1 3D Reconstruction and Scene Representation 2.2 Vision-Language Models for 3D Reasoning 3 Method 3.1 Predictive Spatial Field Modeling 3.2 Learning a Unified Spatial Representation Asymmetric View Aggregator Spa3R Encoder Spa3R Decoder Losses 3.3 Grounding Reasoning in Spatial Context 4 Experiments 4.1 Setup Pre-training Datasets Evaluation Benchmarks Instruction-Tuning Datasets 4.2 Implementation Details Spa3R Pre-training Spa3-VLM Instruction Tuning 4.3 Main Results 4.4 Ablation Studies Effectiveness of Unified Spatial Representation. Synergy of Geometric and Semantic Reconstruction Targets. Architecture for Spa3-VLM Integration. PSFM Mask Ratio. Camera Embedding Mechanism. 4.5 Qualitative Analysis 5 Conclusion Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning Haoyi Jiang 1 Liu Liu 2 Xinjie Wang 2 Yonghao He 3 Wei Sui 3 Zhizhong Su 2 Wenyu Liu 1 Xinggang Wang 1 1 Huazhong University of Science Technology 2 Horizon Robotics 3 D-Robotics Intern at D-Robotics: haoyi_jiang@hust.edu.cn Project leaderCorresponding author: xgwang@hust.edu.cn Abstract While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D spaceâ€”a cornerstone of spatial intelligenceâ€”remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R , a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM , effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R . 1 Introduction The ability to perceive and reason about the 3D world is fundamental to spatial intelligence, underpinning applications from autonomous navigation to robotic manipulation. Despite the remarkable progress of Vision-Language Models (VLMs) in interpreting 2D image content, their comprehension of 3D geometry and spatial relations remains superficial [ 39 ] . This limitation stems from the 2D nature of their pre-training, which lacks the necessary inductive biases to construct a coherent spatial manifold across multiple views. To endow VLMs with 3D spatial awareness, a straightforward strategy involves scaling training with massive multi-view data paired with spatial Question-Answer (QA) annotations. However, this approach imposes immense data demands, as it essentially requires the language model to acquire spatial understanding from first principles. An alternative line of work [ 10 , 5 ] incorporates explicit 3D modalities such as LiDAR point clouds. While being geometrically grounded, their reliance on specialized sensors severely restricts real-world scalability and applicability. Recent advances in geometry foundation models [ 35 , 34 ] have inspired methods that augment VLMs with geometric priors extracted from multiple views [ 46 , 38 , 9 ] . However, these methods typically provide the VLM with only partial, view-conditioned features from a limited set of views due to computational constraints, leaving significant spatial context unobserved. Consequently, the VLMs are tasked with the fundamentally ill-posed problem of implicitly reconstructing a holistic 3D scene from partial visual tokens, under the sparse supervision provided by instruction tuning. In contrast, we propose that spatial intelligence can emerge inherently from 2D vision alone through predictive modeling, dispensing with explicit spatial instruction tuningâ€”akin to how humans develop spatial awareness from multi-view and motion observation. To this end, we introduce Spa3R, a novel self-supervised framework built upon our Predictive Spatial Field Modeling (PSFM) paradigm. The Spa3R Encoder embeds a 3D scene from unposed context views into a unified, view-invariant spatial representation. A corresponding decoder then synthesizes the spatial feature field for arbitrary, unseen views conditioned on this latent representation. This predictive information bottleneck compels the encoder to capture the intrinsic 3D geometry, spatial layout, and semantic relationships of the scene, yielding a holistic understanding that extends beyond the observed images. By decoupling spatial representation learning from reasoning, the pre-trained Spa3R encoder serves as a versatile plug-in module. We integrate it into Qwen2.5-VL [ 1 ] via a lightweight Residual Cross-Attention Adapter to form Spa3-VLM, enabling the VLMâ€™s native visual features to actively query the unified spatial context. This design preserves the generalization capabilities of the base VLM while efficiently grounding its reasoning in 3D space. Extensive experiments demonstrate that Spa3-VLM achieves state-of-the-art performance across multiple spatial reasoning benchmarks, notably achieving 58.6% accuracy on the challenging VSI-Bench [ 39 ] . These results establish PSFM as a novel and scalable paradigm for advancing spatial intelligence. Our contributions are threefold: â€¢ We identify a fundamental bottleneck in existing VLMs for spatial reasoning: relying on the language model to implicitly reconstruct 3D scenes from partial, view-conditioned features constitutes an inefficient and ill-posed learning objective. â€¢ We propose Spa3R , a self-supervised framework based on Predictive Spatial Field Modeling (PSFM) that learns a unified, view-invariant spatial representation by synthesizing feature fields for arbitrary novel views, thereby internalizing intrinsic geometry and spatial layout of the scene. â€¢ We present Spa3-VLM , which integrates the pre-trained Spa3R Encoder to ground VLM reasoning in a holistic spatial context. Extensive experiments on VSI-Bench showcase significant performance improvement, highlighting PSFM as a scalable and effective path toward general spatial intelligence. 2 Related Work 2.1 3D Reconstruction and Scene Representation Recent advances in 3D reconstruction have undergone a fundamental shift from scene-specific optimization toward generalizable, feed-forward inference. Neural Radiance Fields (NeRF) [ 24 ] revolutionized scene representation with implicit neural fields, yet require costly per-scene training. Subsequent works, such as pixelNeRF [ 43 ] and MVSNeRF [ 4 ] , introduced conditional neural fields to learn generalizable 3D priors across multiple scenes. This feed-forward paradigm has recently extended to 3D Gaussian Splatting [ 3 ] and occupancy prediction [ 26 , 13 , 15 ] , facilitating efficient 3D perception from multi-view images. In parallel, the rise of large-scale pre-training has fostered the development of geometry foundation models. The DUSt3R [ 35 , 18 ] and VGGT [ 34 , 36 ] series demonstrated robust 3D reconstruction by effectively unifying the estimation of diverse 3D attributes including point maps, depth, and camera parameters from unposed images. These foundation models provide powerful, spatially grounded priors, inspiring hybrid frameworks [ 41 , 8 , 30 ] that combine such geometric estimation with rendering-based refinement for 3D reconstruction and semantic understanding. A recent, transformative trend involves pure Transformer architectures for Novel View Synthesis (NVS), spearheaded by LVSM [ 17 ] and its variants [ 16 , 33 ] . Eschewing explicit 3D inductive biases, these models leverage the representational capacity and scaling properties of Transformers to implicitly infer geometric structure from 2D data. Spa3R draws inspiration from this predictive paradigm but diverges fundamentally in objective. While LVSM prioritizes pixel-level synthesis for high-fidelity visual reconstruction, our PSFM framework targets representation learning by predicting spatially-grounded feature fields. It thus encapsulates the sceneâ€™s intrinsic geometry and semantic relationships within a unified, view-invariant latent space optimized for downstream spatial reasoning tasks. Figure 1 : Overview of the Spa3R framework and Spa3-VLM integration. (a) The Spa3R Encoder maps unposed context views to a unified, view-invariant spatial latent representation ğ’› \boldsymbol{z} . (b) The Spa3R Decoder synthesizes target features ğ‘­ ^ t \hat{\boldsymbol{F}}_{t} for arbitrary unseen views, conditioned on the spatial latent ğ’› \boldsymbol{z} and target camera embeddings ğ’“ \boldsymbol{r} . (c) For downstream spatial reasoning, the pre-trained Spa3R Encoder is integrated into a VLM to generate spatial representation ğ’› \boldsymbol{z} . A lightweight Adapter fuses the VLMâ€™s native visual features ğ‘­ V \boldsymbol{F}_{V} with spatial latent ğ’› \boldsymbol{z} via cross-attention, effectively grounding its reasoning in spatial context. 2.2 Vision-Language Models for 3D Reasoning While VLMs excel at 2D image-text alignment, extending them to the spatial domain remains challenging. Early approaches relied on explicit 3D modalities, either through object-centric representations [ 37 , 11 ] or by directly processing point clouds [ 10 , 5 , 12 ] to ground language in 3D space. While these approaches benefit from strong geometric grounding, their dependence on specialized sensors ( e.g . , LiDAR) or pre-processed 3D data severely restricts their real-world scalability and applicability. Recent research has shifted toward inferring 3D understanding from widespread 2D video or multi-view data. GPT4Scene [ 28 ] reconstructs Birdâ€™s-Eye View (BEV) representations from video features to capture spatio-temporal context, while approaches such as VG-LLM [ 46 ] , Spatial-MLLM [ 38 ] , and VLM3R [ 9 ] augment VLMs with view-conditioned priors extracted from geometry foundation models. Nevertheless, a fundamental bottleneck persists: these approaches typically expose the VLM to only partial, view-conditioned geometric cues, offloading the burden of holistic 3D reconstruction to the language model under sparse supervision. In contrast, our work addresses this limitation by introducing Predictive Spatial Field Modeling (PSFM), a self-supervised paradigm that explicitly learns a unified, view-invariant spatial representation, providing a robust and coherent foundation that empowers complex spatial reasoning. 3 Method In this section, we first outline the theoretical formulation of Predictive Spatial Field Modeling (PSFM) in Sec. 3.1 . We then elaborate on the Spa3R architecture in Sec. 3.2 . Subsequently, we describe the integration of the pre-trained Spa3R representation into Vision-Language Models to form Spa3-VLM in Sec. 3.3 . An overview of the framework is illustrated in Fig. 1 . 3.1 Predictive Spatial Field Modeling We formulate 3D spatial understanding as a Spatial Field Modeling problem. We conceptualize a 3D scene as a continuous spatial feature field f f , a function mapping any viewpoint defined by camera pose ğ’— âˆˆ ğ’± \boldsymbol{v}\in\mathcal{V} to its corresponding view-centric feature map ğ‘­ âˆˆ â„± \boldsymbol{F}\in\mathcal{F} : f : ğ’± â†’ â„± . f:\mathcal{V}\rightarrow\mathcal{F}. (1) The goal of PSFM is to infer the low-dimensional spatial manifoldâ€”which encapsulates the sceneâ€™s intrinsic geometric structureâ€”from a sparse set of N C N_{C} context views C = { ( ğ’— c , ğ‘­ c ) } c = 1 N C C=\{(\boldsymbol{v}_{c},\boldsymbol{F}_{c})\}_{c=1}^{N_{C}} . We formally frame this problem as a Neural Process. Specifically, we leverage an amortized encoder E Ï• E_{\phi} to encode the context set C C into a unified latent variable ğ’› \boldsymbol{z} . This variable ğ’› \boldsymbol{z} serves as a holistic representation of the spatial manifold, parameterizing the intrinsic properties of the scene. A decoder D Î¸ D_{\theta} then acts as a conditional neural field that reconstructs the target view features ğ‘­ ^ t \hat{\boldsymbol{F}}_{t} for arbitrary target poses ğ’— t \boldsymbol{v}_{t} conditioned on ğ’› \boldsymbol{z} : ğ’› \displaystyle\boldsymbol{z} = E Ï• â€‹ ( C ) = E Ï• â€‹ ( { ( ğ’— c , ğ‘­ c ) } c = 1 N C ) , \displaystyle=E_{\phi}(C)=E_{\phi}\left(\{(\boldsymbol{v}_{c},\boldsymbol{F}_{c})\}_{c=1}^{N_{C}}\right), (2) ğ‘­ ^ t \displaystyle\hat{\boldsymbol{F}}_{t} = D Î¸ â€‹ ( ğ’— t | ğ’› ) . \displaystyle=D_{\theta}(\boldsymbol{v}_{t}|\boldsymbol{z}). (3) During the training phase, we sample a set of views S S from each scene and randomly partition it into a context set C C and a target set T T . The learning objective is to minimize the distance between the predicted features ğ‘­ ^ t \hat{\boldsymbol{F}}_{t} and their ground-truth counterparts ğ‘­ t \boldsymbol{F}_{t} : â„’ PSFM = ğ”¼ C , T âˆ¼ S â€‹ [ âˆ‘ t âˆˆ T dist â€‹ ( D Î¸ â€‹ ( ğ’— t | E Ï• â€‹ ( C ) ) , ğ‘­ t ) ] . \mathcal{L}_{\text{PSFM}}=\mathbb{E}_{C,T\sim S}\left[\sum_{t\in T}\text{dist}\left(D_{\theta}(\boldsymbol{v}_{t}|E_{\phi}(C)),\boldsymbol{F}_{t}\right)\right]. (4) Theoretically, PSFM establishes an information bottleneck that disentangles intrinsic spatial representation from extrinsic, view-conditioned properties. By forcing the encoder E Ï• E_{\phi} to support the synthesis of features at arbitrary points on the manifold via a single latent ğ’› \boldsymbol{z} , the model is compelled to internalize the complete 3D geometry and spatial layout of the scene. 3.2 Learning a Unified Spatial Representation Spa3R instantiates the PSFM paradigm using a specialized encoder-decoder architecture composed of an Asymmetric View Aggregator, a Spa3R Encoder, and a Spa3R Decoder. Asymmetric View Aggregator A critical prerequisite for PSFM is the construction of a canonical spatial feature field f f where context features ğ‘­ c \boldsymbol{F}_{c} and target features ğ‘­ t \boldsymbol{F}_{t} are spatially aligned. To achieve this, we introduce an Asymmetric View Aggregator, which effectively adapts the pre-trained VGGT [ 34 ] to extract spatially-aligned features using its powerful global attention mechanism. We employ an asymmetric attention masking strategy in the global attention layers of VGGT to strictly prevent information l