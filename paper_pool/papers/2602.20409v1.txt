Title: CLIPoint3D: Language-Grounded Few-Shot Unsupervised 3D Point Cloud Domain Adaptation

Abstract: Recent vision-language models (VLMs) such as CLIP demonstrate impressive cross-modal reasoning, extending beyond images to 3D perception. Yet, these models remain fragile under domain shifts, especially when adapting from synthetic to real-world point clouds. Conventional 3D domain adaptation approaches rely on heavy trainable encoders, yielding strong accuracy but at the cost of efficiency. We introduce CLIPoint3D, the first framework for few-shot unsupervised 3D point cloud domain adaptation built upon CLIP. Our approach projects 3D samples into multiple depth maps and exploits the frozen CLIP backbone, refined through a knowledge-driven prompt tuning scheme that integrates high-level language priors with geometric cues from a lightweight 3D encoder. To adapt task-specific features effectively, we apply parameter-efficient fine-tuning to CLIP's encoders and design an entropy-guided view sampling strategy for selecting confident projections. Furthermore, an optimal transport-based alignment loss and an uncertainty-aware prototype alignment loss collaboratively bridge source-target distribution gaps while maintaining class separability. Extensive experiments on PointDA-10 and GraspNetPC-10 benchmarks show that CLIPoint3D achieves consistent 3-16% accuracy gains over both CLIP-based and conventional encoder-based baselines. Codes are available at https://github.com/SarthakM320/CLIPoint3D.

Body: CLIPoint3D: Language-Grounded Few-Shot Unsupervised 3D Point Cloud Domain Adaptation 1 Introduction 2 Related Works 3 Proposed Methodology 3.1 Our CLIPoint3D Framework 3.1.1 Knowledge-Driven Prompt Tuning 3.1.2 Few-Shot PEFT Adaptation 3.1.3 Entropy-Guided View Selection 3.1.4 Domain Alignment Strategies 3.2 Overall Training and Inference 3.3 Generalization Bound 4 Experimental Evaluation 4.1 Comparisons to the Literature 5 Ablation Studies 6 Conclusions A Dataset descriptions B LLM attributes generation C Pseudo-code of CLIPoint3D D Analysis of the LoRA rank E Conventional plug-in UDA methods in CLIP baselines F Effect of Œ± \alpha hyperparameter G Influence of the prompt length H Impact of CLIP variants I Effect of various LLMs CLIPoint3D: Language-Grounded Few-Shot Unsupervised 3D Point Cloud Domain Adaptation ‚àó Mainak Singha 1 equal contribution ‚àó Sarthak Mehrotra 2 Paolo Casari 1,3 Subhasis Chaudhuri 4 Elisa Ricci 1,5 Biplab Banerjee 4 1 University of Trento, Italy 2 MDSR Labs Adobe, India 3 CNIT, Italy 4 IIT Bombay, India 5 Fondazione Bruno Kessler, Italy Abstract Recent vision-language models (VLMs) such as CLIP demonstrate impressive cross-modal reasoning, extending beyond images to 3D perception. Yet, these models remain fragile under domain shifts, especially when adapting from synthetic to real-world point clouds. Conventional 3D domain adaptation approaches rely on heavy trainable encoders, yielding strong accuracy but at the cost of efficiency. We introduce CLIPoint3D , the first framework for few-shot unsupervised 3D point cloud domain adaptation built upon CLIP. Our approach projects 3D samples into multiple depth maps and exploits the frozen CLIP backbone, refined through a knowledge-driven prompt tuning scheme that integrates high-level language priors with geometric cues from a lightweight 3D encoder. To adapt task-specific features effectively, we apply parameter-efficient fine-tuning to CLIP‚Äôs encoders and design an entropy-guided view sampling strategy for selecting confident projections. Furthermore, an optimal transport-based alignment loss and an uncertainty-aware prototype alignment loss collaboratively bridge source-target distribution gaps while maintaining class separability. Extensive experiments on PointDA-10 and GraspNetPC-10 benchmarks show that CLIPoint3D achieves consistent 3-16% accuracy gains over both CLIP-based and conventional encoder-based baselines. Codes are available at https://github.com/SarthakM320/CLIPoint3D . 1 Introduction Point cloud understanding underpins modern 3D vision, driving applications in autonomous driving [ 60 ] , terrain mapping [ 61 ] , augmented reality, and robotics [ 59 ] . Unlike 2D imagery, point clouds explicitly encode fine-grained geometric cues essential for spatial reasoning. Despite the remarkable progress of deep 3D architectures [ 44 , 62 , 19 ] , most assume identical training and deployment distributions. In practice, scans acquired from heterogeneous sensors exhibit large variations in point density, sampling patterns, occlusion, and background clutter, leading to severe performance degradation under domain shifts [ 5 , 43 ] . This issue is exacerbated when transferring from synthetic benchmarks to real-world environments, making unsupervised domain adaptation (UDA) [ 13 , 37 , 14 , 38 , 58 , 32 ] central to achieving scalable 3D perception. Figure 1 : Comparison of CLIPoint3D with SOTA methods on GraspNetPC-10. Encoder-based 3D UDA methods (e.g., PointDAN [ 45 ] , GAST [ 75 ] , MLSP [ 36 ] ) are accurate but computationally expensive, while CLIP-based extensions fail to bridge the synthetic-real gap. CLIPoint3D achieves +16.4% improvement with minimal overhead. Direct fine-tuning on target data can partially mitigate domain gaps but requires dense 3D annotations and high compute, impractical for dynamic or safety-critical applications [ 3 , 48 ] . Since 3D labeling is costly and error-prone [ 29 ] , UDA methods aim to transfer knowledge from labeled sources to unlabeled targets. The key difficulty lies in jointly enforcing statistical alignment (distributional consistency) and semantic alignment (class-level coherence); neglecting either leads to geometrically aligned yet semantically inconsistent features. However, existing unsupervised point cloud domain adaptation (UPDA) techniques generally fall into three paradigms. ( i ) Adversarial alignment [ 14 , 45 ] uses domain discriminators to match latent features but often suffers from model collapse and over-alignment. ( ii ) Self-supervised learning [ 51 , 2 , 70 ] employs pretext tasks such as rotation or deformation prediction, but lacks semantic awareness. ( iii ) Pseudo-labeling and self-paced learning [ 75 , 53 , 36 ] iteratively refine noisy labels, yet degrade under large shifts. Although effective in controlled setups, these models are geometry-centric, computationally heavy, and rarely leverage semantic priors or uncertainty estimation, limiting their robustness to unseen modalities. Recently, vision-language models (VLMs) such as CLIP [ 46 ] have demonstrated impressive zero-shot transfer by coupling visual and textual modalities through large-scale contrastive pretraining. Extending CLIP to 3D [ 71 , 74 , 25 , 52 , 22 ] typically involves projecting point clouds into multi-view depth maps and processing them via CLIP‚Äôs image encoder. While effective for single-domain tasks, such projections expose two fundamental limitations: ( i ) Modality gap: CLIP‚Äôs encoder, trained on RGB images, poorly captures the sparse, textureless, and geometry-dominant nature of 3D depth maps; ( ii ) Domain gap: Existing CLIP-3D models [ 7 , 65 , 64 ] lack mechanisms for cross-domain adaptation, yielding poor generalization beyond their source domain. These issues reveal a crucial research gap: How can we harness CLIP‚Äôs semantic priors to enable unsupervised 3D domain adaptation while bridging both the 2D-3D modality and source-target domain gaps in a compute-efficient way? We hypothesize that CLIP‚Äôs language-grounded latent space can be effectively adapted for 3D UDA if jointly guided by geometric cues and uncertainty-aware optimization. This motivates a framework that (i) injects geometric awareness into CLIP‚Äôs latent space, (ii) aligns distributions across domains without labels, and (iii) achieves parameter-efficient adaptation for few-shot supervision. Our Approach. We introduce CLIPoint3D , a unified framework for few-shot unsupervised 3D domain adaptation built on top of CLIP. It projects each point cloud into multiple depth maps [ 71 ] and reuses CLIP‚Äôs frozen visual backbone, leveraging 2D pretraining for efficient 3D transfer. A knowledge-driven prompt tuning module fuses high-level semantic priors from large language models (LLMs) [ 47 , 1 , 18 , 66 , 42 ] with low-level geometric features from a lightweight 3D encoder, grounding CLIP‚Äôs embeddings in 3D structure. To further reduce compute, we employ parameter-efficient fine-tuning (PEFT) [ 35 ] to adapt a small subset of CLIP parameters while preserving its zero-shot capability. An entropy-guided view sampling strategy [ 54 ] filters ambiguous or redundant views to stabilize multi-view aggregation. Finally, we introduce two novel alignment objectives: (i) an uncertainty-aware prototype alignment loss that performs class-level coupling using entropy-weighted prototypes, and (ii) an entropy-regularized OT alignment that enforces smooth, noise-tolerant global matching. Together, these confidence-aware objectives yield robust semantic and distributional alignment under large 3D domain shifts. In summary, our key contributions are: 1. The first CLIP-based framework for few-shot unsupervised 3D point cloud domain adaptation, achieving strong cross-domain generalization with minimal training cost. 2. A knowledge-driven prompt tuning scheme that unites LLM-derived semantic priors and 3D geometry for multimodal grounding. 3. Dual uncertainty-aware objectives, OT-based statistical alignment and prototype-level semantic regularization, that tighten the adaptation generalization bound (Sec. 3.3 ). 4. An entropy-guided view selection mechanism that enhances stability, interpretability, and efficiency under multi-view uncertainty. Extensive experiments and ablations across standard benchmarks demonstrate that CLIPoint3D achieves superior accuracy-efficiency trade-offs compared to both 3D encoder-based and CLIP-based baselines (Figure 1 ). Figure 2 : Overview of CLIPoint3D , the first CLIP-based unsupervised 3D point cloud domain adaptation framework, comprises four key modules: (1) Knowledge-driven prompt tuning generates LLM-guided textual and 3D-aware visual prompts; (2) Parameter-efficient fine-tuning (PEFT) jointly optimizes these prompts and the encoder while (3) entropy-based view selection filters unreliable projections; (4) Dual objectives, uncertainty-aware prototype loss ùêã proto \mathbf{L}_{\mathrm{proto}} and optimal transport loss ùêã OT \mathbf{L}_{\mathrm{OT}} , achieve joint semantic and statistical alignment. Additional regularizers include ùêã conf = ùêã conf ‚Äã ( S ) + ùêã conf ‚Äã ( T ) \mathbf{L}_{\mathrm{conf}}=\mathbf{L}_{\mathrm{conf(S)}}+\mathbf{L}_{\mathrm{conf(T)}} , and ùêã ortho = ùêã ortho ‚Äã ( S ) + ùêã ortho ‚Äã ( T ) \mathbf{L}_{\mathrm{ortho}}=\mathbf{L}_{\mathrm{ortho(S)}}+\mathbf{L}_{\mathrm{ortho(T)}} , to ensure stable learning across source and target domains. 2 Related Works Unsupervised 3D Point Cloud Domain Adaptation. UPDA seeks to transfer knowledge from labeled source to unlabeled target domains. Existing approaches mainly follow three paradigms. ( i ) Domain adversarial training [ 14 , 45 ] employs discriminators to enforce feature invariance while the feature extractor learns to confuse them. Although conceptually sound, such methods often suffer from unstable convergence, mode collapse, and over-alignment that compromises geometric fidelity critical for fine-grained 3D recognition. ( ii ) Self-supervised learning (SSL) [ 51 , 2 , 70 ] leverages auxiliary pretext tasks such as rotation prediction [ 75 ] or deformation reconstruction [ 2 ] to capture domain-invariant cues. While interpretable, SSL mainly learns low-level geometric invariances with limited semantic alignment. ( iii ) Pseudo-label and self-paced learning [ 75 , 53 , 36 ] iteratively refines pseudo labels for confident target samples, but noisy labels under large shifts amplify confirmation bias. Overall, most UPDA methods are geometry-centric, computationally heavy, and lack semantic grounding or uncertainty modeling, making them less effective for lightweight few-shot adaptation. CLIP for 3D Understanding. Large-scale vision-language models such as CLIP [ 46 ] learn rich multimodal embeddings by aligning image and text representations, inspiring a range of 3D extensions. PointCLIP [ 71 ] , PointCLIP v2 [ 74 ] , DiffCLIP [ 52 ] , and MVFPoint [ 9 ] project 3D point clouds into multi-view depth maps and process them via CLIP‚Äôs image encoder, achieving strong zero-/few-shot classification. CG3D [ 22 ] aligns point cloud-image-text triplets through visual prompt tuning to reduce the RGB-depth gap, while CLIP 2 \text{CLIP}^{2} [ 69 ] leverages proxy alignment from 2D-3D correspondences for transferable 3D features. Few-shot class-incremental frameworks [ 7 , 65 , 64 ] further exploit CLIP‚Äôs semantics to reprogram depth projections for within- and cross-domain generalization. However, these works primarily focus on recognition rather than adaptation. They often freeze CLIP encoders, lacking explicit mechanisms for cross-domain alignment or handling uncertain multi-view projections, limiting robustness under shifts. CLIP for 2D UDA. Recent studies have successfully adapted CLIP for 2D unsupervised domain adaptation. DAPL [ 16 ] introduces pseudo-labeling for target samples, whereas AD-CLIP [ 55 ] aligns source and target domains in the textual prompt space while preserving style semantics. PADCLIP [ 30 ] proposes an adaptive debiasing pseudo-labeling strategy based on forgetting measures. UniMoS [ 34 ] employs modality-ensemble training to balance modality-agnostic and modality-specific features. DAMP [ 10 ] jointly aligns visual and textual embeddings to enhance domain invariance, while PDB [ 21 ] decomposes adaptation into multiple sub-tasks using auxiliary data construction and cascaded semantic filters. Whereas, COSMo [ 41 ] addresses the open-set multi-target DA task, offering a more realistic representation of real-world scenarios while demonstrating CLIP‚Äôs versatility for 2D adaptation and its ability to leverage cross-modal representations to improve generalization to target domains. However, these methods are inherently designed for RGB images and do not account for the unique geometric challenges of 3D data explicitly. 3 Proposed Methodology The UPDA setup involves a labeled source domain and an unlabeled target domain. The source set ùíü S = { ( ùêèùêÇ i S , y i S ) } i = 1 N S \mathcal{D}_{S}=\{(\mathbf{PC}_{i}^{S},y_{i}^{S})\}_{i=1}^{N_{S}} contains N S N_{S} labeled point clouds, while the target set ùíü T = { ùêèùêÇ j T } j = 1 N T \mathcal{D}_{T}=\{\mathbf{PC}_{j}^{T}\}_{j=1}^{N_{T}} includes N T N_{T} unlabeled ones. Each 3D point cloud ùêèùêÇ \mathbf{PC} is projected into M M depth views, yielding ùíü S = { ( x i , m S , y i S ) } \mathcal{D}_{S}=\{(x_{i,m}^{S},y_{i}^{S})\} and ùíü T = { x j , m T } \mathcal{D}_{T}=\{x_{j,m}^{T}\} , where x i , m S , x j , m T x_{i,m}^{S},x_{j,m}^{T} denote the m m -th 2D projections and m ‚àà { 1 , ‚Ä¶ , M } m\!\in\!\{1,\ldots,M\} . Samples follow distinct distributions ùí´ S \mathcal{P}_{S} and ùí´ T \mathcal{P}_{T} ( ùí´ S ‚â† ùí´ T \mathcal{P}_{S}\!\neq\!\mathcal{P}_{T} ) but share a common label space ùí¥ \mathcal{Y} . The goal is to learn a classifier f : ùí≥ S ‚Üí ùí¥ f\!:\!\mathcal{X}_{S}\!\rightarrow\!\mathcal{Y} that generalizes to ùí≥ T \mathcal{X}_{T} by jointly leveraging ùíü S \mathcal{D}_{S} and ùíü T \mathcal{D}_{T} in a transductive manner. UPDA remains challenging due to: (i) large geometric and density variations across sensors and domains; (ii) information loss and redundancy from 3D-to-2D projection, complicating cross-view consistency; (iii) absence of target labels, which blurs the boundary between semantic drift and domain shift; and (iv) the limited transferability of 2D VLMs like CLIP, pretrained on textured RGB data, to sparse, textureless 3D projections. 3.1 Our CLIPoint3D Framework CLIPoint3D builds upon a frozen CLIP backbone composed of a vision encoder ‚Ñ∞ v \mathcal{E}_{v} and a text encoder ‚Ñ∞ t \mathcal{E}_{t} . Following [ 71 ] , we employ online perspective projection [ 17 ] without any post-rendering operations [ 57 ] , directly projecting each 3D point onto a set of predefined image planes to produce scatter-based depth maps. For each projected view x i , m x_{i,m} of a 3D point cloud ùêèùêÇ i \mathbf{PC}_{i} , the vision encoder produces an embedding ùêØ i , m = ‚Ñ∞ v ‚Äã ( x i , m ) ‚àà ‚Ñù 1 √ó d \mathbf{v}_{i,m}=\mathcal{E}_{v}(x_{i,m})