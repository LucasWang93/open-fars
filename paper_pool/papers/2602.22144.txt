Title: NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors

Abstract: Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.

Body: NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors 1 Introduction 2 Related work 2.1 Visual-Language Models 2.2 Hallucination in VLMs 3 Method 3.1 Preliminary experiments 3.2 No-Language-Hallucination Decoding 4 Experiments 4.1 Experimental settings 4.1.1 Datasets evaluation metrics 4.1.2 LVLM baselines 4.2 Decoding baselines 4.3 Experimental results 5 Conclusion and discussion A Appendix A.1 Theoretical proof of NoLan-Plus A.2 Uncertainty analysis and language prior suppression A.3 Correlation study between hallucination and token position A.4 Ablation study A.5 Benchmarking NoLan against the ICD baseline A.6 Supplementary experiments A.7 Contrasting NoLan with attention-based approaches A.8 Qwen-VL series A.9 Consumption of inference A.10 Ethics and reproducibility statements A.11 More case studies NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors Lingfeng Ren 1 Weihao Yu 2 Runpeng Yu 1 Xinchao Wang 1 † † footnotemark: 1 National University of Singapore, Singapore 2 Peking University Shenzhen Graduate School, China {lingfengren, r.yu}@u.nus.edu weihao@pku.edu.cn xinchao@nus.edu.sg https://github.com/lingfengren/NoLan Corresponding author. Abstract Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan , which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code will be made publicly available. 1 Introduction Figure 1 : No-Language-Hallucination Decoding (NoLan). Given an LVLM, an image v v , and a language question x x , NoLan mitigates hallucinations in responses by comparing outputs generated from multimodal and unimodal (text-only) inputs. Step 2 can also be simplified by setting α \alpha to a fixed value of 1 1 . In this example, the hallucinated object “ whale ” is suppressed by reducing the influence of language priors during token generation, while the ground truth object “ bear ” is effectively enhanced. In recent years, Large Language Models (LLMs) ( touvron2023llama , ; chiang2023vicuna , ; chen2023alpagasus , ; zhao2023survey , ; li2023towards , ; wei2023larger , ; Xiao2023LargeLM , ) have revolutionized the field of machine learning with the ability of language understanding and content generation, offering unprecedented capabilities and potentials across a multitude of applications. The integration of LLMs with computer vision systems has given rise to Large Vision-Language Models (LVLMs) ( bubeck2023sparks , ; touvron2023llama , ; zeng2022socratic , ; anas_awadalla_2023_7733589 , ; yang2023mm , ; liu2023visual , ; zhu2023minigpt , ; ye2023mplug , ; liu2020visual , ; Li2023BLIP2BL , ; tran2020transform , ; liang2023modulewise , ) , facilitating various applications through their capacity to produce contextually accurate textual outputs from visual data. These models excel in identifying and converting intricate visual patterns into seamless linguistic expressions ( liu2023visual , ; zhu2023minigpt4 , ; ye2023mplugowl , ; li2023otter , ; dai2023instructblip , ; gong2023multimodalgpt , ; maaz2023videochatgpt , ; zhang2023videollama , ; bai2023qwen , ) . LVLMs with these advanced capabilities have demonstrated their value across multiple domains, such as content generation, image and video annotation, and interactive platforms that require comprehensive visual content interpretation. The development of LVLMs is characterized by continuous enhancements in model structures, training strategies, and data variety, resulting in improved performance and broader application adaptability. Nevertheless, a significant challenge persists: object hallucinations ( li2023evaluating , ; gunjal2023detecting , ; liu2023mitigating , ; lovenia2023negative , ) , where the text generated by LVLMs does not accurately reflect the objects in the provided image. Object hallucinations can lead to misinformation and misinterpretation, posing significant risks for decision-making—particularly in high-stakes areas such as robotics ( mai2023llm , ; liu2023llm , ) , autonomous systems ( chen2023driving , ; wu2023embodied , ) , and healthcare ( wang2023chatcad , ; hu2023advancing , ) . In light of this, various strategies have been investigated to mitigate object hallucinations in LVLMs. Initial efforts focused on small-scale VLMs, employing techniques like fine-grained modality alignment ( biten2022let , ) and data augmentation to reduce statistical biases related to object co-occurrence ( rohrbach2018object , ; kim2023exposing , ) . However, the distinct behaviors of LVLMs render these methods difficult to generalize and scale ( kaplan2020scaling , ; wei2022emergent , ) . Recent research has tackled this challenge by developing hallucination-specific datasets for fine-tuning ( liu2023aligning , ; gunjal2023detecting , ) , training post-hoc revisors to produce outputs with fewer hallucinations ( zhou2023analyzing , ) , and employing factually enhanced Reinforcement Learning from Human Feedback (RLHF) ( sun2023aligning , ) . Despite their effectiveness, these interventions demand significant human effort and computational resources, underscoring the urgent need for a simpler yet efficient solution. LVLMs generally comprise two main components: a vision encoder that perceives visual information and a language decoder that generates text responses. This model composition motivates us to analyze the contributions of the vision and language components within LVLMs to the occurrence of object hallucinations. Through a series of analytical experiments, we find that object hallucinations primarily stem from the language decoder’s priors rather than the vision encoder. Based on this insight, we focus on overcoming language priors and introduce No - Lan guage-Hallucination Decoding ( NoLan ), a simple, effective, and training-free framework designed to mitigate hallucinations in LVLMs. As illustrated in Figure 1 , NoLan works by contrasting the output distributions of multimodal inputs with those of text-only inputs, acting as a corrective mechanism to address the model’s over-reliance on linguistic priors embedded in the LLM. The modulation of the output distribution increases when the similarity between the token distributions of multimodal and text-only inputs is higher, as measured by a Kullback-Leibler divergence-based function. Compared to previous methods ( liu2023aligning , ; gunjal2023detecting , ; zhou2023analyzing , ; sun2023aligning , ) , NoLan eliminates the need for additional training or external tools, such as other pre-trained models. Our experimental results validate the effectiveness of NoLan, demonstrating consistent improvements across various object hallucination benchmarks and LVLM families, including LLaVA-1.5 ( liu2023visual , ; liu2023improved , ) , InstructBLIP ( dai2023instructblip , ) , and Qwen-VL ( bai2023qwen , ) . Specifically, on the POPE benchmark ( li2023evaluating , ) , NoLan achieves significant performance gains, with accuracy improvements of up to 8.38 8.38 and F1 score enhancements of up to 8.78 8.78 , highlighting its robustness and scalability in addressing object hallucinations across diverse LVLM architectures. Overall, our main contributions are as follows: 1. We conduct a series of analytical experiments to investigate the contributions of each component in LVLMs to object hallucinations, finding that hallucinations mainly stem from the language model’s priors rather than the vision model. 2. Building on this insight, we introduce NoLan, a plug-and-play approach designed to mitigate object hallucinations by dynamically suppressing language priors. NoLan achieves this by leveraging the differences in output distributions between multimodal and text-only inputs, ensuring more consistent and contextually accurate content generation. 3. Extensive experiments demonstrate the effectiveness of NoLan in significantly reducing object hallucinations. Notably, our methods do not require additional training or external tools. 2 Related work 2.1 Visual-Language Models The evolution of Vision-Language Models (VLMs) has advanced significantly, shifting from language models that incorporate BERT-like language encoder ( devlin2018bert , ; liu2019roberta , ; koroteev2021bert , ) for the fusion of visual and textual information ( li2019visualbert , ; sun2019videobert , ; wang2022git , ; li2022blip , ) to being driven by the integration of LLMs ( gilardi2023chatgpt , ; touvron2023llama , ; tay2022ul2 , ; raffel2020exploring , ; brown2020language , ; chowdhery2022palm , ; alpaca , ; vicuna2023 , ; bai2023qwenllm , ) . By integrating a general vision encoder with a large language model, LVLMs demonstrate a range of emergent capabilities, enabling them to process and interpret complex visual and textual information more effectively. However, while grafted VLMs inherit strong linguistic capabilities from their base LLM, they also carry over the propensity to generate ungrounded or fabricated information ( huang2021factual , ; bang2023multitask , ) . 2.2 Hallucination in VLMs Hallucination typically refers to instances in which the generated responses include information that is not present in the visual content ( rohrbach2018object , ; biten2022let , ; li2023evaluating , ) . Recent initiatives have aimed to tackle these intricacies, with research focusing on detecting and evaluating object hallucinations in the realm of LVLMs ( wang2023evaluation , ; liu2023aligning , ; li2023evaluating , ; lovenia2023negative , ; yin2024woodpecker , ) , and methods to reduce them ( liu2023aligning , ; yin2024woodpecker , ; Wang2023VIGCVI , ) . For instance, POPE ( li2023evaluating , ) transforms hallucination into a binary classification task to assess the model’s ability to recognize whether a particular object is present in the image. Unlike approaches that simply integrate powerful LLMs with in-context or few-shot learning capabilities ( alayrac2022flamingo , ; li2023blip , ) , efforts to address hallucinations have primarily focused on incorporating external tools for post-processing. For instance, Woodpecker ( yin2024woodpecker , ) utilizes a five-stage process, but many of these stages rely heavily on auxiliary models, such as multiple LLMs and vision foundation models, making the approach resource-intensive. Additionally, adapting factually augmented reinforcement learning from human feedback (RLHF) ( sun2023aligning , ) has emerged as an effective strategy to align model outputs with factual accuracy. However, current strategies ( liu2024visual , ; liu2024improved , ) that involve acquiring additional datasets, performing detailed tuning on initial or new models, or utilizing other pretrained models can be time-intensive, laborious, and computationally demanding. To address these limitations, several training-free methods have been developed. For instance, Visual Contrastive Decoding (VCD) ( leng2024mitigating , ) calibrates visual uncertainty by contrasting output distributions generated from original and distorted visual inputs. Similarly, Multi-Modal Mutual Information Decoding (M3ID) ( favero2024multi , ) and Visual Debias Decoding (VDD) ( zhang2024debiasing , ) enhance the influence of the reference image by comparing probability distributions produced from conditioned and unconditioned inputs. These approaches aim to refine model predictions without requiring additional training. Compared to these methods, our NoLan introduces a fundamentally different, finer-grained assumption. While methods like VCD ( leng2024mitigating , ) and VDD ( zhang2024debiasing , ) simplify the problem by assuming a uniform language prior for all tokens, and M3ID assumes that the prior degree is conditioned only on sequence length ( favero2024multi , ) , our approach makes a more nuanced and realistic assumption. Specifically, our NoLan posits that each token possesses a distinct language prior. We further propose a simple yet effective KL-based method to measure the prior degree of each token. This token-specific and dynamic prior modeling allows our method to more accurately suppress each token’s language prior, leading to performance improvements. Thus, our work’s novelty lies in this novel assumption and the development of an effective mechanism to model it, which fundamentally distinguishes it from prior work. 3 Method 3.1 Preliminary experiments Figure 2 : Experimental pipeline to test whether LLaVA’s vision encoder can detect the presence of an object in an image. Table 1 : The Vision encoder can robustly detect object presence in samples. On the MSCOCO dataset of POPE- random ( li2023evaluating , ) , for samples where LLaVA-1.5 experiences hallucinations, its vision encoder can indeed predict object presence with high accuracy. Samples on COCO of POPE- Random where LLaVA experiences hallucinations Metric Accuracy Precision Recall F1 Score Score 83.01 83.71 98.33 90.43 LVLMs generally comprise two core components: a vision encoder to gain visual information and a language decoder to generate textual responses. This design raises an important question: are these two components responsible for object hallucinations? In this section, we present a comprehensive analysis to investigate the contributions of both the vision encoder and the language decoder to these hallucinations. Vision Encoder. We aim to investigate whether the vision encoder accurately detects object presence in the failing cases of object hallucinations. To this end, we design a pipeline as shown in Figure 2 . Specifically, LLaVA comprises a CLIP vision encoder and a LLaMA (Vicuna) language model, but in this experiment, we use only the CLIP vision encoder. We extract image representation using the CLIP encoder and evaluate whether the representation includes information about a specific object. 