Title: Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages

Abstract: Extracting clinical information from medical transcripts in low-resource languages remains a significant challenge in healthcare natural language processing (NLP). This study evaluates a two-step pipeline combining Aya-expanse-8B as a Persian-to-English translation model with five open-source small language models (SLMs) -- Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, and Gemma-3-1B-it -- for binary extraction of 13 clinical features from 1,221 anonymized Persian transcripts collected at a cancer palliative care call center. Using a few-shot prompting strategy without fine-tuning, models were assessed on macro-averaged F1-score, Matthews Correlation Coefficient (MCC), sensitivity, and specificity to account for class imbalance. Qwen2.5-7B-Instruct achieved the highest overall performance (median macro-F1: 0.899; MCC: 0.797), while Gemma-3-1B-it showed the weakest results. Larger models (7B--8B parameters) consistently outperformed smaller counterparts in sensitivity and MCC. A bilingual analysis of Aya-expanse-8B revealed that translating Persian transcripts to English improved sensitivity, reduced missing outputs, and boosted metrics robust to class imbalance, though at the cost of slightly lower specificity and precision. Feature-level results showed reliable extraction of physiological symptoms across most models, whereas psychological complaints, administrative requests, and complex somatic features remained challenging. These findings establish a practical, privacy-preserving blueprint for deploying open-source SLMs in multilingual clinical NLP settings with limited infrastructure and annotation resources, and highlight the importance of jointly optimizing model scale and input language strategy for sensitive healthcare applications.

Body: Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages 1 Introduction 2 Results 2.1 Qwen2.5-7B-Instruct demonstrated the highest overall performance among the evaluated models 2.2 Extraction performance was highly dependent on the type of clinical feature 2.2.1 Physiological symptoms were reliably detected across most models 2.2.2 Complex somatic and psychological features showed inconsistent extraction 2.2.3 Administrative requests and multifaceted complaints remained difficult for all models 2.3 Translating Persian transcripts to English enhanced sensitivity and reduced missing outputs 2.4 Larger models demonstrated superior performance under class imbalance 2.5 Larger models favored sensitivity while smaller models achieved comparable or superior specificity 3 Discussion 4 Methods 4.1 Ethical consideration 4.2 Data collection and dataset 4.3 Data preprocessing 4.4 Generation and post-processing 4.5 Assessment 4.6 Computational environment Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages Mohammadreza Ghaffarzadeh-Esfahani 1,† Nahid Yousefian 1 Ebrahim Heidari-Farsani 1 Ali Akbar Omidvarian 1 Sepehr Ghahraei 1 Atena Farangi 1 AmirBahador Boroumand 2,† 1 Student Research Committee Isfahan University of Medical Sciences Isfahan Iran 2 Department of Emergency Medicine Isfahan University of Medical Sciences Isfahan Iran † Correspondence: Mohammadreza Ghaffarzadeh-Esfahani: mreghafarzadeh@gmail.com Tel/Fax: +98-3136700479 ORCID: 0009-0009-9322-5471 AmirBahador Boroumand: ab.boroumand@med.mui.ac.ir Tel/Fax: +98-36688597 ORCID: 0000-0001-5055-6881 Abstract Extracting clinical information from medical transcripts in low-resource languages remains a significant challenge in healthcare natural language processing (NLP). This study evaluates a two-step pipeline combining Aya-expanse-8B as a Persian-to-English translation model with five open-source small language models (SLMs) — Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, and Gemma-3-1B-it — for binary extraction of 13 clinical features from 1,221 anonymized Persian transcripts collected at a cancer palliative care call center. Using a few-shot prompting strategy without fine-tuning, models were assessed on macro-averaged F1-score, Matthews Correlation Coefficient (MCC), sensitivity, and specificity to account for class imbalance. Qwen2.5-7B-Instruct achieved the highest overall performance (median macro-F1: 0.899; MCC: 0.797), while Gemma-3-1B-it showed the weakest results. Larger models (7B–8B parameters) consistently outperformed smaller counterparts in sensitivity and MCC. A bilingual analysis of Aya-expanse-8B revealed that translating Persian transcripts to English improved sensitivity, reduced missing outputs, and boosted metrics robust to class imbalance, though at the cost of slightly lower specificity and precision. Feature-level results showed reliable extraction of physiological symptoms across most models, whereas psychological complaints, administrative requests, and complex somatic features remained challenging. These findings establish a practical, privacy-preserving blueprint for deploying open-source SLMs in multilingual clinical NLP settings with limited infrastructure and annotation resources, and highlight the importance of jointly optimizing model scale and input language strategy for sensitive healthcare applications. Keywords: Natural language processing (NLP); Small language model (SLM); Few-shot prompting; Clinical information extraction; Low-resource languages; Machine translation 1. Introduction In the era of digital health, natural language processing (NLP) has emerged as a transformative tool for data extraction from unstructured clinical text [ 1 ] . This extraction is crucial in medicine, as it converts raw descriptions into structured, quantifiable information, facilitating advanced analytics like predictive modeling for disease progression [ 2 ] and evidence-based decision support systems that enhance patient outcomes and operational efficiency [ 3 ] . Particularly in palliative oncology, where patients often report multifaceted symptoms, automated information extraction from patient-provider interactions can alleviate clinician workload, reduce diagnostic delays, and optimize support, especially in resource-constrained settings [ 4 ] . However, the practical implementation of NLP solutions, especially across diverse languages and settings, faces technical and methodological hurdles [ 5 ] . Traditional NLP pipelines for medical entity recognition and data extraction have relied on rule-based systems [ 6 ] or supervised machine learning models trained on annotated corpora [ 7 ] , but these approaches face challenges like training data scarcity and struggle to understand linguistic nuances [ 8 ] . Recently, the advent of large language models (LLMs) like GPT-5 [ 9 ] has promised capabilities for cross-lingual tasks [ 10 ] , yet their proprietary nature, reliant on API access, raises critical data safety and security concerns, risking patient privacy [ 11 ] . Meanwhile, their large open-source counterparts demand substantial computational resources, posing barriers to deployment in low-infrastructure environments [ 12 ] . This led to a growing interest in small language models (SLMs) with compact architectures (1–10 billion parameters), fine-tuned for instruction-following, which provide a compelling balance of efficiency, accessibility, and performance [ 13 , 14 ] . Recent benchmarks, such as those on the MedNLI and MIMIC-III datasets, demonstrate SLMs’ viability for English-centric clinical tasks, achieving high F1-scores for symptom detection [ 15 ] . Yet, empirical evidence on their efficacy for medical extraction in low-resource languages, especially Persian, remains understudied. Significant gaps persist in understanding how translation artifacts, class imbalance, and language-specific prompting affect performance in real-world clinical settings. In this study, we address these gaps by designing a two-step pipeline containing a translator model, Aya-expanse-8B [ 16 ] , plus five open-source SLMs, Qwen2.5-7B-Instruct [ 17 ] , Llama-3.1-8B-Instruct [ 18 ] , Llama-3.2-3B-Instruct [ 19 ] , Qwen2.5-1.5B-Instruct [ 20 ] , and Gemma-3-1B-it [ 21 ] , on a novel dataset of 1,221 anonymized Persian transcripts from a cancer palliative care call center. Leveraging a few-shot prompting strategy [ 22 ] for binary extraction of 13 clinical features, we assess model performance across metrics robust to imbalance, including macro-averaged F1-score, Matthews Correlation Coefficient (MCC), sensitivity, and specificity. We additionally conducted a bilingual assessment of Aya-expanse-8B, contrasting direct Persian processing against English-translated inputs to quantify translation’s trade-offs. Our findings reveal that while larger SLMs, such as Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct, delivered superior performance for symptom detection, the use of original Persian inputs in the Aya-expanse-8B model notably enhanced the recognition of more subjective conditions, such as psychological complaints. These insights not only benchmark SLMs’ potential for equitable, low-resource healthcare NLP but also inform hybrid strategies that combine native-language inference with translation safeguards to enhance robustness in sensitive domains. 2. Results Our evaluation of the proposed two-step pipeline, which combines the Aya-expanse-8B translator model with five open-source SLMs, demonstrated variability in the extraction of structured clinical information from Persian palliative oncology call transcripts. Benchmarking against a manually annotated dataset of 1,221 calls revealed that model capability was strongly influenced by parameter count and input language, with no single model dominating across all 13 clinical features. To provide a comprehensive overview of the models’ capabilities, the results are presented across four key dimensions: the overall comparative performance of models, their feature-specific analysis, a focused analysis on the impact of language translation for the multilingual model, and an evaluation of model robustness through imbalance-aware performance metrics and error analysis. This multifaceted presentation reveals critical insights into models’ behavior, the influence of parameter scale and language, and the practical implications of deploying such systems in sensitive domains such as palliative care. A visual summary illustrating the key aspects of the study is presented in Figure 1 . Figure 1 : Schematic overview of the study. The upper panel shows the dataset preprocessing, inference generation, and postprocessing, starting from 1,221 Persian palliative care phone-call transcripts, followed by translation into English, prompt construction with input–output examples, and inference using multiple small language models (SLMs). The models’ structured outputs are then post-processed to extract tabular data. The lower panel illustrates the multi-facet analysis framework, comparing manual extraction of 13 reference features with model-derived features through performance metrics (accuracy, sensitivity, specificity, precision, F1-score), assessment of translation effects, crobustness analysis (Matthews correlation coefficient (MCC), missing values), and sensitivity–specificity trade-offs. 2.1. Qwen2.5-7B-Instruct demonstrated the highest overall performance among the evaluated models Evaluation of the five SLMs demonstrated heterogeneous performance in extracting binary clinical features from the translated English transcripts. Overall, Qwen2.5-7B-Instruct exhibited the strongest balanced performance, achieving the highest median specificity (0.987 [0.975, 0.992]), macro-averaged F1-score (0.899 [0.832, 0.908]), precision (0.814 [0.759, 0.878]), and accuracy (0.96 [0.947, 0.984]) across all features. This model demonstrated robustness in handling class imbalance (Figure 2 A and Supplementary File 1). Figure 2 : Comparative performances of different models on validation metrics. (A) The median value for 5 metrics of accuracy, sensitivity, specificity, macro-averaged F1 score, and precision among 13 extracted features compared to the manually-extracted ground truth. (B) Matthews Correlation Coefficient (MCC) values for each evaluated model across the 13 extracted clinical features, comparing model-generated outputs with the manually extracted ground-truth annotations. (C) Total number of missing counts for each model among different extracted features. In contrast, Gemma-3-1B-it showed the lowest median sensitivity (0.613 [0.294, 0.735]) and macro-averaged F1-score (0.74 [0.676, 0.79]), reflecting challenges in detecting positive instances. Additionally, the multilingual Aya-expanse-8B model performed comparably when using different prompt variants (macro-averaged F1-score: English 0.855 [0.804, 0.87] and Persian 0.842 [0.753, 0.868]). Finally, larger models like Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct generally outperformed their smaller counterparts (e.g., Llama-3.2-3B-Instruct and Qwen2.5-1.5B-Instruct) in sensitivity and overall accuracy, with median accuracies exceeding 0.90 in most cases. 2.2. Extraction performance was highly dependent on the type of clinical feature To gain deeper insight into model performance, we conducted a feature-wise analysis comparing model predictions with manually extracted data. The results revealed substantial variability in how each model identified specific clinical features from patients’ phone calls. The following section presents a comparison of macro-averaged F1 scores for each feature across all evaluated models (Table 1 ). Other evaluation metrics, including accuracy, precision, sensitivity, and specificity, are provided in Supplementary File 1. Table 1 : Macro-averaged F1 scores for the binary extraction of 13 clinical features from Persian palliative care transcripts across the evaluated small language models. Values are reported for Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, Gemma-3-1B-it, and Aya-expanse-8B (English-translated and direct Persian variants). Feature Llama-3.1 Qwen2.5 Llama-3.2 Qwen2.5 Gemma-3 Aya-expanse-8B -8B-Inst. -7B-Inst. -3B-Inst. -1.5B-Inst. -1B-it English Persian Doctor’s visit request 0.832 0.678 0.723 0.733 0.633 0.493 0.411 Psychological complaints 0.825 0.832 0.818 0.800 0.782 0.835 0.859 Sleep disorders 0.804 0.838 0.766 0.788 0.740 0.739 0.753 Loss of appetite 0.905 0.915 0.895 0.874 0.732 0.865 0.874 Seizures 0.869 0.908 0.880 0.819 0.790 0.869 0.798 Weakness and fatigue 0.892 0.907 0.856 0.921 0.865 0.753 0.742 Decreased level of consciousness 0.792 0.909 0.769 0.802 0.748 0.843 0.804 Fever 0.907 0.924 0.907 0.921 0.867 0.891 0.918 Respiratory complaints 0.918 0.891 0.908 0.883 0.655 0.908 0.918 Insurance/treatment cost issues 0.816 0.899 0.866 0.791 0.689 0.870 0.674 Urinary tract issues 0.870 0.795 0.866 0.766 0.598 0.888 0.842 Pain 0.930 0.903 0.893 0.873 0.828 0.855 0.868 Gastrointestinal issues 0.874 0.818 0.848 0.828 0.676 0.804 0.849 Median [IQR 1 , IQR 3 ] 0.870 [0.825, 0.905] 0.899 [0.832, 0.908] 0.866 [0.818, 0.893] 0.819 [0.791, 0.874] 0.74 [0.676, 0.79] 0.855 [0.804, 0.87] 0.842 [0.753, 0.868] 2.2.1. Physiological symptoms were reliably detected across most models Several features were consistently identified with a high macro-averaged F1 score ( 0.85) by most models. Pain was the top-performing feature overall, with Llama-3.1-8B-Instruct achieving the highest score (0.93). Fever and respiratory complaints also showed robust performance, with multiple models (Qwen2.5-7B-Instruct, Qwen2.5-1.5B-Instruct, Aya-expanse-8B (Persian) for Fever; Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Aya-expanse-8B (Persian) for respiratory complaints) scoring above 0.91. Similarly, loss of appetite and seizures were well-detected, particularly by the Qwen2.5-7B-Instruct model (0.915 and 0.908, respectively). 2.2.2. Complex somatic and psychological features showed inconsistent extraction Performance was more varied for features like weakness and fatigue, decreased level of consciousness, and psychological complaints. While some models excelled (for example, Qwen2.5-7B-Instruct on decreased level of consciousness (0.909) and Aya-expanse-8B (Persian) on psychological complaints (0.859)), others, particularly smaller models like Gemma-3-1B-it, struggled more on these conceptually related symptoms. 2.2.3. Administrative requests and multifaceted complaints remained difficult for all models Certain features proved challenging across the board. The doctor’s visit request was notably difficult, with the highest score (0.832 from Llama-3.1-8B-Instruct) and the lowest score (0.410 from Aya-expanse-8B (Persian)). Sleep disorders and insurance/treatment cost issues also showed lo