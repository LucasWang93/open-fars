Title: FlashOptim: Optimizers for Memory Efficient Training

Abstract: Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.   We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.   Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.

Body: FlashOptim: Optimizers for Memory Efficient Training Report GitHub Issue × Title: Content selection saved. Describe the issue below: Description: Submit without GitHub Submit in GitHub Back to arXiv Why HTML? Report Issue Back to Abstract Download PDF Abstract 1 Introduction 2 Related Work 3 Method 3.1 Weight Splitting 3.2 Companded Optimizer State Quantization 3.3 Optimizer Update 3.4 Implementation 4 Experiments 4.1 Experimental Setup 4.2 Convergence and Accuracy 4.3 Memory and Speed 4.4 Weight Error Correction 4.5 Optimizer State Quantization 5 Limitations 6 Conclusion References A Method Details B Experimental Details B.1 Image Classification Memory and Speed Profiling. B.2 LLM Pretraining Memory and Speed Profiling. B.3 In-Context Learning Benchmarks B.4 LLM Finetuning Training and Evaluation. License: CC BY-SA 4.0 arXiv:2602.23349v1[cs.LG] 26 Feb 2026 FlashOptim: Optimizers for Memory Efficient Training Jose Javier Gonzalez Ortiz Databricks AI Research j.gonzalez@databricks.com Equal contribution Abhay Gupta Databricks AI Research abhay.gupta@databricks.com Chris Renard Databricks AI Research chris@standardkernel.co Now at Standard Kernel Co. Davis Blalock 1 1 footnotemark: 1 Databricks AI Research daviswblalock@gmail.com Now at Google DeepMind Abstract Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory. We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half. Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning. 1 Introduction Recent advances in deep learning have been driven largely by scaling: larger models trained on more data consistently yield better results across language ( Kaplan et al. , 2020 ; Hoffmann et al. , 2022 ; Chowdhery et al. , 2023 ) and vision ( Rosenfeld et al. , 2019 ; Tan and Le , 2019 ; Dehghani et al. , 2023 ) domains. Training large models can require a great deal of accelerator memory, with each training iteration requiring memory to store parameters, activations, gradients, and optimizer state. How much memory do these tensors require? Table 1 shows a typical breakdown. Excluding activations, which scale with batch size rather than parameter count, training with Adam uses about 16 bytes per parameter. Training a 7-billion-parameter LLM therefore requires at least 112GB of accelerator memory, plus more memory for activations. Table 1 : Memory per parameter (bytes) for model training . FlashOptim reduces Adam from 16 to 7 bytes and SGD from 12 to 6 bytes. ( ⋆ ) (\star) With gradient release, we further reduce total memory requirements by 2 bytes. Tensor SGD FlashSGD Adam FlashAdam Master Weights 4 2 (0 ⋆ ) 4 2 (0 ⋆ ) Weight Correction 1 (0 ⋆ ) 1 (0 ⋆ ) Gradients 4 2 (0 ⋆ ) 4 2 (0 ⋆ ) Momentum 4 1 (0 ⋆ ) 4 1 (0 ⋆ ) Variance 4 1 (0 ⋆ ) Total 12 6 (4 ⋆ ) 16 7 (5 ⋆ ) Several approaches mitigate this memory consumption. Distributed training with tensor sharding ( Rajbhandari et al. , 2020 ) divides the memory load across multiple accelerators. While this is standard practice in well-resourced organizations, it requires access to multiple accelerators that many practitioners lack. Another alternative is to perform CPU offloading ( Ren et al. , 2021 ) , which moves some tensors to host memory at the cost of added overhead and complexity. Third, parameter-efficient methods ( Li and Liang , 2021 ; Hu et al. , 2022 ) reduce trainable parameters by freezing most weights and training either a small subset of the original weights or a small set of new auxiliary weights, but fundamentally alter the training dynamics ( Biderman et al. , 2024 ) . In this work, we describe FlashOptim, a set of techniques to reduce parameter-associated memory in common deep learning optimizers. Figure 1 shows an example: with FlashOptim, finetuning Llama-3.1-8B drops from 175 GiB to 113 GiB peak memory. Crucially, these memory savings are effectively free—FlashOptim runs just as fast as standard optimizers and causes no measurable loss of model quality across a suite of established training tasks (§ 4 ). This allows our optimizer implementations to serve as drop-in replacements for their unoptimized counterparts. FlashOptim incorporates existing enhancements, such as gradient release ( Zhang et al. , 2023 ; Warner , 2024 ) , while also introducing improved float splitting ( Zamirai et al. , 2020 ; Warner , 2024 ) and simplified 8-bit optimizer state quantization ( Dettmers et al. , 2022 ; Peng et al. , 2023 ; Xi et al. , 2025 ; Fishman et al. , 2025 ) . Furthermore, FlashOptim composes cleanly with existing memory-reduction techniques, such as sharding tensors across accelerators, offloading to CPU, or freezing parameters. Figure 1 : Memory breakdown for finetuning Llama-3.1-8B. FlashOptim reduces peak memory from 175 to 113 GiB by compressing parameters and optimizer states. We make the following contributions: • Improved float splitting : Instead of materializing both a 32-bit master weight and a 16-bit downcast weight for forward and backward, one can split each master weight into a low-precision weight and a correction term stored in the optimizer ( Zamirai et al. , 2020 ; Warner , 2024 ) . We improve on existing float splitting techniques by (a) enabling either 8- or 16-bit error correction and (b) achieving much lower reconstruction error for a given number of correction bits. This allows us to use 24-bit master weights with no loss of model quality. • Companded optimizer state quantization : Several works have shown that one can compress optimizer states to 8 bits per element given sufficient software complexity. We demonstrate that one can do this much more simply, with nothing more than a one-line preprocessing function before standard group-wise linear quantization. Our ablations across different tensor types suggest that designing custom companding functions is a fruitful direction for future research. • Fused optimized kernels : We implement FlashOptim as optimizer step kernels that fuse all compression and quantization operations, reducing memory while preserving throughput during training. Our implementation is publicly available at https://github.com/databricks/flashoptim . 2 Related Work Low-Precision Training. Mixed-precision training ( Micikevicius et al. , 2018 ) executes forward and backward passes in FP16 to reduce memory and compute, while retaining FP32 precision for optimizer states and master weights to preserve numerical stability. Kalamkar et al. ( 2019 ) showed that BFloat16 ( Google , 2019 ) works equally well, and Zamirai et al. ( 2020 ) explored pure BF16 master weights with stochastic rounding and Kahan summation. Recent work has pushed further with FP8 training ( Wang et al. , 2018 ; Mellempudi et al. , 2019 ; Micikevicius et al. , 2022 ; Fishman et al. , 2025 ; Narayan et al. , 2025 ) , though these approaches primarily target compute formats and retain higher-precision storage for master weights. FlashOptim extends this line of work with an improved float splitting mechanism that reduces storage to 3 bytes per parameter, down from 4-byte FP32, while maintaining FP32-equivalent training semantics. Optimizer State Compression. Dettmers et al. ( 2022 ) applied 8-bit block-wise dynamic quantization to Adam’s momentum and variance, reducing optimizer state from 8 to 2 bytes per parameter. Follow-up work explored FP8 representations ( Peng et al. , 2023 ; Xi et al. , 2025 ; Fishman et al. , 2025 ) , and Li et al. ( 2023 ) compressed both moments to 4-bit using row and column-wise quantization. MicroAdam ( Modoranu et al. , 2024 ) instead compresses gradients before updating optimizer states. Rather than design elaborate quantization methods or number formats, we show that one can obtain 8-bit optimizer states with no quality loss using simple, one-line preprocessing functions. Beyond optimizer states, we address additional sources of per-parameter memory, eliminating entire bytes from other tensors. Gradient Memory and Communication. LOMO ( Lv et al. , 2024b ) , AdaLOMO ( Lv et al. , 2024a ) , and Adam Accumulation ( Zhang et al. , 2023 ) fuse parameter updates into the backward pass to release gradient memory eagerly. However, this conflicts with gradient accumulation, which requires the full accumulated gradient before updating. In distributed settings, gradient communication can also become a bottleneck. One can reduce this bottleneck by, e.g., compressing gradients to 1-bit with error feedback ( Tang et al. , 2021 ) , or using low-rank approximations ( Vogels et al. , 2019 ) . FlashOptim supports gradient release when compatible and could be used alongside communication compression techniques. Memory-Efficient Optimization. Alternative optimizer designs reduce memory by restructuring update rules and stored buffers. Adafactor ( Shazeer and Stern , 2018 ) achieves sublinear memory by factorizing the second moment into row and column statistics; SM3 ( Anil et al. , 2019 ) stores structured maxima; NovoGrad ( Ginsburg et al. , 2019 ) replaces per-parameter variance with layer-wise normalization. Adam-mini ( Zhang et al. , 2025 ) shares variance terms across parameter blocks, while Adapprox ( Zhao et al. , 2024b ) uses a low-rank approximation. Other approaches eliminate the second moment entirely: Lion ( Chen et al. , 2023 ) uses sign-based momentum, and Muon ( Jordan et al. , 2024 ; Liu et al. , 2025 ) applies orthogonalized updates. Pethick et al. ( 2025 ) extend Muon to unify gradient accumulation with momentum, removing dedicated optimizer memory altogether. Low-rank decompositions approximate full tensors while requiring less memory. For fine-tuning, LoRA ( Hu et al. , 2022 ) and QLoRA ( Dettmers et al. , 2023 ) freeze base weights and train only low-rank adapters. For pretraining, GaLore ( Zhao et al. , 2024a ) projects gradients to a low-rank subspace, and APOLLO ( Zhu et al. , 2025 ) approximates adaptive scaling with random projections. Unlike these approaches that modify the optimizer’s update rule, FlashOptim preserves standard optimizer semantics and can be combined with these techniques. System-Level Memory Optimizations. System-level approaches reduce accelerator memory without changing optimization semantics. Activation checkpointing ( Chen et al. , 2016 ; Korthikanti et al. , 2023 ) trades compute for memory by recomputing activations during the backward pass. ZeRO ( Rajbhandari et al. , 2020 ) partitions optimizer states, gradients, and parameters across data-parallel ranks, while offloading ( Rajbhandari et al. , 2021 ; Ren et al. , 2021 ) moves state to CPU or NVMe memory. FlashOptim is orthogonal to these approaches: it reduces the per-rank footprint and can be used with ZeRO, FSDP ( Zhao et al. , 2023 ) , and activation checkpointing. 3 Method This section describes the two key techniques behind FlashOptim: weight splitting (§ 3.1 ) and companded optimizer state quantization (§ 3.2 ). We then describe how to integrate these ideas into common optimizer updates while minimizing associated overhead (§ 3.3 ). 3.1 Weight Splitting Mixed-precision training uses 16-bit weights for forward and backward passes, but accumulating gradient updates requires higher precision to avoid stagnation ( Micikevicius et al. , 2018 ) . Thus, the standard practice is to maintain FP32 precision master weights during training. However, this introduces waste: the downcast weights take space, but store no information beyond what’s saved in the master weights. To eliminate this redundancy, weight splitting ( Zamirai et al. , 2020 ; Warner , 2024 ) instead stores the downcast weights and narrow error-correction values. By combining a 16-bit weight θ ′ \theta^{\prime} with a 16-bit error correction value ρ \rho , one has enough information to reconstruct a 32-bit master weight θ \theta with no redundancy. The core questions in a weight splitting scheme are 1) how to set ( θ ′ , ρ ) (\theta^{\prime},\rho) given θ \theta and 2) how to estimate θ \theta given ( θ ′ , ρ ) (\theta^{\prime},\rho) . One obvious approach is to use the high 16 bits of θ \theta as θ ′ \theta^{\prime} and the low 16 bits as ρ \rho . This admits exact reconstruction of θ \theta for the special case of BF16 θ ′ \theta^{\prime} and FP32 θ \theta , since these formats happen to share the same exponent sizes and offsets. However, this approach doesn’t generalize to other pairs of number formats. It also rounds towards zero instead of towards the nearest low-precision value. A more general alternative, used in previous work ( Zamirai et al. , 2020 ; Warner , 2024 ) , is to set ρ = θ − θ ′ \rho=\theta-\theta^{\prime} , represented as a BF16 value. The problem with this is that the difference of two floating-point numbers requires as many bits to store exactly as the wider of the two floats. 1 1 1 Consider, e.g., storing the minimum float32 subnormal, which will be rounded to zero by any narrower datatype. This means that a BF16 ρ \rho incurs approximation error. E.g., if the rounding error were 1e-5, BF16 could only represent 1.0014e-5. In general, while BF16’s wide exponent lets it represent nearly the full range of FP32, its 7 mantissa bits only guarantee a relative error bound of 1/256. Our observation is that all exponent bits in this scheme are wasted . The exponent of e ≜ θ − θ ′ e\triangleq\theta-\theta^{\prime} can always be inferred from θ ′ \theta^{\prime} : under round-to-nearest, θ \theta must lie within [ θ ′ − u / 2 , θ ′ + u / 2 ] [\theta^{\prime}-u/2,\theta^{\prime}+u/2] , where u = ULP ​ ( θ ′ ) u=\text{ULP}(\theta^{\prime}) is the unit in the last place ( Goldberg , 1991 ) . If θ \theta were outside this interval, it would have rounded to a different value. It therefore suffices to encode where e e falls within this tiny interval rather than across the full FP32 range. To exploit this observation, we rescale e e such that [ − u / 2 , u / 2 ] [-u/2,u/2] maps to [ − N , N ] [-N,N] ; N ≜ 2 b − 1 N\triangleq 2^{b}-1 and then quantize this rescaled e e to the nearest