Title: When Will Gradient Regularization Be Harmful?

Abstract: Gradient regularization (GR), which aims to penalize the gradient norm atop the loss function, has shown promising results in training modern over-parameterized deep neural networks. However, can we trust this powerful technique? This paper reveals that GR can cause performance degeneration in adaptive optimization scenarios, particularly with learning rate warmup. Our empirical and theoretical analyses suggest this is due to GR inducing instability and divergence in gradient statistics of adaptive optimizers at the initial training stage. Inspired by the warmup heuristic, we propose three GR warmup strategies, each relaxing the regularization effect to a certain extent during the warmup course to ensure the accurate and stable accumulation of gradients. With experiments on Vision Transformer family, we confirm the three GR warmup strategies can effectively circumvent these issues, thereby largely improving the model performance. Meanwhile, we note that scalable models tend to rely more on the GR warmup, where the performance can be improved by up to 3\% on Cifar10 compared to baseline GR. Code is available at \href{https://github.com/zhaoyang-0204/gnp}{https://github.com/zhaoyang-0204/gnp}.

Body: When Will Gradient Regularization Be Harmful? 1 Introduction 2 Background 2.1 Gradient Regularization: an Overview 2.2 Related Works 3 The Incompatibility Between Gradient Regularization and Adaptive Optimization 3.1 Practical Training with Gradient Regularization in Adaptive Optimization Vanilla Adaptive Optimization Training Implementation 3.2 Performance Degradation When Using GR and Adaptive Optimization Simultaneously LR warmup can improve training performance. Gradient regularization can have side-effect with LR warmup techniques. 4 Understanding the Incompatibility Between GR and Adaptive Optimization Compatibility issues arise with an initial decline in gradient norm. GR can lead to large adaptive leraning rate variance in theory 5 GR Warmup Strategies 5.1 r ğ‘Ÿ r italic_r -warmup Gradient Regularization 5.2 Î» ğœ† \lambda italic_Î» -warmup Gradient Regularization 5.3 Zero-warmup Gradient Regularization 6 Conclusion A Proof of Theorems B Implementation Details C Additional Results When Will Gradient Regularization Be Harmful? Yang Zhao Hao Zhang Xiuyuan Hu Abstract Gradient regularization (GR), which aims to penalize the gradient norm atop the loss function, has shown promising results in training modern over-parameterized deep neural networks. However, can we trust this powerful technique? This paper reveals that GR can cause performance degeneration in adaptive optimization scenarios, particularly with learning rate warmup. Our empirical and theoretical analyses suggest this is due to GR inducing instability and divergence in gradient statistics of adaptive optimizers at the initial training stage. Inspired by the warmup heuristic, we propose three GR warmup strategies, each relaxing the regularization effect to a certain extent during the warmup course to ensure the accurate and stable accumulation of gradients. With experiments on Vision Transformer family, we confirm the three GR warmup strategies can effectively circumvent these issues, thereby largely improving the model performance. Meanwhile, we note that scalable models tend to rely more on the GR warmup, where the performance can be improved by up to 3% on Cifar10 compared to baseline GR. Code is available at https://github.com/zhaoyang-0204/gnp . Machine Learning, ICML 1 Introduction Advancements in computational hardware have catalyzed the design of modern deep neural networks such as the Transformers (Dosovitskiy et al., 2021 ; Vaswani et al., 2017b ; Brown et al., 2020 ; Liu et al., 2021 ; Touvron et al., 2021 ) , characterized by an extraordinarily vast number of parameters, far exceeding their predecessors. In this context, regularization techniques emerge as a more pivotal role to resist overfitting in training these over-parameterized networks (Srivastava et al., 2014 ; Ioffe Szegedy, 2015 ; Ba et al., 2016 ; Foret et al., 2021 ) . Figure 1: Comparison of test error rates (lower values are preferable) of the ViT-B model on the Cifar10 dataset under Base training, GR, and our three proposed GR warmup strategies. All the training instances have also applied the LR warmup. Notably, the performance with LR warmup and normal GR (red line) can be worse compared to training with only LR warmup (blue line). Recent studies highlight the gradient regularization (GR) as an effective regularization strategy (Barrett Dherin, 2020 ; Smith et al., 2021 ; Zhao et al., 2022b ; Karakida et al., 2023 ; Reizinger HuszÃ¡r, 2023 ) . By imposing an additional penalty concerning gradient norm atop the loss function, this technique deliberately biases the optimization process towards the attainment of flat minima, fostering better generalization. Meanwhile, it has been revealed that a strong association exists between GR and Sharpness-Aware Minimization (SAM) family (Foret et al., 2021 ) , which posits SAM as a special parameter configuration in the first-order solution of GR (Zhao et al., 2022b ; Karakida et al., 2023 ) . However, despite its practical utility, the scope and limitations of GR are yet to be fully understood, particularly in terms of establishing when it can be beneficial or safe to apply this technique. We find that GR can lead to serious performance degeneration (see Figure 1 ) in the specific scenarios of adaptive optimization such as Adam (Kingma Ba, 2015 ) and RMSProp (Hinton et al., 2012 ) . With both our empirical observations and theoretical analysis, we find that the biased estimation introduced in GR can induce the instability and divergence in gradient statistics of adaptive optimizers at the initial stage of training, especially with a learning rate warmup technique which originally aims to benefit gradient statistics (Vaswani et al., 2017a ; Popel Bojar, 2018 ; Liu et al., 2020 ) . Notably, this issue tends to become more severe as the complexity of the model increases. To mitigate this issue, we draw inspirations from the idea of warmup techniques, and propose three GR warmup strategies: Î» ğœ† \lambda italic_Î» -warmup, r ğ‘Ÿ r italic_r -warmup and zero-warmup GR. Each of the three strategies can relax the GR effect during warmup course in certain ways to ensure the accuracy of gradient statistics. Then, training reverts to normal GR for the rest of training, allowing the optimization to fully enjoy the performance gain derived from the GR. Finally, we empirically confirm that all the three GR warmup strategies can not only successfully avoid this issue but further enhance the performance for almost all training cases. Of these strategies, the zero-warmup GR can give the best improvements, significant outperforming the baseline. 2 Background 2.1 Gradient Regularization: an Overview Gradient regularization typically aims to impose an additional gradient norm penalty on top of the loss function (Barrett Dherin, 2020 ; Smith et al., 2021 ; Zhao et al., 2022b ; Karakida et al., 2023 ) , L ( g â¢ r ) â¢ ( ğœ½ ) = L â¢ ( ğœ½ ) + Î» â¢ â€– âˆ‡ ğœ½ L â¢ ( ğœ½ ) â€– 2 superscript ğ¿ ğ‘” ğ‘Ÿ ğœ½ ğ¿ ğœ½ ğœ† subscript norm subscript âˆ‡ ğœ½ ğ¿ ğœ½ 2 L^{(gr)}({\bm{\theta}})=L({\bm{\theta}})+\lambda||\nabla_{{\bm{\theta}}}L({\bm% {\theta}})||_{2} italic_L start_POSTSUPERSCRIPT ( italic_g italic_r ) end_POSTSUPERSCRIPT ( bold_italic_Î¸ ) = italic_L ( bold_italic_Î¸ ) + italic_Î» | | âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT italic_L ( bold_italic_Î¸ ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT (1) where ğœ½ ğœ½ {\bm{\theta}} bold_italic_Î¸ is the model parameter and Î» ğœ† \lambda italic_Î» denotes the regularization degree, effectively controlling the extent to which this regularization influences the overall process. Note that some research choose to penalize the â€– âˆ‡ ğœ½ L â¢ ( ğœ½ ) â€– 2 2 superscript subscript norm subscript âˆ‡ ğœ½ ğ¿ ğœ½ 2 2 ||\nabla_{{\bm{\theta}}}L({\bm{\theta}})||_{2}^{2} | | âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT italic_L ( bold_italic_Î¸ ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , which actually results in the same effect as â€– âˆ‡ ğœ½ L â¢ ( ğœ½ ) â€– 2 subscript norm subscript âˆ‡ ğœ½ ğ¿ ğœ½ 2 ||\nabla_{{\bm{\theta}}}L({\bm{\theta}})||_{2} | | âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT italic_L ( bold_italic_Î¸ ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . In practical applications, Hessian-free techniques are employed to approximate the involved gradient L ( g â¢ r ) â¢ ( ğœ½ ) superscript ğ¿ ğ‘” ğ‘Ÿ ğœ½ L^{(gr)}({\bm{\theta}}) italic_L start_POSTSUPERSCRIPT ( italic_g italic_r ) end_POSTSUPERSCRIPT ( bold_italic_Î¸ ) , g ( g â¢ r ) = ( 1 âˆ’ Î» r ) â¢ âˆ‡ ğœ½ L â¢ ( ğœ½ ) + Î» r â¢ âˆ‡ ğœ½ L â¢ ( ğœ½ + Ïµ ) superscript ğ‘” ğ‘” ğ‘Ÿ 1 ğœ† ğ‘Ÿ subscript âˆ‡ ğœ½ ğ¿ ğœ½ ğœ† ğ‘Ÿ subscript âˆ‡ ğœ½ ğ¿ ğœ½ bold-italic-Ïµ \begin{split}g^{(gr)}=(1-\frac{\lambda}{r})\nabla_{{\bm{\theta}}}L({\bm{\theta% }})+\frac{\lambda}{r}\nabla_{{\bm{\theta}}}L({\bm{\theta}}+{\bm{\epsilon}})% \end{split} start_ROW start_CELL italic_g start_POSTSUPERSCRIPT ( italic_g italic_r ) end_POSTSUPERSCRIPT = ( 1 - divide start_ARG italic_Î» end_ARG start_ARG italic_r end_ARG ) âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT italic_L ( bold_italic_Î¸ ) + divide start_ARG italic_Î» end_ARG start_ARG italic_r end_ARG âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT italic_L ( bold_italic_Î¸ + bold_italic_Ïµ ) end_CELL end_ROW (2) with Ïµ = r â‹… âˆ‡ ğœ½ L â¢ ( ğœ½ ) â€– âˆ‡ ğœ½ L â¢ ( ğœ½ ) â€– 2 bold-italic-Ïµ â‹… ğ‘Ÿ subscript âˆ‡ ğœ½ ğ¿ ğœ½ subscript norm subscript âˆ‡ ğœ½ ğ¿ ğœ½ 2 \begin{split}{\bm{\epsilon}}=r\cdot\frac{\nabla_{{\bm{\theta}}}L({\bm{\theta}}% )}{||\nabla_{{\bm{\theta}}}L({\bm{\theta}})||_{2}}\end{split} start_ROW start_CELL bold_italic_Ïµ = italic_r â‹… divide start_ARG âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT italic_L ( bold_italic_Î¸ ) end_ARG start_ARG | | âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT italic_L ( bold_italic_Î¸ ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG end_CELL end_ROW (3) where the parameter r ğ‘Ÿ r italic_r denotes as a small, positive scalar representing neighborhood perturbation, which is used for Taylor approximation. Notably, the gradients in SAM (Foret et al., 2021 ) can be expressed as, g ( s â¢ a â¢ m ) = âˆ‡ ğœ½ L â¢ ( ğœ½ + Ïµ ) superscript ğ‘” ğ‘  ğ‘ ğ‘š subscript âˆ‡ ğœ½ ğ¿ ğœ½ bold-italic-Ïµ \begin{split}g^{(sam)}=\nabla_{{\bm{\theta}}}L({\bm{\theta}}+{\bm{\epsilon}})% \end{split} start_ROW start_CELL italic_g start_POSTSUPERSCRIPT ( italic_s italic_a italic_m ) end_POSTSUPERSCRIPT = âˆ‡ start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT italic_L ( bold_italic_Î¸ + bold_italic_Ïµ ) end_CELL end_ROW (4) which clearly means that SAM essentially regularizes gradient norm with a special parameter configuration in GR where Î» = r ğœ† ğ‘Ÿ \lambda=r italic_Î» = italic_r for any case. 2.2 Related Works Recent research has been burgeoning in the field of GR with a specific emphasis on its applications and effects in deep learning. Barrett Dherin ( 2020 ) along with Smith et al. ( 2021 ) have conducted studies first exploring the aspects of GR in deep learning. They have revealed that the stepped update of standard gradient descent implicitly regularizes the gradient norm through its inherent dynamics, especially when correlated with its continuous equivalent, which is identified as implicit GR. When considering explicit GR, they suggest that applying GR explicitly can yield even better results. Recently, Zhao et al. ( 2022b ) and Karakida et al. ( 2023 ) proposes to directly penalize the gradient norm atop the loss and shows that such techniques can potentially give state-of-the-art performance. Meanwhile, it has been highlighted that GR exhibits a strong correlation with SAM (Zhao et al., 2022b ; Karakida et al., 2023 ; Reizinger HuszÃ¡r, 2023 ) . Considering the close connections between GR and SAM family, we would also like to discuss works associated with flat minima. In (Hochreiter Schmidhuber, 1997 ) , the authors are the first to point out that the flatness of minima could be associated with the model generalization, where models with better generalization should converge to flat minima. And such claim has been supported extensively by both empirical evidences and theoretical demonstrations (Keskar et al., 2017 ; Dinh et al., 2017 ) . In the meantime, researchers are also fascinating by how to implement practical algorithms to force the models to converge to such flat minima. By summarizing this problem to a specific minimax optimization, (Foret et al., 2021 ) introduce the SAM training scheme, which successfully guides optimizers to converge to flat minima. Since then, many SAM variants are proposed, such as improved approaches (Kwon et al., 2021 ; Zhuang et al., 2022 ) and efficient approaches (Du et al., 2021 ; Zhao et al., 2022a ) , in expectation to contributing the original SAM from various perspectives. Table 1: Testing error rate of ViT models using Adam and RMSProp optimizers on Cifar-{10, 100} when employing a cross-utilization of LR warmup and standard GR techniques. Model Architecture Base Optimizer LR Warmup Cifar-10 Error [%] Cifar-100 Error [%] Vanilla GR Vanilla GR ViT-Ti Adam âœ• 15.94 Â± 0.33 subscript 15.94 plus-or-minus 0.33 \ \ 15.94_{\pm 0.33}\ 15.94 start_POSTSUBSCRIPT Â± 0.33 end_POSTSUBSCRIPT 14.37 Â± 0.31 subscript 14.37 plus-or-minus 0.31 \ \ 14.37_{\pm 0.31}\ 14.37 start_POSTSUBSCRIPT Â± 0.31 end_POSTSUBSCRIPT 41.19 Â± 0.21 subscript 41.19 plus-or-minus 0.21 \ \ 41.19_{\pm 0.21}\ 41.19 start_POSTSUBSCRIPT Â± 0.21 end_POSTSUBSCRIPT 39.99 Â± 0.39 subscript 39.99 plus-or-minus 0.39 \ \ 39.99_{\pm 0.39}\ 39.99 start_POSTSUBSCRIPT Â± 0.39 end_POSTSUBSCRIPT âœ“ 14.82 Â± 0.64 subscript 14.82 plus-or-minus 0.64 \ \ 14.82_{\pm 0.64}\ 14.82 start_POSTSUBSCRIPT Â± 0.64 end_POSTSUBSCRIPT 13.92 Â± 0.19 subscript 13.92 plus-or-minus 0.19 \ \ 13.92_{\pm 0.19}\ 13.92 start_POSTSUBSCRIPT Â± 0.19 end_POSTSUBSCRIPT 39.25 Â± 0.41 subscript 39.25 plus-or-minus 0.41 \ \ 39.25_{\pm 0.41}\ 39.25 start_POSTSUBSCRIPT Â± 0.41 end_POSTSUBSCRIPT 39.28 Â± 0.50 subscript 39.28 plus-or-minus 0.50 \ \ 39.28_{\pm 0.50}\ 39.28 start_POSTSUBSCRIPT Â± 0.50 end_POSTSUBSCRIPT RMSProp âœ• 17.05 Â± 0.45 subscript 17.05 plus-or-minus 0.45 \ \ 17.05_{\pm 0.45}\ 17.05 start_POSTSUBSCRIPT Â± 0.45 end_POSTSUBSCRIPT 16.32 Â± 0.30 subscript 16.32 plus-or-minus 0.30 \ \ 16.32_{\pm 0.30}\ 16.32 start_POSTSUBSCRIPT Â± 0.30 end_POSTSUBSCRIPT 41.62 Â± 0.16 subscript 41.62 plus-or-minus 0.16 \ \ 41.62_{\pm 0.16}\ 41.62 start_POSTSUBSCRIPT Â± 0.16 end_POSTSUBSCRIPT 41.06 Â± 0.90 subscript 41.06 plus-or-minus 0.90 \ \ 41.06_{\pm 0.90}\ 41.06 start_POSTSUBSCRIPT Â± 0.90 end_POSTSUBSCRIPT âœ“ 16.47 Â± 0.39 subscript 16.47 plus-or-minus 0.39 \ \ 16.47_{\pm 0.39}\ 16.47 start_POSTSUBSCRIPT Â± 0.39 end_POSTSUBSCRIPT 15.79 Â± 0.25 subscript 15.79 plus-or-minus 0.25 \ \ 15.79_{\pm 0.25}\ 15.79 start_POSTSUBSCRIPT Â± 0.25 end_POSTSUBSCRIPT 40.31 Â± 0.50 subscript 40.31 plus-or-minus 0.50 \ \ 40.31_{\pm 0.50}\ 40.31 start_POSTSUBSCRIPT Â± 0.50 end_POSTSUBSCRIPT 40.02 Â± 0.22 subscript 40.02 plus-or-minus 0.22 \ \ 40.02_{\pm 0.22}\ 40.02 start_POSTSUBSCRIPT Â± 0.22 end_POSTSUBSCRIPT ViT-S Adam âœ• 15.43 Â± 0.52 subscript 15.43 plus-or-minus 0.52 \ \ 15.43_{\pm 0.52}\ 15.43 start_POSTSUBSCRIPT Â± 0.52 end_POSTSUBSCRIPT 14.41 Â± 0.42 subscript 14.41 plus-or-minus 0.42 \ \ 14.41_{\pm 0.42}\ 14.41 start_POSTSUBSCRIPT Â± 0.42 end_POSTSUBSCRIPT 42.83 Â± 0.34 subscript 42.83 plus-or-minus 0.34 \ \ 42.83_{\pm 0.34}\ 42.83 start_POSTSUBSCRIPT Â± 0.34 end_POSTSUBSCRIPT 39.16 Â± 0.27 subscript 39.16 plus-or-minus 0.27 \ \ 39.16_{\pm 0.27}\ 39.16 start_POSTSUBSCRIPT Â± 0.27 end_POSTSUBSCRIPT âœ“ 12.07 Â± 0.53 subscript 12.07 plus-or-minus 0.53 \ \ 12.07_{\pm 0.53}\ 12.07 start_POSTSUBSCRIPT Â± 0.53 end_POSTSUBSCRIPT 12.40 Â± 0.17 subscript 12.40 plus-or-minus 0.17 \ \ \mathbf{12.40_{\pm 0.17}}\ bold_12.40 start_POSTSUBSCRIPT Â± bold_0.17 end_POSTSUBSCRIPT 37.17 Â± 0.24 subscript 37.17 plus-or-minus 0.24 \ \ 37.17_{\pm 0.24}\ 37.17 start_POSTSUBSCRIPT Â± 0.24 end_POSTSUBSCRIPT 36.65 Â± 0.23 subscript 36.65 plus-or-minus 0.23 \ \ 36.65_{\pm 0.23}\ 36.65 start_POSTSUBSCRIPT Â± 0.23 end_POSTSUBSCRIPT RMSProp âœ• 19.94 Â± 0.44 subscript 19.94 plus-or-minus 0.44 \ \ 19.94_{\pm 0.44}\ 19.94 start_POSTSUBSCRIPT Â± 0.44 end_POSTSUBS