Title: Controllable 3D Object Generation with Single Image Prompt

Abstract: Recently, the impressive generative capabilities of diffusion models have been demonstrated, producing images with remarkable fidelity. Particularly, existing methods for the 3D object generation tasks, which is one of the fastest-growing segments in computer vision, pre-dominantly use text-to-image diffusion models with textual inversion which train a pseudo text prompt to describe the given image. In practice, various text-to-image generative models employ textual inversion to learn concepts or styles of target object in the pseudo text prompt embedding space, thereby generating sophisticated outputs. However, textual inversion requires additional training time and lacks control ability. To tackle this issues, we propose two innovative methods: (1) using an off-the-shelf image adapter that generates 3D objects without textual inversion, offering enhanced control over conditions such as depth, pose, and text. (2) a depth conditioned warmup strategy to enhance 3D consistency. In experimental results, ours show qualitatively and quantitatively comparable performance and improved 3D consistency to the existing text-inversion-based alternatives. Furthermore, we conduct a user study to assess (i) how well results match the input image and (ii) whether 3D consistency is maintained. User study results show that our model outperforms the alternatives, validating the effectiveness of our approaches. Our code is available at GitHub repository:https://github.com/Seooooooogi/Control3D_IP/

Body: Controllable 3D object Generation with Single Image Prompt 1 Introduction 2 Related Work 3 Methods 3.1 Controllable Image Prompt Score Distillation Sampling 3.2 Depth Conditioned Warmup Strategy 4 Experiments 4.1 Implementation Details 4.2 Quantitative Results 4.3 Qualitative Results 4.4 Discussion 4.5 User Study 5 Conclusion 5.0.1 Acknowledgements 1 1 institutetext: College of Computer Science, Kookmin University, Seoul, Korea * Correspondence: jaekoo@kookmin.ac.kr Controllable 3D object Generation with Single Image Prompt Jaeseok Lee Jaekoo Lee ‚àó Abstract Recently, the impressive generative capabilities of diffusion models have been demonstrated, producing images with remarkable fidelity. Particularly, existing methods for the 3D object generation tasks, which is one of the fastest-growing segments in computer vision, predominantly use text-to-image diffusion models with textual inversion which train a pseudo text prompt to describe the given image. In practice, various text-to-image generative models employ textual inversion to learn concepts or styles of target object in the pseudo text prompt embedding space, thereby generating sophisticated outputs. However, textual inversion requires additional training time and lacks control ability. To tackle this issues, we propose two innovative methods: (1) using an off-the-shelf image adapter that generates 3D objects without textual inversion, offering enhanced control over conditions such as depth, pose, and text. (2) a depth conditioned warmup strategy to enhance 3D consistency. In experimental results, ours show qualitatively and quantitatively comparable performance and improved 3D consistency to the existing text-inversion-based alternatives. Furthermore, we conduct a user study to assess (i) how well results match the input image and (ii) whether 3D consistency is maintained. User study results show that our model outperforms the alternatives, validating the effectiveness of our approaches. Our code is available at GitHub repository: https://github.com/Seooooooogi/Control3D_IP/ Figure 1: Comparison between (a) existing method using text-guided priors via textual inversion and (b) our method that employs a single image prompt. 1 Introduction Generative models are widely used in real-world industrial applications, including creating artworks [ Rombach_2022_CVPR ] , video generation [ singer2022makeavideo , sora ] , automatic colorization [ lee2022bridging , zabari2023diffusing ] , virtual try-on [ Zhu_2023_CVPR_tryondiffusion ] , and 3D computer graphics [ poole2022dreamfusion , melaskyriazi2023realfusion , Mohammad_Khalid_2022 , chen2023text2tex , qian2023magic123 , Chen_2023_ICCV ] . Especially, 3D object generation tasks have significant demand in computer vision industries such as Artificial Reality (AR), Virtual Reality (VR), and Gaming. Conventionally, generating 3D objects has been a delicate and intricate process that requires experts well-versed in 3D graphics tools with a considerable amount of time. However, recent breakthroughs have introduced 3D generative models (e.g., text-to-3D model and image-to-3D model) that allow even non-experts to effortlessly produce 3D objects. Especially, several methods [ melaskyriazi2023realfusion , qian2023magic123 ] have achieved significant advancements in image-to-3D generation by using text-guided priors and textual inversion [ gal2022image ] , which is subsequently used in text-to-image diffusion model. However, as shown in Fig. 1 , textual inversion requires additional time and cost to get a new prompt because it needs training process to optimize pseudo text prompt. To address these issues, we propose the following contributions. ‚Ä¢ We propose a controllable image prompt score distillation sampling(called SDS) method that uses a single image as a prompt without textual inversion to generate novel views of 3D object. ‚Ä¢ Our method leverages depth estimated from the 3D object as a condition for ControlNet [ zhang2023adding ] to prevent the degradation of 3D consistency, and employs a depth conditioned warmup strategy to alleviate instability of depth in early training epochs. ‚Ä¢ We experimentally show that our proposed method generates diverse, controllable 3D objects using a given single image and optional prompts such as depth, pose, and text. Our method quantitatively and qualitatively outperforms alternative approaches. ‚Ä¢ We also conduct a user study that participants score two evaluation metrics: fidelity and 3D consistency. Our method exhibits superior performance compared to SOTA methods. 2 Related Work Neural 3D Representations. Neural Radiance Fields (NeRF) [ mildenhall2020nerf ] trains deep neural network with sparse input views and camera poses to optimize the inherent continuous volumetric scene function, synthesizing novel views of given scenes. Although NeRF has capability of 3D reconstruction task, optimization process is slow and hard to get high-resolution results. To overcome computational cost, Instant-NGP [ mueller2022instant ] uses smaller neural networks, leverages multi resolution hash grids without sacrificing quality. On the other hand, Deep Marching Tetrahedra (DMTet) [ shen2021dmtet ] leverages novel hybrid implicit-explicit 3D representations, achieving high resolution, finer geometric details with fewer artifacts with efficient memory consumption. In here, we utilize Instant-NGP and DMTet for generate 3D object from a single image, aiming for higher quality results while maintaining memory efficiency. Diffusion Models. In computer vision tasks such as inpainting [ lugmayr2022repaint , saharia2022palette ] , image editing [ meng2022sdedit , sinha2021d2c ] , super-resolution [ li2021srdiff , saharia2021image ] , image generation [ ramesh2022hierarchical , Rombach_2022_CVPR ] and video generation [ singer2022makeavideo , sora ] , diffusion models are gaining popularity due to great result quality. Diffusion models train neural networks to denoise images blurred with Gaussian noise and to reverse the diffusion process [ ho2020denoising ] . Especially, Stable diffusion [ rombach2022highresolution ] , which is open source, achieved great success with datasets consisting of large scale text-image pairs [ schuhmann2022laion ] . Subsequent work, ControlNet, leverages the expressiveness of the Stable diffusion model, expanding it to incorporate a variety of conditional conditional priors such as sketches, depth, and human poses [ zhang2023adding ] . Gal, Rinon et al. [ gal2022image ] introduce textual inversion, which optimizes a pseudo text prompt by using only 3-5 images of a user-provided concept to utilize the expressiveness of Stable diffusion model. However, it requires additional training time and usually needs several images. To overcome these shortcomings, IP-Adapter [ ye2023ip-adapter ] proposes a lightweight adapter to directly input an image prompt into a pretrained text-to-image diffusion model, thereby facilitating multi-modal image generation without additional training time. 3D Generative Models. By integrating NeRF and diffusion models, several approaches aim to train 3D generative models, either by using text [ poole2022dreamfusion , lin2023magic3d , Chen_2023_ICCV ] or a single image [ melaskyriazi2023realfusion , qian2023magic123 ] . For text-to-3D generation, DreamFusion [ poole2022dreamfusion ] proposes score distillation sampling to guide the training of NeRF models using a pretrained text-to-image diffusion model. Although DreamFusion successfully achieved first text-to-3D generative task with pretrained diffusion models, it fails to generate high-resolution objects, requiring up to one day to train a single 3D object. To produce high-resolution 3D objects, several methods are proposed. Magic3D [ lin2023magic3d ] integrates DMTet [ shen2021dmtet ] during the training process. Fantasia3D [ Chen_2023_ICCV ] disentangles geometry and appearance to generate high-fidelity 3D objects that closely align with real graphics rendering. For image-to-3D generation task, RealFusion [ melaskyriazi2023realfusion ] employs textual inversion from an image to derive custom tokens for training a 3D generative model instead of text prompts. On the other hand, Zero-1-to-3 [ liu2023zero1to3 ] finetunes the Stable diffusion model to simultaneously input an image and geometry-related camera pose priors, facilitating the synthesis of images from specific 3D viewpoints. Magic123 [ qian2023magic123 ] combines the Stable diffusion model with textual inversion and Zero-1-to-3 simultaneously. Despite RealFusion and Magic123 utilize textual inversion for single image, these methods often fail to capture fine details of given images and requires additional training time. Figure 2: Overall architecture of our proposed 3D generative model. During training, our method iteratively use the following stages: (i) controllable image prompt score distillation sampling, and (ii) depth conditioned warmup strategy. 3 Methods The main goal of our method is to synthesize a 3D object without relying on text prompts obtained by textual inversion, instead directly using controllable image prompts (e.g., only image or with optional conditions). To achieve this goal, as shown in Fig. 2 , we introduce controllable image prompt score distillation sampling and depth conditioned warmup strategy . We build the proposed model, which consists of two main components, using the Magic123 [ qian2023magic123 ] architecture as the baseline. The proposed model performs alternately two stage training process (i.e., coarse-to-fine training). During the 1st stage for training coarse grained representation, we train NeRF. In detail, initialized NeRF model with learnable parameters Œ∏ \theta , predicts the volume density œÉ \sigma and color c c of each pixel given a random camera pose c c . ( c , œÉ ) = N ‚Äã e ‚Äã R ‚Äã F ‚Äã ( Œ∏ ; c ) (c,\sigma)=NeRF(\theta;c) (1) By computing each pixel‚Äôs volume density and color, NeRF predicts image x ^ ‚àà ‚Ñù H √ó W √ó 3 \hat{x}\in\mathbb{R}^{H\times W\times 3} and depth d ^ ‚àà ‚Ñù H √ó W √ó 1 \hat{d}\in\mathbb{R}^{H\times W\times 1} . When the 1st training stage finished, we finetune the NeRF with DMTet to represent the target 3D object at high resolution during 2nd stage for training fine grained representation. Unlike NeRF which has entangled geometry and texture representations, DMTet has disentangled geometry and texture representations. For geometry representation, DMTet uses a deformable tetrahydral grid ( V T , T ) (V_{T},T) [ shen2021dmtet ] , where tetrahydral grid denotes T T and its vertices denote V T V_{T} . DMTet represents the 3D object using a Signed Distance Function (SDF) s s at vertex v i ‚àà V T v_{i}\in V_{T} and a triangle deformation vector Œî ‚Äã v i \Delta v_{i} . In here, s s is initialized from the NeRF, while triangle deformation vector Œî ‚Äã v i \Delta v_{i} is initialized as zero. For texture representation, DMTet uses a neural color field, as mentioned in Magic3D [ lin2023magic3d ] , is also initialized from the NeRF. Figure 3: An overview of controllable image prompt score distillation sampling. See details on Sec 3.1 . 3.1 Controllable Image Prompt Score Distillation Sampling As shown in Fig. 3 , a single input image x x , optionally including various conditions such as text, depth, sketch, is embedded into a controllable image prompt through the well-known pretrained Image Prompt Adapter (IP-Adapter) [ ye2023ip-adapter ] , which consists of the pretrained CLIP [ radford2021learning ] image encoder and the single linear layer. The embedded controllable image prompt and depth d ^ \hat{d} , obtained from NeRF/DMTet, are then injected as conditions into a stable diffusion model like ControlNet [ zhang2023adding ] to predict œµ ^ 2 ‚Äã D \hat{\epsilon}_{2D} . The x ^ \hat{x} obtained from NeRF/DMTet is added time-dependent noise œµ \epsilon to produce a noisy latent x ^ t \hat{x}_{t} . x ^ t \hat{x}_{t} is then fed into the ControlNet [ zhang2023adding ] . Consequently, ControlNet outputs œµ ^ 2 ‚Äã D \hat{\epsilon}_{2D} , corresponding to the random diffusion timestep t t . The loss function ‚Ñí I ‚Äã P ‚àí 2 ‚Äã D \mathcal{L}_{IP-2D} for 2D score distillation sampling is as follows: Figure 4: An overview of depth conditioned warmup strategy. See details on Sec 3.2 . ‚Ñí I ‚Äã P ‚àí 2 ‚Äã D = ùîº t , œµ ‚Äã [ w ‚Äã ( t ) ‚Äã ( œµ ^ 2 ‚Äã D ‚Äã ( z ^ t ; x , d ^ , t ) ‚àí œµ ) ‚Äã ‚àÇ x ‚àÇ Œ∏ ] \mathcal{L}_{IP-2D}=\mathbb{E}_{t,\epsilon}\Big[w(t)(\hat{\epsilon}_{2D}(\hat{z}_{t};x,\hat{d},t)-\epsilon)\frac{\partial x}{\partial\theta}\Big] (2) where w ‚Äã ( t ) w(t) represents the time-dependent weighting function. By directly injecting input image into the stable diffusion model, the 2D score distillation sampling loss guides NeRF/DMTet to adhere to the input image from any camera pose. Similar to ControlNet, Zero-1-to-3 predicts œµ ^ 3 ‚Äã D \hat{\epsilon}_{3D} when given x ^ t \hat{x}_{t} as input. The loss function ‚Ñí 3 ‚Äã D \mathcal{L}_{3D} for 3D score distillation sampling is as follows: ‚Ñí 3 ‚Äã D = ùîº t , œµ ‚Äã [ w ‚Äã ( t ) ‚Äã ( œµ ^ 3 ‚Äã D ‚Äã ( z ^ t ; x , c , t ) ‚àí œµ ) ‚Äã ‚àÇ x ‚àÇ Œ∏ ] \mathcal{L}_{3D}=\mathbb{E}_{t,\epsilon}\Big[w(t)(\hat{\epsilon}_{3D}(\hat{z}_{t};x,c,t)-\epsilon)\frac{\partial x}{\partial\theta}\Big] (3) Although ‚Ñí 3 ‚Äã D \mathcal{L}_{3D} serves the same purpose as ‚Ñí I ‚Äã P ‚àí 2 ‚Äã D \mathcal{L}_{IP-2D} , Magic123 [ qian2023magic123 ] found that training with ‚Ñí 3 ‚Äã D \mathcal{L}_{3D} can lead NeRF/DMTet to have better 3D consistency. Additionally, a primary requirement of image-to-3D generative models is to reconstruct input image from a reference camera pose c r c^{r} . Given the input image x x masked with ‚Ñ≥ \mathcal{M} , which is acquired by a dense prediction transformer [ Ranftl2021 ] , and the predicted image x r ^ \hat{x^{r}} masked with ‚Ñ≥ r \mathcal{M}^{r} , we perform the reconstruction by comparing x x and x r ^ \hat{x^{r}} . The reconstruction loss ‚Ñí r ‚Äã e ‚Äã c \mathcal{L}_{rec} is as follows: ‚Ñí r ‚Äã e ‚Äã c = ‚Äñ ‚Ñ≥ ‚äô ( x ‚àí x ^ r ) ‚Äñ 2 2 + ‚Äñ ‚Ñ≥ ‚àí ‚Ñ≥ r ‚Äñ 2 2 \mathcal{L}_{rec}=\|\mathcal{M}\odot(x-\hat{x}^{r})\|_{2}^{2}+\|\mathcal{M}-\mathcal{M}^{r}\|_{2}^{2} (4) where ‚äô \odot is element-wise product. Xu, Dejia et al. [ xu2023neurallift ] introduce that reconstructing 3D object from 2D images often results in flat geometry. To alleviate this issue, we incorporate a monocular depth regularization loss ‚Ñí d \mathcal{L}_{d} , inspired by Magic123 [ qian2023magic123 ] . Given predicted depth d ^ r \hat{d}^{r} from the reference camera pose c r c^{r} and the input image‚Äôs pseudo depth d x d^{x} , estimated by pretrained monocular depth estimator [ ranftl2020towards ] , the loss function ‚Ñí d \mathcal{L}_{d} is calculated using the negative Pearson correlation between d ^ r \hat{d}^{r} and d x {d}^{x} , where are masked with ‚Ñ≥ \mathcal{M} . ‚Ñí d = 1 2 ‚Äã [ 1 ‚àí C ‚Äã o ‚Äã v ‚Äã ( d x ‚äô ‚Ñ≥ , d ^ r ‚äô ‚Ñ≥ ) œÉ ‚Äã ( d x ‚äô ‚Ñ≥ ) , œÉ ‚Äã ( d ^ r ‚äô ‚Ñ≥ ) ] \mathcal{L}_{d}=\frac{1}{2}\left[1-\frac{Cov(d^{x}\odot\mathcal{M},\hat{d}^{r}\odot\mathcal{M})}{\sigma(d^{x}\odot\mathca