Title: Image Generation with a Sphere Encoder

Abstract: We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .

Body: Image Generation with a Sphere Encoder 1 Introduction 2 Method 2.1 Spherical Latent Space 2.2 Spherifying with Noise 2.3 Training Objective 2.4 Model Architecture 3 Quantitative Experiments 3.1 Small Image Size 3.2 Large Image Size 3.3 Lower FID scores? 4 Qualitative Experiments 5 Image Editing 6 Main Ablations 7 Related Work 8 Conclusion A Additional Results on CIFAR-10 B Memorization Risk on CIFAR-10 C Additional Ablations C.1 CFG Position C.2 Dialing in the Noise Distribution C.3 Explicit Distribution Regularization C.4 BatchNorm Recalibration C.5 Volume Compression Ratio C.6 Noise Prior Distribution D Hyperparameters Image Generation with a Sphere Encoder Kaiyu Yue Menglin Jia Ji Hou Tom Goldstein Abstract We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at sphere-encoder.github.io . Sphere Encoders, Few-step Generation, One-step Generation, Autoencoders, Generative Models Figure 1 : Selected images generated by the Sphere Encoder in one-step for CIFAR-10 ( 32 Ã— 32 32\times 32 ) and Animal-Faces, two-steps for Oxford-Flowers, and four-steps for ImageNet ( 256 Ã— 256 256\times 256 ). 1 Introduction Most generative image models rely on either diffusion (Ho et al., 2020 ; Lipman et al., 2022 ) or autoregressive next-token prediction (Tian et al., 2024 ) . With either paradigm, image generation is extremely slow and costly, requiring many forward passes to produce a single image. We propose an alternative paradigm that is capable of generating sharp images with as little as one forward pass. Our approach, which we call a sphere encoder , works by training two complementary models: an encoder model that maps the distribution of natural images uniformly onto the sphere, and a decoder that maps points on the sphere back to natural images ( Figure 2 ). The term aligns with the autoencoder convention, reflecting its encoder-decoder architecture. At test time, an image is generated quickly by sampling a random point on the sphere and passing it through the decoder. Although the sphere encoder does not employ diffusion processes explicitly, it supports several key capabilities commonly associated with its diffusion-based cousins (Dhariwal Nichol, 2021 ; Rombach et al., 2022 ; Esser et al., 2024 ) . These include conditional generation using AdaLN (Perez et al., 2018 ; Peebles Xie, 2023 ) , classifier-free guidance (CFG) (Ho Salimans, 2022 ) , and few-step iteration to enhance sample quality (Goodfellow et al., 2014 ; Kingma Dhariwal, 2018 ; Song et al., 2023 ) . Experiments demonstrate that our approach achieves competitive one-step generation, and state-of-the-art performance in few-step regimes ( e.g . , fewer than 5 5 steps) on a range of datasets. Figure 2 : A sphere encoder E E maps the natural image distribution uniformly onto a global sphere S S . The decoder D D then generates a realistic image by decoding a random point on the sphere. Motivation and Relation to Autoencoders Autoencoders (LeCun, 1987 ; Bourlard Kamp, 1988 ; Hinton Zemel, 1993 ) have been widely used in representation learning and generative modeling. A lower-dimensional latent bottleneck between the encoder and decoder forces the model to learn an undercomplete representation of the input (Goodfellow et al., 2016 ) . To regularize the latent space, variational autoencoders (VAEs) (Kingma Welling, 2013 , 2019 ; Tolstikhin et al., 2018 ; Davidson et al., 2018 ; Ke Xue, 2025 ) minimize the divergence between the latent distribution and a (typically) Gaussian prior. Unfortunately, in the standard VAE formulation, the divergence loss and image reconstruction loss are at odds with one another; zero divergence loss cannot be achieved simultaneously with perfect image reconstruction. As a result, the learned posterior fails to strongly match the prior â€“ an issue known as the posterior hole problem (Makhzani et al., 2015 ; Rezende Viola, 2018 ; Tomczak Welling, 2018 ; Dai Wipf, 2019 ; Ghosh et al., 2020 ; Aneja et al., 2021 ) . Direct samples from the Gaussian prior fail to yield valid images. Realistic images are currently possible only by decoding samples from the posterior ( i.e . , adding noise to latents derived from real images), as illustrated in Figure 3 . Our approach does not suffer from this problem. Like a classical VAE, our approach relies on an autoencoder. Unlike the VAE, which tries to force the latent vectors into a Gaussian distribution, we instead force latents to be uniformly distributed on a sphere. Due to the bounded and rotationally symmetric nature of the sphere, this can be achieved simply by forcing embeddings of natural images away from one another, causing them to spread throughout the sphere. Moreover, this objective is not in contradiction with the image reconstruction objective; we can achieve both uniformity and accurate reconstruction simultaneously. Figure 3 : Posterior hole problem in VAEs . Columns: (1) Input images; (2) Autoencoder reconstructions; (3) Samples from standard Gaussian prior; and (4) Samples from estimated Gaussian posterior on Animal-Faces training set. Unlike modern FLUX.1/2 (Labs et al., 2025 ) and SD-VAE (Podell et al., 2024 ) , our sphere encoder produces realistic images by decoding random points sampled from the sphere. Many contemporary state-of-the-art diffusion models are actually latent diffusion models (Rombach et al., 2022 ; Peebles Xie, 2023 ; Liu et al., 2023 ; Ma et al., 2024 ; Esser et al., 2024 ; Podell et al., 2024 ; Wan et al., 2025 ) â€“ hybrid models built on top of VAEs. The VAE partially Gaussianizes the image distribution, but not well enough to be sampled. A diffusion pipeline picks up the slack in the VAE, going the last mile of producing a valid latent sample for the decoder. Concurrent works have shown that more powerful representation encoders (Yu et al., 2024 ; Tong et al., 2026 ) , and even spherical manifold encoders (Zheng et al., 2025 ) , result in faster training of the diffusion layer. In our work, we show that a spherical latent space 1 1 1 In contrast to prior vMF-based approaches, we create our spherical space using simple vector RMS normalization. can be learned so precisely that the expensive diffusion step is irrelevant. 2 Method 2.1 Spherical Latent Space We employ an encoder E E based on a Transformer (Dosovitskiy, 2020 ; Vaswani et al., 2017 ) to map an input image ğ± âˆˆ H Ã— W Ã— 3 \mathbf{x}\in^{H\times W\times 3} into a latent representation ğ³ âˆˆ h Ã— w Ã— d \mathbf{z}\in^{h\times w\times d} . The latent resolution is determined by the patch size P P , such that h = H / P h=H/P and w = W / P w=W/P , with d d denoting the channel depth. To construct a global spherical latent space, we define a spherifying function, denoted as f f . This function flattens ğ³ \mathbf{z} into a vector of dimension L = h Ã— w Ã— d L=h\times w\times d and then projects it onto a sphere with radius L \sqrt{L} via RMS normalization: ğ¯ = f â€‹ ( ğ³ ) = f â€‹ ( E â€‹ ( ğ± ) ) . \displaystyle\mathbf{v}=f(\mathbf{z})=f(E(\mathbf{x}))\;. (1) Subsequently, a decoder D D reconstructs the image from ğ¯ \mathbf{v} : ğ± ^ = D â€‹ ( ğ¯ ) , \displaystyle\hat{\mathbf{x}}=D(\mathbf{v})\;, (2) where ğ± ^ \hat{\mathbf{x}} denotes the reconstructed image. If the encoder maps images uniformly onto a sphere, then we can generate images by decoding random points on the sphere: ğ± ^ = D â€‹ ( f â€‹ ( ğ ) ) , \displaystyle\hat{\mathbf{x}}=D(f(\mathbf{e}))\;, (3) where ğ âˆ¼ ğ’© â€‹ ( 0 , ğˆ ) âˆˆ L \mathbf{e}\sim\mathcal{N}(0,\mathbf{I})\in^{L} is random anisotropic Gaussian and f â€‹ ( ğ ) f(\mathbf{e}) is uniformly distributed on the sphere. For simplicity, we use ğ± ^ \hat{\mathbf{x}} to denote the decoder output in both reconstruction and generation scenarios. Figure 4 : Spherifying latent with noise . Encoder E E maps image ğ± \mathbf{x} to a latent, which f f projects to ğ¯ \mathbf{v} on sphere S S . During training, random Gaussian noise Ïƒ â‹… ğ \sigma\cdot\mathbf{e} is added to ğ¯ \mathbf{v} , where Ïƒ \sigma is jittered magnitude. Decoder D D reconstructs the image ğ± ^ \hat{\mathbf{x}} from the re-projected noisy latent f â€‹ ( ğ¯ + Ïƒ â‹… ğ ) f(\mathbf{v}+\sigma\cdot\mathbf{e}) . 2.2 Spherifying with Noise Our training process uses embedding vectors of natural images, and also noisy versions of those embedding vectors. The purpose of training with noisy vectors is two-fold. First, noisy clouds of vectors densely cover the latent space, enabling us to train the decoder on the continuous global latent sphere, rather than only on the finite set of embedding vectors. Second, by using a loss that promotes accurate decoding of noisy latent vectors, we force the noisy clouds produced by each training image to spread apart and cover the entire latent sphere. From a Normal distribution, we randomly sample a noise vector ğ âˆ¼ ğ’© â€‹ ( 0 , ğˆ ) âˆˆ L \mathbf{e}\sim\mathcal{N}(0,\mathbf{I})\in^{L} to perturb the direction of ğ¯ \mathbf{v} : ğ¯ NOISY = f â€‹ ( ğ¯ + Ïƒ â‹… ğ ) , \displaystyle\mathbf{v}_{\text{NOISY}}=f(\mathbf{v}+\sigma\cdot\mathbf{e})\;, (4) where the scalar Ïƒ \sigma controls the noise magnitude. Note that f f is applied again here to project the perturbed vector back onto the spherical surface. Jittering Sigma . To cover diverse directions on the sphere, we jitter Ïƒ \sigma during the training. By sampling a scalar r r uniformly from [ 0 , 1 ] [0,1] , we compute Ïƒ \sigma as: Ïƒ = r â‹… Ïƒ max , \displaystyle\sigma=r\cdot\sigma_{\text{max}}\;, (5) where the Ïƒ max \sigma_{\text{max}} is the maximum noise limit. The case of r = 0 r=0 reduces to the naive spherifying in Equation 1 . Later we determine the optimal value for Ïƒ max \sigma_{\text{max}} with experiments. This core design is illustrated in Figure 4 . 2.3 Training Objective Consider two perturbed latent vectors, ğ¯ NOISY \mathbf{v}_{\text{NOISY}} and ğ¯ noisy \mathbf{v}_{\text{noisy}} , with large and small noise. ğ¯ NOISY \mathbf{v}_{\text{NOISY}} is defined as in Equation 4 with Ïƒ âˆˆ [ 0 , Ïƒ max ] \sigma\in[0,\sigma_{\text{max}}] . The other perturbed ğ¯ noisy \mathbf{v}_{\text{noisy}} has less jitter: ğ¯ noisy = f â€‹ ( ğ¯ + Ïƒ sub â‹… ğ ) , \displaystyle\mathbf{v}_{\text{noisy}}=f(\mathbf{v}+\sigma_{\text{sub}}\cdot\mathbf{e})\;, (6) where Ïƒ sub = s â‹… Ïƒ \sigma_{\text{sub}}=s\cdot\sigma , and s s is uniformly sampled from [ 0 , 0.5 ] [0,0.5] . Note that ğ¯ noisy \mathbf{v}_{\text{noisy}} shares the same noise direction ğ \mathbf{e} as ğ¯ NOISY \mathbf{v}_{\text{NOISY}} . Pixel Reconstruction Loss . This loss ensures that the decoder is an approximate inverse of the encoder, and that the decoder creates valid images. We have the standard pixel-level reconstruction loss, which combines of smoothed L1 loss (Girshick, 2015 ) and perceptual loss (Johnson et al., 2016 ) . This loss encourages the decoder to reconstruct the input image ğ± \mathbf{x} from its noisy latent representation ğ¯ noisy \mathbf{v}_{\text{noisy}} : â„’ pix-recon = â„’ L1 + perceptual â€‹ ( D â€‹ ( ğ¯ noisy ) , ğ± ) . \displaystyle\mathcal{L}_{\text{pix-recon}}=\mathcal{L}_{\text{L1 + perceptual}}\left(D(\mathbf{v}_{\text{noisy}}),\mathbf{x}\right)\;. (7) Pixel Consistency Loss . This consistency loss ensures that the latent space is smooth and well structured by promoting that nearby latent vectors produce similar images: â„’ pix-con = â„’ L1 + perceptual â€‹ ( D â€‹ ( ğ¯ NOISY ) , sg â€‹ ( D â€‹ ( ğ¯ noisy ) ) ) , \displaystyle\mathcal{L}_{\text{pix-con}}=\mathcal{L}_{\text{L1 + perceptual}}(D(\mathbf{v}_{\text{NOISY}}),\text{sg}(D(\mathbf{v}_{\text{noisy}})))\;, (8) which also uses the combination of smooth L1 loss and perceptual loss, and sg â€‹ ( â‹… ) \text{sg}(\cdot) denotes stop-gradient operation. Latent Consistency Loss . It is well known that image similarity is better measured in latent space than in pixel space (Zhang et al., 2018 ; Radford et al., 2021 ) . This is the reason why our pixel similarities use a perceptual loss, which relies on features produced by a static VGG model. To achieve a stronger consistency loss, we also measure image similarity using the latent space of our own encoder. We want a natural image ğ± \mathbf{x} and its noisy decoded representation D â€‹ ( ğ¯ NOISY ) D(\mathbf{v}_{\text{NOISY}}) to be semantically similar. The semantic similarity is measured by applying the encoder to both, and computing the cosine similarity between their latent representations. This yields the following loss: â„’ lat-con = â„’ cosine similarity â€‹ ( ğ¯ , E â€‹ ( D â€‹ ( ğ¯ NOISY ) ) ) . \displaystyle\mathcal{L}_{\text{lat-con}}=\mathcal{L}_{\text{cosine similarity}}(\mathbf{v},E(D(\mathbf{v}_{\text{NOISY}})))\;. (9) This loss serves an additional important purpose: It improves the iterative generation process we discuss later by encouraging the encoder to map distorted images, D â€‹ ( ğ¯ NOISY ) D(\mathbf{v}_{\text{NOISY}}) , that may be off the image manifold to â€œcleaned upâ€ latent vectors that reflect on-manifold images. Overall Loss. The overall training loss is a weighted sum of the three components: â„’ = â„’ pix-recon + â„’ pix-con + â„’ lat-con . \displaystyle\mathcal{L}=\mathcal{L}_{\text{pix-recon}}+\mathcal{L}_{\text{pix-con}}+\mathcal{L}_{\text{lat-con}}\;. (10) More details about loss weights and training hyperparameters are provided in Appendix D . 2.4 Model Architecture Our architecture employs the standard ViT (Dosovitskiy, 2020 ) for both encoder and decoder. We insert 4-layer MLP-Mixers (Tolstikhin et al., 2021 ) in the end of the encoder (before spherification) and the beginning of the decoder. This aims to improve cross-token mixing and globalization of features without the expense of linear layers on the full flattened vector. A final RMSNorm layer (Zhang Sennrich, 2019 ) with learned affine parameters is added to each MLP-Mixer to bound the latent magnitude ( â‰¤ L \leq\sqrt{L} ). This regularization proves critical for stabilizing training, especially when there is a dramatic divergence between the decoder outputs of ğ¯ noisy \mathbf{v}_{\text{noisy}} and ğ¯ NOISY \mathbf{v}_{\text{NOISY}} . We use both RoPE (Su et al., 2024 ) positional embedding and sinusoidal absolute positional encoding. We found that removing the sinusoidal positional embedding hurts generation quality. For class-conditional generation, we implement AdaLN-Zero (Perez et al., 2018 ; Peebles Xie, 2023 ) in both the encoder and 