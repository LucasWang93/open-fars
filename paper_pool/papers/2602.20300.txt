Title: What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance

Abstract: Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.

Body: What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance 1 Introduction 2 Related Work 3 Methodology 3.1 Linguistic features 3.2 Observed risk via semantics-preserving perturbations 3.3 Ordinal risk model 3.4 Metrics and diagnostics 3.5 Robustness 4 Experimental Setup 5 Results: A Query-Feature Risk Landscape for Hallucination 5.1 Feature and Dataset Effects 5.2 Distributional Effects 5.3 Task Format Moderates Absolute Risk But Not Direction 5.4 Propensity overlap uplifts 5.5 Robustness Across Datasets 5.6 Linguistic Trends (Figure 1 ) 5.7 Correlation Clusters (Figure 6 ) 5.8 Regression-Based Associations with Risk 5.9 Findings with respect to hypotheses 5.10 Practical Applications: Risk Triage and Low-effort Rewrites 6 Conclusion A Distribution of Hallucination Across Query Type B Linguistic Features B.1 Structural Features B.2 Scenario-Based Features B.3 Lexical Features B.4 Stylistic Complexity B.5 Semantic Grounding C Reward-Weight Simplex Analysis: D Feature Calculation Methodology E Ordinal Logistic Regression Details Analysis of Results. Calibration. F Propensity and IPW Uplift Computation IPW uplift. G Prompt Templates What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance William Watson Nicole Cho 1 1 footnotemark: 1 Sumitra Ganesh Manuela Veloso J.P. Morgan AI Research nicole.cho@jpmorgan.com Equal Contribution Abstract Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query‚Äôs form can also shape a listener‚Äôs (and model‚Äôs) response. We operationalize this insight by constructing a 17-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies. What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance William Watson ‚Ä† ‚Ä† thanks: Equal Contribution Nicole Cho 1 1 footnotemark: 1 Sumitra Ganesh Manuela Veloso J.P. Morgan AI Research nicole.cho@jpmorgan.com Figure 1: Prevalence of binary linguistic features across hallucination risk categories ( Safe , Borderline , Risky ). Warmer colors indicate higher frequency. Lack of specificity , clause complexity , and polysemous words show a pronounced rise from Safe to Risky . 1 Introduction Large Language Models (LLMs) have transformed natural language processing, yet their propensity to hallucinate, producing plausible but factually incorrect outputs, remains a critical challenge, especially in high-stakes domains such as finance and law (Huang et al. , 2025 ; Dahl et al. , 2024 ; Naveed et al. , 2024 ) . The societal, financial, and legal costs of hallucinations are already evident, with multiple lawsuits emerging in response to LLM-generated errors (Milmo, 2023 ) , underscoring the impracticality of relying on users to detect such failures. While most prior work emphasizes reactive, post-generation mitigations (e.g., self-verification, logit-based detection) (Lewis et al. , 2021 ; Madaan et al. , 2023 ) and proactive, pre-generation strategies (e.g., RAG) (Lewis et al. , 2021 ; Watson et al. , 2025 ) , comparatively fewer studies take a proactive, input-side view beyond ambiguity detection (Zhang et al. , 2024 ; Kuhn et al. , 2023 ) . Drawing on classical linguistics, we define a 17-dimensional query feature vector capturing structural, lexical, stylistic, and semantic aspects known to shape human comprehension and obfuscate understanding. While a few of these features have been studied for their influence on general LLM performance (Truong et al. , 2023 ; Cho and Watson, 2025 ) , to our knowledge there is no large-scale empirical mapping from such features to hallucination behavior. Following Blevins et al. ( 2023 ) , we leverage an LLM to extract these features from 369,837 real-world queries spanning 13 QA datasets (3 scenarios, 16 configurations). Using an semantics-preserving paraphrase neighborhood with an offline Monte Carlo correctness proxy, we provide empirical evidence of strong correlations between specific linguistic markers and observed hallucination rates, yielding a consistent "risk landscape". Features that destabilize interpretation (underspecification, deep clause nesting) align with higher hallucination propensity, whereas features that tighten semantics (clear intention grounding, answerability) align with lower risk rates. Others (e.g., domain specificity) show mixed, dataset- and model-dependent effects. Surprisingly, several linguistic features traditionally known to confuse human readers (e.g., word rarity, superlatives, complex negation) show minimal association with hallucination in LLMs, suggesting that human and model failure modes need not coincide. Our contributions are threefold: ‚ñ∂ \blacktriangleright Feature taxonomy and extraction: A linguistically grounded, 17-feature representation of queries known to impact language understanding for humans. We bring this perspective to LLMs and understand whether or not these features are associated with hallucinatory behavior. ‚ñ∂ \blacktriangleright Risk landscape at scale: An empirical, distributional map derived from ordinal modeling with dataset/scenario fixed effects and ECDF separations, linking query features to hallucination propensity over 369,837 queries. ‚ñ∂ \blacktriangleright Proactive guidance: We highlight practical, feature-aware triage and low-effort rewrites that can complement reactive defenses. Therefore, we advocate a practical approach to proactively mitigate hallucinations before generation by optimizing queries‚Äîrather than relying on post hoc human inspection, which is often impractical. 2 Related Work Hallucinations in LLMs: Prior work addresses hallucinations both proactively and reactively (Ji et al. , 2023a ; Varshney et al. , 2023 ; Li et al. , 2024 ) . Proactive, input-time methods (e.g., Retrieval-Augmented Generation, external tool use) enrich the context before decoding (Lewis et al. , 2021 ; Schick et al. , 2023 ; Qin et al. , 2023 ) . Reactive, post-generation methods (e.g., self-consistency, logit-based detectors) evaluate or re-rank outputs after the model decodes a response (Wang et al. , 2023b ; Manakul et al. , 2023 ) . Pre-Generation Query Evaluation: More recent research has begun to address hallucination proactively by examining the input query itself (Ji et al. , 2023b ; Karpukhin et al. , 2020 ) . Studies have shown that query structure and semantic properties, such as polysemy, contextual nuance, and specificity, play a crucial role in shaping LLM outputs (Brown et al. , 2020 ; Jiao et al. , 2023 ) . For example, ReLA (Zhang et al. , 2021 ) demonstrates that sparse attention can improve both interpretability and performance without additional overhead. HalluciBot (Watson et al. , 2025 ) further illustrated that perturbing queries can effectively estimate hallucination likelihood. In contrast, our work systematically extracts 22 linguistic features from queries and empirically analyzes their correlation with hallucination risk. This proactive approach lays the foundation for query pre-filtering techniques aimed at enhancing the reliability of LLM outputs. 3 Methodology Ordinal ( Œ≤ \beta -only) Correlation Feature Coef SE z z -value p p -value OR œÅ \rho œÑ \tau Adj. p p p 0.05 p 0.05 Lack of Specificity 0.868 0.010 85.898 10 -5 2.382 0.271 0.256 10 -5 ‚úì Clause Complexity 0.568 0.010 57.363 10 -5 1.764 0.155 0.147 10 -5 ‚úì Negation Usage 0.311 0.016 19.499 10 -5 1.364 0.028 0.026 10 -5 ‚úì Excessive Details 0.247 0.026 9.668 10 -5 1.281 0.066 0.063 10 -5 ‚úì Anaphora Usage 0.214 0.009 23.827 10 -5 1.238 0.107 0.101 10 -5 ‚úì Polysemous Words 0.096 0.007 13.840 10 -5 1.101 0.104 0.098 10 -5 ‚úì Rare Word Usage 0.095 0.011 8.997 10 -5 1.100 0.055 0.052 10 -5 ‚úì Pragmatic Features 0.072 0.008 8.496 10 -5 1.074 0.132 0.125 10 -5 ‚úì Presupposition 0.056 0.010 5.565 10 -5 1.058 0.046 0.044 10 -5 ‚úì Contextual Constraints 0.044 0.007 5.812 10 -5 1.045 -0.081 -0.077 10 -5 ‚úì Parse Tree Height 0.011 0.005 2.312 0.021 1.011 -0.149 -0.121 10 -5 ‚úì Named Entities Present 0.009 0.007 1.269 0.205 1.009 0.003 0.002 0.115 ‚úó Domain Specificity 0.003 0.009 0.396 0.692 1.003 -0.013 -0.012 10 -5 ‚úì Query-Scenario Mismatch -0.064 0.014 -4.734 10 -5 0.938 0.153 0.145 10 -5 ‚úì Superlative Usage -0.103 0.012 -8.674 10 -5 0.902 -0.012 -0.011 10 -5 ‚úì Dependency Depth -0.128 0.005 -24.353 10 -5 0.879 -0.203 -0.159 10 -5 ‚úì Intention Grounding -0.168 0.023 -7.272 10 -5 0.846 -0.159 -0.151 10 -5 ‚úì Subjectivity -0.168 0.019 -8.885 10 -5 0.846 0.044 0.041 10 -5 ‚úì Query Token Length -0.212 0.010 -20.973 10 -5 0.809 -0.274 -0.214 10 -5 ‚úì Number of Clauses -0.262 0.009 -28.652 10 -5 0.769 -0.272 -0.228 10 -5 ‚úì Answerability -1.106 0.017 -63.425 10 -5 0.331 -0.228 -0.216 10 -5 ‚úì Table 1: Results for Observed Risk analyses. Left: Ordinal logistic regression estimates (using both binary and scaled numeric predictors. Right: Spearman‚Äôs œÅ \rho and Kendall‚Äôs œÑ \tau correlation coefficients between each feature and Observed Risk, with adjusted p p -values and a significance indicator. Features in italics (e.g., Lack of Specificity , Clause Complexity , Query Token Length , Number of Clauses , and Answerability ) highlight particularly intriguing effects. All adjusted p p -values were below 10 ‚àí 5 10^{-5} except for ‚ÄúNamed Entities Present‚Äù ( p = 0.115 p=0.115 , not significant). Problem setup. We study how the linguistic form of a user query modulates large language model reliability. Each query i i receives an ordinal triage label y i ‚àà { 0 , 1 , 2 } y_{i}\in\{0,1,2\} corresponding to Safe \,{ }\, Borderline \,{ }\, Risky . Let x i ‚àà { 0 , 1 } p x_{i}\in\{0,1\}^{p} be a binary feature vector capturing human-confusing linguistic phenomena (¬ß B ), and c i c_{i} the observed covariates (dataset d ‚Äã ( i ) d(i) , scenario s ‚Äã ( i ) s(i) ). We model Pr ‚Å° ( y i ‚à£ x i , c i ) \Pr(y_{i}\mid x_{i},c_{i}) to quantify (i) marginal effects of features, (ii) distributional shifts in predicted risk, and (iii) robustness under dataset shifts‚Äì without rewriting queries. 3.1 Linguistic features We operationalize p = 17 p{=}17 query-level features spanning ambiguity ( Lack of Specificity , Polysemous Words , Pragmatic Features ), referential structure ( Anaphora ), complexity ( Clause Complexity ), polarity ( Negation ), grounding ( Answerability , Intention Grounding , Contextual Constraints ), and others (¬ß B ). Detectors return structured outputs (label+rationale) via typed prompts; positive/negative 5-shot examples appear in App. G . Detector noise is treated as classical measurement error and expected to attenuate magnitudes rather than flip signs (Blevins et al. , 2023 ) . 3.2 Observed risk via semantics-preserving perturbations Benchmark items can be memorized, biasing raw hallucination rates (Carlini et al. , 2021 ; Nasr et al. , 2023 ; Aerni et al. , 2024 ; Watson et al. , 2025 ) . For each original query q orig q_{\text{orig}} , we generate a local semantic equivalence class ùí© ‚Äã ( q orig ) = { q 1 , ‚Ä¶ , q m } \mathcal{N}(q_{\text{orig}}){=}\{q_{1},\dots,q_{m}\} by sampling paraphrases at T = 1.0 T{=}1.0 with the instruction "Produce a semantically indifferent but lexically perturbed version of the query." We retain the first six paraphrases whose hybrid similarity meets s ‚Äã ( q orig , q i ) ‚â• 0.85 s(q_{\text{orig}},q_{i})\!\geq\!0.85 , s ‚Äã ( q orig , q i ) = Œª bi ‚ãÖ cos ‚Å° ( ùêû bi ‚Äã ( q orig ) , ùêû bi ‚Äã ( q i ) ) + Œª cross ‚ãÖ 1 2 [ Pr cross ( q orig , q i ) + Pr cross ( q i , q orig ) ] \begin{split}s(q_{\mathrm{orig}},q_{i}) =\lambda_{\mathrm{bi}}\cdot\cos\bigl(\mathbf{e}_{\mathrm{bi}}(q_{\mathrm{orig}}),\mathbf{e}_{\mathrm{bi}}(q_{i})\bigr)\\ \quad+\lambda_{\mathrm{cross}}\cdot\frac{1}{2}\Biggl[\Pr_{\mathrm{cross}}(q_{\mathrm{orig}},q_{i})\\ \qquad\qquad\qquad+\Pr_{\mathrm{cross}}(q_{i},q_{\mathrm{orig}})\Biggr]\end{split} with ( Œª bi , Œª cross ) = ( 0.6 , 0.4 ) (\lambda_{\text{bi}},\lambda_{\text{cross}})=(0.6,0.4) , ùêû bi \mathbf{e}_{\text{bi}} from Text-Embedding-3-Large (3,072-d), and Pr cross \Pr_{\text{cross}} from ms-marco-MiniLM-L6-v2 (Reimers and Gurevych, 2019 ) . Empirical hallucination estimation. For each q i ‚àà ùí© ‚Äã ( q orig ) q_{i}\!\in\!\mathcal{N}(q_{\text{orig}}) we compute a convex proxy h ^ ‚Äã ( q i ) = w 0 ‚Äã s llm + w 1 ‚Äã s fuzz + w 2 ‚Äã s bleu \hat{h}(q_{i})=w_{0}\,s_{\text{llm}}+w_{1}\,s_{\text{fuzz}}+w_{2}\,s_{\text{bleu}} , combining a binary LLM-judge decision s llm ‚àà 0 , 1 s_{\text{llm}}\!\in\!{0,1} (semantic; Wang et al. , 2023a ; Liu et al. , 2023b ; Adlakha et al. , 2024 ), fuzzy string similarity s fuzz ‚àà [ 0 , 1 ] s_{\text{fuzz}}\!\in\![0,1] (surface; Bachmann, 2024 ), and BLEU-1 s bleu ‚àà [ 0 , 1 ] s_{\text{bleu}}\!\in\![0,1] (lexical; Papineni et al. , 2002 ; Lin and Och, 2004 ; Callison-Burch et al. , 2006 ). We use ( w 0 , w 1 , w 2 ) = ( 0.6 , 0.3 , 0.1 ) (w_{0},w_{1},w_{2})=(0.6,0.3,0.1) , selected on a small human-labeled set by sweeping the ( w 0 ‚Ä≤ , w 1 ‚Ä≤ , w 2 ‚Ä≤ ) (w_{0}^{\prime},w_{1}^{\prime},w_{2}^{\prime}) simplex; the ROC‚ÄìAUC surface is flat for w 0 ¬± 0.2 w_{0}\pm 0.2 , drops quickly with larger w 1 w_{1} , and is worst for BLEU-only, placing our mix on a Pareto plateau (App. C , Fig. 8 ). A perturbation counts as hallucinated if h ^ ‚Äã ( q i ) 0.5 \hat{h}(q_{i}) 0.5 . Aggregating across the six paraphrases yields query-level categories: Safe (0/6), Borderline (1‚Äì3/6), Risky (4‚Äì6/6). Figure 2: ECDFs of predicted P ‚Äã ( Risky ) P(\text{Risky}) for Present vs. Absent (top six by KS). Shaded regions indicate dominance; inset shows KS and Œî \Delta median. Lack of Specificity, Excessive Details, Clause Complexity, and Query‚ÄìScenario Mismatch shift mass toward higher risk; Answerability and Intention Grounding shift mass lower. 3.3 Ordinal risk model We fit a proportional-odds (cumulative logit) model log ‚Å° Pr ‚Å° ( Y i ‚â§ k | x i , c i ) Pr ‚Å° ( Y i k | x i , c i ) = œÑ k ‚àí Œ∑ i \log\frac{\Pr(Y_{i}\leq k\,|\,x_{i},c_{i})}{\Pr(Y_{i} k\,|\,x_{i},c_{i})}=\tau_{k}-\eta_{i} (1) with k ‚àà 0 , 1 k\in{0,1} , linear predictor Œ∑ i = x i ‚ä§ ‚Äã Œ≤ + Œ± d ‚Äã ( i ) + Œ≥ s ‚Äã ( i ) \eta_{i}=x_{i}^{\top}\beta+\alpha_{d(i)}+\gamma_{s(i)} and ordered cutpoints œÑ 0 œÑ 1 \tau_{0}\! \!\tau_{1} . Clas