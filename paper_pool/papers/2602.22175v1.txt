Title: DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs

Abstract: Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO, a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads--a subset of attention heads specialized for long-context retrieval--to identify task-relevant tokens at each decoding step and explicitly up-weight them. By doing so, DySCO dynamically adjusts attention during generation to better utilize relevant context. The method is training-free and can be applied directly to any off-the-shelf LMs. Across multiple instruction-tuned and reasoning models, DySCO consistently improves performance on challenging long-context reasoning benchmarks, yielding relative gains of up to 25% on MRCR and LongBenchV2 at 128K context length with modest additional compute. Further analysis highlights the importance of both dynamic attention rescaling and retrieval-head-guided selection for the effectiveness of the method, while providing interpretability insights into decoding-time attention behavior. Our code is available at https://github.com/princeton-pli/DySCO.

Body: DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs 1 Introduction 2 Background and Motivation 2.1 Preliminaries: Retrieval Heads Retrieval Heads. Query-Focused Retrieval Heads ( QRHead ). 2.2 Retrieval Heads Stay Focused on Relevant Context Diagnostic task: Path Traversal . Severe performance degradation on Path Traversal . Behavior of retrieval heads. Steering overall attention with retrieval heads. 3 DySCO : Dynamic Attention Scaling Overview 3.1 The DySCO Algorithm Aggregation. Selection. Rescaling. 3.2 Efficient Implementation Early stopping of attention aggregation. Overhead analysis. 4 Experiments 4.1 Models and Baselines Models. Head selection. Baselines. Setting parameters for DySCO and UniAttnS . 4.2 Diagnostic Test on Path Traversal 4.3 Long Context Reasoning Tasks (with CoT) Datasets. Settings. Results. 4.4 Long Context Recall Tasks Datasets. Results. 4.5 Comparison with RAG and Prompt Compression Results. 5 Analysis Ablations 5.1 Impact of CoT Reasoning and Context Length Interplay between DySCO and CoT reasoning. Impact of context length. 5.2 Ablations Ablation: Importance of head selection. Ablation: Importance of dynamic rescaling. Hyperparameter robustness. 6 Related Work Improving long-context LMs. Long-context inference techniques. Scaffolding and external systems for long context. 7 Conclusion A Additional Details on Query-Focused Retrieval Heads ( QRHead ). B Parameter Selection for DySCO and UniAttnS B.1 Choosing Parameters for DySCO Hyperparameters. B.2 Choosing Parameters for UniAttnS C Robustness to Hyperparameter Variations D Detailed Setup for Retrieval-Augmented Generation and Prompt Compression Setup. E Prompt Example for Path Traversal Task DySCO : Dynamic Attention-Scaling Decoding for Long-Context LMs Xi Ye Wuwei Zhang Fangcong Yin Howard Yen Danqi Chen Abstract Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO , a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads ‚Äîa subset of attention heads specialized for long-context retrieval‚Äîto identify task-relevant tokens at each decoding step and explicitly up-weight them. By doing so, DySCO dynamically adjusts attention during generation to better utilize relevant context. The method is training-free and can be applied directly to any off-the-shelf LMs. Across multiple instruction-tuned and reasoning models, DySCO consistently improves performance on challenging long-context reasoning benchmarks, yielding relative gains of up to 25% on MRCR and LongBenchV2 at 128K context length with modest additional compute. Further analysis highlights the importance of both dynamic attention rescaling and retrieval-head-guided selection for the effectiveness of the method, while providing interpretability insights into decoding-time attention behavior. Our code is available at https://github.com/princeton-pli/DySCO . Machine Learning, ICML 1 Introduction Recent advances in language models (LMs) have enabled processing of extremely long context windows, unlocking applications such as repository-level code understanding and long-document question answering. Driven by improvements in data curation and transformer architectures, modern LMs now support context lengths of 128K tokens and beyond (Gemini Team, 2025 ; Yang et al., 2025 ; Kimi Team, 2025 ; OpenAI, 2025 ; Anthropic, 2026 ) . However, model performance often degrades significantly as input length increases, even on simple tasks, commonly known as ‚Äúcontext rot‚Äù (Hong et al., 2025 ; Goldman et al., 2024 ) . As illustrated in Figure 1 , even on a simple Path Traversal task (Ye et al., 2025 ) , models such as Qwen3-8B and Llama-3.1-8B see accuracy drop from around 60% at 4K tokens to below 20% at 16K tokens, despite supporting context lengths of up to 128K tokens. Figure 1 : Top: An illustrative Path Traversal task (simplified). Solving the task requires dynamically locating relevant context during decoding. Bottom: Accuracy as a function of context length for models with and without DySCO . Despite the total context being only 16K tokens, both models exhibit severe performance degradation as context length increases. Figure 2 : Overview of DySCO algorithm. At each decoding step, DySCO consists of three stages: (1) Aggregation : We run a partial forward pass over the input sequence to obtain attentions of retrieval heads, such as QRHead , and use them to assign relevance scores to context tokens; (2) Selection : We use the relevance scores to select the important tokens; (3) Rescaling : We up-weight the important tokens by intervening attention logits of all attention heads and run a full forward pass to sample the next token. We attribute this degradation to a key challenge in how LMs utilize long and information-dense contexts during genera- tion: effective long-context reasoning requires models to continuously focus attention on the most task-relevant parts of the context as the generation state evolves. In practice, however, vanilla decoding often fails to exhibit this behavior over long contexts. For example, in the Path Traversal task shown in Figure 1 , LMs must iteratively identify the next edge from the context based on the current node. Our analysis (¬ß 2.2 ) shows that attention is often insufficiently focused on the relevant context at each generation step, leading to errors even when the required information is present. In this work, we propose a novel decoding algorithm, DySCO ( Dy namic Attention- S caling De CO ding), which dynamically adjusts attention on the fly during generation. DySCO is lightweight, training-free, and directly applicable to off-the-shelf language models. DySCO operates exclusively at the decoding stage, after the long-context prefilling phase that accounts for the majority of computation. As a result, it introduces only modest additional overhead‚Äîfor example, approximately 4% extra FLOPs when generating 8K tokens from 128K-token inputs. The key idea of DySCO is to identify relevant tokens at each decoding step using retrieval heads (Wu et al., 2025a ; Zhang et al., 2025c ) , and then upweight attention to these tokens during generation. Retrieval heads are a specialized subset (1-2%) of attention heads that are responsible for long-context retrieval: compared to other heads, they assign higher and more stable attention to context that is critical for next-token prediction. Notably, we find that retrieval heads remain consistently focused on relevant tokens even when overall model performance degrades with increasing context length (¬ß 2.2 ). After prefilling, DySCO operates in three stages at each decoding step (Figure 2 ). 1) aggregation : we run a partial forward pass (¬ß 3.1 ) to aggregate attention scores from retrieval heads; 2) selection : we identify a small set of context tokens with the highest aggregated attention scores; 3) rescaling : we upweight attention to the selected tokens by intervening on the attention logits of all heads and run a full forward pass to generate the next token. DySCO introduces a new paradigm for attention shaping to improve long-context reasoning. Prior work on attention scaling focuses on long-context extension by uniformly scaling all attention logits by a constant factor (Peng et al., 2024 ; Chen et al., 2026 ; Nakanishi, 2025 ) ; in contrast, DySCO performs dynamic, token-selective scaling at each decoding step, yielding substantial gains even within the model‚Äôs native context window. More broadly, most prior long-context inference methods, such as KV-cache eviction and compression (Xiao et al., 2024 ; Li et al., 2024a ; Bhaskar et al., 2025 ) , prioritize efficiency and may degrade reasoning performance, while DySCO explicitly trades a small amount of additional compute for improved accuracy. Unlike higher-level scaffolding approaches such as agentic pipelines or external context-management systems (Zhang et al., 2025a ; Yu et al., 2025 ; Wu et al., 2025b ) , DySCO operates directly at the attention level, is independent of any specific scaffold, and can be naturally combined with such systems. We evaluate DySCO across a diverse set of models, including both instruction-tuned and reasoning LMs, on a wide range of long-context tasks. DySCO consistently improves performance on challenging long-context reasoning benchmarks. Notably, for Qwen3-8B, DySCO delivers up to 25% relative improvements on MRCR and LongBenchV2 compared to YaRN alone, with ‚â§ \leq 4% additional FLOPs. Further analysis highlights the importance of both dynamic scaling and the use of retrieval heads for identifying important tokens. We summarize our contributions: ‚Ä¢ We explore a new axis for long-context inference methods: improving accuracy rather than efficiency. ‚Ä¢ We propose DySCO , a training-free decoding algorithm that yields strong gains on long-context reasoning tasks. ‚Ä¢ We provide mechanistic insights into how retrieval heads can help mitigate long-context failures. 2 Background and Motivation We first set up the preliminaries of retrieval heads (Wu et al., 2025a ; Zhang et al., 2025c ) , an important building block of our approach (¬ß 2.1 ). We then study Path Traversal , a synthetic task designed to stress-test basic reasoning over long and information-dense context, and use it to reveal the connection between retrieval heads and long-context reasoning capabilities (¬ß 2.2 ). 2.1 Preliminaries: Retrieval Heads We consider an autoregressive transformer LM ‚Ñ≥ \mathcal{M} (Vaswani et al., 2017 ) that generates the next token x t + 1 x_{t+1} conditioned on the prefix x ‚â§ t x_{\leq t} , which includes both the input context and previously generated tokens. Let ‚Ñã = { h i } \mathcal{H}=\{h_{i}\} denote the set of attention heads in ‚Ñ≥ \mathcal{M} . At decoding step t t , each head h ‚àà ‚Ñã h\in\mathcal{H} produces an attention distribution: ùú∂ t ( h ) ‚àà ‚Ñù t \boldsymbol{\alpha}_{t}^{(h)}\in\mathbb{R}^{t} , where Œ± t , i ( h ) \alpha_{t,i}^{(h)} is the attention mass for token x i ‚àà ùíô ‚â§ t x_{i}\in\boldsymbol{x}_{\leq t} . Retrieval Heads. Wu et al. ( 2025a ) discovered a universal set of attention heads that exhibit copy-like behavior during decoding, which they term retrieval heads . Concretely, when generating token x t x_{t} , a retrieval head h h concentrates its attention on a prior occurrence of the same token in the context. That is, Œ± t , i ( h ) \alpha^{(h)}_{t,i} is high for some i t i t such that x i = x t x_{i}=x_{t} . These heads are typically sparse (less than 5% of all heads) and provide a mechanistic explanation for how language models perform explicit token lookup and copy-paste from long context. Query-Focused Retrieval Heads ( QRHead ). Retrieval heads can be identified using different criteria: Wu et al. ( 2025a ) rely on copy-paste behavior measured in synthetic retrieval tasks (e.g., Needle-in-a-Haystack). Zhang et al. ( 2025c ) proposed QRHead , an improved approach that instead aggregates query-context attention mass across examples from realistic long-context tasks (e.g., long-form question answering). This yields a more semantic and task-aligned selection, which Zhang et al. ( 2025c ) show to be more effective for long-context reasoning. We therefore adopt QRHead in this work; see Appendix A for details. 2.2 Retrieval Heads Stay Focused on Relevant Context Diagnostic task: Path Traversal . Long-context reasoning requires dynamically attending to relevant information at each decoding step, conditioned not only on the inputs but also on the intermediate state encoded in x ‚â§ t x_{\leq t} . To analyze this capability in a controlled setting, we use a synthetic task, Path Traversal (Ye et al., 2025 ) . The input to Path Traversal consists of a long list of graph edges E = { ‚ü® v i , v j ‚ü© } E=\{\langle v_{i},v_{j}\rangle\} between nodes. The task is to find a path ùíØ = ( ‚ü® v start , v 1 ‚ü© ‚Äã ‚Ä¶ , ‚ü® v t ‚àí 1 , v target ‚ü© ) \mathcal{T}=(\langle v_{\text{start}},v_{1}\rangle\,\ldots,\langle v_{t-1},v_{\text{target}}\rangle) connecting a given start node to a target node. Crucially, the graph is designed in a way that each node along the gold path has exactly one outgoing edge. As a result, solving the task reduces to repeatedly identifying the next correct outgoing node and chaining them together. This constraint is explicitly stated in the prompt, ensuring a deterministic reasoning procedure. By varying the number of nodes, we can control the input length and construct tasks with different context sizes. See Appendix E for a full prompt example. Severe performance degradation on Path Traversal . Specifically, we generate instances with approximately 4, 8, 16, and 32K tokens, corresponding to roughly 250, 500, 1000, and 2000 edges, respectively. LMs are required to find a path of 5 nodes (four edges). As shown in Figure 3 (left), although the task is structurally simple, model performance degrades rapidly as context length increases to 32K, well below the claimed context size, with step-level accuracy (correctness of each ‚ü® v t ‚àí 1 , v t ‚ü© \langle v_{t-1},v_{t}\rangle along the path) dropping from near-perfect to approximately 20%. Path Traversal thus isolates a central challenge of long-context reasoning: the need for repeated dynamic key-context lookup during decoding. Behavior of retrieval heads. We further analyze the behavior of retrieval heads, which provides insight into LMs‚Äô failure modes on long-context reasoning tasks. We partition the context into edge-level spans and compute the sum of attention mass assigned by QRHead to each span at every decoding step. Our analysis focuses on two metrics: 1) the fraction of decoding steps for which the gold edge (i.e., the next edge on the correct path) appears among the top 5% most-attended spans (Gold Edge in Top 5%). 2) the total attention mass assigned to the gold edge at each decoding step (Gold Attention). For this analysis (Figure 3 , middle and right), we only consider the second step (edge) among the four steps to explicitly study the attention dynamics occurring in the middle of the reasoning process. Figure 3 : Left : Performance of Qwen3-8B on Path Traversal as context length increases. Middle : Fractions that the gold edge appear among the top-5% edges ranked by attention score (sum of attention over all tokens in the span) from QRHead versus random heads. Right : Attention mass assigned to gold edges by QRHead and random heads. Despite severe performance degradation and a reduction in attention mass on gold edges, QRHead consistently allocates substantially higher attention to the gold edges. We additionally include randomly selected attention heads as a baseline for comparison. As shown in Figure 3 (middle), retrieval head r