Title: JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments

Abstract: Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D perception, relying on RGB video and monaural audio. This design choice introduces a fundamental dimensionality mismatch that precludes reliable source localization and spatial reasoning in complex 3D environments. We address this limitation by presenting JAEGER, a framework that extends AV-LLMs to 3D space, to enable joint spatial grounding and reasoning through the integration of RGB-D observations and multi-channel first-order ambisonics. A core contribution of our work is the neural intensity vector (Neural IV), a learned spatial audio representation that encodes robust directional cues to enhance direction-of-arrival estimation, even in adverse acoustic scenarios with overlapping sources. To facilitate large-scale training and systematic evaluation, we propose SpatialSceneQA, a benchmark of 61k instruction-tuning samples curated from simulated physical environments. Extensive experiments demonstrate that our approach consistently surpasses 2D-centric baselines across diverse spatial perception and reasoning tasks, underscoring the necessity of explicit 3D modelling for advancing AI in physical environments. Our source code, pre-trained model checkpoints and datasets will be released upon acceptance.

Body: JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments 1 Introduction 2 Related Work 2.1 Spatial Audio Understanding 2.2 Visual 3D Object Grounding 2.3 AV-LLMs for 3D Understanding 3 SpatialSceneQA 3.1 Data Simulation Pipeline 3.2 Creation of QA Dataset 4 Method 4.1 Intensity Vector for Audio Localization 4.2 Neural Intensity Vector 4.3 3D-aware Visual Encoding 4.4 Overall Model Architecture 5 Experiments 5.1 Implementation Details Model Specifications. Data and Training Specifications. 5.2 Main Results 5.3 Ablation Studies 6 Conclusion A Diversity Analysis of Speaker Point Clouds B Coordinate System Definition JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments Zhan Liu Changli Tang Yuxin Wang Zhiyuan Zhu Youjun Chen Yiwen Shao Tianzi Wang Lei Ke Zengrui Jin Chao Zhang Abstract Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D perception, relying on RGB video and monaural audio. This design choice introduces a fundamental dimensionality mismatch that precludes reliable source localization and spatial reasoning in complex 3D environments. We address this limitation by presenting JAEGER, a framework that extends AV-LLMs to 3D space, to enable joint spatial grounding and reasoning through the integration of RGB-D observations and multi-channel first-order ambisonics. A core contribution of our work is the neural intensity vector (Neural IV), a learned spatial audio representation that encodes robust directional cues to enhance direction-of-arrival estimation, even in adverse acoustic scenarios with overlapping sources. To facilitate large-scale training and systematic evaluation, we propose SpatialSceneQA, a benchmark of 61k instruction-tuning samples curated from simulated physical environments. Extensive experiments demonstrate that our approach consistently surpasses 2D-centric baselines across diverse spatial perception and reasoning tasks, underscoring the necessity of explicit 3D modelling for advancing AI in physical environments. Our source code, pre-trained model checkpoints and datasets will be released upon acceptance. Machine Learning, ICML 1 Introduction Despite the rapid evolution of audio-visual large language models (AV-LLMs), most existing systems continue to rely on RGB video and monaural audio (Xu et al. , 2025a , b ; Tang et al. , 2025 ; Cheng et al. , 2024 ) , a design choice that fundamentally constrains their ability to perceive and reason about the three-dimensional (3D) physical world. Although 3D grounding has recently attracted significant attention, current research remains fragmented, with most approaches addressing spatial cues from vision or audio in isolation. In the visual domain, recent advances extend a 2D visual LLM by incorporating RGB-D inputs and chain-of-thought reasoning, leading to improved 3D grounding and spatial relationship understanding (Wang et al. , 2025 ) . In parallel, auditory approaches leverage binaural spatial encoders (Zheng et al. , 2024 ) or intensity-vector representations (Tang et al. , 2024 ) to enable relative sound source localization. However, a unified paradigm for comprehensive 3D audio-visual scene understanding remains largely unexplored. Early multimodal efforts, such as Hear You Are (Ryu et al. , 2025 ) , pair spatial audio with panoramic RGB image and assume a single active source per scene, which precludes evaluating overlap-robust localization and depth-aware 3D grounding. Similarly, while SAVVY (Chen et al. , 2025 ) integrates RGB-D perception with multi-channel audio, it relies on a cascaded pipeline that depends on traditional signal processing for sound source localization. This modular design hinders end-to-end learning and prevents the AV-LLM from performing fully integrated spatial reasoning, underscoring the need for a more unified, learned approach. In this paper, we introduce JAEGER, a j oint 3D a udio-visual r e asoning and g rounding method in simulated physical e nvi r onments, an end-to-end framework that extends 2D audio-visual large language models (AV-LLMs) to 3D settings by explicitly modeling visual depth and multi-channel spatial audio. Initialized from Qwen2.5-Omni (Xu et al. , 2025a ) and efficiently adapted via low-rank adaptation (LoRA) (Hu et al. , 2022 ) , JAEGER integrates an RGB-D visual stream augmented with depth-projected 3D positional encodings together with a dual-path audio stream, in which semantic content extracted from the omnidirectional first-order ambisonics (FOA) channel is disentangled from spatial directional cues. To further enhance azimuth perception in reverberant environments and under overlapping sound sources, we propose a novel neural intensity vector (Neural IV) approach, which is a learnable FOA-based representation that encodes robust directional information for improved sound localization and cross-modal alignment. To support instruction tuning and systematic evaluation, we construct SpatialSceneQA, a benchmark comprising 61k high-fidelity RGB-D scenes paired with spatial audio and fine-grained 3D annotations. The dataset covers a diverse set of tasks, including single- and overlapping-source direction-of-arrival (DoA) estimation, 3D visual grounding of sound-emitting objects, and multi-speaker audio-visual matching. Experimental results show that JAEGER achieves a median angular error (MAE) of 2.21° for single sources and 13.13° under overlapping conditions, while showing strong generalization across varied source configurations. Leveraging explicit depth cues, the model attains a 3D intersection over union (IoU) of 0.32 with a median localization error of 0.16 meter (m). When FOA-based spatial cues are jointly modeled with RGB-D perception, JAEGER further achieves 99.2% accuracy on joint audio-visual reasoning in simulated multi-speaker physical environments. In summary, our main contributions are as follows: • We introduce joint grounding and reasoning with spatial audio and RGB-D geometry, which exposes a fundamental modality gap for current AV-LLMs even after fine-tuning. We adapt an AV-LLM with depth-aware visual encoding and FOA spatial cues for end-to-end DoA estimation, 3D box grounding, and multi-speaker matching. • We present SpatialSceneQA , a 61k-sample dataset that pairs 3D RGB-D images with 3D 4-channel FOA audio and dense object-level spatial annotations. To the best of our knowledge, it is the first spatial audio-visual benchmark with degree-level azimuth and elevation supervision, enabling precise localization, 3D grounding, and multi-speaker matching under source overlap. • We propose the Neural Intensity Vector , a learnable FOA spatial representation that replaces STFT-based intensity features with a neural encoder, yielding more stable azimuth cues under reverberation and overlapping sources and improving cross-scenario generalization. Figure 1 : Overview of the SpatialSceneQA 61k dataset. . Left: Example question-answer pairs demonstrating diverse spatial tasks, including sound source localization (azimuth/elevation), visual grounding (bounding boxes), and overlapping sound source identification. Right: The data synthesis pipeline leveraging Habitat-Sim and SoundSpaces 2.0 . The process consists of four stages: (1) selecting an HM3D scene, (2) sampling random source and receiver poses, (3) inserting 3D visual sound sources (e.g., speakers generated by Hunyuan3D-1.0), and (4) exporting synchronized RGB-D frames, FOA audio, and semantic and camera intrinsic/extrinsic metadata. 2 Related Work 2.1 Spatial Audio Understanding Spatial audio understanding seeks to infer the spatial configuration of acoustic events from multi-channel recordings, including binaural signals, FOA, and microphone arrays. Early work has primarily focused on sound event localization and detection (Adavanne et al. , 2018 ; Cao et al. , 2021 ; Shimada et al. , 2021 , 2022 ) , where models jointly estimate event activity and coarse spatial attributes. Recent studies have begun to integrate spatial audio perception with large language models (LLMs), enabling instruction following and higher-level reasoning grounded in spatial acoustic cues. A major design choice is the spatial representation. Binaural modeling leverages interaural time and level differences and head-related transfer functions (HRTFs) induced cues, and recent geometry-aware encoders improve the use of such signals (Zheng et al. , 2024 ; Biswas et al. , 2025 ) . However, binaural cues are tightly coupled to recording geometry and the underlying HRTFs, which hinders generalization across devices. FOA has a hardware-agnostic alternative by encoding spatial information in the B-format. (Devnani et al. , 2024 ) aligns FOA-based spatial embeddings with semantics via contrastive learning, while (Tang et al. , 2024 ) injects intensity vector cues into an auditory LLM to support localization and localization-conditioned speech processing. Progress in this area is further constrained by data availability. Real-world multi-channel datasets with dense spatial annotations remain scarce, with STARSS23 (Shimada et al. , 2023 ) serving as a representative benchmark. However, STARSS23 pairs multi-channel audio with RGB-only paranomic video, and does not provide depth aligned to the visual stream. This limitation has motivated simulation-driven data generation, where configurable simulators such as SoundSpaces 2.0 (Chen et al. , 2022 ) enable scalable synthesis of reverberant or anechoic audio-visual data with controllable scene geometry and precise ground-truth spatial annotations. 2.2 Visual 3D Object Grounding Three-dimensional visual grounding and spatial reasoning have become increasingly important for modern vision–language models (Huang et al. , 2024 ; Chen et al. , 2024b ; Bai et al. , 2025 ; Guo et al. , 2025 ; Zhu et al. , 2025b ) and AV-LLMs (Senocak et al. , 2023 ; Chowdhury et al. , 2024 ; Li et al. , 2024 ) , as many downstream tasks demand object-level localization and relational reasoning beyond two-dimensional captioning. Existing approaches primarily differ along two dimensions: (i) how 3D geometric information is incorporated and (ii) whether the model natively predicts 3D outputs. A first line of work conditions models on explicit 3D structure, such as point clouds or object-centric 3D tokens, to impose direct geometric constraints (Huang et al. , 2024 ; Mao et al. , 2025 ) . An alternative relies on multi-view video or RGB-D streams as implicit 3D supervision (Qi et al. , 2025 ; Zheng et al. , 2025 ; Chen et al. , 2024a ; Zhu et al. , 2025a ) . (Zheng et al. , 2025 ) projects depth into global coordinates and injects 3D positional encodings into video representations, improving spatial awareness through geometry-aligned tokens. Similarly, RGB-D-based approaches incorporate depth-derived cues into visual tokens to strengthen geometric grounding (Chen et al. , 2024a ; Zhu et al. , 2025a ) . Several approaches rely on auxiliary components to obtain reliable 3D signals, such as external 3D segmenters for candidate proposal generation or specialized decoders for 3D grounding (Zheng et al. , 2025 ; Zhu et al. , 2025a ) . More recently, N3D-VLM (Wang et al. , 2025 ) advances a more native paradigm by explicitly reasoning about and directly predicting 3D bounding boxes within the model. 2.3 AV-LLMs for 3D Understanding Recent AV-LLMs show strong multimodal understanding (Cheng et al. , 2024 ; Xu et al. , 2025a , b ; Tang et al. , 2025 ) , but are largely trained on RGB video and monaural audio, leaving spatial structure and directional acoustics implicit. This limitation motivates a line of work that augments AV-LLMs with explicit spatial cues for precise localization and geometric reasoning. One representative approach builds modular pipelines that estimate geometry and spatial acoustics using specialized components, while delegating higher-level reasoning to an LLM. (Chen et al. , 2025 ) introduces a benchmark for dynamic 3D audio-visual spatial reasoning with synchronized spatial audio and proposes a training-free pipeline that estimates object trajectories and constructs a global spatial map to answer queries. Complementary simulation-based efforts, such as Hear You Are (Ryu et al. , 2025 ) , exploit panoramic imagery and binaural audio cues for audio-visual spatial reasoning, but continue to rely on RGB-only visual inputs, leaving explicit depth information unmodeled. In contrast, our work adapts an existing AV-LLM to directly perceive and reason about spatial cues by incorporating RGB-D observations and FOA-based multi-channel audio in an end-to-end manner. We introduce a learned spatial audio representation, the Neural IV, and construct a large-scale instruction-tuning dataset, SpatialSceneQA, to jointly support spatial perception, grounding, and language-based reasoning within a unified framework. 3 SpatialSceneQA Training a 3D AV-LLM demands large-scale supervision with tightly synchronized (i) metric geometry (RGB-D with camera calibration), (ii) multi-channel spatial audio, and (iii) 3D object-level annotations in a consistent reference frame. To this end, we introduce SpatialSceneQA 61K, a synthetically generated dataset for high-fidelity 3D audio-visual instruction tuning. Each sample contains synchronized RGB-D renderings, 4-channel FOA, and precise 3D annotations, enabling supervision for both spatial perception and audio-visual spatial reasoning. Table 1 : Statistics of the SpatialSceneQA 61K dataset. Tasks are grouped into “Perception” and “Reasoning categories. #Sources denotes active audio sources for A–B and inserted loudspeaker assets for C–E. Regarding definitions of different types of tasks, A–B estimate source azimuth and elevation; C predicts the precise Bounding Box of the inserted speaker; D–E identify the correspondence between a target audio speaker and a visual loudspeaker based on spatial audio-visual cues. Task Type #Sources #Samples Question Answer Example Perception A: Single-Source Audio DoA 1 32K Q: Based on the audio cues, please output the precise azimuth and elevation. A: azimuth: -7; elevation: -22 B: Overlap-Source Audio DoA 2 30K Q: Where is the female voice coming from? Output azimuth and elevation. A: azimuth: 28; elevation: -15 C: Visual Grounding 1 17K Q: Identify the 3D box for the speaker in this scene. A: bbox_0 = Bbox ​ ( speaker , 0.14 , − 0.48 , − 1.15 , 0.33 , 0.88 , 0.32 ) \texttt{bbox\_0}=\texttt{Bbox}(\text{speaker},0.14,-0.48,-1.15,0.33,0.88,0.32) 2 34K 3 0 9K Reasoning D: Single-Source Multi-speakers Matching 2 10K Q: Determine which speaker corresponds to the audio source. 3 0 4K A: Left E: Overlap-Source Multi-speakers Matching 2 24K Q: Can you tell which speaker the male voice originates from? 3 0 5K A: Center 3.1 Data Simulation Pipeline We synthesize SpatialSceneQA using SoundSpaces 2.0 (Chen et al. , 2022 ) , which supports continuous, geometry-aware acoustic rendering 