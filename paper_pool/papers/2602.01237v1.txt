Title: Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models

Abstract: Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.

Body: Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models 1 Introduction 2 Related Work 2.1 Chain-of-Thought and Meta-Decoding 2.2 Batch Scheduling and Token Reordering 2.3 Adaptive Compute Within Transformers 2.4 Per-Query Token-Budget Prediction and Greedy Allocation 2.5 Curriculum Learning and Difficulty-Aware Batching 2.6 Our Contribution 3 Methods 3.1 Dataset Preprocessing and Experimental Setup 3.1.1 Data Processing 3.1.2 Training Data Generation 3.1.3 Data Splits and Validation 3.2 Early Stopping Prediction 3.2.1 MLP-based Early Stopping Predictors Architecture and Training. 3.2.2 LoRA Fine-tuned Model for Early Stopping Prediction Architecture and Implementation. Prediction Pipeline. Training Details. 3.3 Predicting Problem Difficulty for Token Budget Allocation 3.3.1 Few-Shot Classification with LLMs Implementation. 3.3.2 LoRA Fine-tuned Classification Model Architecture and Implementation. Training Methodology. 3.3.3 Greedy Algorithm 3.4 Difficulty-Based Token Budget Allocation 4 Results 4.1 Early Stopping Prediction Results 4.1.1 Layer-wise Analysis of Hidden State Features 4.1.2 Linguistic Features and Reasoning Complexity 4.1.3 Token Budget Allocation Using Early Stopping Predictions 4.2 Discrete Difficulty Classification Results 4.2.1 Validation of Difficulty-Based Categorization 4.2.2 Comparative Analysis of Difficulty Classification Models 4.2.3 Difficulty-Based Token Allocation Performance 4.3 Size vs. Difficulty: Which Prediction Framework Yields Better Allocation? 5 Conclusion 6 Appendix 6.1 Answer Correctness Evaluation 6.2 MLP Architecture and Hyperparameter Search 6.3 Runtime and Hardware 6.4 Few-Shot Classification of Question Difficulty Prompt Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models Katrina Brown Harvard College katrinabrown@college.harvard.edu Equal contribution. Aneesh Muppidi 1 1 footnotemark: 1 Harvard College aneeshmuppidi@college.harvard.edu Rana Shahout Harvard SEAS rana@seas.harvard.edu (April 2025) Abstract Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors—an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text—to estimate each query’s optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12–17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute–accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments. Project Website Code: https://aneeshers.github.io/predictive-scheduling/ 1 Introduction Modern large language models (LLMs) deliver exceptional chain-of-thought (CoT) reasoning capabilities, powering applications from real-time code autocomplete to interactive tutoring and decision support. Yet inference-time costs remain a critical bottleneck: applying a fixed token budget per query either wastes tokens on trivial inputs or starves hard ones, leading to unnecessary latency and inflated cloud bills—key concerns for production LLM services and on-device deployments. We first ask the problem of per-query budget heterogeneity by asking: can we predict, before any generation, how many tokens each reasoning trace needs, or the difficulty of each query, in order to guide budget allocation? Prior approaches either rely on few-shot heuristics to guess a single optimal trace length (Han et al. , 2024 ) , perform single-checkpoint early-termination checks during generation (Li et al. , 2024 ; Fu et al. , 2024a ) , or schedule batches based on surface-level signals such as queue length or past runtimes (Liu and others, 2023 ; Damani et al. , 2024 ) . None leverage the rich internal features of transformer hidden states to learn fine-grained, pre-run estimates of required trace length and query difficulty, nor integrate these estimates into a global batch allocator under a fixed total token budget. Concretely, we present five contributions 1. We introduce 2 lightweight predictors that map intermediate transformer hidden states to an estimate of the token length required for a reasoning trace to achieve correctness. 2. We develop a lightweight difficulty classifier —using both few-shot prompting and LoRA fine-tuning—to categorize queries as easy, medium, or hard before generation. 3. We perform a systematic layer-wise analysis, revealing that middle transformer layers (12–17) carry the strongest predictive signal for reasoning-length estimation. 4. We design and implement a greedy batch allocation algorithm that dynamically assigns per-trace token budgets to maximize expected accuracy gains under a fixed total budget. 5. We demonstrate on the GSM8K arithmetic reasoning benchmark that our combined predictive scheduling yields up to 7.9% absolute accuracy improvement at equal token cost relative to nonadaptive scheduling, closing over half the gap to an oracle with perfect size and difficulty estimates. Our predictive scheduling framework offers a practical plug-in method for latency- and cost-sensitive LLM deployments, showing that pre-run budget and difficulty prediction can substantially improve inference efficiency without any modifications to the underlying language model. 2 Related Work 2.1 Chain-of-Thought and Meta-Decoding Large language models improve multi-step reasoning by generating intermediate “thought” steps. Chain-of-Thought prompting decomposes problems into sub-steps to boost accuracy on arithmetic and logical tasks (Wei and others, 2022 ) , while self-consistency decoding aggregates multiple sampled chains to reduce variance and error (Wang and others, 2022 ) . Tree-of-Thoughts further explores a tree of partial solutions to guide search (Yao and others, 2023 ) . These methods yield strong gains but assume a fixed chain length or uniform budget per query, incurring substantial extra computation. 2.2 Batch Scheduling and Token Reordering When serving mixed-difficulty requests, naive FIFO or uniform batching can suffer from head-of-line blocking and wasted tokens on easy queries. The vLLM scheduler predicts remaining tokens to reorder and preempt queries for better throughput (Liu and others, 2023 ) , and related pre-scheduling techniques reduce latency in multi-tenant serving (Zhang and others, 2021 ) . Dynasor dynamically allocates extra compute to queries deemed “hard” at a fixed checkpoint in the reasoning process (Fu et al. , 2024a ) . These external schedulers improve average latency but rely on surface-level heuristics or single-checkpoint decisions rather than learned per-query estimates of needed reasoning length. 2.3 Adaptive Compute Within Transformers A parallel line of work adapts internal model computation based on input difficulty. Depth-adaptive Transformers vary the number of layers executed per input (Elbayad and others, 2020 ) , and Mixture-of-Experts architectures route tokens through a subset of expert sub-modules to save compute (Fedus and others, 2021 ) . These approaches adjust per-input compute “on the fly” but do not provide pre-run, per-query length estimates or batch-level scheduling under a fixed token budget. 2.4 Per-Query Token-Budget Prediction and Greedy Allocation Estimating each query’s optimal reasoning length in advance can eliminate wasted tokens and avoid under-reasoning. Wu et al. analyze how accuracy first increases then plateaus or degrades as chain length grows, demonstrating an optimal CoT length exists per problem (Wu et al. , 2025 ) . TALE uses few-shot prompts and small post-trained models to predict the optimal token budget for each query (Han et al. , 2024 ) . Fu et al. train a length-ordering predictor via learning-to-rank to sort requests by relative size before batching (Fu et al. , 2024b ) . Li et al. introduce a learned early-stopping criterion that decides, during generation, whether additional chains are likely to improve correctness (Li et al. , 2024 ) . Damani et al. train lightweight predictors (MLP or LoRA) to estimate the marginal reward of adding more reasoning traces and employ a greedy algorithm to allocate the number of traces under a fixed budget (Damani et al. , 2024 ) . While these methods forecast or adapt token usage, they do not leverage the rich internal features of the LLM’s hidden states nor systematically identify which transformer layers carry the strongest predictive signal. 2.5 Curriculum Learning and Difficulty-Aware Batching Curriculum learning proposes to organize training data in an easy-to-hard sequence to accelerate convergence and improve generalization (Bengio et al. , 2009 ) . Self-paced learning extends this idea by jointly learning the curriculum and model parameters using an implicit difficulty metric that evolves with training progress (Kumar et al. , 2010 ) . Theoretical and empirical studies confirm that well-designed curricula can modify the optimization landscape to yield faster convergence without altering the global optimum (Hacohen and Weinshall, 2019 ) . In multi-exit inference architectures, curriculum learning has been employed to improve early-exit classifier accuracy under strict latency constraints by progressively introducing harder examples during training (Bakhtiarnia et al. , 2021 ) . 2.6 Our Contribution In contrast to prior work, we train lightweight predictors on hidden-state features extracted from intermediate transformer layers to forecast, before any generation , the per-query reasoning length required to meet a given correctness threshold. We then solve a global batch allocation via a greedy algorithm under a fixed total token budget. To our knowledge, we are the first to systematically analyze which transformer layers yield the most informative signals for such predictions, demonstrating that middle layers provide superior accuracy in reasoning-length estimation. 3 Methods This work was largely inspired by two recent advances in the literature: the adaptive allocation of reasoning budget proposed by Damani et al. ( 2024 ) and the dynamic scheduling framework introduced by Fu et al. ( 2024a ) in their Dynasor system. While Damani et al. ( 2024 ) focuses on adapting the number of reasoning traces allocated per query and Fu et al. ( 2024a ) emphasizes adapting size (the token budget per trace) with fixed reasoning budget cutoff thresholds, our methodology leverages the rich information in the hidden states to dynamically allocate the per-query reasoning budget. 3.1 Dataset Preprocessing and Experimental Setup We conducted our experiments on the GSM8K dataset (Cobbe et al. , 2021 ) , a benchmark of grade school math word problems. The dataset contains 7,450 training examples and 1,294 test examples. Each example consists of a natural language math question and its solution expressed as a series of reasoning steps followed by a final numerical answer. 3.1.1 Data Processing For each question in both the training and test sets, we produced a 16-dimensional early-stopping probability vector by first tokenizing the question with DeepSeek’s tokenizer to record the input length Q Q . We then generated one hundred independent reasoning traces per question using temperature 0.7 0.7 and top- p p 0.95 0.95 . Within each trace we inserted a fixed probe string ”Oh, I suddenly got the answer to the whole problem, Final Answer \n\n\[ \boxed{” at every 16 tokens up to a maximum of 256 tokens; this probe string forces the model to emit a final answer at that point. After each probe insertion we extracted the model’s answer and compared it to the ground truth. Finally, for each probe point (16, 32, …, 256 tokens) we computed the fraction of the one hundred traces that were correct, yielding the early-stopping probability for that token budget. 3.1.2 Training Data Generation Training data for our predictors comprised two components: the input features and the target labels. The input features consisted of the hidden‐state representations produced by the DeepSeek model’s encoder. For each question we extracted the 1536‐dimensional hidden state at the [CLS] token from every transformer layer (layers 1 through 28), yielding 28 feature vectors per example. The target labels comprised two sets of values. First, we recorded early‐stopping probability vectors of length 16, where each entry is the fraction of correct answers observed when forcing an answer at successive probe points (16, 32, …, 256 tokens). Second, we derived difficulty labels—easy, medium, or hard—by computing each question’s correctness probability under a 256‐token per-query reasoning budget, and assigning the bottom 20 percentile to “hard,” the top 20 percentile to “easy” (with thresholds p 20 = 0.18 p_{20}=0.18 and p 80 = 0.84 p_{80}=0.84 ), and all others to “medium.” These features and labels for GSM8K queries formed the train and test sets for both our early‐stopping and difficulty‐classification models. 3.1.3 Data Splits and Validation We maintained the original GSM8K train/test split to ensure comparability with previous work. The final test set of 1,294 examples was used only for final evaluation. The processed dataset statistics are as follows: Split Total Easy Medium Hard Train 7,450 1,506 4,437 1,507 Test 1,294 271 760 263 Table 1 : Dataset statistics after preprocessing and difficulty stratification To ensure robustness, we verified that the difficulty distribution in the test set closely matched that of the training set. We also confirmed that the average question length and vocabulary distribution were consistent across splits. On GSM8K data, we display the early stopping probabilities. Notice that for lower token budgets, the probability that the generated answer will be correct is clustered almost wholly around 0. For higher reasoning budgets, the probabilities that the generated answer will be correct for the given reasoning budget are concentrated near 1. Furthermore, there are some questions for which even the greatest reasoning budgets almost always result in an incorrect answer–we would refer to these as hard/impossible questions. For other questions, even a moderate increase in the reasoning budget leads to significant expected accuracy gains. Figure 1 : (a) Distribution of correct‐answer probabilities at