Title: Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems

Abstract: Enterprise Retrieval-Augmented Generation (RAG) assistants operate in multi-turn, case-based workflows such as technical support and IT operations, where evaluation must reflect operational constraints, structured identifiers (e.g., error codes, versions), and resolution workflows. Existing RAG evaluation frameworks are primarily designed for benchmark-style or single-turn settings and often fail to capture enterprise-specific failure modes such as case misidentification, workflow misalignment, and partial resolution across turns.   We present a case-aware LLM-as-a-Judge evaluation framework for enterprise multi-turn RAG systems. The framework evaluates each turn using eight operationally grounded metrics that separate retrieval quality, grounding fidelity, answer utility, precision integrity, and case/workflow alignment. A severity-aware scoring protocol reduces score inflation and improves diagnostic clarity across heterogeneous enterprise cases. The system uses deterministic prompting with strict JSON outputs, enabling scalable batch evaluation, regression testing, and production monitoring.   Through a comparative study of two instruction-tuned models across short and long workflows, we show that generic proxy metrics provide ambiguous signals, while the proposed framework exposes enterprise-critical tradeoffs that are actionable for system improvement.

Body: Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems 1 Introduction 1.1 Contributions 2 Background and Enterprise Evaluation Requirements 2.1 LLM-as-a-Judge for Multi-Turn Evaluation 3 Related Work 4 Framework Overview Inputs and scope. Outputs. Case-aware evaluation. Case-aware evaluation. 4.1 Explainability and Actionability 4.2 Metric Definitions 5 Severity-Based Scoring and Aggregation 5.1 Severity-Based Scoring 5.2 Aggregated Score 6 Implementation 6.1 Cost Analysis 7 Evaluation Methodology 7.1 Dataset and Experimental Setup Sampling Procedure. 7.2 Evaluation Paradigms 7.3 Models Evaluated 8 Results and Analysis 8.1 Key Findings 8.2 Input Complexity 8.3 Statistical Significance Unit of analysis. Tests. 8.4 Judge Robustness 8.5 Comparison with Generic Proxy Metrics 8.6 Enterprise Diagnosis and Operational Use 8.7 Human Alignment Validation 9 Limitations 10 Conclusion 11 Reproducibility and Artifacts 12 Ethical and Safety Considerations A Extended Experimental Analysis A.1 Evaluation Dataset Characteristics A.2 Operational Evaluation Considerations B Severity Band Definitions B.1 Severity-Based Scoring C Enterprise Metric Suite M1: Hallucination (Grounding Fidelity). M2: Retrieval Correctness. M3: Context Sufficiency. M4: Answer Helpfulness. M5: Answer Type Fit. M6: Identifier Integrity. M7: Case Issue Identification. M8: Resolution Alignment. D Dataset Composition Details D.1 Inter-Metric Correlation Analysis Observations. D.2 Per-Metric Score Distributions Distributional Insights. D.3 Justification Length as a Complexity Proxy Key Findings. D.4 Aggregate Metric Validity Construct Validity. E Extended Metric Interpretation E.1 Retrieval and Sufficiency Distributions E.2 Enterprise Metric Profile Visualization F Enterprise Motivation and Operational Context G Extended Positioning Against Prior Work G.1 Limitations of Generic RAG Evaluation Assumptions G.2 Comparison to RAGAS, ARES, and RAGChecker G.3 Grounding and Attribution Literature G.4 LLM-as-a-Judge Reliability Considerations H Evaluation Schema and Implementation Details H.1 Evaluation Record Schema H.2 Continuous Scoring Interpretation H.3 Batch Processing and Validation I Detailed Limitations of Existing RAG Evaluation Frameworks I.1 Independent Metric Evaluation I.2 Single-Turn Assumptions I.3 Lack of Severity Awareness I.4 Limited Actionability J Extended Metric Definitions and Examples J.1 Hallucination vs. General Guidance J.2 Retrieval Failure Examples J.3 Illustrative Failure Case: Workflow Violation Despite Correct Retrieval J.4 Severity of Identifier Corruption J.5 Workflow Misalignment K Statistical Robustness Considerations K.1 Variance Across Conversation Turns K.2 Inter-Judge Consistency L LLM-as-a-Judge Prompt Design M Batch Evaluation Pipeline N Enterprise Failure Pattern Analysis O Reproducibility and Deployment Considerations Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems Mukul Chhabra Dell Technologies Austin, TX, USA mukul.chhabra@dell.com Luigi Medrano Dell Technologies Austin, TX, USA Luigi.Medrano@dell.com Arush Verma Dell Technologies Austin, TX, USA Arush.Verma@dell.com Abstract Enterprise Retrieval-Augmented Generation (RAG) assistants operate in multi-turn, case-based workflows such as technical support and IT operations, where evaluation must reflect operational constraints, structured identifiers (e.g., error codes, versions), and resolution workflows. Existing RAG evaluation frameworks are primarily designed for benchmark-style or single-turn settings and often fail to capture enterprise-specific failure modes such as case misidentification, workflow misalignment, and partial resolution across turns. We present a case-aware LLM-as-a-Judge evaluation framework for enterprise multi-turn RAG systems. The framework evaluates each turn using eight operationally grounded metrics that separate retrieval quality, grounding fidelity, answer utility, precision integrity, and case/workflow alignment. A severity-aware scoring protocol reduces score inflation and improves diagnostic clarity across heterogeneous enterprise cases. The system uses deterministic prompting with strict JSON outputs, enabling scalable batch evaluation, regression testing, and production monitoring. Through a comparative study of two instruction-tuned models across short and long workflows, we show that generic proxy metrics provide ambiguous signals, while the proposed framework exposes enterprise-critical tradeoffs that are actionable for system improvement. Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems Mukul Chhabra Dell Technologies Austin, TX, USA mukul.chhabra@dell.com Luigi Medrano Dell Technologies Austin, TX, USA Luigi.Medrano@dell.com Arush Verma Dell Technologies Austin, TX, USA Arush.Verma@dell.com 1 Introduction Retrieval-Augmented Generation (RAG) is widely used to deploy large language models (LLMs) in enterprise environments by combining retrieval over proprietary content with conditional generation. While effective in reducing hallucinations, enterprise deployments differ substantially from benchmark-style QA: support cases are multi-turn, operationally constrained, and require precise handling of structured identifiers (e.g., error codes, versions) and workflow alignment. In production systems, responses that appear relevant may still fail to resolve a case, misinterpret structured signals, or violate troubleshooting order. Generic RAG evaluation metrics—such as faithfulness and relevance—often conflate retrieval accuracy, grounding, and resolution quality into coarse signals (Es et al. , 2023 , 2024 ) . As a result, they provide limited diagnostic value for enterprise iteration and deployment decisions. We propose an enterprise-focused evaluation framework based on the LLM-as-a-Judge paradigm (Zheng et al. , 2023 ) . Our key contribution is case-aware evaluation : the judge conditions on multi-turn history, case metadata, and retrieved evidence while enforcing structured scoring across eight enterprise-aligned metrics. The resulting framework exposes operational failure modes—such as retrieval mismatch, hallucination, case misidentification, and workflow misalignment—and provides actionable signals for production monitoring and system improvement. 1.1 Contributions This paper makes the following contributions: • We formalize evaluation requirements for enterprise multi-turn RAG systems and identify recurring operational failure modes not captured by generic metric suites. • We introduce eight case-aware evaluation metrics that disentangle retrieval quality, grounding fidelity, answer utility, precision integrity, and resolution alignment. • We propose a severity-aware scoring protocol that improves diagnostic clarity across heterogeneous enterprise cases. • We design an audit-friendly LLM-as-a-Judge implementation with deterministic prompting and structured JSON outputs for scalable batch evaluation. • We empirically demonstrate that the proposed framework yields more actionable diagnostic insights than generic proxy metrics. 2 Background and Enterprise Evaluation Requirements Enterprise RAG deployments differ from benchmark-style QA in several key respects: queries are multi-turn, retrieval may be partially correct, and answers must comply with operational workflows and structured identifiers (e.g., error codes, versions). In such settings, a response can be relevant yet misaligned with case resolution, or factually grounded yet operationally incorrect. Generic RAG evaluation frameworks, such as RAGAS (Es et al. , 2023 , 2024 ) , decompose quality into dimensions such as faithfulness and relevance. While effective for reference-free evaluation, these metrics do not explicitly capture enterprise-specific failure modes such as workflow compliance, precision integrity, and correct case interpretation. Our framework complements this line of work by introducing case-aware, operationally grounded evaluation signals. 2.1 LLM-as-a-Judge for Multi-Turn Evaluation LLM-as-a-Judge methods provide scalable evaluation without exhaustive human labeling (Zheng et al. , 2023 ) . Prior work identifies potential biases (e.g., verbosity and position bias) and proposes structured rubric-based prompting to improve reliability (Liu et al. , 2023 ; Dubois et al. , 2024 ) . We adopt this paradigm while constraining the judge to evidence-only inputs and enforcing structured JSON outputs to maximize auditability and actionability in enterprise settings. 3 Related Work Automated RAG evaluation has progressed toward decomposed and component-aware metrics. RAGAS introduces reference-free faithfulness and relevance metrics (Es et al. , 2023 , 2024 ) . ARES trains lightweight judges for retrieval and generation scoring (Saad-Falcon et al. , 2024 ) , and RAGChecker provides diagnostic benchmarking across retrieval-generation interactions (Ru et al. , 2024 ) . Complementary work studies grounding and attribution. RARR proposes retrieval-driven revision pipelines (Gao et al. , 2023 ) , AttributionBench evaluates attribution reliability (Li et al. , 2024 ) , and FActScore decomposes long-form generations into atomic factual units (Min et al. , 2023 ) . Our framework builds on these directions but targets enterprise RAG systems, introducing case-aware, workflow-sensitive, and precision-focused metrics designed for multi-turn operational support environments. 4 Framework Overview Figure 1 summarizes the batch evaluation pipeline. Each row corresponds to one evaluated turn with structured case fields, retrieved contexts, and the model answer. For each row, the system normalizes inputs, constructs a single deterministic judge prompt, invokes the LLM once, validates a strict JSON schema, and aggregates per-metric scores into S final S_{\text{final}} . Inputs H , q , c s , c d , R , a H,q,c_{s},c_{d},R,a (+ optional g g ) Normalize Validate Build Judge Prompt LLM Judge (single call) Parse JSON Sanity Check 8 Metric Scores + Justifications Aggregate S final S_{\text{final}} Outputs eval table + compact table Figure 1: End-to-end evaluation pipeline for case-aware LLM-as-a-Judge scoring of multi-turn RAG responses. Inputs and scope. Let q q be the current query, H H the conversation history, c s c_{s} the case subject, c d c_{d} the case description, R = { r 1 , … , r n } R=\{r_{1},\dots,r_{n}\} the retrieved chunks, and a a the model answer (optionally a reference g g ). The judge is explicitly constrained to { H , q , c s , c d , R , a , g } \{H,q,c_{s},c_{d},R,a,g\} and must not use external knowledge. Outputs. The judge returns eight continuous scores s i ∈ [ 0 , 1 ] s_{i}\in[0,1] with brief justifications, plus a weighted aggregate S final S_{\text{final}} for monitoring and regression tests (0=failure, 1=full compliance; intermediate values reflect partial rubric satisfaction). Case-aware evaluation. We call the evaluation case-aware because scoring conditions on (i) case metadata ( c s , c d ) (c_{s},c_{d}) , (ii) multi-turn state H H capturing prior attempts and constraints, and (iii) retrieved evidence R R , while restricting judgment to these inputs. This contrasts with common single-turn RAG evaluation that ignores workflow constraints and identifier-critical correctness. Case-aware evaluation. We define case-aware LLM-judge evaluation as scoring that conditions on (i) structured case metadata (e.g., subject/description), (ii) multi-turn history capturing attempted steps and constraints, and (iii) retrieved evidence, while restricting judgments to these inputs only. This differs from standard RAG evaluation, which typically assumes single-turn independence and does not model workflow constraints or identifier-critical correctness. 4.1 Explainability and Actionability Each metric is tied to an engineering lever: Retrieval Correctness and Context Sufficiency diagnose retriever, chunking, and evidence coverage; Identifier Integrity flags high-severity command/ID corruption; and metric-level justifications make regressions auditable and accelerate production triage. 4.2 Metric Definitions We score eight case-aware dimensions spanning evidence quality, grounded response quality, and workflow safety: • Evidence quality: Retrieval Correctness (retrieved chunks contain needed facts) and Context Sufficiency (retrieval covers all required evidence). • Grounded response quality: Hallucination / Grounding Fidelity (claims supported by R R ), Answer Helpfulness (actionable and clear), and Answer Type Fit (diagnose vs. instruct vs. clarify). • Workflow safety: Identifier Integrity (no corruption of commands/IDs/paths), Case Issue Identification (correct issue given H , c s , c d H,c_{s},c_{d} ), and Resolution Alignment (steps satisfy constraints and likely resolve). Formal definitions and scoring rubrics are provided in Appendix C . 5 Severity-Based Scoring and Aggregation 5.1 Severity-Based Scoring To reflect enterprise risk, composite scores are mapped to four severity bands (Critical, Major, Moderate, Minor); definitions appear in Appendix B . 5.2 Aggregated Score For monitoring and regression testing, we compute a weighted sum: S final = ∑ i = 1 8 w i ​ s i , S_{\text{final}}=\sum_{i=1}^{8}w_{i}s_{i}, (1) where weights w i w_{i} reflect organizational risk tolerance, emphasizing grounding fidelity and retrieval quality. We use: Hallucination 0.20, Retrieval Correctness 0.15, Context Sufficiency 0.10, Answer Helpfulness 0.15, Answer Type Fit 0.10, Identifier Integrity 0.10, Case Issue Identification 0.10, Resolution Alignment 0.10. 6 Implementation The framework is a deterministic batch pipeline: each turn is serialized into a structured judge prompt and evaluated with a single LLM call. Outputs must match a strict JSON schema; invalid JSON triggers bounded retries and otherwise fails closed for inspection. Judge configuration: GPT-4 via Azure OpenAI, temperature=0.0, top_p=1.0, max_tokens=1024. We verify weight-robustness using (i) uniform weights ( w i = 0.125 w_{i}{=}0.125 ) and (ii) a retrieval-heavy profile (Retrieval Correctness and Context Sufficiency set to 0.20 each; others scaled proportionally). Conclusions are stable: the long-query ordering is unchanged and composite scores shift minimally (typically 3 % 3\% ). Metric GPT-OSS (Short) LLaMA (Short) GPT-OSS (Long) LLaMA (Long) Hallucination (Grounding Fidelity) 0.6890 0.7431 0.7586 0.7132 Retrieval Correctness 0.7429 0.7647 0.7941 0.7578 Context Sufficiency 0.6526 0.6429 0.7411 0.6546 Answer Helpfulness 0.6943 0.6143 0.7724 0.6143 Answer Type Fit 0.7603 0.7274 0.8360 0.6825 Identifier Integrity 0.9421 0.9354 0.9559 0.9329 Case Issue Identification 0.7636 0.6956 0.8783 0.7052 Resolution Alignment 0.7004 0.6459 0.8208 0.6759 Weighted Aggregate 0.7353 0.7202 0.8099 0.7136 Table 1: Corrected retrieval-aware evaluation results. Scores reflect full judge evaluation with retrieved contexts provided. 6.1 Cost Analysis Evaluation cost is linear in turns ( O ​ ( N ) O