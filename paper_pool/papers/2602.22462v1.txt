Title: MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation

Abstract: Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.

Body: MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation 1 Introduction 2 Literature Background 3 Methodology 3.1 Data Collection 3.2 Data Preprocessing 3.3 Model Selection 3.4 Prompt Design 3.5 Experimental Design 3.5.1 Base Configuration 3.5.2 RAG Configuration 3.5.3 Fine-Tune Configuration 3.6 Evaluating Model Response 4 Results Discussion 4.1 Report generation (textual similarity) 4.2 Prompting vs RAG for structured labels 4.3 Effect of QLoRA fine-tuning 5 Conclusion MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation Raiyan Jahangir, Nafiz Imtiaz Khan, Amritanand Sudheerkumar, Vladimir Filkov University of California, Davis, CA, USA Abstract Screening mammography is high-volume, time-sensitive, and documentation-heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast-density categories, and narrative reports. Although recent Vision Language Models (VLMs) make image-to-text reporting plausible, many demonstrations rely on closed, cloud-hosted systems (raising privacy, cost, and reproducibility concerns) or on tightly coupled, task-specific architectures that are hard to adapt across models, datasets, and workflows. We present MammoWise , a local, multi-model pipeline that turns open-source VLMs into clinically-styled mammogram report generators and multi-task classifiers. MammoWise accepts any Ollama-hosted VLM and any mammography dataset, supports zero-shot, few-shot, and Chain-of-Thought prompting, and includes an optional multimodal Retrieval Augmented Generation (RAG) mode that retrieves image-text exemplars from a vector database to provide case-specific context. To illustrate the utility of MammoWise in complex use cases, we evaluate three open VLMs ( MedGemma , LLaVA-Med , Qwen2.5-VL ) on VinDr-Mammo and DMID datasets, and assess report quality (BERTScore, ROUGE-L), BI-RADS classification (5-class), breast density (ACR A–D), and key findings (mass, calcification, asymmetry). Across models and datasets, report generation is consistently strong. It generally improves with few-shot prompting and RAG, whereas prompting-only classification is feasible but highly sensitive to the (dataset, model, prompt) combination. To improve reliability, we added parameter-efficient fine-tuning (QLoRA) to MedGemma on VinDr-Mammo, yielding substantial gains, including BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341, while preserving report quality. Overall, MammoWise provides a practical, extensible scaffold for deploying and studying local VLMs for mammography reporting, spanning prompting, RAG, and fine-tuning within a unified, reproducible workflow. 1 Introduction Screening mammography is among the most widely used imaging modalities in modern preventive care [ 33 ] . Every study requires careful inspection of bilateral views, such as Cranio-Caudal (CC) and Medio-Lateral Oblique (MLO), identification of subtle findings (masses, asymmetries, calcifications), and conversion of visual evidence into standardized clinical language: BI-RADS evaluation and recommendation, breast-density assessment, and a structured narrative report [ 18 ] . This translation step is essential, as reports drive follow-up imaging, biopsies, and longitudinal follow-up [ 41 ] . Still, it is also labor-intensive and vulnerable to variability in phrasing, completeness, and coding-relevant details. As imaging volumes continue to grow, tools that can help draft consistent, clinically styled mammogram reports have immediate practical value, without compromising privacy or introducing unsafe hallucinations [ 26 ] . VLMs offer a promising direction because they can jointly reason over images and text and generate natural-language output [ 69 ] . In principle, a VLM could ingest mammogram views and produce (i) a narrative report that follows radiology conventions and (ii) structured labels such as BI-RADS and density. However, in practice, two friction points limit the usefulness of the real world. First, many of the strongest demonstrations of report generation use large, closed, cloud-hosted models [ 51 ] . For clinical imaging, this creates deployment barriers. For example, protected health information may be transmitted off-premises, operational costs can be high, and results are difficult to reproduce or audit. Second, open-source VLMs can be run locally, but "out of the box,” they are often not specialized for mammography and may produce clinically implausible details unless carefully guided [ 27 ] . Bridging this gap typically requires either prompt engineering [ 14 ] (lightweight but unreliable for fine-grained classification) or fine-tuning [ 21 , 42 ] (more reliable but model- and dataset-dependent and often expensive). Existing multimodal mammography-focused systems illustrate these trade-offs. Several lines of work emphasize specialized architectures or training recipes optimized for specific tasks (e.g., malignancy, breast density, or lesion detection), while others focus on reporting using proprietary models [ 57 ] . As a result, practitioners and researchers who want to (a) run locally, (b) compare multiple open VLMs, and (c) evaluate multiple adaptation strategies (prompting, retrieval-augmented prompting, and fine-tuning) often face fragmented codebases, inconsistent experimental protocols, and limited support for producing complete, clinically styled reports rather than only labels. This paper takes a different stance. Instead of proposing yet another bespoke model, we introduce MammoWise , a multi-model, locally runnable pipeline that makes open VLMs usable for mammogram report generation and associated classification tasks. Conceptually, MammoWise is a modular one-stop shop, comparable to middleware/IDE, that provides a unified interface to launch local VLMs, configure multimodal RAG, and run standardized evaluations across multiple mammography use cases and metrics. The design goal is to enable practical, reproducible experimentation under realistic constraints. The pipeline can plug in any Ollama-hosted VLM [ 43 ] , ingest standard mammography datasets, and run a consistent end-to-end workflow that includes preprocessing, prompt templating, output parsing, and evaluation. MammoWise supports zero-shot [ 50 ] , few-shot [ 63 ] , and Chain-of-Thought (CoT) prompting [ 65 ] . It also adds an optional RAG [ 48 ] mode that selects image-text exemplars from a multimodal vector database to provide a customized context to the input mammogram. When prompting alone is insufficient, especially for stable, high-fidelity classification, we also support Parameter-Efficient Fine-Tuning (PEFT) [ 12 ] (QLoRA), which adapts an open VLM to the target label space without full model retraining. We evaluate MammoWise across two datasets (VinDr-Mammo [ 45 ] and DMID [ 46 ] ) and three open VLMs ( MedGemma [ 56 ] , LLaVA-Med [ 36 ] , Qwen2.5-VL [ 3 ] ). The results highlight a consistent pattern. Report generation is robust and improves further with few-shot prompting and RAG. At the same time, prompting-only classification is highly variable across model/dataset/prompt choices. To reduce this sensitivity, we fine-tune MedGemma on VinDr-Mammo using QLoRA [ 10 ] and observe substantial improvements in classification quality, reaching 0.7545 BI-RADS accuracy, 0.8840 density accuracy, and 0.9341 calcification accuracy (F1-score 0.9313) while maintaining strong report-generation behavior. These findings suggest a pragmatic division of labor: prompting and retrieval are often sufficient to draft clinically styled reports, but dependable structured classification benefits from lightweight fine-tuning. Contributions This paper makes the following contributions: • A local, multi-model pipeline ( MammoWise ) that turns open VLMs into mammogram report generators and multi-task classifiers across datasets and prompting regimes. • A multimodal RAG workflow for case-specific few-shot context using an image-text vector database. • A systematic comparison of prompting, RAG, and QLoRA fine-tuning across models and datasets, with unified evaluation for both narrative report quality and clinical labels. • Evidence that parameter-efficient fine-tuning can substantially stabilize and improve classification performance in this setting. Based on these contributions, we formulate the following research questions: RQ : 1 {}_{1}: How well do local medical VLMs generate structured reports under prompting vs RAG? RQ 2 : When does lightweight fine-tuning outperform prompting/RAG for classification? The remainder of the paper reviews related work, describes the MammoWise architecture and experimental design, presents results and analyses across adaptation strategies, and closes with practical implications and limitations. Figure 1 : Overall methodology of the study 2 Literature Background Several recent studies have explored how VLMs can assist radiologists with mammogram interpretation. Haver et al. [ 16 ] investigated the potential of ChatGPT to determine BI-RADS assessments from mammogram images. Pesapane et al. [ 49 ] used GPT-4 to generate mammogram reports and identify abnormalities, such as masses and microcalcifications. While these works demonstrated that large, closed-source models can describe mammogram findings, they also reported hallucinations, low sensitivity and specificity, and the inherent privacy and cost issues associated with cloud-based models. Other work has investigated CLIP-style models as backbones for mammography. Moura et al. [ 9 ] evaluated several Contrastive Language-Image Pretraining (CLIP) variants- CLIP [ 37 ] , BiomedCLIP [ 70 ] , and PubMedCLIP [ 11 ] for breast-density and BI-RADS classification. Khan et al. [ 61 ] used MedCLIP [ 64 ] to construct diverse teaching cases for radiology education via image-text retrieval, exploring zero-shot, few-shot, and supervised prompting scenarios. Several specialized VLM architectures have also been proposed. Chen et al. introduced LLaVA-Mammo [ 6 ] , a fine-tuned variant of LLaVA [ 39 ] tailored for BI-RADS, breast density, and malignancy classification. Increasing the language model size from 7B to 13B parameters improved density accuracy from 72.1 to 76.6 and malignancy AUC from 0.687 to 0.723. Their later LLaVA-MultiMammo model [ 7 ] extended capabilities to breast cancer risk prediction while retaining core classification tasks. Ghosh et al. [ 13 ] proposed MammoCLIP , a VLM combining CNN-based vision [ 52 ] and BioBERT [ 35 ] language encoders trained on mammogram-report pairs. Their model supports the classification of malignancy, density, mass, and calcification, and provides visual explanations via a novel feature attribution method (Mammo-Factor). Jain et al. [ 24 ] developed MMBCD , a multimodal model that integrates mammogram images and clinical histories using ViT and RoBERTa -based embeddings [ 40 ] . Cao et al. [ 5 ] introduced MammoVLM , a GLM-4-9B -based VLM with a dedicated visual encoder, fine-tuned to provide diagnostic assistance at a level comparable to a junior radiologist. Very few works provide a single reusable pipeline that supports both report generation and multi-task labels across models/datasets. Our work addresses this gap by framing MammoWise as a reusable pipeline rather than a single fixed model, and by evaluating its behavior across multiple VLMs, datasets, and adaptation strategies. 3 Methodology The study’s overall methodology is shown in Figure 1 and discussed in detail in the following subsections. 3.1 Data Collection We used two mammography datasets for our work. The primary dataset, VinDr-Mammo, was prepared by Nguyen et al. [ 45 ] and is used for most experiments, including fine-tuning. The second dataset is the DMID dataset prepared by Oza et al. [ 46 ] . The VinDr-Mammo dataset consists of 20,000 images from 5,000 Vietnamese patients, with four images per patient (two per breast): Cranio-Caudal (CC) and Medio-Lateral Oblique (MLO) for both left and right breasts. The dataset also includes metadata describing acquisition devices, study numbers, and image identifiers, as well as breast-level information like tissue density (ACR A/B/C/D) and BI-RADS score (1–5), and abnormal findings such as masses, calcifications, and other abnormalities. The dataset is publicly available via PhysioNet (https://physionet.org/content/vindr-mammo/1.0.0/) . DMID is a relatively small but lesion-enriched dataset that contains 510 mammogram images from India, of which 300 include abnormalities. Each image is paired with a corresponding text report describing breast density, BI-RADS score, and any abnormal findings. This image-report pairing makes DMID particularly suitable for evaluating report-generation behavior and prompting strategies. The dataset is accessible via Figshare (https://figshare.com/authors/Parita_Oza/17353984) . 3.2 Data Preprocessing To prevent memory overload and ensure compatibility with the VLMs, we resize all images from both datasets to a fixed size of 512 × 512 512\times 512 pixels. For VinDr-Mammo, we then merge the four views (right/left CC and right/left MLO) of each patient into a single composite image, as shown in Figure 1 , arranged symmetrically: CC views are side by side at the top, and MLO views are side by side at the bottom. This layout mimics the way radiologists routinely inspect mammograms, comparing the right and left breasts on both projections to assess asymmetries and subtle patterns [ 55 , 8 ] . From the 20,000 single-view images, this merging process yields 5,000 composite images, one per patient. For each merged image, we generate a corresponding report, yielding 5,000 composite image-report pairs. In the report: • Breast density is taken directly from the breast-level labels. • For BI-RADS, when different views have different categories, we take the highest BI-RADS across the four views as the patient-level label, reflecting clinical caution. • The findings of all views are combined into a single sentence summarizing the presence and location of masses, calcifications, asymmetries, and other abnormalities. • We define a field of derived suspicion that indicates whether the case is labeled “healthy” (BI-RADS 1), “benign” (BI-RADS 2–3), or “suspicious” (BI-RADS 4-5). Importantly, this label reflects radiologic suspicion implied by BI-RADS, not biopsy-confirmed cancer. For the DMID dataset, there were already 510 corresponding reports for each image. We extract key information, such as “breast density”, “BI-RADS”, and “findings”, from those reports and store them in JavaScript Object Notation (JSON) format. For RAG implementation, we convert these image-report pairs into multimodal embeddings. We divide both the data into an 80:20 train-test split. For VinDr-Mammo, it is a patient-level split, whereas in DMID, it is an image-level split. The image and text embeddings are generated using a multimodal OpenCLIPEmbeddin