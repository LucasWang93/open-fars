Title: Theory of Optimal Learning Rate Schedules and Scaling Laws for a Random Feature Model

Abstract: Setting the learning rate for a deep learning model is a critical part of successful training, yet choosing this hyperparameter is often done empirically with trial and error. In this work, we explore a solvable model of optimal learning rate schedules for a powerlaw random feature model trained with stochastic gradient descent (SGD). We consider the optimal schedule $Î·_T^\star(t)$ where $t$ is the current iterate and $T$ is the total training horizon. This schedule is computed both numerically and analytically (when possible) using optimal control methods. Our analysis reveals two regimes which we term the easy phase and hard phase. In the easy phase the optimal schedule is a polynomial decay $Î·_T^\star(t) \simeq T^{-Î¾} (1-t/T)^Î´$ where $Î¾$ and $Î´$ depend on the properties of the features and task. In the hard phase, the optimal schedule resembles warmup-stable-decay with constant (in $T$) initial learning rate and annealing performed over a vanishing (in $T$) fraction of training steps. We investigate joint optimization of learning rate and batch size, identifying a degenerate optimality condition. Our model also predicts the compute-optimal scaling laws (where model size and training steps are chosen optimally) in both easy and hard regimes. Going beyond SGD, we consider optimal schedules for the momentum $Î²(t)$, where speedups in the hard phase are possible. We compare our optimal schedule to various benchmarks in our task including (1) optimal constant learning rates $Î·_T(t) \sim T^{-Î¾}$ (2) optimal power laws $Î·_T(t) \sim T^{-Î¾} t^{-Ï‡}$, finding that our schedule achieves better rates than either of these. Our theory suggests that learning rate transfer across training horizon depends on the structure of the model and task. We explore these ideas in simple experimental pretraining setups.

Body: Theory of Optimal Learning Rate Schedules and Scaling Laws for a Random Feature Model 1 Introduction 1.1 Related Works 2 Setting 2.1 Power Law Linear Random Feature Model 2.2 Analytical Expression for Test Loss Dynamics 2.3 Power-law spectra 3 Optimal LR schedules via optimal control 3.1 Numerical optimal control 3.2 Analytical derivation of optimal schedules Easy phase. Hard phase. 3.3 Comparison with benchmarks 3.4 Joint optimization of learning rate and batch size 3.5 Compute-optimal scaling 3.6 Optimal Momentum Schedules Compared to SGD 4 Experiments in Deep Networks 5 Conclusions Limitations and Future Directions A Long-time dynamics B Analytical derivation of optimal learning-rate schedules B.1 Power-law spectra C Comparison with benchmarks C.1 Constant learning rate C.2 Power-law schedules C.3 General scaling form C.3.1 The edge term is subleading for the optimal schedule D Joint optimization of learning rate and batch size E Loss Dynamics for SGD + Momentum F Optimal Control Implementation Theory of Optimal Learning Rate Schedules and Scaling Laws for a Random Feature Model Blake Bordelon Francesco Mori Abstract Setting the learning rate for a deep learning model is a critical part of successful training, yet choosing this hyperparameter is often done empirically with trial and error. In this work, we explore a solvable model of optimal learning rate schedules for a powerlaw random feature model trained with stochastic gradient descent (SGD). We consider the optimal schedule Î· T â‹† â€‹ ( t ) \eta_{T}^{\star}(t) where t t is the current iterate and T T is the total training horizon. This schedule is computed both numerically and analytically (when possible) using optimal control methods. Our analysis reveals two regimes which we term the easy phase and hard phase. In the easy phase the optimal schedule is a polynomial decay Î· T â‹† â€‹ ( t ) â‰ƒ T âˆ’ Î¾ â€‹ ( 1 âˆ’ t / T ) Î´ \eta_{T}^{\star}(t)\simeq T^{-\xi}(1-t/T)^{\delta} where Î¾ \xi and Î´ \delta depend on the properties of the features and task. In the hard phase, the optimal schedule resembles warmup-stable-decay with constant (in T T ) initial learning rate and annealing performed over a vanishing (in T T ) fraction of training steps. We investigate joint optimization of learning rate and batch size, identifying a degenerate optimality condition. Our model also predicts the compute-optimal scaling laws (where model size and training steps are chosen optimally) in both easy and hard regimes. Going beyond SGD, we consider optimal schedules for the momentum Î² â€‹ ( t ) \beta(t) , where speedups in the hard phase are possible. We compare our optimal schedule to various benchmarks in our task including (1) optimal constant learning rates Î· T â€‹ ( t ) âˆ¼ T âˆ’ Î¾ \eta_{T}(t)\sim T^{-\xi} (2) optimal power laws Î· T â€‹ ( t ) âˆ¼ T âˆ’ Î¾ â€‹ t âˆ’ Ï‡ \eta_{T}(t)\sim T^{-\xi}t^{-\chi} , finding that our schedule achieves better rates than either of these. Our theory suggests that learning rate transfer across training horizon depends on the structure of the model and task. We explore these ideas in simple experimental pretraining setups. Machine Learning, ICML 1 Introduction Training deep learning models requires choosing many hyperparameters such as the learning rate, batch size, training horizon, total data, and architectural details, resulting in a complicated decision space. To make matters worse, it is challenging to characterize how these various hyperparameters jointly interact as one scales up the model size or training horizon. One strategy to reduce this complexity is to identify scaling protocols that allow for transfer of optimal hyperparameters, where small models can provide reliable proxies for tuning for large models. Such approximate hyperparameter transfer across model sizes can be achieved by principled parameterization and optimizer design (Yang et al. , 2022 ; Bordelon et al. , 2023 ; Yang et al. , 2023 ; Dey et al. , 2025 ) . However, these schemes do not automatically lead to transfer over training horizons (Bjorck et al. , 2024 ; Everett et al. , 2024 ) . In Figure 1 we illustrate the failure of transfer over training horizons T T despite having successful transfer over model sizes (widths N N ). This failure motivates theory that can account for the behavior of optimal learning rates of SGD as the training horizon varies. Further, beyond optimal base learning rates themselves, one can also adopt a learning rate schedule . While many schedules such as linear decay, warmup-stable-decay, and cosine annealing are commonly used in practice, it is unclear under which conditions one should favor using a type of schedule over the other. (a) ResNet SGD Training on CIFAR-5M (b) Power Law Random Feature Model with SGD Figure 1 : SGD learning rates do not automatically transfer over training horizons T T . This motivates theory that can identify not only how to scale Î· \eta with T T , but also how to set the entire learning rate schedule Î· â€‹ ( t ) \eta(t) with T T . (a) The loss of a deep ResNet trained on CIFAR-5M. (b) Test loss of a random feature model trained with SGD as a function of fixed learning rate Î· \eta . The optimal learning rate shifts leftwards in this model, mimicking the behavior of the real network training (Bjorck et al. , 2024 ) . To address these questions in a theoretical framework, we make the following contributions: â€¢ We analyze SGD dynamics in a powerlaw random feature model (Bordelon and Pehlevan, 2022 ; Bordelon et al. , 2024 ; Paquette et al. , 2024 ) . We show that SGD fluctuations can lead to shifts in optimal learning rates across training horizons T T that are task and model dependent . â€¢ We characterize both numerically and analytically the optimal learning rate annealing schedule for SGD in our model using optimal control, minimizing the final test loss at step T T . This analysis reveals two phases of tasks, easy and hard. For easy tasks, our schedule takes the form Î· â€‹ ( t ) = T âˆ’ Î¾ â€‹ f â€‹ ( t / T ) \eta(t)=T^{-\xi}f(t/T) , while for hard tasks our schedule resembles warmup-stable-decay (WSD). â€¢ We compare our optimal schedules to various benchmarks including optimized constant learning rate and optimal powerlaw schedules, finding that our schedules can achieve better scaling exponents. â€¢ This analysis is generalized to optimal batch size schedules and optimal momentum schedules. Optimizing batch size and learning rate jointly enables a reduction in total wall-clock time, while optimizing the momentum parameter can provide an improvement in the scaling exponent for hard tasks beyond SGD. 1.1 Related Works Based on the correspondence between neural networks in the lazy training regime (Chizat et al. , 2019 ) , many works have attempted to characterize neural scaling laws (Kaplan et al. , 2020 ; Hoffmann et al. , ) by computing the generalization of random feature models with powerlaw features (Varre et al. , 2021 ; Bordelon and Pehlevan, 2022 ; Bahri et al. , 2024 ; Bordelon et al. , 2024 ; Paquette et al. , 2024 ; Lin et al. , 2024 ; Ferbach et al. , 2025 ) . Several works have examined easy and hard phases of this model in the noise dominated regime (Dieuleveut and Bach, 2015 ; Lin and Rosasco, 2017 ) . Notably, Pillaud-Vivien et al. ( 2018 ) demonstrated that optimal rates for SGD can be achieved in the hard phase with data repetition and model averaging strategies. Meterez et al. ( 2025 ) recently analyzed this model to motivate invariances between learning rate and batch sizes when designing schedules. (Qiu et al. , 2025 ) identified a scaling collapse phenomenon where the compute optimal models exhibit universal loss dynamics across scales for a variety of learning rate schedules and developed a theoretical model of SGD that accounted for this supercollapse. Our work similarly utilizes this powerlaw random feature model, however we study the optimal learning rate schedule for SGD using optimal control theory. Optimal control theory provides a principled framework for deriving hyperparameter schedules in reduced theoretical models. Pioneering work in the 1990s characterized optimal learning rate schedules for two-layer neural networks within the teacher-student formalism (Saad and Rattray, 1997 ; Rattray and Saad, 1998 ) . More recently, these techniques have been generalized to several hyperparameter protocols (Mori et al. , 2025 ; Mignacco and Mori, 2025 ) . Analogous optimization procedures have also recently been studied in deep linear networks (Carrasco-Davis et al. , 2023 ) and cognitive science (Njaradi et al. , 2026 ) . Most closely related to our work is (Li et al. , 2025 ) , which examined learning rate schedules within the context of power-law random feature models. However, their analysis was restricted to pre-defined, parametric schedules and did not address the theoretical derivation of the optimal control policy. 2 Setting 2.1 Power Law Linear Random Feature Model We consider a random-feature model widely studied in the literature of scaling laws (Bordelon and Pehlevan, 2022 ; Bordelon et al. , 2024 ; Lin et al. , 2024 ; Paquette et al. , 2024 ) . Let the inputs ğ’™ âˆˆ â„ D {\bm{x}}\in\mathbb{R}^{D} be drawn from a probability distribution p â€‹ ( ğ’™ ) p({\bm{x}}) . The associated labels are generated as y â€‹ ( ğ’™ ) = ğ’˜ âˆ— â‹… ğ â€‹ ( ğ’™ ) + Ïƒ 0 â€‹ z , y({\bm{x}})={\bm{w}}^{*}\cdot{\bm{\psi}}({\bm{x}})+\sigma_{0}z\,, (1) where Ïƒ 0 0 \sigma_{0} 0 is the amplitude of the label noise and z âˆ¼ ğ’© â€‹ ( 0 , 1 ) z\sim\mathcal{N}(0,1) . The vector ğ â€‹ ( ğ’™ ) âˆˆ â„ M {\bm{\psi}}({\bm{x}})\in\mathbb{R}^{M} (with M M possibly infinite) is a feature map and ğ’˜ âˆ— âˆˆ â„ M {\bm{w}}^{*}\in\mathbb{R}^{M} a vector of parameters. We work in the feature eigenbasis, so that âŸ¨ ğ i â€‹ ( ğ’™ ) â€‹ ğ j â€‹ ( ğ’™ ) âŸ© ğ’™ âˆ¼ p â€‹ ( ğ’™ ) = Î´ i â€‹ j â€‹ Î» i . \langle{\bm{\psi}}_{i}(\bm{x}){\bm{\psi}}_{j}(\bm{x})\rangle_{\bm{x}\sim p(\bm{x})}=\delta_{ij}\lambda_{i}\,. (2) We consider a linear student model y ^ â€‹ ( ğ’™ ) = ğ’˜ â‹… ğ ~ â€‹ ( ğ’™ ) \hat{y}(\bm{x})={\bm{w}}\cdot\tilde{\bm{\psi}}(\bm{x}) , with parameters ğ’˜ âˆˆ â„ N {\bm{w}}\in\mathbb{R}^{N} and projected features ğ ~ â€‹ ( ğ’™ ) = ğ‘® â€‹ ğ â€‹ ( ğ’™ ) \tilde{\bm{\psi}}(\bm{x})={\bm{G}}\bm{\psi}(\bm{x}) , where ğ‘® âˆˆ â„ N Ã— M {\bm{G}}\in\mathbb{R}^{N\times M} is a projection matrix. To describe model-size effects, we assume N â‰¤ M N\leq M and that the top- N N features are selected, i.e., ğ‘® = [ ğ‘° N , ğŸ N Ã— ( M âˆ’ N ) ] {\bm{G}}=[{\bm{I}}_{N},{\bm{0}}_{N\times(M-N)}] . We train via online stochastic gradient descent (SGD) on the square loss ğ’˜ t + 1 = ğ’˜ t âˆ’ Î· t m t â€‹ âˆ‘ Î¼ = 1 m t ğ ~ â€‹ ( ğ’™ Î¼ , t ) â€‹ ( ğ’˜ t â‹… ğ ~ â€‹ ( ğ’™ t , Î¼ ) âˆ’ y â€‹ ( ğ’™ t , Î¼ ) ) , {\bm{w}}_{t+1}={\bm{w}}_{t}-\frac{\eta_{t}}{m_{t}}\sum_{\mu=1}^{m_{t}}\tilde{\bm{\psi}}(\bm{x}_{\mu,t})({\bm{w}_{t}}\cdot\tilde{\bm{\psi}}(\bm{x}_{t,\mu})-y(\bm{x}_{t,\mu}))\,, (3) with learning rate Î· t \eta_{t} and minibatch size m t m_{t} . The samples ğ’™ Î¼ , t \bm{x}_{\mu,t} and the label noise variables z Î¼ , t z_{\mu,t} are assumed to be independent. The parameters are initialized to zero. 2.2 Analytical Expression for Test Loss Dynamics Following (Bordelon and Pehlevan, 2022 ) , for k = 1 â€‹ â€¦ â€‹ N k=1\,\ldots N , we define c t , k = âŸ¨ ( w t , k âˆ’ w k âˆ— ) 2 âŸ© ğ’Ÿ t âˆ’ 1 c_{t,k}=\langle(w_{t,k}-w^{*}_{k})^{2}\rangle_{\mathcal{D}_{t-1}} , where âŸ¨ â‹… âŸ© ğ’Ÿ t âˆ’ 1 \langle\cdot\rangle_{\mathcal{D}_{t-1}} indicates the average over all noise sources up to time t âˆ’ 1 t-1 . Assuming that the features ğ â€‹ ( ğ’™ ) {\bm{\psi}}({\bm{x}}) are zero-mean Gaussian variables, one can show that the coefficients c t , k c_{t,k} satisfy the recursion relation (Bordelon and Pehlevan, 2022 ) c t + 1 , k \displaystyle c_{t+1,k} = ( 1 âˆ’ 2 â€‹ Î· t â€‹ Î» k + Î· t 2 â€‹ m t + 1 m t â€‹ Î» k 2 ) â€‹ c t , k \displaystyle=\left(1-2\eta_{t}\lambda_{k}+\eta_{t}^{2}\frac{m_{t}+1}{m_{t}}\lambda_{k}^{2}\right)c_{t,k} + Î· t 2 m t â€‹ Î» k â€‹ âˆ‘ â„“ = 1 N Î» â„“ â€‹ c t , â„“ + Î· t 2 m t â€‹ Ïƒ 2 â€‹ Î» k , \displaystyle+\frac{\eta_{t}^{2}}{m_{t}}\lambda_{k}\sum_{\ell=1}^{N}\lambda_{\ell}c_{t,\ell}+\frac{\eta^{2}_{t}}{m_{t}}\sigma^{2}\lambda_{k}\,, (4) where we have defined the total irreducible noise as Ïƒ 2 = Ïƒ 0 2 + âˆ‘ k = N + 1 M ( w k âˆ— ) 2 â€‹ Î» k . \sigma^{2}=\sigma_{0}^{2}+\sum_{k=N+1}^{M}(w^{*}_{k})^{2}\lambda_{k}\,. (5) The test loss L t L_{t} at time t t has the form L t \displaystyle L_{t} â‰¡ âŸ¨ âŸ¨ ( ğ’˜ âˆ— â‹… ğ â€‹ ( ğ’™ ) + Ïƒ 0 â€‹ z âˆ’ ğ’˜ â‹… ğ ~ â€‹ ( ğ’™ ) ) 2 âŸ© ğ’Ÿ t âˆ’ 1 âŸ© ğ’™ , z \displaystyle\equiv\langle\langle({\bm{w}}^{*}\cdot{\bm{\psi}}({\bm{x}})+\sigma_{0}z-{\bm{w}}\cdot\tilde{\bm{\psi}}(\bm{x}))^{2}\rangle_{\mathcal{D}_{t-1}}\rangle_{\bm{x},z} = âˆ‘ k = 1 N c t , k â€‹ Î» k + Ïƒ 2 . \displaystyle=\sum_{k=1}^{N}c_{t,k}\lambda_{k}+\sigma^{2}\,. (6) It is therefore sufficient to track c t , k c_{t,k} in order to predict the test loss dynamics. 2.3 Power-law spectra Motivated by empirical observations that natural data often exhibit power-law spectral decay (Bordelon and Pehlevan, 2022 ; Bahri et al. , 2024 ) , we assume power-law scalings for the data covariance eigenvalues and the teacher weights: Î» k âˆ¼ k âˆ’ b \lambda_{k}\sim k^{-b} and ( w k âˆ— ) 2 â€‹ Î» k âˆ¼ k âˆ’ a (w^{*}_{k})^{2}\lambda_{k}\sim k^{-a} , with exponents a , b 1 a,b 1 . These parameters characterize the effective dimension and difficulty of the learning task. Under these assumptions, for a constant learning rate Î· \eta and batch size m m , the excess loss scales as (see Appendix C ): L T âˆ’ Ïƒ 2 âˆ¼ ( Î· â€‹ T ) âˆ’ a âˆ’ 1 b + Ïƒ 2 â€‹ Î· m , L_{T}-\sigma^{2}\sim(\eta T)^{-\frac{a-1}{b}}+\frac{\sigma^{2}\eta}{m}\,, (7) where âˆ¼ \sim denotes proportionality up to constant factors. Optimizing over a fixed learning rate yields the optimal constant strategy Î· âˆ¼ T âˆ’ ( a âˆ’ 1 ) / ( a + b âˆ’ 1 ) \eta\sim T^{-(a-1)/(a+b-1)} , which results in L T âˆ’ Ïƒ 2 âˆ¼ T âˆ’ ( a âˆ’ 1 ) / ( a + b âˆ’ 1 ) L_{T}-\sigma^{2}\sim T^{-(a-1)/(a+b-1)} . A fundamental question is whether this scaling exponent can be improved by modulating the learning rate over time, and if so, what is the optimal schedule Î· T âˆ— â€‹ ( t ) \eta_{T}^{*}(t) . (a) Optimal Schedule ( b a b a ) (b) Loss vs. Time ( b a b a ) (c) Final Loss ( b a b a ) (d) Optimal Schedule ( b a b a ) (e) Loss vs. Time ( b a b a ) (f) Final Loss ( b a b a ) Figure 2 : Comparison of optimal learning rate schedules in the hard (top row, b a b a ) and easy (bottom row, b a b a ) phases. (a, d) Profile of the optimal learning rate Î· T âˆ— â€‹ ( t ) \eta_{T}^{*}(t) . In the hard phase (a) , the schedule maintains a constant maximum value for t t s t t_{s} followed by a rapid annealing phase, where the annealing fraction 1 âˆ’ t s / T 1-t_{s}/T vanishes as T â†’ âˆ T\to\infty (see Fig. 7 ). In the easy phase (d) , the schedule collapses onto the scaling form Î· T âˆ— â€‹ ( t ) â‰ˆ T b / a âˆ’ 1 â€‹ f â€‹ ( t / T ) \eta_{T}^{*}(t)\approx T^{b/a-1}f(t/T) (dashed theoretical curve, see Eq. ( 12 )). (b, e) Evolution of the loss over training time t t for the optimal schedule. (c, f) Scaling of the final excess loss L T âˆ’ Ïƒ 2 L_{T}-\sigma^{2} with the training horizon T T . The optimal schedule improves scaling exponent compared to the optimal constant and power-law basel