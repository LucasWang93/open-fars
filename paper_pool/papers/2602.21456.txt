Title: Revisiting Text Ranking in Deep Research

Abstract: Deep research has emerged as an important task that aims to address hard queries through extensive open-web exploration. To tackle it, most prior work equips large language model (LLM)-based agents with opaque web search APIs, enabling agents to iteratively issue search queries, retrieve external evidence, and reason over it. Despite search's essential role in deep research, black-box web search APIs hinder systematic analysis of search components, leaving the behaviour of established text ranking methods in deep research largely unclear. To fill this gap, we reproduce a selection of key findings and best practices for IR text ranking methods in the deep research setting. In particular, we examine their effectiveness from three perspectives: (i) retrieval units (documents vs. passages), (ii) pipeline configurations (different retrievers, re-rankers, and re-ranking depths), and (iii) query characteristics (the mismatch between agent-issued queries and the training queries of text rankers). We perform experiments on BrowseComp-Plus, a deep research dataset with a fixed corpus, evaluating 2 open-source agents, 5 retrievers, and 3 re-rankers across diverse setups. We find that agent-issued queries typically follow web-search-style syntax (e.g., quoted exact matches), favouring lexical, learned sparse, and multi-vector retrievers; passage-level units are more efficient under limited context windows, and avoid the difficulties of document length normalisation in lexical retrieval; re-ranking is highly effective; translating agent-issued queries into natural-language questions significantly bridges the query mismatch.

Body: Revisiting Text Ranking in Deep Research 1 Introduction 2 Task definition 3 Methodology 3.1 Research questions and experimental design 3.2 Experimental setup 3.2.1 Deep research agents 3.2.2 Text ranking methods to be reproduced 3.2.3 Dataset. 3.2.4 Passage corpus construction 3.2.5 Query-to-question (Q2Q) reformulation 3.2.6 Evaluation. 3.2.7 Implementation details. 4 Results and Discussions 4.1 Sanity check for reproduction 4.2 Retrievers on passage and document corpora 4.3 Re-ranking in deep research 4.4 Training–inference query mismatch 5 Related Work 6 Conclusions Future Work Revisiting Text Ranking in Deep Research Chuan Meng 0000-0002-1434-7596 The University of Edinburgh Edinburgh United Kingdom chuan.meng@ed.ac.uk , Litu Ou 0009-0005-8380-1368 The University of Edinburgh Edinburgh United Kingdom litu.ou@ed.ac.uk , Sean MacAvaney 0000-0002-8914-2659 University of Glasgow Glasgow United Kingdom sean.macavaney@glasgow.ac.uk and Jeff Dalton 0000-0003-2422-8651 The University of Edinburgh Edinburgh United Kingdom jeff.dalton@ed.ac.uk Abstract. Deep research has emerged as an important task that aims to address hard queries through extensive open-web exploration. To tackle it, most prior work equips large language model (LLM)-based agents with opaque web search APIs, enabling agents to iteratively issue search queries, retrieve external evidence, and reason over it. Despite search’s essential role in deep research, black-box web search APIs hinder systematic analysis of search components, leaving the behaviour of established text ranking methods in deep research largely unclear. To fill this gap, we reproduce a selection of key findings and best practices for IR text ranking methods in the deep research setting. In particular, we examine their effectiveness from three perspectives: (i) retrieval units (documents vs. passages), (ii) pipeline configurations (different retrievers, re-rankers, and re-ranking depths), and (iii) query characteristics (the mismatch between agent-issued queries and the training queries of text rankers). We perform experiments on BrowseComp-Plus, a deep research dataset with a fixed corpus, evaluating 2 open-source agents, 5 retrievers, and 3 re-rankers across diverse setups. We find that agent-issued queries typically follow web-search-style syntax (e.g., quoted exact matches), favouring lexical, learned sparse, and multi-vector retrievers; passage-level units are more efficient under limited context windows, and avoid the difficulties of document length normalisation in lexical retrieval; re-ranking is highly effective; translating agent-issued queries into natural-language questions significantly bridges the query mismatch. Text ranking, Retrieval, Re-ranking, Deep research † † copyright: none † † ccs: Information systems Information retrieval 1. Introduction Text ranking is a core part of information retrieval (IR) (Robertson et al. , 1995 ; Lassance et al. , 2024 ; Ma et al. , 2024 ; Zhang et al. , 2025 ; Santhanam et al. , 2022 ; Nogueira et al. , 2020 ; Weller et al. , 2025b ) . It aims to produce an ordered list of texts retrieved from a corpus in response to a query (Lin et al. , 2022 ) . Text ranking methods have been studied across diverse settings, from single-hop (Bajaj et al. , 2018 ; Kwiatkowski et al. , 2019 ) and multi-hop search (Ho et al. , 2020 ; Yang et al. , 2018 ) to reasoning-intensive search (Shao et al. , 2025 ; SU et al. , 2025 ) and retrieval-augmented generation (RAG) (Su et al. , 2025 ; Mo et al. , 2025 ; Asai et al. , 2024 ) . Recently, the IR community has witnessed the emergence of deep research (Shi et al. , 2025 ) scenarios. Deep research aims to answer multi-hop, reasoning-intensive queries that require extensive exploration of the open web and are difficult for both humans and large language models (LLMs) (Wei et al. , 2025 ) . To tackle this task, a growing number of studies build agents that interact with live web search APIs to obtain information (Zhou et al. , 2025 ; Li et al. , 2025a ; Liu et al. , 2025 ) . Such agents typically use LLMs as their decision-making core and perform multiple rounds of chain-of-thought (CoT) reasoning (Wei et al. , 2022 ) and search invocations (Jin et al. , 2025 ; Song et al. , 2025 ; Li et al. , 2025b ) . Motivation . Despite the essential role of search in deep research, how existing text ranking methods perform in this setting remains poorly understood. Most prior studies rely on black-box live web search APIs (Xu et al. , 2026b ; Zhou et al. , 2025 ) , which lack transparency, thereby hindering systematic analysis and a clear understanding of the contribution of search components (Chen et al. , 2025 ; Hu et al. , 2026 ) . Although recent work (Chen et al. , 2025 ) builds a deep research dataset with a fixed document corpus and human-verified relevance judgments, it evaluates only two retrievers on the dataset. Research goal . In this paper, we examine to what extent established findings and best practices in text ranking methods are generalisable to the deep research setup . We identify three research gaps in text ranking for deep research that have received limited attention. First, passage-level information units have received little attention in deep research . Most existing studies build deep research agents that search and read at the document level (i.e., full web pages). Because feeding full web pages to LLM-based agents quickly exhausts the context window, prior work (Sharifymoghaddam and Lin, 2026 ; Chen et al. , 2025 ) typically returns truncated documents to agents, which may remove relevant content and lead to information loss. Although prior work introduces an additional full-document read tool that agents can invoke to access complete documents (Chen et al. , 2025 ) , it adds system complexity. There is strong motivation to explore passage-level units in deep research: (i) their concise nature makes them more efficient under limited context-window budgets; (ii) they allow agents to access any relevant segments within a document, avoiding information loss from document truncation; (iii) passages can enhance lexical retrieval by avoiding the difficulties of document length normalisation (Kaszkiel and Zobel, 1997 ) ; and (iv) a large body of neural retrievers has been developed for passage retrieval (Lassance et al. , 2024 ; Santhanam et al. , 2022 ; Formal et al. , 2021a , b ) , but it remains unclear how well they perform in deep research. To address this gap, we ask RQ1: To what extent are existing retrievers effective in deep research under passage-level and document-level retrieval units? In this RQ, we revisit key findings that neural retrievers often underperform lexical methods (e.g., BM25 (Robertson et al. , 1995 ) ) on out-of-domain data (Thakur et al. , 2021 ) , and that learned sparse and multi-vector dense (a.k.a. late-interaction) retrievers generalise better than single-vector dense retrievers on out-of-domain data (Formal et al. , 2021a ; Thakur et al. , 2021 ) . Second, our understanding of re-ranking in deep research remains limited . Re-ranking plays an important role in lifting relevant documents to the top of ranked lists in traditional search settings (Nogueira and Cho, 2019 ) . It remains unclear whether widely-used re-ranking methods (Weller et al. , 2025b ; Ma et al. , 2024 ; Nogueira et al. , 2020 ) provide consistent benefits when the content consumer is an LLM-based agent rather than a human user. Despite recent work (Sharifymoghaddam and Lin, 2026 ) examining one specific re-ranking method with a single retriever, a systematic evaluation across various ranking configurations is still lacking in deep research. To address this gap, we ask RQ2: To what extent is a re-ranking stage effective in deep research under different initial retrievers, re-ranker types, and re-ranking cut-offs? In this RQ, we revisit established findings that re-ranking effectively improves ranking performance (Ma et al. , 2024 ; Thakur et al. , 2021 ; Nogueira and Cho, 2019 ) , deeper re-ranking generally produces better results (Meng et al. , 2024 ) , and reasoning-based re-rankers outperform their non-reasoning counterparts (Weller et al. , 2025b ; Yang et al. , 2025b ) . Third, the potential mismatch between agent-issued queries and the queries used to train existing text ranking methods remains underexplored . Many text ranking methods in the IR community are trained on natural-language-style questions, such as those in MS MARCO (Bajaj et al. , 2018 ) . However, agent-issued queries may not align with the queries these methods expect. Prior studies show that a query-format mismatch between training and inference can significantly hurt neural retrieval quality (Meng et al. , 2025 ; Zhuang et al. , 2022 ) . It remains unclear how such a potential mismatch affects the performance of existing text ranking methods in deep research. To address this gap, we ask RQ3: To what extent does the mismatch between agent-issued queries and the training queries used for text ranking methods affect their performance? Experiments . We perform experiments on BrowseComp-Plus (Chen et al. , 2025 ) , a deep research dataset that provides a fixed document corpus and human-verified relevance judgments. 1 1 1 To the best of our knowledge, at the time of writing, this is the only publicly available deep research dataset providing this setup. To ensure broad coverage, we use widely-used retrievers spanning 4 main paradigms in modern IR: lexical-based sparse (BM25 (Robertson et al. , 1995 ) ), learned sparse (SPLADE-v3 (Lassance et al. , 2024 ) ), single-vector dense (RepLLaMA (Ma et al. , 2024 ) , Qwen3-Embed (Zhang et al. , 2025 ) ), and multi-vector dense retrievers (ColBERTv2 (Santhanam et al. , 2022 ) ). For re-ranking, we select methods that trade off effectiveness and efficiency at 3 operational points: a relatively inexpensive re-ranker (monoT5-3B (Nogueira et al. , 2020 ) ), an LLM-based re-ranker (RankLLaMA-7B (Ma et al. , 2024 ) ), and a CoT-based reasoning re-ranker (Rank1-7B (Weller et al. , 2025b ) ), which generates additional reasoning tokens. We use two open-source LLM-based agents: gpt-oss-20b (Agarwal et al. , 2025 ) and GLM-4.7-Flash (30B) (Zeng et al. , 2025 ) . Findings . For RQ1, our findings are fivefold: (i) The concise nature of passage-level units enables more search and reasoning iterations before reaching context-window limits, resulting in higher answer accuracy than document-level units (without a full-document reader), particularly for the gpt-oss-20b agent that has shorter context windows. (ii) BM25 on the passage corpus outperforms neural retrievers in most cases (gpt-oss-20b with BM25 achieves the highest accuracy of 0.572 across all retrieval settings in our study). We find agent-issued queries tend to follow a web-search style with keywords, phrases, and quotation marks for exact matching (see Table 5 ), favouring lexical retrievers such as BM25. (iii) On the document corpus, BM25 performs worst under the parameter settings used in prior work (Chen et al. , 2025 ) . We find that these parameters lack proper document-length normalisation. With document-oriented parameters, however, BM25 becomes highly competitive. This highlights BM25’s sensitivity to length normalisation and the advantage of passage-level units, which reduce reliance on document-length normalisation. (iv) learned sparse (Lassance et al. , 2024 ) and multi-vector dense (Santhanam et al. , 2022 ) retrievers with only millions of parameters generalise better to web-search-style queries than 7B/8B single-vector dense models. (v) Enabling a full-document reader on truncated documents generally improves answer accuracy and reduces search calls, suggesting that the reader complements truncated inputs. In contrast, adding the reader to the passage corpus slightly degrades performance, likely because passage retrieval already provides access to any segments within a document, rendering the reader redundant. For RQ2, re-ranking consistently improves ranking effectiveness and answer accuracy while reducing search calls, confirming its important role in deep research. These gains are further amplified by deeper re-ranking depths and stronger initial retrievers. Notably, the BM25–monoT5-3B pipeline with gpt-oss-20b achieves the best results in our work, reaching 0.716 recall and 0.689 accuracy. Despite using only a 20B agent with BM25 and a 3B re-ranker, this setup approaches the 0.701 accuracy of a GPT-5–based agent (Table 1 in (Chen et al. , 2025 ) ). The reasoning-based Rank1 (Weller et al. , 2025b ) shows no clear advantage over non-reasoning methods, as it often misinterprets the intent of keyword-rich web-search queries, limiting the benefits of reasoning. For RQ3, we propose a query-to-question (Q2Q) method to translate agent-issued web search queries into natural-language questions (similar to MS MARCO-style questions (Bajaj et al. , 2018 ) ), significantly improving neural retrieval and re-ranking performance. This indicates that the mismatch between agent-issued queries and queries used for training neural rankers can severely degrade neural ranking effectiveness. Mitigating this training–inference query mismatch is therefore critical for improving neural rankers in deep research. Contributions . Our main contributions are as follows: • To the best of our knowledge, we are the first to reproduce a comprehensive set of text ranking methods in the context of deep research. • We construct a passage corpus for the recent deep research dataset BrowseComp-Plus. • Experiments across 2 open-source agents, 5 retrievers, and 3 re-rankers reveal the effectiveness of the following components in deep research: passage-level information units, retrievers suited to web-search-style queries, re-ranking, and mitigation of the training–inference query mismatch in neural ranking. • We open-source our code, data, and all agent-generated traces of reasoning and search calls at https://github.com/ChuanMeng/text-ranking-in-deep-research . Following the BrowseComp-Plus protocol, the traces are released in an encrypted format and can be locally decrypted for post-hoc analyses of agent behaviour. 2. Task definition Text ranking has been extensively studied in ad-hoc search, which typically follows a single-shot paradigm over user-issued queries. Given a query q u q_{u} issued by a user and a corpus of documents C = { d 1 , d 2 , … , d n } C=\{d_{1},d_{2},\dots,d_{n}\} with n n documents, the goal of a text ranking method f f is to return a ranked list D ⊆ C D\subseteq C of size k k , i.e., D = f ​ ( q u , C ) D=f(q_{u},C) . Text ranking in deep research . We follow the ReAct (Yao et al. , 2022 ) paradigm, widely used in recent work (Xu et al. , 2026a ; Chen et al. , 2025 ) , to define text ranking in deep research. Given a query q u q_{u} issued by a user, a deep research agent A A takes q u q_{u} as input and performs multiple iterations of reasoni