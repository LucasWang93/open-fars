Title: Component-Aware Pruning Framework for Neural Network Controllers via Gradient-Based Importance Estimation

Abstract: The transition from monolithic to multi-component neural architectures in advanced neural network controllers poses substantial challenges due to the high computational complexity of the latter. Conventional model compression techniques for complexity reduction, such as structured pruning based on norm-based metrics to estimate the relative importance of distinct parameter groups, often fail to capture functional significance. This paper introduces a component-aware pruning framework that utilizes gradient information to compute three distinct importance metrics during training: Gradient Accumulation, Fisher Information, and Bayesian Uncertainty. Experimental results with an autoencoder and a TD-MPC agent demonstrate that the proposed framework reveals critical structural dependencies and dynamic shifts in importance that static heuristics often miss, supporting more informed compression decisions.

Body: Component-Aware Pruning Framework for Neural Network Controllers via Gradient-Based Importance Estimation 1 Introduction 2 Related Work 2.1 Magnitude-Based Criteria 2.2 Importance Estimation and Second-Order Methods 2.3 Optimization Frameworks 3 Methodology 3.1 Gradient-Based Importance Metrics 3.1.1 Gradient Magnitude Accumulation 3.1.2 Fisher Information (Diagonal Approximation) 3.1.3 Bayesian (Empirical) 3.2 Training Regime 3.2.1 Temporal Smoothing 3.2.2 Coefficient Parameters 3.2.3 Loss Function Formulation 3.2.4 Scheduler for Regularization Coefficients 4 Experimental Setup 4.1 Use Cases 4.1.1 Autoencoder on MNIST 4.1.2 TD-MPC for Balancing an Inverted Pendulum 4.2 Component-Aware Pruning Group Formation 4.3 Estimating Pruning Group Importance 4.3.1 Limitations of Conventional Approaches 4.3.2 Component-Specific and Coupling Groups 5 Results and Discussion 5.1 Use Case 1: MNIST Autoencoder 5.2 Use Case 2: TD-MPC for Balancing an Inverted Pendulum 6 Conclusion and Future Work Component-Aware Pruning Framework for Neural Network Controllers via Gradient-Based Importance Estimation Ganesh Sundaram Jonas Ulmen Daniel G√∂rges Department of Electrical and Computer Engineering, RPTU University Kaiserslautern-Landau, Germany (e-mail: {ganesh.sundaram, jonas.ulmen, daniel.goerges}@rptu.de). Abstract The transition from monolithic to multi-component neural architectures in advanced neural network controllers poses substantial challenges due to the high computational complexity of the latter. Conventional model compression techniques for complexity reduction, such as structured pruning based on norm-based metrics to estimate the relative importance of distinct parameter groups, often fail to capture functional significance. This paper introduces a component-aware pruning framework that utilizes gradient information to compute three distinct importance metrics during training: Gradient Accumulation, Fisher Information, and Bayesian Uncertainty. Experimental results with an autoencoder and a TD-MPC agent demonstrate that the proposed framework reveals critical structural dependencies and dynamic shifts in importance that static heuristics often miss, supporting more informed compression decisions. keywords: AI-driven modeling and control, Machine learning for modeling and prediction, Reinforcement learning and deep learning in control, Neural network Compression, Structured pruning, Neural network controllers ‚Ä† ‚Ä† thanks: Authors have contributed equally. 1 Introduction The adoption of Deep Neural Networks (DNNs) has transformed numerous domains, including perception and control, by facilitating advanced representation learning and generalization. Despite these advances, the increasing complexity of multi-component model architectures for environmental understanding has widened the gap between research-grade models and the strict resource limitations of real-world embedded systems (Liu et al., 2025 ) . Closing this gap necessitates effective model compression. However, conventional approaches such as unstructured pruning or magnitude-based structured pruning often treat networks as indivisible entities. Traditional methods typically use static, rule-based heuristics that fail to account for cross-component dependencies. This oversight can result in the elimination of critical connections that are essential for maintaining the stability and performance of complex closed-loop controllers. This work addresses these limitations by introducing a component-aware structured pruning framework. Unlike conventional methods, the proposed approach explicitly distinguishes between component-specific groups and inter-component coupling groups, enabling a more granular and adaptable pruning strategy. Three prevailing heuristics for assessing group importance are systematically evaluated using two representative scenarios: a canonical Multi-Component Neural Architecture (MCNA) autoencoder and a control-oriented Temporal Difference Learning for Model Predictive Control (TD-MPC) agent. The empirical results challenge the prevailing assumption that coupling groups or early layers are inherently critical. Instead, the findings indicate that group importance is conditional, architecture-dependent, and highly dynamic, with significant shifts occurring throughout training. To capture these dynamics, the framework incorporates a unified pruning pipeline that assesses importance using three distinct metrics: Gradient Accumulation , Fisher Information , and Bayesian Uncertainty . This allows for informed, data-driven pruning decisions without incurring the high computational cost of post-training sensitivity analysis. By revealing essential structural dependencies, the method preserves control performance under aggressive compression and offers interpretable insights into the contributions of different components to overall controller fidelity. The remainder of this paper is organized as follows: Section 2 reviews prior pruning methods, Section 3 details the structured pruning formalism and importance measures, Section 4 describes the experimental setup, and Section 5 presents the results and analysis. 2 Related Work Structured pruning methods are typically classified into magnitude-based heuristics, importance estimation techniques, and learning-based optimization approaches. 2.1 Magnitude-Based Criteria Early heuristic methods performed pruning based on activation patterns or magnitude ranking, sometimes employing an ‚Ñì 1 \ell_{1} penalty to promote sparsity (Hu et al., 2016 ; Li et al., 2016 ; Liu et al., 2017 ) . Structured sparsity can be applied to channels, filters, or blocks and may be guided by reconstruction error or gating functions (Luo et al., 2017 ; He et al., 2017a , b ) . Gradually increasing sparsity regularization has been shown to reduce redundancy (Wang et al., 2020 ) . However, recent surveys caution that naive heuristics risk discarding essential parameters and emphasize that training the target architecture from scratch can achieve performance comparable to pruned models (He and Xiao, 2024 ; Cheng et al., 2024 ; Blalock et al., 2020 ) . 2.2 Importance Estimation and Second-Order Methods Second-order methods, including Optimal Brain Damage, Optimal Brain Surgeon, and Taylor-series pruning, approximate the Hessian matrix to evaluate parameter importance (LeCun et al., 1989 ; Frantar et al., 2023 ; Molchanov et al., 2017 ) . More recent approaches use gradient accumulation and Bayesian uncertainty to estimate the importance of parameter groups (Molchanov et al., 2019a , b ; Ke and Fan, 2022 ) . 2.3 Optimization Frameworks Learning-based approaches conceptualize pruning as a search or optimization problem. Reinforcement learning and automated machine learning (AutoML) methods are employed to explore optimal sparsity ratios (Huang et al., 2018 ) . Single-shot saliency techniques, such as SNIP (Lee et al., 2019 ) , assess parameter importance at initialization. Hybrid frameworks enable the simultaneous pruning of layers, attention heads, and hidden units in transformer architectures (Michel et al., 2019 ; Xia et al., 2022 ) . Collectively, these methods demonstrate that pruning can be integrated within a comprehensive optimization pipeline rather than relying solely on fixed heuristics. 3 Methodology This section presents our primary contributions. First, we propose three importance metrics for each pruning group, each derived from the gradients of the corresponding parameters. Second, we introduce a training regime that accumulates gradients using an exponential moving average and incorporates a scheduling mechanism to prevent vanishing gradients in the importance metric as the model converges on the task loss. 3.1 Gradient-Based Importance Metrics Although frameworks such as Torch-Pruning offer essential tools for managing architectural dependencies during structured pruning, they do not specify methods for measuring the importance of each parameter group (Fang et al., 2023 ) . Users are therefore required to define appropriate scoring metrics to guide pruning decisions. Traditional approaches often use static, heuristic-based metrics, such as weight norms. However, these can be inadequate for complex models in which a group‚Äôs functional importance does not necessarily align with its magnitude. To address this limitation, this work investigates dynamic, gradient-based metrics that quantify a group‚Äôs influence on task loss, thereby providing a more principled basis for pruning decisions. Previous approaches, such as (Sundaram et al., 2025b ) , employ grid search or gradient-based optimization to determine pruning-group coefficients, which specify the number of parameters each group must remove to achieve the desired pruning objective. However, these coefficient weightings can only be determined post-training, limiting their utility for importance estimation during training. To address this limitation, a set of gradient-based importance metrics is introduced, computed online, and integrated into the training process. These metrics leverage group-wise gradients of the training loss and capture complementary aspects of importance, including instantaneous sensitivity, curvature, and accumulated learning activity. Combining these metrics provides a more robust estimate of importance than relying on a single metric. 3.1.1 Gradient Magnitude Accumulation For a pruning group g g with parameter set Œò g \Theta_{g} , define the per-parameter average absolute gradient at one iteration t t of an epoch as I g grad ‚Äã ( t ) = 1 N g ‚Äã ‚àë Œ∏ ‚àà Œò g | ‚àá Œ∏ ‚Ñí ‚Äã ( t ) | , I_{g}^{\mathrm{grad}}(t)\;=\;\frac{1}{N_{g}}\sum_{\theta\in\Theta_{g}}\left|\nabla_{\theta}\mathcal{L}(t)\right|, Here, ‚àá Œ∏ ‚Ñí \nabla_{\theta}\mathcal{L} represents the gradient of the loss ‚Ñí \mathcal{L} with respect to the parameters Œ∏ \theta , and N g ‚àà ‚Ñï N_{g}\in\mathbb{N} denotes the number of parameters in the group g g . This metric quantifies the model‚Äôs immediate tendency to update a group, reflecting the steepness of the loss landscape for that group and the group‚Äôs instantaneous importance. Higher values indicate groups that are updated more frequently during training. While this metric captures instantaneous learning activity, it may be subject to noise. 3.1.2 Fisher Information (Diagonal Approximation) The Fisher Information Matrix quantifies the sensitivity of the loss to parameter changes. Parameters with high Fisher information have a strong influence on model predictions. It is computed as the diagonal approximation of the Fisher Information Matrix, i.e. I g Fisher ‚Äã ( t ) = 1 N g ‚Äã ‚àë Œ∏ ‚àà Œò g ùîº ‚Äã [ ( ‚àá Œ∏ ‚Ñí ‚Äã ( t ) ) 2 ] I_{g}^{\text{Fisher}}(t)=\frac{1}{N_{g}}\sum_{\theta\in\Theta_{g}}\mathbb{E}[(\nabla_{\theta}\mathcal{L}(t))^{2}] In practice, we approximate the expectation with the current batch, i.e. I g Fisher ‚Äã ( t ) ‚âà 1 N g ‚Äã ‚àë Œ∏ ‚àà Œò g ( ‚àá Œ∏ ‚Ñí ‚Äã ( t ) ) 2 I_{g}^{\text{Fisher}}(t)\approx\frac{1}{N_{g}}\sum_{\theta\in\Theta_{g}}(\nabla_{\theta}\mathcal{L}(t))^{2} Fisher information quantifies the curvature of the loss landscape and indicates the sensitivity of the loss function to small parameter perturbations. Parameters associated with large Fisher values exert a strong influence on model predictions and are thus more critical for overall performance. Additionally, the Fisher matrix provides a tractable approximation for the Hessian, as it is the expected outer product of gradients and is formulated as ‚Ñ± ‚Äã ( t ) = ùîº ‚Äã [ ‚àá ‚Ñí ‚Äã ( t ) ‚ãÖ ‚àá ‚Ñí ‚Äã ( t ) T ] ‚âà ùîº ‚Äã [ ‚àá 2 ‚Ñí ‚Äã ( t ) ] \mathcal{F}(t)=\mathbb{E}[\nabla\mathcal{L}(t)\cdot\nabla\mathcal{L}(t)^{T}]\approx\mathbb{E}[\nabla^{2}\mathcal{L}(t)] 3.1.3 Bayesian (Empirical) In this context, standard Bayesian inference is challenging because it requires specifying a likelihood for the gradient observations and computing posterior integrals at each training step. Both requirements are often computationally expensive or intractable. Therefore, a lightweight empirical Bayes formulation is adopted. The per-group gradient energies are modeled as samples from an exponential distribution with the rate parameter Œª g \lambda_{g} , i.e. E g ‚àº Exponential ‚Äã ( Œª g ) E_{g}\sim\mathrm{Exponential}(\lambda_{g}) with density p ‚Äã ( E g ‚à£ Œª g ) = Œª g ‚Äã e ‚àí Œª g ‚Äã E g , E g ‚â• 0 . p(E_{g}\mid\lambda_{g})=\lambda_{g}e^{-\lambda_{g}E_{g}},\;E_{g}\geq 0. The conjugate prior for the rate Œª g \lambda_{g} is a Gamma distribution Œì ‚Äã ( Œ± g , Œ≤ g ) \Gamma(\alpha_{g},\beta_{g}) , which enables simple additive updates. Instead of maintaining the full posterior p ‚Äã ( Œª g ‚à£ data ) p(\lambda_{g}\mid\text{data}) , only its mean is tracked via the sufficient statistics ( Œ± g , Œ≤ g ) (\alpha_{g},\beta_{g}) . To capture cumulative evidence of a group‚Äôs learning activity, the per-iteration group gradient energy is defined as follows. E g ( t ) = 1 N g ‚Äã ‚àë Œ∏ ‚àà N g | ‚àá Œ∏ ‚Ñí ‚Äã ( t ) | . E_{g}^{(t)}\;=\;\frac{1}{N_{g}}\sum_{\theta\in N_{g}}\left|\nabla_{\theta}\mathcal{L}(t)\right|. We maintain a Gamma prior in the ( shape = Œ± , rate = Œ≤ ) (\text{shape}=\alpha,\ \text{rate}=\beta) parameterization and update it online with a fractional pseudo-count Œ∫ 0 \kappa 0 : Œ± g ( t + 1 ) = Œ± g ( t ) + Œ∫ , Œ≤ g ( t + 1 ) = Œ≤ g ( t ) + Œ∫ ‚ãÖ E g ( t ) Œ∑ , \alpha_{g}^{(t+1)}\;=\;\alpha_{g}^{(t)}+\kappa,\qquad\beta_{g}^{(t+1)}\;=\;\beta_{g}^{(t)}+\kappa\cdot\frac{E_{g}^{(t)}}{\eta}, Here, Œ∑ 0 \eta 0 is a scaling factor and Œ∫ \kappa controls the effective observation weight (with Œ∫ = 0.25 \kappa=0.25 used in experiments). The posterior mean of the rate is then Œº g ( t ) = Œ± g ( t ) Œ≤ g ( t ) , \mu_{g}^{(t)}\;=\;\frac{\alpha_{g}^{(t)}}{\beta_{g}^{(t)}}, This value summarizes the accumulated learning activity for group g g . Subsequently, this historical activity is combined with the current sensitivity as follows. I g Bayes ‚Äã ( t ) = log ‚Å° ( 1 + Œº g ( t ) ) ‚Äã ( 1 + I g Fisher ‚Äã ( t ) ) , I_{g}^{\mathrm{Bayes}}(t)\;=\;\log\!\bigl(1+\mu_{g}^{(t)}\bigr)\,\bigl(1+I_{g}^{\mathrm{Fisher}}(t)\bigr), where the log ( 1 + ‚ãÖ ) \log(1+\cdot) term improves numerical stability as Œº g \mu_{g} grows and modulates instantaneous Fisher sensitivity with long-term activity. Interpretation: The Gamma model interprets large, persistent gradient energies as evidence that the network actively utilizes a group. The Gamma prior accumulates this evidence over time, and its mean Œº g ( t ) \mu_{g}^{(t)} provides a smoothed estimate of long-term group activity. Consequently, the Bayesian importance score I g Bayes I_{g}^{\mathrm{Bayes}} assigns higher ranks to groups that are both currently sensitive to the loss (exhibiting high Fisher information) and historically active (with large Œº g ( t ) \mu_{g}^{(t)} ), thereby prioritizing groups that consistently contribute rather than those relevant only during isolated updates. 3.2 Training Regime Training the network for subsequent pruning with the proposed metrics requires additional gradient information, which must be collected during training. Two phenomena need to be