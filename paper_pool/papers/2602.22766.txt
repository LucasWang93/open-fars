Title: Imagination Helps Visual Reasoning, But Not Yet in Latent Space

Abstract: Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.

Body: Imagination Helps Visual Reasoning, But Not Yet in Latent Space 1 Introduction 2 Related Work 2.1 Visual Reasoning with Tools 2.2 Visual Reasoning through Imagination 3 Analysis: Latent Tokens Hardly Helps 3.1 Formulation 3.2 Causal Analysis of X ‚Üí Y X\rightarrow Y 3.3 Causal Analysis of Z ‚Üí Y Z\rightarrow Y 3.3.1 Intervation on Latent Tokens Z Z 3.3.2 Probing Analysis on Latent tokens Z Z 4 CapImagine 4.1 Method Design 4.2 Dataset Construction 5 Experiments 5.1 Experiment Setup 5.2 Main Results 5.3 Ablation Study 5.4 Dependency Analysis 5.5 Efficiency Analysis 6 Conclusion A Examples for Derived Questions in Probing Analysis. B Detailed Results for Intervention on Z Z . C Limitations and Future Work Imagination Helps Visual Reasoning, But Not Yet in Latent Space You Li Chi Chen Yanghao Li Fanhu Zeng Kaiyu Huang Jinan Xu Maosong Sun Abstract Latent visual reasoning aims to mimic human‚Äôs imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect : dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect : perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine , which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination. Machine Learning, ICML Figure 1 : Comparison between visual reasoning with tools and through imagination. (a) Reasoing with tools perceive visual content through function calling such as zoom-in or drawing. (b) Latent-space imagination exploits the hidden states of MLLMs to conduct visual reasoning. (c) We show that imagination can be more effective in text-space. 1 Introduction The field of Visual Reasoning within Multimodal Large Language Models (MLLMs) has witnessed a surge in interest, driven by steady progress in complex mathematical reasoning, spatial understanding, long-horizon planning, and fine-grained visual perception (Yang et al. , 2025b ; Bai et al. , 2025c ; Li et al. , 2025d ; Hong et al. , 2025 ; Yang et al. , 2025a ; Luo et al. , 2026 ; Lou et al. , 2025 ; Ma et al. , 2025 ) . The increasing complexity of visual reasoning tasks requires MLLM to employ more active percpetion of visual content. To meet this demand, visual reasoning with tools (Zhang et al. , 2025b ; Hong et al. , 2025 ; Zhao et al. , 2025 ) generates interleaved multimodal reasoning trajectories, effectively incorporating visual semantics during the reasoning process. However, the obtained auxiliary image during reasoning process mostly comes from limited set of rigid tools, posing huge gap with human-like native imagination. To address this gap, Latent Visual Reasoning (LVR) (Yang et al. , 2025c ) emerges as a novel paradigm that reasons through the hidden state in MLLMs, which we refers as latent token. Since input embeddings from different modalities have been aligned within the model, LVR trains MLLMs to output latent tokens that are compatible with visual embeddings and encode rich visual semantics. By deliberating in this high-dimensional latent space, LVR enables a broader and less constrained form of Visual Imagination (Atwood, 1971 ; Pylyshyn, 2002 ) . Building on these properties, a series of latent-space visual reasoning methods have been proposed (Li et al. , 2025b ; Wang et al. , 2025b ; Tong et al. , 2025 ; Li et al. , 2025b ; Liu et al. , 2025 ; Wang et al. , 2026 ; Zhang et al. , 2025a , c ) . These approches typically supervise latent tokens using visual features or hidden representations from the teacher model. Empirically, they exhibit strong performance across various vision-centric tasks. Despite these promising results, the internal mechanism of Latent Visual Reasoning and the behavior of latent tokens are still poorly understood. In particular, it is unclear whether and how MLLM actually performs deliberative reasoning within the latent space. To address this gap, we adopt Causal Mediation Analysis framework and conceptualize latent reasoning as a causal process from input X X to intermediate latent tokens Z Z , and finally to output Y Y , i.e., X ‚Üí Z ‚Üí Y X\rightarrow Z\rightarrow Y . Our analysis focuses on systematic perturbations on both X X and Z Z to examine the full causality. We begin by conducting instance-level perturbations on the input X X , where the entire input sequence is altered. Surprisingly, the resulting latent tokens Z Z exhibit a high degree of homogeneity measured by cosine similarity, even across diverse inputs and tasks. This similarity indicates a disconnect in the X ‚Üí Z X\rightarrow Z causality. Taken a step further, we implement systemtic intervention analysis and probing analysis on Z Z . Across multiple latent reasoning methods and diverse benchmarks, we find that drastic perturbations to Z Z lead to negligible changes in final answer. Moreover, probing analysis reveals that latent tokens encode only minimal task-relevant visual semantics and are insufficient to support downstream reasoning on their own. The intervention and probing analysis demonstrates a disconnect in the Z ‚Üí Y Z\rightarrow Y causality Collectively, these results show that latent tokens neither vary meaningfully according to the inputs, nor do they actually affect the final answer. These observations naturally leads to a second question: how can MLLMs perform visual reasoning? To explore this, we propose a simple yet effective method under a strictly controlled setting. Specifically, we convert the original Monet-SFT-125K (Wang et al. , 2025b ) training data into a text-space imagination format. For each interleaved reasoning image, we generate textual descriptions of visual manipulations, such as highlighting or zooming into regions of interest, and train the MLLM to internalize these operations purely through text. Using the same data source as Monet, this simple data reformulation strategy yields substantially stronger results than latent-space approaches. Extensive evaluations on V* (Wu and Xie, 2024 ) , HR-Bench (Wang et al. , 2025c ) , MME-RealWorld-Lite (Zhang et al. , 2024 ) , and other vision-centric benchmarks show consistent improvements, surpassing Monet by 4.0% on HR-Bench-8K and 4.9% on MME-RealWorld-Lite. We believe our work sheds light on how to build more faithful, interpretable, and causally effective visual reasoning methods. In conclusion, our contribution can be concluded as follows: ‚Ä¢ We conduct a systematic study on latent tokens in visual reasoning through Causal Mediation Analysis, revealing that latent tokens contributes little to the causal reasoning process. ‚Ä¢ We propose a simple yet effective text-space imagination method CapImagine , showing better causality than latent-space methods. ‚Ä¢ Our method substantially outperforms latent-space approaches across multiple vision-centric benchmarks, demonstrating strong effectiveness and generality. 2 Related Work 2.1 Visual Reasoning with Tools Tool-augmented visual reasoning approaches actively engage with the visual modality, adaptively perceiving visual content through explicit manipulation to lead to the final answer. These methods can be further distinguished by how intermediate visual observations are produced. Some works rely on fixed tool set such as zoom-in or image-drawing operations (Zheng et al. , 2025 ; Qi et al. , 2024 ; Lai et al. , 2025 ; Jiang et al. , 2025 ; Cao et al. , 2025 ; Fu et al. , 2025 ; Chen et al. , 2025 ) to actively perceive the visual elements, dramatically expanding perceptual bandwidth compared with static perception. From the cognition and knowledge perspective, another stream of work (Wu et al. , 2025a ; Yu et al. , 2026 ; Narayan et al. , 2025 ) seeks to utilize retrieval or web-search tools for factual verification and external multimodal knowledge injection. Expanding the scope of predefined tool set, other approaches leverage self-rendered code to enable more flexible and free-form visual manipulations (Zhao et al. , 2025 ; Geng et al. , 2025 ; Hong et al. , 2025 ) , faciliating agentic MLLM in visual reasoning. 2.2 Visual Reasoning through Imagination Visual Imagination could be achieved through self-generation or latent-space reasoning. Unified multimodal models attempt to visually imagine through its inner generation ability, explicitly instantiating internal reasoning states (Deng et al. , 2025 ; Li et al. , 2025c ; Shi et al. , 2025 ) . Latent visual reasoning proposes to conduct imagination through the hidden states in MLLMs, without decoding it into specific text token. Latent visual reasoning was first introduced by Mirage (Yang et al. , 2025c ) , which addresses the challenge of latent supervision design by compressing visual features extracted from intermediate reasoning images. Subsequent works (Li et al. , 2025b ; Tong et al. , 2025 ; Dong et al. , 2025 ; Zhang et al. , 2025a ) largely follow adopting vision encoder features as supervision signals, and further extend latent reasoning to broader perception scenarios, more flexible latent formats, and improved strategies for selecting supervisory visual features. However, visual features are inherently continuous and semantically sparse. The compression strategy in Mirage tends to dilute discriminative semantics, and directly supervising latents with entire visual token sequences (Li et al. , 2025b ) can lead to latent mode collapse during inference. Monet (Wang et al. , 2025b ) introduces a distillation-based framework (Shen et al. , 2025 ) that restricts gradient propagation exclusively to latent tokens, thereby preserving informative semantics from both intermediate images and key textual cues. Despite these advances, the LVR field still lacks rigorous investigation of many core design choices and mechanisms, an issue this paper aims to address. 3 Analysis: Latent Tokens Hardly Helps 3.1 Formulation Latent Visual Reasoning refers to a reasoning paradigm in which the last hidden states of the final transformer layer are treated as latent tokens for solving visual question answering tasks. Given a set of input images I i i = 0 N {I_{i}}_{i=0}^{N} and a question q q , the model is required to produce an answer conditioned on the joint input X = ( { I i } i = 0 N , q ) X=(\{I_{i}\}_{i=0}^{N},q) . During inference, the model ‚Ñ≥ \mathcal{M} can adaptively switch between decoding normal text tokens and latent tokens. The inference process is formally defined as: h i = ‚Ñ≥ ‚Äã ( E ‚Äã ( x ) ; y i ) , y 0 = ‚àÖ h_{i}=\mathcal{M}\left(E(x);y_{ i}\right),\quad y_{0}=\emptyset (1) y i = ùïÄ ‚Äã ( i ‚àà ‚Ñê L ) ‚ãÖ œï ‚Äã ( h i ) + ùïÄ ‚Äã ( i ‚àâ ‚Ñê L ) ‚ãÖ E ‚Äã ( Decode ‚Äã ( h i ) ) y_{i}=\mathbb{I}(i\in\mathcal{I}_{L})\cdot\phi(h_{i})+\mathbb{I}(i\notin\mathcal{I}_{L})\cdot E\left(\text{Decode}(h_{i})\right) (2) where ‚Ñê L \mathcal{I}_{L} denotes the index set of latent tokens, œï ‚Äã ( h i ) \phi(h_{i}) is an optional projection layer applied to hidden states, and E ‚Äã ( ‚ãÖ ) E(\cdot) represents the embedding process. The indicator function ùïÄ ‚Äã ( ‚ãÖ ) \mathbb{I}(\cdot) determines whether the current decoding step operates in latent mode or normal text mode. Figure 2 : Our systematic latent analysis framework for investigating the internal mechanisms and behavioral patterns of latent tokens. (a) Model Inference illustrates the latent inference process. (b) and (c) respectively illustrate two causal analysis approaches. In diagram Intervention on Z Z , œÑ \tau denotes a fixed tensor, œµ \epsilon represents random Gaussian noise with œµ ‚àº ùí© ‚Äã ( 0 , œÉ 2 ) \epsilon\sim\mathcal{N}(0,\sigma^{2}) , and Œº \mu is a small value close to zero. In practice, the number of latent tokens is usually predefined. The latent mode starts immediately after the model outputs |latent_start| . During latent mode, the model takes the last hidden state as the input for the next step. The model exits latent mode when the current hidden state is decoded as |latent_end| , after which normal text decoding resumes. When text and latent tokens are interleaved together, our primary research focus are the latent tokens, denoted as Z Z . Given the original input X X and the whole reasoning process, the model ultimately produces the final answer Y Y . Explicitly tracing the role of latent tokens in visual reasoning, we abstract the overall reasoning process as: X ‚Üí Z ‚Üí Y X\rightarrow Z\rightarrow Y (3) In the following content, we will conduct targeted interventions P ‚Äã ( Z ‚à£ d ‚Äã o ‚Äã ( X ) ) P(Z\mid do(X)) and P ‚Äã ( Y ‚à£ d ‚Äã o ‚Äã ( Z ) ) P(Y\mid do(Z)) to elucidate the role of latent tokens in visual reasoning. 3.2 Causal Analysis of X ‚Üí Y X\rightarrow Y Finding 1: Latent tokens are similar across instances and tasks, and progressively collapse into highly identical states. We begin with a causal mediation analysis (Pearl, 2009 ) conducting instance-level perturbations on the input X X and measure how the latent reasoning tokens change accodingly with the entire input sequence altering, i.e., P ‚Äã ( Z ‚à£ d ‚Äã o ‚Äã ( x ) ) P(Z\mid do(x)) . Experiment Setting We evaluate three representative baselines: (1) Monet (Wang et al. , 2025b ) , a distillation-based model focused on general scenarios; (2) LVR (Li et al. , 2025b ) , which leverages image features as supervision in general scenarios; and (3) Mirage (Yang et al. , 2025c ) , which also uses image features but is fine-tuned for task-specific settings. For general VQA scenarios, we uniformly sample instances from V* (Wu and Xie, 2024 ) , MME (Yin et al. , 2024 ) , OCRBench-v2 (Fu et al. , 2024a ) , MME-Realworld-Lite (Zhang et al. , 2024 ) , and TableVQA (Kim et al. , 2024 ) , resulting in a total of 100 testing instances. These instances have been sorted and grouped in results visualization. For the task-specific Mirage model, we adopt its released Visual Spatial Planning dataset, which requires the model to reach the destination circumventing obstacles such as frozen lakes. During inference, all three models are prompted to perform latent reasoning and only instances with valid 