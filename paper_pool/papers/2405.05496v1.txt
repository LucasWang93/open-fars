Title: Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis

Abstract: Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment analysis, which aims to extract the aspects and predict their sentiments. Most existing studies focus on improving the performance of the target domain by fine-tuning domain-specific models (trained on source domains) based on the target domain dataset. Few works propose continual learning tasks for ABSA, which aim to learn the target domain's ability while maintaining the history domains' abilities. In this paper, we propose a Large Language Model-based Continual Learning (\texttt{LLM-CL}) model for ABSA. First, we design a domain knowledge decoupling module to learn a domain-invariant adapter and separate domain-variant adapters dependently with an orthogonal constraint. Then, we introduce a domain knowledge warmup strategy to align the representation between domain-invariant and domain-variant knowledge. In the test phase, we index the corresponding domain-variant knowledge via domain positioning to not require each sample's domain ID. Extensive experiments over 19 datasets indicate that our \texttt{LLM-CL} model obtains new state-of-the-art performance.

Body: Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis 1 Introduction 2 Related Work 2.1 Aspect-based Sentiment Analysis 2.2 Continual Learning for NLP 2.3 Continual Learning for ABSA 3 Our Method 3.1 LLMs-Based ABSA Model 3.2 Domain Knowledge Decoupling 3.3 Domain Knowledge Warmup 3.4 Domain Positioning 4 Experimental Setups 4.1 Datasets and Metrics 4.2 Selected Baselines 4.3 Experimental Settings 5 Experimental Analysis 5.1 Catastrophic Forgetting of LLMs 5.2 Main Results Peformance on ABSC. Peformance on AE and JOINT. 5.3 Ablation Studies 5.4 Further Analysis Comparison with SOTA LLMs. The Influence of Rank r ğ‘Ÿ r italic_r . 5.5 Conclusions and Further Work Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis Xuanwen Ding 1 , Jie Zhou 1, â€  â€  thanks: Jie Zhou is the corresponding author. , Liang Dou 1 , Qin Chen 1 , Yuanbin Wu 1 , Chengcai Chen 2 , Liang He 1 1 East China Normal University, 2 Xiao-i Research xwding@stu.ecnu.edu.cn, jzhou@cs.ecnu.edu.cn Abstract Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment analysis, which aims to extract the aspects and predict their sentiments. Most existing studies focus on improving the performance of the target domain by fine-tuning domain-specific models (trained on source domains) based on the target domain dataset. Few works propose continual learning tasks for ABSA, which aim to learn the target domainâ€™s ability while maintaining the history domainsâ€™ abilities. In this paper, we propose a Large Language Model-based Continual Learning ( LLM-CL ) model for ABSA. First, we design a domain knowledge decoupling module to learn a domain-invariant adapter and separate domain-variant adapters dependently with an orthogonal constraint. Then, we introduce a domain knowledge warmup strategy to align the representation between domain-invariant and domain-variant knowledge. In the test phase, we index the corresponding domain-variant knowledge via domain positioning to not require each sampleâ€™s domain ID. Extensive experiments over 19 datasets indicate that our LLM-CL model obtains new state-of-the-art performance. \useunder Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis Xuanwen Ding 1 , Jie Zhou 1, â€  â€  thanks: Jie Zhou is the corresponding author. , Liang Dou 1 , Qin Chen 1 , Yuanbin Wu 1 , Chengcai Chen 2 , Liang He 1 1 East China Normal University, 2 Xiao-i Research xwding@stu.ecnu.edu.cn, jzhou@cs.ecnu.edu.cn 1 Introduction Aspect-based sentiment analysis (ABSA) Pontiki et al. ( 2016 ); Do et al. ( 2019 ); Zhang et al. ( 2022 ) plays an important role in the field of natural language processing. This task can be divided into two sub-tasks: aspect extract (AE), which aims to identify the aspects in the sentence and aspect-based sentiment classification (ABSC) Zhou et al. ( 2019 ) , which aims to infer the polarities of the corresponding aspects. For example, in the review â€œThe service is bad but the food is delicious!", the user expresses negative and positive sentiments for aspects â€œservice" and â€œfood" respectively. Figure 1: Continual learning for a sequence of ABSA domains. The blue color is domain-invariant knowledge, and the other is domain-variant knowledge. The previous work for ABSA mainly trained a domain-specific model with designed architectures, which largely relies on the size of the target dataset Li et al. ( 2018 ); Fei et al. ( 2022 ); Zhou et al. ( 2024 ) . To utilize the datasets of other domains, transfer learning-based methods are proposed to learn the knowledge from source domains to the target domain Marcacini et al. ( 2018 ); Zhou et al. ( 2021 ) . However, these studies focus on improving the performance of the target domain, while ignoring the accuracy of source domains. To address this problem, a few studies introduced continual learning for a sequence of ABSA domains Wang et al. ( 2018 , 2020b ); Ke et al. ( 2021c , d , a ) . Wang et al. Wang et al. ( 2018 ) used a memory network to accumulate aspect sentiment knowledge by itself from big (past) unlabeled data and then used it to better guide its new/future task learning. Wang et al. Wang et al. ( 2020b ) integrated a lifelong machine learning into Positive-Unlabeled (PU) learning model for target-based sentiment analysis. Ke et al. Ke et al. ( 2021c ) introduced a novel contrastive continual learning method for knowledge transfer and distillation, and task masks to isolate task-specific knowledge to avoid catastrophic forgetting. To overcome catastrophic forgetting and transfer knowledge across domains, Ke et al. Ke et al. ( 2021d , a ) presented a novel capsule network based on pre-trained language models (e.g., BERT) to learn task-shared and task-specific knowledge via a masking strategy. They used a task-specific module for all the tasks, while the knowledge in different domains may conflict. Moreover, the relationships between the shared knowledge and specific knowledge are ignored by them. There are still several challenges to continual learning for ABSA. First (C1) , this task requires rich commonsense knowledge to infer the sentiment. For example, the word â€œhot" expresses a negative sentiment polarity for the aspect â€œCPU" in the Laptop domain and has a positive sentiment for the aspect â€œpizza" in the Restaurant domain (See Figure 1 ). Second (C2) , the sentiment knowledge is inconsistent among different domains. The knowledge in each domain can be divided into domain-invariant knowledge (e.g., good, happy) and domain-variant knowledge (e.g., long, hot, fast). For instance, the general sentiment words are domain-invariant knowledge, which does not change among various domains. To address these problems, we propose large language model-based continual learning ( LLM-CL ) for ABSA. Particularly, for C1 , we integrate LLMs to utilize the large-scale commonsense knowledge in the model. Existing work has proved that LLMs can serve as a knowledge base Petroni et al. ( 2019 ); Suchanek and Luu ( 2023 ) . Then, for C2 , we individually consider the domain-invariant and domain-variant knowledge via a domain knowledge decoupling module with an orthogonal constraint. All the domains learn separate adapters for different domains with a shared adapter. Also, we propose a domain knowledge warmup mechanism to align the domain-invariant and -variant representation using replay data. In the test phase, we design a domain positioning strategy to index the correct domain-variant knowledge without knowing the domain the sample belongs to. In the experiments, we first analyze the catastrophic forgetting problem of LLMs for ABSA. Although LLMs can reduce the catastrophic forgetting problem, it is still challenging for LLMs. Comparing our LLM-CL model on ABSC, AE, and JOINT tasks with several strong baselines, our model obtains new state-of-the-art performance on 19 datasets. The ablation studies show the effectiveness of the main components consisting of our LLM-CL model. The key contributions are summarized as follows: â€¢ We propose an LLMs-based CL framework for ABSA to leverage the rich commonsense knowledge in LLMs. â€¢ We decouple domain-invariant and -variant knowledge by modeling the relationships among them using an orthogonal constraint. Then, a domain knowledge warmup strategy is proposed to align the representations of domain-invariant and -variant knowledge. â€¢ We conduct extensive experiments on three subtasks over 19 domain datasets. The results show our LLM-CL model outperforms the existing typical baselines. 2 Related Work 2.1 Aspect-based Sentiment Analysis Aspect-based sentiment analysis (ABSA) emerges as an advanced iteration of sentiment analysis, honing in on the intricate task of identifying specific aspects within a given text and subsequently extracting the associated polarity Zhou et al. ( 2019 ) . In this study, our focus is on its subtasks: aspect extraction (AE), which aims to pinpoint the aspects within a sentence, and aspect-based sentiment classification (ABSC), which seeks to deduce the polarities associated with the corresponding aspects. Neural network-based ABSA models designed domain-specific structures, such as attention Wang et al. ( 2016 ) , memory network Tang et al. ( 2016 ) , sequence to sequence Yan et al. ( 2021 ) and graph neural network Li et al. ( 2021 ); Wang et al. ( 2020a ) . All these models are based on large-scale labeled datasets, which is time-consuming and labor-intensive. Then, transfer learning is adopted for ABSA to transfer the knowledge from the source domain to the target domain He et al. ( 2018 ) , which focuses on improving the performance of the target domain. 2.2 Continual Learning for NLP Continual learning (CL) is dedicated to acquiring new knowledge while addressing the prevalent issue of catastrophic forgetting, a subject extensively explored in NLP Biesialska et al. ( 2020 ); Ke et al. ( 2023 ) . Current research can be broadly categorized into three main approaches: rehearsal-based, regularization-based, and architecture-based methods. Rehearsal-based methods involve conducting experience replay by retaining historical information, which may take the form of preserved data Li et al. ( 2022b ); Scialom et al. ( 2022 ) , or pseudo-data generators Sun et al. ( 2019 ); Qin and Joty ( 2022 ) . Regularization-based methods enhance the loss function by introducing an additional term, commonly implemented through techniques such as knowledge distillation Varshney et al. ( 2022 ); Monaikul et al. ( 2021 ) or parameter importance Li et al. ( 2022a ); Liu et al. ( 2019 ) . This modification aims to discourage alterations to crucial parameters acquired during a prior task when the model adapts to a new one. Architecture-based methods Wang et al. ( 2023b , a ); Razdaibiedina et al. ( 2023 ) allocate sets of task-specific parameters and dynamically integrate them with the frozen base model. These studies mainly focus on reducing the catastrophic forgetting problem based on pre-trained language models (e.g., BERT) whose parameters are much smaller than LLMs. 2.3 Continual Learning for ABSA The most related works to our paper are Ke et al. ( 2021c , d ) , which delved into the CL performance of pre-trained language models in ABSC. These works primarily designed a CL framework that performs well on the target domain while keeping the performance over the history domains. To overcome catastrophic forgetting, they shared a domain-specific module across all the domains and learned the domain-shared or domain-specific knowledge independently. However, domain-variant sentiment knowledge may conflict between the two domains. Moreover, domain-variant knowledge and domain-invariant knowledge are mutually exclusive with rich commonsense knowledge. Leveraging the capabilities of large language models, we model the relationships among domain-invariant and domain-variant knowledge and extend our investigation into ABSA, which performs AE and ABSC jointly. Figure 2: The framework of our LLM-CL . 3 Our Method In this paper, we propose an LLMs-based CL framework for ABSA, which consists of domain knowledge decoupling, domain knowledge warmup and domain positioning (Figure 2 ). Our framework is based on an LLMs-based ABSA model, which trains a generative model using instruction tuning. We first introduce a domain knowledge decoupling module to learn a domain-invariant adapter with individual domain-variant adapters for each domain. Then, we align the domain-invariant and domain-variant representations via a domain knowledge warmup strategy. Finally, we utilize a domain positioning mechanism to index the domain-variant adapter without requiring the domain ID of each sample in the test stage. Formally, given a sequence of domains { ğ’Ÿ 1 , ğ’Ÿ 2 , â€¦ , ğ’Ÿ N } superscript ğ’Ÿ 1 superscript ğ’Ÿ 2 â€¦ superscript ğ’Ÿ ğ‘ \{\mathcal{D}^{1},\mathcal{D}^{2},...,\mathcal{D}^{N}\} { caligraphic_D start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , caligraphic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , â€¦ , caligraphic_D start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT } , we aim to sequentially learn a model f ğ‘“ f italic_f to maximize the function f ğ‘“ f italic_f at the domain ğ’Ÿ i superscript ğ’Ÿ ğ‘– \mathcal{D}^{i} caligraphic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and history domains ğ’Ÿ 1 , â€¦ , ğ’Ÿ i âˆ’ 1 superscript ğ’Ÿ 1 â€¦ superscript ğ’Ÿ ğ‘– 1 \mathcal{D}^{1},...,\mathcal{D}^{i-1} caligraphic_D start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , â€¦ , caligraphic_D start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT . Each domain ğ’Ÿ i superscript ğ’Ÿ ğ‘– \mathcal{D}^{i} caligraphic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT contains training samples { ( x j i , y j i ) } 1 | ğ’Ÿ i | superscript subscript subscript superscript ğ‘¥ ğ‘– ğ‘— subscript superscript ğ‘¦ ğ‘– ğ‘— 1 superscript ğ’Ÿ ğ‘– \{(x^{i}_{j},y^{i}_{j})\}_{1}^{|\mathcal{D}^{i}|} { ( italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | caligraphic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | end_POSTSUPERSCRIPT , where ( x j i , y j i ) subscript superscript ğ‘¥ ğ‘– ğ‘— subscript superscript ğ‘¦ ğ‘– ğ‘— (x^{i}_{j},y^{i}_{j}) ( italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) are the j ğ‘— j italic_j -th example in domain domain ğ’Ÿ i superscript ğ’Ÿ ğ‘– \mathcal{D}^{i} caligraphic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , and | ğ’Ÿ i | superscript ğ’Ÿ ğ‘– |\mathcal{D}^{i}| | caligraphic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | is the number of training samples in domain ğ’Ÿ i superscript ğ’Ÿ ğ‘– \mathcal{D}^{i} caligraphic_D start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT . Let x j i subscript superscript ğ‘¥ ğ‘– ğ‘— x^{i}_{j} italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT be a text in AE and JOINT or a text combined with a term in ABSC. Additionally let y j i subscript superscript ğ‘¦ ğ‘– ğ‘— y^{i}_{j} italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT be the aspect term in AE, or sentiment polarity (e.g., positive, negative and neutral) in ABSC or their combination in JOINT. Notably, in the test phase, we need to predict we randomly merge all the test samples selected from all domains without domain IDs. 3.1 LLMs-Based ABSA Model Using a generative framework, we first build an LLMs-based ABSA model to integrate the rich latent knowledge in LLMs. We construct instructions to convert the input and output of ABSC and AE subtasks into a unified structure so that our model can perform all the tasks simultaneously. S