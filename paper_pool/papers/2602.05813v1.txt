Title: Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers

Abstract: We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.   Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup

Body: 1 Introduction 1 Introduction 2 Related Work 3 Theoretical Analysis 3.1 Assumptions and Practical Motivation 3.2 Deterministic Case without Weight Decay 3.3 Deterministic Case with Weight Decay 4 Stochastic Extensions 5 Practical Scheduler with Adaptive Warmup (1) Peak learning rate. (2) Warm-up floor. (3) Target-shape matching. 6 Experiments 6.1 Illustration of the Adaptive Scheduler 6.2 Experiment Results 6.3 Ablation Study on Target Loss A Automatic Boundedness for the Euclidean Norm B Empirical Motivation for Assumption 2 C Connection to Specific Optimizers: Detailed Derivations Normalized Gradient Descent ( â„“ 2 \ell_{2} norm). signSGD ( â„“ âˆ \ell_{\infty} norm). Lion ( â„“ âˆ \ell_{\infty} norm with momentum). Muon (spectral norm on matrices). Layer-wise optimizers (supremum norm over layers). D Details of the Adaptive Scheduler from Section 5 D.1 Closed-form expressions for Îº \kappa D.2 Closed-form solution for K 0 , K 1 , K 2 K_{0},K_{1},K_{2} and selection of Î” â€² \Delta^{\prime} Step 1: critical point. Step 2: value constraints. Step 3: solving for K 2 , K 1 , K 0 K_{2},K_{1},K_{0} . Step 4: selecting Î” â€² \Delta^{\prime} . E Connection ( Ï , K 0 , K 1 , K Ï ) (\rho,K_{0},K_{1},K_{\rho}) -smoothness with ( Ï , L 0 , L Ï ) (\rho,L_{0},L_{\rho}) -smoothness F Proofs F.1 Proof of Theorem 1 F.2 Proof of Theorem 2 F.3 Proof of Theorem 3 G Experimental Details G.1 Model Architecture G.2 Assumption Validation Experiments G.3 Main Experiments Hyperparameters BRAIn Lab Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers Artem Riabinin 1 , Andrey Veprikov 1, 2 , Arman Bolatov 3 , Martin TakĞ±Ä 3 , Aleksandr Beznosikov 1, 2, 4 1 Basic Research of Artificial Intelligence Laboratory (BRAIn Lab) 2 Federated Learning Problems Laboratory 3 Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) 4 Innopolis University We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically. Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup . 1 Introduction In this paper, we consider the problem of training large language models (LLMs), which can be formulated as the following optimization problem: f â‹† := min x âˆˆ ğ’³ â¡ f â€‹ ( x ) , f^{\star}:=\min_{x\in\mathcal{X}}f(x), (1) where f : ğ’³ â†’ â„ f:\mathcal{X}\to\mathbb{R} denotes the loss of the model x x with a parameter space ğ’³ := { ( W 1 , â€¦ , W L ) âˆ£ W i âˆˆ â„ m i Ã— n i } \mathcal{X}:=\{(W_{1},\ldots,W_{L})\mid W_{i}\in\mathbb{R}^{m_{i}\times n_{i}}\} representing the collection of L L modelâ€™s layers. Nowadays, classical method is to solve ( 1 ) using norm-constrained optimizers, where the update direction is given by a Linear Minimization Oracle (LMO) over a unit ball. This framework has emerged as a powerful family of methods for training deep networks, with recent successes including Kimi K2 [Team et al., 2025 ] and Moonlight [Liu et al., 2025a ] . It unifies several modern optimizers, including normSGD [Hazan et al., 2015a ] , signSGD [Bernstein et al., 2018 ] , Lion [Chen et al., 2023 ] , and Muon [Jordan et al., 2024 ] . Specifically, the LMO-based update rule is: x t + 1 = x t + Î· t â€‹ LMO â¡ ( g t ) , LMO â¡ ( g t ) := arg â€‹ min q âˆˆ ğ’³ : â€– q â€– = 1 â€‹ âŸ¨ g t , q âŸ© , \displaystyle x^{t+1}=x^{t}+\eta^{t}\operatorname{LMO}(g^{t}),\quad\operatorname{LMO}(g^{t}):=\mathrm{arg\penalty 10000\ min}_{q\in\mathcal{X}:\|q\|=1}\langle g^{t},q\rangle, (2) where t t is the optimization step, Î· t 0 \eta^{t} 0 is the learning rate, g t g^{t} is a gradient approximation (e.g., momentum), and âˆ¥ â‹… âˆ¥ \|\cdot\| refers to an arbitrary, possibly non-Euclidean norm. This formulation arises naturally from minimizing a quadratic approximation of the loss function around the point x t x^{t} : f â€‹ ( x t + Î” â€‹ x t ) â‰ˆ f â€‹ ( x t ) + âŸ¨ âˆ‡ f â€‹ ( x t ) , Î” â€‹ x t âŸ© + Î» 2 â€‹ â€– Î” â€‹ x t â€– 2 . f(x^{t}+\Delta x^{t})\approx f(x^{t})+\langle\nabla f(x^{t}),\Delta x^{t}\rangle+\frac{\lambda}{2}\|\Delta x^{t}\|^{2}. (3) The update Î” â€‹ x t = x t + 1 âˆ’ x t \Delta x^{t}=x^{t+1}-x^{t} from ( 2 ) is the arg â¡ min \arg\min of ( 3 ) with respect to Î” â€‹ x t âˆˆ ğ’³ \Delta x^{t}\in\mathcal{X} up to multiplication factors. Different choices of norms âˆ¥ â‹… âˆ¥ \|\cdot\| yield different optimizers: the Euclidean norm recovers normSGD, the â„“ 1 \ell_{1} norm gives signSGD, and the spectral norm leads to Muon. For a detailed derivation of the resulting updates see Appendix C . The success of LMO-based methods depends not only on the appropriate choice of the norm in ( 2 ), but also on the proper selection of the learning rate Î· t \eta^{t} [Goyal et al., 2017 ] . In practice, empirically designed schedules are commonly used, such as linear warm-up followed by cosine decay [Loshchilov and Hutter, 2017 ] . In this work, we focus on the warm-up phase: starting with small learning rates and gradually increasing them before decay. Although warm-up has become nearly ubiquitous in practice [Goyal et al., 2017 ; Vaswani et al., 2017 ; Loshchilov and Hutter, 2017 ] , its theoretical necessity has not been fully understood. Therefore, in this paper we address the following research questions: (i) Can learning rate warm-up be theoretically justified for LMO-based optimizers, rather than being treated as a purely empirical heuristic? (ii) Can the warm-up duration be determined adaptively during training, eliminating the need for manual tuning? Guided by this research questions, we make the following contributions: â€¢ We introduce a new generalized smoothness assumption where local curvature decreases with the suboptimality gap, and empirically verify that this behavior holds along optimization trajectories. â€¢ We provide a theoretical analysis establishing convergence guarantees for LMO-based optimizers under this assumption, where warm-up followed by decay emerges naturally from the proof rather than being imposed heuristically. â€¢ Based on the theory, we develop a practical learning rate scheduler with adaptive warm-up that relies only on standard hyperparameters and automatically determines the warm-up duration at the beginning of training. â€¢ We validate our approach on language model pretraining with LLaMA architectures, showing that the proposed adaptive warm-up matches or outperforms hand-tuned schedules across Muon, Lion, and normalized SGD without hyperparameter search. 2 Related Work Norm-constrained optimizers. Norm-constrained optimizers have recently attracted significant attention in deep learning. One of the most prominent examples of such optimizers is Muon [Jordan et al., 2024 ] , which demonstrates strong performance for training deep neural networks [Liu et al., 2025a ] . Numerous studies have developed practical variants of Muon and related LMO-based algorithms for large-scale models, analyzing their empirical behavior under spectral or orthogonality constraints [Pethick et al., 2025 ; Riabinin et al., 2025 ; Amsel et al., 2025 ; Liu et al., 2025b ; Huang et al., 2025 ; Kovalev, 2025 ; He et al., 2025 ] . Adaptive and parameter-free optimizers. A related line of work focuses on designing optimizers that require minimal or no hyperparameter tuning. In this domain, Adam [Kingma and Ba, 2015 ] and its variant AdamW [Loshchilov and Hutter, 2019 ] are adaptive coordinate-wise optimizers that have long been considered the default choice in deep learning. More recent advancements include D-Adaptation [Defazio and Mishchenko, 2023 ] , which automatically estimates the distance to the solution to set learning rates, and Prodigy [Mishchenko and Defazio, 2024 ] , which improves on this with tighter distance estimates and faster adaptation. Furthermore, the recently introduced Schedule-Free methods [Defazio et al., 2024 ] eliminate the need for learning rate schedules entirely by maintaining iterate averages that converge without explicit decay. However, despite these advances, all discussed methods, even the Schedule-Free approaches, still rely on heuristically defined learning rate warm-up phases. Learning rate warm-up. The learning rate warm-up is a widely used heuristic to train deep neural networks, dating back at least to He et al. [ 2016 ] , which used a small constant learning rate during the initial training phase. The linear warm-up strategy, introduced by Goyal et al. [ 2017 ] , has since become standard for training ResNets [He et al., 2016 ] and transformers [Vaswani et al., 2017 ] . Empirical studies have shown that warm-up enhances training stability to allow larger learning rates, reduces gradient variance, and improves model performance [Gotmare et al., 2019 ; Liu et al., 2019 ; Kosson et al., 2024 ] . From a geometric perspective, Gilmer et al. [ 2021 ] and Kalra and Barkeshli [ 2024 ] observed that warm-up induces a sharpness reduction phase where the largest Hessian eigenvalue decreases, enabling larger learning rates in subsequent training. Generalized smoothness and warm-up theory. The standard L L -smoothness assumption â€– âˆ‡ f â€‹ ( x ) âˆ’ âˆ‡ f â€‹ ( y ) â€– â‰¤ L â€‹ â€– x âˆ’ y â€– \|\nabla f(x)-\nabla f(y)\|\leq L\|x-y\| is insufficient to explain the necessity of warm-up, as it implies a uniform bound on curvature throughout the training landscape. Similarly, the ( L 0 , L 1 ) (L_{0},L_{1}) -smoothness condition introduced by Zhang et al. [ 2020 ] , which bounds the Hessian norm by L 0 + L 1 â€‹ â€– âˆ‡ f â€‹ ( x ) â€– L_{0}+L_{1}\|\nabla f(x)\| , has not, to the best of our knowledge, yielded theoretical justifications for warm-up strategies. Recent works address this limitation by linking smoothness to the suboptimality gap, bounding the Hessian by K 0 + K 1 â€‹ ( f â€‹ ( x ) âˆ’ f â‹† ) K_{0}+K_{1}(f(x)-f^{\star}) [Alimisis et al., 2025 ] or K 0 + K Ï â€‹ ( f â€‹ ( x ) âˆ’ f â‹† ) Ï K_{0}+K_{\rho}(f(x)-f^{\star})^{\rho} [Liu et al., 2025c ] , where f â‹† f^{\star} is a target (perfect) loss for the problem ( 1 ). However, the theoretical results in these studies are limited to GD and SGD optimizers and derive schedules consisting solely of a warm-up phase without any decay. In contrast, our Assumption 2 adopts this suboptimality-dependent framework to naturally derive warm-up and subsequent decay phases specifically for LMO-based methods. Target loss estimation. As was mentioned in the previous paragraph, all the theoretical frameworks about warmup rely on knowledge of the target value f â‹† f^{\star} . It commonly appears in adaptive stepsize methods, including Polyak-type step sizes [Polyak, 1963 ; Orabona and Dâ€™Orazio, 2025 ] , its stochastic variant [Loizou et al., 2021 ] and Polyak step method for mirror descent [You et al., 2022 ] . A range of techniques has been proposed to estimate or adapt such target values in practice, including adaptive running lower bounds constructed from past function values [Hazan and Kakade, 2019 ] and level-type schemes that replace f â‹† f^{\star} by evolving target levels in Polyak-type step sizes [You and Li, 2022 ] . In this work, we do not employ these mechanisms and instead fix a reasonable estimate of f â‹† f^{\star} for each setup. In Section 6.3 , we perform an ablation over f â‹† f^{\star} and show that choosing f â‹† f^{\star} within a reasonable neighborhood of the optimal loss yields stable and robust behavior, consistently outperforming the best manually tuned warm-up schedules. 3 Theoretical Analysis 3.1 Assumptions and Practical Motivation We begin our analysis with the set of assumptions required to study LMO-based optimizers for problem ( 1 ). Assumption 1 . The function f : ğ’³ â†’ â„ f:\mathcal{X}\to\mathbb{R} is star-convex, i.e., the following inequality holds: f â€‹ ( Î² â€‹ x â‹† + ( 1 âˆ’ Î² ) â€‹ x ) â‰¤ Î² â€‹ f â€‹ ( x â‹† ) + ( 1 âˆ’ Î² ) â€‹ f â€‹ ( x ) , f(\beta x^{\star}+(1-\beta)x)\leq\beta f(x^{\star})+(1-\beta)f(x), for all x âˆˆ ğ’³ x\in\mathcal{X} and Î² âˆˆ [ 0 , 1 ] \beta\in[0,1] , where x â‹† x^{\star} is a global minimizer of f f , i.e. f â€‹ ( x â‹† ) = f â‹† f(x^{\star})=f^{\star} . Unlike classical convexity, which requires the inequality to hold for every pair of points in ğ’³ \mathcal{X} , star-convexity only enforces convexity with respect to the optimum. This reflects a setting in which the loss landscape may be highly nonconvex globally, yet becomes progressively well behaved along trajectories that approach x â‹† x^{\star} . Such star-shaped conditions are now standard in modern theoretical analyses of deep neural networks [Zhou et al., 2019 ] . In particular, recent works analyzing LMO-based optimizers show that star-convexity, together with classical Lipschitz smoothness, is sufficient to guarantee convergence with a constant learning rate [Pethick et al., 2025 ; Kovalev, 2025 ] . However, since these works rely on the standard smoothness assumption, they fail to explain the emergence of learning rate warm-up. This motivates Assumption 2 , which we introduce next. Assumption 2 . The function f : ğ’³ â†’ â„ f:\mathcal{X}\to\mathbb{R} is ( Ï , K 0 , K 1 , K Ï ) (\rho,K_{0},K_{1},K_{\rho}) -smooth, i.e., there exist K 0 , K 1 , K Ï â‰¥ 0 K_{0},K_{1},K_{\rho}\geq 0 and Ï 0 \rho 0 such that for all x , y âˆˆ ğ’³ x,y\in\mathcal{X} it holds that: â€– âˆ‡ f â€‹ ( x ) âˆ’ âˆ‡ f â€‹ ( y ) â€– â‹† â‰¤ ğ’¦ â€‹ ( x ) â€‹ â€– x âˆ’ y â€– , \displaystyle\|\nabla f(x)-\nabla f(y)\|_{\star}\leq\mathcal{K}(x)\|x-y\|, ğ’¦ â€‹ ( x ) := K 0 + K 1 â€‹ ( f â€‹ ( x ) âˆ’ f â‹† ) + K Ï â€‹ ( f â€‹ ( x ) âˆ’ f â‹† ) Ï , \displaystyle\mathcal{K}(x):=K_{0}+K_{1}\left(f(x)-f^{\star}\right)+K_{\rho}\left(f(x)-f^{\star}\right)^{\rho}, where âˆ¥ â‹… âˆ¥ â‹† \|\cdot\|_{\star} is the conjugate norm for âˆ¥ â‹… âˆ¥ \|\cdot\| used in the update rule ( 2 ). This condition strengthens the classical smoothness model in a manner that is aligned with the geometry of deep learning. It recovers the ( Ï , L 0 , L Ï ) (\rho,L_{0},L_{\rho}) -smoothness (see Section E for details). When K Ï = 0 K_{\rho}=0 and âˆ¥ â‹… âˆ¥ := âˆ¥ â‹… âˆ¥ 2 \|\cdot\|:=\|\cdot\|_{2} , Assumption 2 reduces to the version of assumption from [Alimisis et al., 2025 ] , which corresponds to a bound of the type: â€– âˆ‡ f â€‹ ( x ) âˆ’ âˆ‡ f â€‹ ( y ) â€– â‰¤ ( K 0 + K 1 â€‹ ( f â€‹ ( x ) âˆ’ f â‹† ) ) â€‹ â€– x âˆ’ y â€– . \|\nabla f(x)-\nabla f(y)\|\leq\left(K_{0}+K_{1}(f(x)-f^{\star})\right)\|x-y\|. However, our findings (both theoretical and empirical) ind