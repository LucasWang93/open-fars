Title: MoBind: Motion Binding for Fine-Grained IMU-Video Pose Alignment

Abstract: We aim to learn a joint representation between inertial measurement unit (IMU) signals and 2D pose sequences extracted from video, enabling accurate cross-modal retrieval, temporal synchronization, subject and body-part localization, and action recognition. To this end, we introduce MoBind, a hierarchical contrastive learning framework designed to address three challenges: (1) filtering out irrelevant visual background, (2) modeling structured multi-sensor IMU configurations, and (3) achieving fine-grained, sub-second temporal alignment. To isolate motion-relevant cues, MoBind aligns IMU signals with skeletal motion sequences rather than raw pixels. We further decompose full-body motion into local body-part trajectories, pairing each with its corresponding IMU to enable semantically grounded multi-sensor alignment. To capture detailed temporal correspondence, MoBind employs a hierarchical contrastive strategy that first aligns token-level temporal segments, then fuses local (body-part) alignment with global (body-wide) motion aggregation. Evaluated on mRi, TotalCapture, and EgoHumans, MoBind consistently outperforms strong baselines across all four tasks, demonstrating robust fine-grained temporal alignment while preserving coarse semantic consistency across modalities. Code is available at https://github.com/bbvisual/ MoBind.

Body: MoBind: Motion Binding for Fine-Grained IMU‚ÄìVideo Pose Alignment 1 Introduction 2 Related Work 2.1 Multi-modal contrastive learning 2.2 Temporal synchronization 2.3 IMU-to-Subject Association 3 MoBind 3.1 Modality-Specific Modules 3.2 Hierarchical Contrastive Alignment 3.3 Masked Token Prediction (MTP) 4 Experiments 4.1 Datasets Training 4.2 Cross-Modal Retrieval 4.3 Temporal Synchronization 4.4 Subject and Body-part Localization 4.5 Human Action Recognition 4.6 Ablation Study 5 Conclusion MoBind: Motion Binding for Fine-Grained IMU‚ÄìVideo Pose Alignment Duc Duy Nguyen Tat-Jun Chin Minh Hoai Australian Institute for Machine Learning, Adelaide University, Adelaide, SA 5000, Australia Abstract We aim to learn a joint representation between inertial measurement unit (IMU) signals and 2D pose sequences extracted from video, enabling accurate cross-modal retrieval, temporal synchronization, subject and body-part localization, and action recognition. To this end, we introduce MoBind, a hierarchical contrastive learning framework designed to address three challenges: (1) filtering out irrelevant visual background, (2) modeling structured multi-sensor IMU configurations, and (3) achieving fine-grained, sub-second temporal alignment. To isolate motion-relevant cues, MoBind aligns IMU signals with skeletal motion sequences rather than raw pixels. We further decompose full-body motion into local body-part trajectories, pairing each with its corresponding IMU to enable semantically grounded multi-sensor alignment. To capture detailed temporal correspondence, MoBind employs a hierarchical contrastive strategy that first aligns token-level temporal segments, then fuses local (body-part) alignment with global (body-wide) motion aggregation. Evaluated on mRi, TotalCapture, and EgoHumans, MoBind consistently outperforms strong baselines across all four tasks, demonstrating robust fine-grained temporal alignment while preserving coarse semantic consistency across modalities. Code is available at https://github.com/bbvisual/MoBind . 1 Introduction Understanding human motion is critical for a wide range of applications, including action recognition [ m_Wang-etal-CVPR20 , m_Wang-Hoai-FG18 ] , sports performance analysis [ mtlaqa , Xu2024FineParserAF , 10.1145/2556288.2557116 , yifeng_exrac_aaai_2024 , duc_caracount_tpami_2025 ] , and rehabilitation monitoring [ 9207296 , Ahad_2019_CVPR_Workshops ] . However, human motion is often subtle or hard to sense from a single modality. This limitation can be mitigated by integrating signals from complementary sensing sources, such as video recordings and IMUs. Video can offer rich spatial and semantic information but is sensitive to occlusion, viewpoint changes, and limited frame rates, while IMUs provide precise, temporally dense motion signals but lack visual context, making the captured motion difficult to interpret. To fully leverage the strengths of both modalities, it is essential to develop a joint representation that establishes meaningful correspondence between them‚Äîboth at the coarse-grained action category level and the fine-grained, sub-second temporal alignment. Figure 1 : Proposed framework for motion binding between IMUs and 2D pose sequence from video. Contrastive learning is applied at both the local space, aligning each IMU with its corresponding body-part, and the global space, aligning full-body representations. This representation supports several downstream tasks, including cross-modal retrieval, temporal synchronization, subject and body parts localization, and human action recognition. Such a joint representation that aligns IMU signals and video-based body motion would unlock several important capabilities. First, it enables IMU-Video temporal synchronization without cumbersome, explicit calibration procedures. Traditional calibration often requires global timestamps, trigger pulses, or manual alignment‚Äîmethods that can be technically demanding and error-prone, especially for non-expert users. A learned joint representation enables synchronization based purely on the content of the signals, making it more accessible to collect and use multimodal data. This is important because multimodal data is only truly useful when the different streams are temporally aligned. Second, it supports cross-modal retrieval, where one modality can be used to query relevant information from a database in the other. This is especially valuable in privacy-sensitive scenarios where synchronized video data may be unavailable or restricted, yet similar examples can still be retrieved from the database for visualization and enhanced understanding. Third, it facilitates spatial localization, i.e., associating each IMU with the correct person in the video, which in turn stabilizes tracking in multi-person scenes, and identities can be maintained under occlusions or re-entries. Together, these capabilities make multimodal datasets more reliable to collect, curate, and analyze beyond controlled lab settings. In practice, much recent work on IMU‚Äìvision coupling targets human activity recognition (HAR) by learning a shared embedding space with contrastive objectives [ moon2022imu2clip , girdhar2023imagebind , das2025primus , tan2023egodistill , zhang2025masked ] . Typically, each clip or window is projected to a single global embedding, and training relies on matched/mismatched pairs. This design might excel at coarse semantic discrimination (e.g., action categories) but overlooks fine-grained temporal structure: segments that differ only by phase shifts, short lags, or repetition boundaries (i.e., distinct cycles of the same action) collapse to nearby codes. Consequently, the resulting representations become insensitive to true temporal synchrony, a limitation that hinders calibration-free temporal synchronization, sub-second cross-modal retrieval, and spatial localization. This limitation motivates us to develop a joint representation that explicitly models the fine-grained temporal dynamics between IMU and video while keeping the coarse semantics. In searching for an effective approach to fine-grained IMU‚Äìvideo alignment, we found no prior work that directly targets this setting. We therefore first examined joint audio‚Äìvideo representation learning designed for sub-second alignment [ Chung16a , Afouras20b , 10.5555/3327757.3327874 , Fernandez-Labrador_2024_CVPR ] , but these techniques do not transfer well to the IMU‚Äìvideo setting. Unlike audio, which often correlates with multiple visual instances in the scene and provides scene-level cues, IMU signals are localized and strictly motion-centric, making most visual background irrelevant. Moreover, IMUs are commonly deployed in multi-sensor configurations, each attached to a different body part, naively concatenating these signals fails to capture their spatial and temporal specificity. Lastly, synchronization cues in audio‚Äìvideo data exist in multiple forms‚Äîsome are momentary and localized (e.g., door closures, hand claps), while others are more continuous and span broader contexts (e.g., background music aligned with scene changes) [ sparse2022iashin ] . Both types are often informative for alignment. In contrast, human motion tends to exhibit continuous and highly repetitive patterns (e.g., walking cycles), producing abundant but highly similar synchronization cues. This repetition can lead to ambiguous alignments, particularly when distinct motion segments appear nearly identical. Taken together, these factors limit the direct applicability of audio‚Äìvideo synchronization techniques and highlight the need for tailored strategies that model sensor-specific dynamics and their alignment with motion-derived visual cues. In this paper, we introduce a contrastive learning framework illustrated in Fig. 1 , that addresses key limitations of prior works. To focus on motion-relevant cues and reduce irrelevant visual background, we learn a joint representation between IMU signals and video-derived skeletal motion sequences instead of raw pixels. To support multi-sensor IMU setups, we decompose the extracted skeleton into local body-parts and align each with its corresponding IMU signal, enabling structured, semantically grounded association. To achieve fine-grained temporal alignment, we use a hierarchical objective: a local term aligns each IMU‚Äìpart pair on short segments (sub-second synchrony), and a global term aggregates local features into full-body, multi-IMU embeddings. To retain action-level semantics, we introduce a Masked Token Prediction (MTP) auxiliary task, optimized jointly with the contrastive loss. Evaluated on mRi [ an2022mri ] , TotalCapture [ Trumble:BMVC:2017 ] , and EgoHumans [ Khirodkar_2023_ICCV ] , our method consistently outperforms competing approaches across cross-modal retrieval, temporal synchronization, subject/body-part localization, and action recognition. 2 Related Work 2.1 Multi-modal contrastive learning In the context of contrastive learning, most IMU work has focused on improving encoders for human activity recognition (HAR) [ haresamudram2021contrastive , 10.1145/3699744 , zhang2025masked , 10.1145/3485730.3485937 , das2025primus ] . However, following the success of CLIP [ pmlr-v139-radford21a ] in vision‚Äìlanguage modeling, attention has shifted toward learning semantically meaningful cross-modal representations involving IMU signals. IMU2CLIP [ moon2022imu2clip ] and ImageBind [ girdhar2023imagebind ] align IMU features with the CLIP space, typically via egocentric video, to enable IMU and video retrieval. UniMTS [ zhang2024unimts ] maps synthetic IMU from motion data also into the CLIP space to pair with a textual description, enabling zero-shot HAR, while DeSPITE [ despite2025 ] builds a joint embedding across LiDAR, skeletons, and IMU, optionally incorporating CLIP text when annotations exist, enabling cross-modal retrieval. Although these approaches share the goal of learning joint representations, they generally operate at a global (clip-level), which fails to preserve fine-grained temporal alignment. As our experiments indicate, such formulations lack the precision required for sub-second cross-modal synchronization. Figure 2 : Overview of the proposed MoBind. The framework first encodes each IMU stream together with the motion of its corresponding body part, yielding token-level and local-level representations per sensor. These local representations are then aggregated across sensors to form global-level embeddings. The contrastive objective applies at all three levels. In addition, a Masked Token Prediction (MTP) module is used only during training to preserve coarse semantic structure, preventing the model from over-focusing on fine-grained alignment. 2.2 Temporal synchronization Temporal synchronization across modalities has been extensively studied in the audio-visual (AV) domain [ NIPS2000_9f699296 , marcheret2015detecting , Chung16a , chung2019perfect , chen2021audio , Fernandez-Labrador_2024_CVPR ] . An early approach [ NIPS2000_9f699296 ] leveraged statistical correlations between MFCCs from audio and mouth appearance in video using correlation to solve AV synchronization. SyncNet [ Chung16a ] was among the first to introduce a deep contrastive framework that aligns speech and mouth motion at clip- and frame-level granularity, and DiVAS [ Fernandez-Labrador_2024_CVPR ] extended this line with a transformer-based model that is robust to varying frame rates. More recent AV methods [ sparse2022iashin , synchformer2024iashin ] replace pure contrastive objectives by tokenizing audio and video and formulating synchronization as temporal offset classification. Inspired by these two paradigms‚Äîcontrastive alignment and offset classification‚Äîwe instantiate two strong baselines for IMU-video synchronization by aligning IMU signals with video-extracted skeletal motion. Despite the rich AV literature, IMU-video synchronization remains underexplored. The most closely related work is SyncWISE [ 10.1145/3411824 ] , which estimates temporal offsets by computing correlations between IMU signals and optical-flow derivatives. In contrast, our work provides a fully automatic, learning-based solution that avoids manual gestures or ad-hoc heuristics, making synchronization practical in real-world settings. 2.3 IMU-to-Subject Association The task of associating an IMU with the correct individual in a video has been previously explored. This task is conceptually similar to active speaker detection in audio-visual research [ Chung16a , 9423336 , 10.1145/3474085.3475587 , Fernandez-Labrador_2024_CVPR ] , whose goal is to link audio sources to corresponding visual entities. VIT [ Henschel_2019_CVPR_Workshops ] formulates the association as a graph-labeling task, assigning IMU identities to visual tracklets based on orientation consistency. VIPL [ 10.1109/IROS45743.2020.9341739 ] learns a shared visual-inertial feature space using a contrastive loss to match a person‚Äôs wearable-IMU motion with their video motion. Vi-Fi [ 9826015 ] introduces a recurrent multimodal network that estimates affinity matrices between camera tracks and IMU/device IDs. In contrast to these methods, our approach addresses both spatial association (who is carrying the IMU) and temporal synchronization within a unified learning framework. Moreover, by leveraging the locality of IMU signals, our model also infers body-part attribution-determining not only who is wearing a given IMU, but also where on the body it is worn. 3 MoBind This section presents MoBind, an end-to-end framework for learning joint representations between wearable IMU signals and video-based human motion (Fig. 2 ). From the video, skeletal joint sequences are extracted, while raw IMU streams from N N body-mounted sensors are simultaneously processed. Modality-specific modules produce representations trained with a contrastive objective to align synchronized IMU‚Äìvideo pairs and separate mismatched ones. Unlike prior work that compresses inputs into a single global vector, MoBind preserves spatial and temporal structure by capturing body-part motion in local representations, aggregating them into a global one, and modeling token-level temporal dynamics. This hierarchical design can capture fine-grained motion details. However, an exclusive fine-grained focus can under-represent coarse semantics beneficial for HAR. To address this, we add a Masked Token Prediction (MTP) module tailored to IMU inputs, encouraging embeddings to retain high-level semantics while emphasizing temporal granularity. Together, hierarchical alignment and MTP enable the model to learn localized motion patterns and fine-grained dynamics, yielding robust cross-modal alignment and improved HAR performance. 3.1 Modality-Specific Modules IMU Module. The input to the IMU module consists of N N IMU signals, each captured from a sensor mounted on a limb of the human body. Let ùêó ‚àà ‚Ñù F √ó C \mathbf{X}\in\mathbb{R}^{F\times C} de