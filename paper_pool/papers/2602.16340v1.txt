Title: The Implicit Bias of Adam and Muon on Smooth Homogeneous Neural Networks

Abstract: We study the implicit bias of momentum-based optimizers on homogeneous models. We first extend existing results on the implicit bias of steepest descent in homogeneous models to normalized steepest descent with an optional learning rate schedule. We then show that for smooth homogeneous models, momentum steepest descent algorithms like Muon (spectral norm), MomentumGD ($\ell_2$ norm), and Signum ($\ell_\infty$ norm) are approximate steepest descent trajectories under a decaying learning rate schedule, proving that these algorithms too have a bias towards KKT points of the corresponding margin maximization problem. We extend the analysis to Adam (without the stability constant), which maximizes the $\ell_\infty$ margin, and to Muon-Signum and Muon-Adam, which maximize a hybrid norm. Our experiments corroborate the theory and show that the identity of the margin maximized depends on the choice of optimizer. Overall, our results extend earlier lines of work on steepest descent in homogeneous models and momentum-based optimizers in linear models.

Body: The Implicit Bias of Adam and Muon on Smooth Homogeneous Neural Networks 1 Introduction 2 Preliminaries 2.1 Setting and Notations 2.2 Optimizers 2.3 Assumptions Model Assumptions. Learning Rate Assumptions. Realizability and Trajectory Assumptions. Adam Well-Definability Assumption. 2.4 Margin Maximization and KKT Conditions 3 Results 3.1 Normalized Steepest Descent with a LR Schedule 3.2 Momentum Steepest Descent, Muon and Muon-Signum 3.3 Adam and Muon-Adam 4 Non-Smooth Models 5 Main Proof Ideas 5.1 Approximate Steepest Descent 5.2 Asymptotic Momentum-Gradient Relations 6 Experiments 7 Conclusion A Clarke Subgradients and Chain Rule B Momentum (EMA) B.1 Discrete Momentum B.2 Continuous Momentum C Proof Details C.1 Losses C.2 Algorithm-Independent KKT Stationarity C.3 Normalized Steepest Descent with a LR Schedule C.4 Approximate Steepest Descent C.5 Momentum Steepest Descent C.6 Composite MSD Algorithms, Muon and Muon-Signum C.7 Adam C.8 Muon-Adam D Experimental Details The Implicit Bias of Adam and Muon on Smooth Homogeneous Neural Networks Eitan Gronich Gal Vardi Weizmann Institute of Science {eitan.gronich, gal.vardi}@weizmann.ac.il Abstract We study the implicit bias of momentum-based optimizers on homogeneous models. We first extend existing results on the implicit bias of steepest descent in homogeneous models to normalized steepest descent with an optional learning rate schedule. We then show that for smooth homogeneous models, momentum steepest descent algorithms like Muon (spectral norm), MomentumGD ( â„“ 2 \ell_{2} norm), and Signum ( â„“ âˆ \ell_{\infty} norm) are approximate steepest descent trajectories under a decaying learning rate schedule, proving that these algorithms too have a bias towards KKT points of the corresponding margin maximization problem. We extend the analysis to Adam (without the stability constant), which maximizes the â„“ âˆ \ell_{\infty} margin, and to Muon-Signum and Muon-Adam, which maximize a hybrid norm. Our experiments corroborate the theory and show that the identity of the margin maximized depends on the choice of optimizer. Overall, our results extend earlier lines of work on steepest descent in homogeneous models and momentum-based optimizers in linear models. 1 Introduction Deep neural networks show remarkable generalization performance despite often being overparameterized, and even when trained with no explicit regularization. A well-established line of work attempts to explain this phenomenon with the notion of the implicit bias (tendency) of gradient-based optimization algorithms to converge to well-generalizing solutions. This bias is often realized in the form of maximizing a certain margin for the training points (cf. vardi2023implicit ). While earlier works studied mostly gradient descent and showed its implicit bias towards maximizing the â„“ 2 \ell_{2} margin in increasingly complex models, recent years have witnessed an interest in the study of the implicit bias of other optimizers, such as Adam ( kingma2014adam ) , AdamW ( loshchilov2019_adamw ) , and recently Muon ( jordan2024muon ) , hand-in-hand with their rising popularity. Indeed, as these algorithms are used near-universally for training large language models (LLMs) and vision transformers, there is a growing imperative to understand their inner workings. In this work, we study smooth homogeneous models and show a margin-maximization bias of Adam and Muon. Previous work analyzed the implicit bias of Adam and Muon on linear predictors ( zhang2024_adam_linear ; fan2025implicit ) , and we extend these results to the substantially broader class of smooth homogeneous models. Moreover, our analysis of Muon is a special case of a more general framework that we develop, which is applicable to all momentum-based optimizers built on top of steepest descent algorithms. All of our results hold for a family of exponentially tailed losses that includes the logistic and exponential losses. Our main contributions are as follows: 1. We show that any limit point of ğœ½ t â€– ğœ½ t â€– \frac{\bm{\theta}_{t}}{\left\lVert\bm{\theta}_{t}\right\rVert} in a normalized steepest descent trajectory with a learning rate schedule Î· â€‹ ( t ) \eta(t) is a KKT point of the âˆ¥ â‹… âˆ¥ \left\lVert\cdot\right\rVert -max-margin problem, as long as âˆ« 0 âˆ Î· â€‹ ( t ) â€‹ ğ‘‘ t = âˆ \int_{0}^{\infty}\eta(t)dt=\infty . This result holds for any locally Lipschitz, C 1 C^{1} -stratifiable homogeneous model, including ReLU networks. It extends a result by tsilivis2025 that considered (unnormalized) steepest descent with a constant learning rate. 2. We show that when ğœ½ t â€– ğœ½ t â€– \frac{\bm{\theta}_{t}}{\left\lVert\bm{\theta}_{t}\right\rVert} converges, it converges to the direction of a KKT point of the âˆ¥ â‹… âˆ¥ \left\lVert\cdot\right\rVert -max-margin problem even for trajectories which approximate steepest descent. This allows us to focus on momentum-based optimizers on smooth homogeneous models and show: (a) Muon has an implicit bias towards margin maximization with respect to a norm defined using spectral norms of the weight matrices, under a decaying learning rate regime. In fact, the bias towards margin maximization holds for any normalized Momentum Steepest Descent (MSD) algorithm, for the appropriate norm. We show this includes composite MSD algorithms such as Muon-Signum. In addition, we prove an implicit bias of Muon-Adam. (b) Adam (without the stability constant) has an implicit bias towards â„“ âˆ \ell_{\infty} margin maximization under a decaying learning rate regime. Related Work soudry2017_gd_linear first showed that gradient descent in linear models maximizes the â„“ 2 \ell_{2} margin. This result was followed by several works on margin maximization in linear fully-connected, convolutional and diagonal networks (e.g., ji2018gradient ; gunasekar2018implicit ; yun2020unifying ; moroshko2020implicit ). Going beyond linear networks, chizat2020implicit studied the implicit bias in infinitely-wide two-layer smooth homogeneous networks, and proved margin maximization w.r.t. a certain function norm, known as the variation norm. lyu_li_homogeneous studied homogeneous models under gradient descent, demonstrating that any limit point of the direction of the vector of parameters ğœ½ t â€– ğœ½ t â€– \frac{\bm{\theta}_{t}}{\left\lVert\bm{\theta}_{t}\right\rVert} is the direction of a KKT point of the max-margin problem. In a complementary result, ji2020directional showed that directional convergence of the parameters is indeed guaranteed when optimizing homogeneous models definable in an o-minimal structure with gradient descent. The implicit bias of gradient descent in certain non-homogeneous neural networks was studied in nacson2019lexicographic ; kunin2022asymmetric ; cai2025_nearlyhomogeneous . For a more comprehensive survey on the implicit bias of gradient descent, see vardi2023implicit . A general treatment of the implicit bias of the family of steepest descent algorithms was given for linear models by gunasekar2018characterizing , who proved maximization of the appropriate norm-dependent margin. tsilivis2025 generalized that result and the result by lyu_li_homogeneous and proved for homogeneous models under steepest descent that any limit point of ğœ½ t â€– ğœ½ t â€– \frac{\bm{\theta}_{t}}{\left\lVert\bm{\theta}_{t}\right\rVert} is a KKT point of the max-margin problem. Adam in the context of homogeneous models was studied by wang2021_adam_eps_homoegeneous , who showed a bias towards â„“ 2 \ell_{2} -margin maximization. Notably, this work studied Adam without momentum in the numerator and with a stability constant Îµ \varepsilon which asymptotically dominates the denominator, driving behavior to be similar to gradient descent. Follow-up works have argued that the analysis of the implicit bias with the stability constant is less faithful to the characteristics of Adam in practice, as the stability constant is typically negligible throughout the trajectory. Such works on Adam without the stability constant have so far focused on linear models and include zhang2024_adam_linear and fan2025implicit , in the binary and multiclass settings respectively, who showed â„“ âˆ \ell_{\infty} margin maximization, and baek2025implicit who showed that the implicit bias of Adam under a deterministic batching routine can deviate from the full-batch case. We generalize the result by zhang2024_adam_linear to smooth homogeneous models. AdamW, contrasting with other algorithms by its utilization of explicit weight decay, was studied for smooth models and losses by xie2024implicit , who showed that limit points of the trajectory are KKT points of the loss under the constraint that the â„“ âˆ \ell_{\infty} norm of the parameters is bounded. fan2025implicit studied normalized steepest descent and its momentum counterparts on linear models in the multiclass setting, including spectral descent and Muon respectively, and showed maximization of the appropriate margins. We generalize their result, albeit in the binary classification setting. 2 Preliminaries 2.1 Setting and Notations Throughout this work, we consider a fixed binary classification dataset { ( ğ± i , y i ) } i = 1 m âŠ† â„ d Ã— { Â± 1 } \{\left(\mathbf{x}_{i},y_{i}\right)\}_{i=1}^{m}\subseteq\mathbb{R}^{d}\times\{\pm 1\} , a parameterized model f â€‹ ( ğ± ; ğœ½ ) f(\mathbf{x};\bm{\theta}) for parameters ğœ½ âˆˆ â„ p \bm{\theta}\in\mathbb{R}^{p} , and a log-concave, exponentially tailed loss of the form: â„’ â€‹ ( ğœ½ ) = âˆ‘ i = 1 m â„“ â€‹ ( y i â€‹ f â€‹ ( ğ± i ; ğœ½ ) ) = âˆ‘ i = 1 m e âˆ’ Ï† â€‹ ( y i â€‹ f â€‹ ( ğ± i ; ğœ½ ) ) , \mathcal{L}(\bm{\theta})=\sum_{i=1}^{m}\ell\left(y_{i}f(\mathbf{x}_{i};\bm{\theta})\right)=\sum_{i=1}^{m}e^{-\varphi({y_{i}f(\mathbf{x}_{i};\bm{\theta})})}~, (1) where Ï† \varphi is twice continuously differentiable, strictly monotone increasing and convex, with bounded first and second derivatives (see Appendix C.1 ), notably allowing for the exponential ( â„“ â€‹ ( u ) = e âˆ’ u \ell(u)=e^{-u} ) and logistic ( â„“ â€‹ ( u ) = log â¡ ( 1 + e âˆ’ u ) \ell(u)=\log(1+e^{-u}) ) losses. We denote for brevity z i â€‹ ( ğœ½ ) = y i â€‹ f â€‹ ( ğ± i ; ğœ½ ) z_{i}(\bm{\theta})=y_{i}f(\mathbf{x}_{i};\bm{\theta}) and q min â€‹ ( ğœ½ ) = min i âˆˆ [ m ] â¡ z i â€‹ ( ğœ½ ) q_{\mathrm{min}}(\bm{\theta})=\min_{i\in[m]}z_{i}(\bm{\theta}) . For a trajectory ğœ½ t \bm{\theta}_{t} we often write z i t = z i â€‹ ( ğœ½ t ) , q min t = q min â€‹ ( ğœ½ t ) z_{i}^{t}=z_{i}(\bm{\theta}_{t}),q_{\mathrm{min}}^{t}=q_{\mathrm{min}}(\bm{\theta}_{t}) or z i , q min z_{i},q_{\mathrm{min}} when t t is clear from context. For a vector ğ¯ âˆˆ â„ n \mathbf{v}\in\mathbb{R}^{n} we denote by ğ¯ â€‹ [ j ] \mathbf{v}[j] the j j â€™th coordinate of ğ¯ \mathbf{v} . For n âˆˆ â„• n\in\mathbb{N} , we denote [ n ] = { 1 , â€¦ , n } [n]=\{1,\ldots,n\} . We denote by âˆ¥ â‹… âˆ¥ p \left\lVert\cdot\right\rVert_{p} the â„“ p \ell_{p} norm for p âˆˆ [ 1 , âˆ ] p\in[1,\infty] . For an arbitrary norm âˆ¥ â‹… âˆ¥ \left\lVert\cdot\right\rVert we denote by âˆ¥ â‹… âˆ¥ â‹† {\left\lVert\cdot\right\rVert}_{\star} the dual norm, defined by â€– ğ± â€– â‹† = max â€– ğ® â€– = 1 â¡ âŸ¨ ğ® , ğ± âŸ© {\left\lVert\mathbf{x}\right\rVert}_{\star}=\max_{\left\lVert\mathbf{u}\right\rVert=1}\left\langle\mathbf{u},\mathbf{x}\right\rangle . We denote by â€– W â€– sp {\left\lVert W\right\rVert}_{\mathrm{sp}} the standard spectral norm of a matrix W W , and â€– ( W 1 , â€¦ , W K ) â€– msp := max k âˆˆ [ K ] â¡ â€– W k â€– sp {\left\lVert(W_{1},...,W_{K})\right\rVert}_{\mathrm{msp}}:=\max_{k\in[K]}{\left\lVert W_{k}\right\rVert}_{\mathrm{sp}} (short for max-spectral). We use the standard asymptotic notations ğ’ª , Î© , Î˜ , o , Ï‰ \mathcal{O},\Omega,\Theta,\mathrm{o},\mathrm{\omega} . We denote by C k â€‹ ( X ) C^{k}(X) for X âŠ† â„ n , n âˆˆ â„• X\subseteq\mathbb{R}^{n},n\in\mathbb{N} the class of k k -times continuously differentiable functions from X X to â„ \mathbb{R} . By log â¡ u \log u we refer to the natural logarithm. By ess â€‹ lim , ess â€‹ liminf , ess â€‹ limsup \operatorname*{ess\,lim},\operatorname*{ess\,liminf},\operatorname*{ess\,limsup} we refer to essential limits holding up to sets of measure 0 . 2.2 Optimizers The optimization algorithms we study are derivatives of the steepest descent family, a generalization of gradient descent defined with respect to a norm âˆ¥ â‹… âˆ¥ \left\lVert\cdot\right\rVert (and its dual norm âˆ¥ â‹… âˆ¥ â‹† {\left\lVert\cdot\right\rVert}_{\star} ). We study the infinitesimal step size (flow) limit of the optimization trajectories. We define steepest descent and its normalized variant in the general case of a subdifferentiable model f f , allowing for a learning rate schedule Î· â€‹ ( t ) 0 \eta(t) 0 , as follows: Steepest Descent: d â€‹ ğœ½ t d â€‹ t âˆˆ { Î· â€‹ ( t ) â‹… arg â¡ min â€– ğ® â€– = â€– ğ  t â€– â‹† â¡ âŸ¨ ğ® , ğ  t âŸ© âˆ£ ğ  t âˆˆ âˆ‚ â„’ â€‹ ( ğœ½ t ) } , \frac{d\bm{\theta}_{t}}{dt}\in\left\{\eta(t)\cdot\arg\min_{\left\lVert\mathbf{u}\right\rVert={\left\lVert\mathbf{g}_{t}\right\rVert}_{\star}}\left\langle\mathbf{u},\mathbf{g}_{t}\right\rangle\mid\mathbf{g}_{t}\in\partial\mathcal{L}(\bm{\theta}_{t})\right\}~, (2) Normalized Steepest Descent: d â€‹ ğœ½ t d â€‹ t âˆˆ { Î· â€‹ ( t ) â‹… arg â¡ min â€– ğ® â€– = 1 â¡ âŸ¨ ğ® , ğ  t âŸ© âˆ£ ğ  t âˆˆ âˆ‚ â„’ â€‹ ( ğœ½ t ) } , \frac{d\bm{\theta}_{t}}{dt}\in\left\{\eta(t)\cdot\arg\min_{\left\lVert\mathbf{u}\right\rVert=1}\left\langle\mathbf{u},\mathbf{g}_{t}\right\rangle\mid\mathbf{g}_{t}\in\partial\mathcal{L}(\bm{\theta}_{t})\right\}~, (3) for almost every t â‰¥ 0 t\geq 0 , where âˆ‚ â„’ \partial\mathcal{L} is the Clarke subdifferential of â„’ \mathcal{L} (see Appendix A ), which reduces to âˆ‡ â„’ \nabla\mathcal{L} wherever f f is differentiable. Notably, gradient descent and coordinate descent are recovered with âˆ¥ â‹… âˆ¥ = âˆ¥ â‹… âˆ¥ 2 , âˆ¥ â‹… âˆ¥ 1 \left\lVert\cdot\right\rVert=\left\lVert\cdot\right\rVert_{2},\left\lVert\cdot\right\rVert_{1} respectively from Equation 2 , and sign gradient descent is recovered with âˆ¥ â‹… âˆ¥ = âˆ¥ â‹… âˆ¥ âˆ \left\lVert\cdot\right\rVert=\left\lVert\cdot\right\rVert_{\infty} from Equation 3 . Introducing momentum-based optimizers, we consider a choice of subgradients ğ  t âˆˆ âˆ‚ â„’ â€‹ ( ğœ½ t ) \mathbf{g}_{t}\in\partial\mathcal{L}(\bm{\theta}_{t}) along the trajectory, and denote the momentum estimate by the following ODE with the given explicit solution: d â€‹ ğ¦ t d â€‹ t = c 1 â€‹ ( ğ  t âˆ’ ğ¦ t ) , ğ¦ 0 = ğŸ ( Explicitly: ğ¦ t = âˆ« 0 t c 1 â€‹ e âˆ’ c 1 â€‹ ( t âˆ’ s ) â€‹ ğ  s â€‹ ğ‘‘ s ) . \begin{gathered}\frac{d\mathbf{m}_{t}}{dt}=c_{1}(\mathbf{g}_{t}-\mathbf{m}_{t}),\quad\mathbf{m}_{0}=\mathbf{0}\\ \left(\text{Explicitly:}\quad\mathbf{m}_{t}=\int_{0}^{t}c_{1}e^{-c_{1}(t-s)}\mathbf{g}_{s}ds\right)~.\end{gathered} (4) For the full derivation of the above continuous analogue of momentum, see Appendix B . Here, c 1 0 c_{1} 0 is the momentum smoothing parameter , and 1 c 1 \frac{1}{c_{1}} is the characteristic time frame in which past gradients are accumulated ( c 1 c_{1} is analogous to âˆ’ log â¡ ( Î² 1 ) -\log(\beta_{1}) for a discrete momentum parameter Î² 1 âˆˆ ( 0 , 1 ) \beta_{1}\in(0,1) , and is roughly 1 âˆ’ Î² 1 1-\beta_{1} when Î² 1 \beta_{1} is close to 1). We now define momentum steepest descent and its normalized counterpart: Momentum Steepest Descent: d â€‹ ğœ½ t d â€‹ t âˆˆ { Î· â€‹ ( t ) â‹… arg â¡ min â€– ğ® â€– = â€– ğ¦ t â€– â‹† â¡ âŸ¨ ğ® , ğ¦ t 