Title: Interactive Learning of Single-Index Models via Stochastic Gradient Descent

Abstract: Stochastic gradient descent (SGD) is a cornerstone algorithm for high-dimensional optimization, renowned for its empirical successes. Recent theoretical advances have provided a deep understanding of how SGD enables feature learning in high-dimensional nonlinear models, most notably the \textit{single-index model} with i.i.d. data. In this work, we study the sequential learning problem for single-index models, also known as generalized linear bandits or ridge bandits, where SGD is a simple and natural solution, yet its learning dynamics remain largely unexplored. We show that, similar to the optimal interactive learner, SGD undergoes a distinct ``burn-in'' phase before entering the ``learning'' phase in this setting. Moreover, with an appropriately chosen learning rate schedule, a single SGD procedure simultaneously achieves near-optimal (or best-known) sample complexity and regret guarantees across both phases, for a broad class of link functions. Our results demonstrate that SGD remains highly competitive for learning single-index models under adaptive data.

Body: Interactive Learning of Single-Index Models via Stochastic Gradient Descent 1 Introduction Notation. 1.1 Main results 1.2 Related work Single-index models. Generalized linear bandits. Gradient descent in online learning and bandits. 1.3 Organization 2 Analysis of the SGD update 3 Analysis of the learning phase 3.1 Pure exploration Regime I: t â‰¤ s T 0 t\leq s T_{0} . Regime II: s â‰¥ T 0 s\geq T_{0} . 3.2 Regret minimization 4 Analysis of the burn-in phase 4.1 Link function with derivative lower bound Regime I: t â‰¤ T 0 t\leq T_{0} . Regime II: T 0 â‰¤ t â‰¤ T T_{0}\leq t\leq T . 4.2 Convex link function 5 Discussion Comparison with other descent algorithms. Necessity of monotonicity. Comparison with information exponent. Dropping the convexity assumption. A Proofs of main lemmas A.1 Proof of Section 1.1 A.2 Proof of Section 2 A.3 Proof of Section 2 A.4 Proof of Section 2 A.5 Proof of Section 2 A.6 Proof of Section 3.2 Regime I: t â‰¤ s T 0 t\leq s T_{0} . Regime II: s â‰¥ T 0 s\geq T_{0} . A.7 Proof of Section 4.2 Case I: T 0 t + Î” T_{0} t+\Delta . Case II: T 0 â‰¤ t + Î” T_{0}\leq t+\Delta . A.8 Proof of Claim I: max t âˆˆ [ T 1 ] â¡ m t â‰¤ 0.2 \max_{t\in[T_{1}]}m_{t}\leq 0.2 with probability at least 1 âˆ’ Î´ â€‹ T 1 / ( 4 â€‹ T ) 1-\delta T_{1}/(4T) . Claim II: min t âˆˆ [ T 1 ] â¡ m t â‰¤ 0.1 \min_{t\in[T_{1}]}m_{t}\leq 0.1 with probability at least 1 âˆ’ Î´ â€‹ T 1 / ( 4 â€‹ T ) 1-\delta T_{1}/(4T) . B Auxiliary results Interactive Learning of Single-Index Models via Stochastic Gradient Descent Nived Rajaraman, Yanjun Han Nived Rajaraman is with Microsoft Research NYC, email: nrajaraman@microsoft.com . Yanjun Han is with the Courant Institute of Mathematical Sciences and Center for Data Science, New York University, email: yanjunhan@nyu.edu . Abstract Stochastic gradient descent (SGD) is a cornerstone algorithm for high-dimensional optimization, renowned for its empirical successes. Recent theoretical advances have provided a deep understanding of how SGD enables feature learning in high-dimensional nonlinear models, most notably the single-index model with i.i.d. data. In this work, we study the sequential learning problem for single-index models, also known as generalized linear bandits or ridge bandits, where SGD is a simple and natural solution, yet its learning dynamics remain largely unexplored. We show that, similar to the optimal interactive learner, SGD undergoes a distinct â€œburn-inâ€ phase before entering the â€œlearningâ€ phase in this setting. Moreover, with an appropriately chosen learning rate schedule, a single SGD procedure simultaneously achieves near-optimal (or best-known) sample complexity and regret guarantees across both phases, for a broad class of link functions. Our results demonstrate that SGD remains highly competitive for learning single-index models under adaptive data. 1 Introduction Stochastic gradient descent (SGD) and its many variants have achieved remarkable empirical success in solving high-dimensional optimization problems in machine learning. Recent theoretical advances have provided rigorous analyses of SGD in high-dimensional, non-convex settings for a range of statistical and machine learning tasks, such as tensor decomposition [ GHJ+15 ] , PCA [ WML17 ] , phase retrieval [ CCF+19 , TV23 ] , to name a few. A particularly intriguing setting is that of single-index models [ DH18 , BGJ21 ] (and generalizations to multi-index models [ AAM22 , AAM23 , DLS22 , ASK+23 , BBP25 ] ) with Gaussian data. In this framework, each observation ( x t , y t ) (x_{t},y_{t}) consists of a Gaussian feature x t âˆ¼ ğ’© â€‹ ( 0 , I d ) x_{t}\sim\mathcal{N}(0,I_{d}) and a noisy outcome y t = f â€‹ ( âŸ¨ Î¸ â‹† , x t âŸ© ) + Îµ t , \displaystyle y_{t}=f(\langle{\theta^{\star},x_{t}}\rangle)+\varepsilon_{t}, where f : â„ â†’ â„ f:\mathbb{R}\to\mathbb{R} is a known link function, Î¸ â‹† âˆˆ ğ•Š d âˆ’ 1 \theta^{\star}\in\mathbb{S}^{d-1} is an unknown parameter vector on the unit sphere in â„ d \mathbb{R}^{d} , and Îµ t \varepsilon_{t} denotes the unobserved noise. With a learning rate Î· t 0 \eta_{t} 0 and a random initialization Î¸ 1 âˆ¼ Unif â€‹ ( ğ•Š d âˆ’ 1 ) \theta_{1}\sim\mathrm{Unif}(\mathbb{S}^{d-1}) , the SGD update for learning single-index models is given by Î¸ t + 1 / 2 = Î¸ t âˆ’ Î· t â€‹ ( f â€‹ ( âŸ¨ Î¸ t , x t âŸ© ) âˆ’ y t ) â€‹ f â€² â€‹ ( âŸ¨ Î¸ t , x t âŸ© ) â‹… ( I âˆ’ Î¸ t â€‹ Î¸ t âŠ¤ ) â€‹ x t , Î¸ t + 1 = Î¸ t + 1 / 2 â€– Î¸ t + 1 / 2 â€– . \displaystyle\begin{split}\theta_{t+1/2}=\theta_{t}-\eta_{t}(f(\langle{\theta_{t},x_{t}}\rangle)-y_{t})f^{\prime}(\langle{\theta_{t},x_{t}}\rangle)\cdot(I-\theta_{t}\theta_{t}^{\top})x_{t},\quad\theta_{t+1}=\frac{\theta_{t+1/2}}{\|\theta_{t+1/2}\|}.\end{split} (1) Here, the first update is a descent step of the population loss Î¸ â†¦ 1 2 â€‹ ğ”¼ â€‹ ( f â€‹ ( âŸ¨ Î¸ , x âŸ© ) âˆ’ y ) 2 \theta\mapsto\frac{1}{2}\mathbb{E}\left(f(\langle{\theta,x}\rangle)-y\right)^{2} at Î¸ = Î¸ t \theta=\theta_{t} , whose spherical gradient 1 1 1 Recall that the spherical gradient of a function f : ğ•Š d âˆ’ 1 â†’ â„ f:\mathbb{S}^{d-1}\to\mathbb{R} is defined as âˆ‡ f = D â€‹ f âˆ’ âˆ‚ f âˆ‚ r â€‹ âˆ‚ âˆ‚ r \nabla f=Df-\frac{\partial f}{\partial r}\frac{\partial}{\partial r} , where D â€‹ f Df is the Euclidean gradient, and âˆ‚ âˆ‚ r \frac{\partial}{\partial r} is the derivative in the radial direction. is estimated based on the current sample ( x t , y t ) (x_{t},y_{t}) . It is well known (cf. e.g. [ BGJ21 ] ) that the evolution of SGD in this context exhibits two distinct phases: an initial â€œsearchâ€ phase, during which the correlation âŸ¨ Î¸ t , Î¸ â‹† âŸ© \langle{\theta_{t},\theta^{\star}}\rangle gradually improves from O â€‹ ( d âˆ’ 1 / 2 ) O(d^{-1/2}) to Î© â€‹ ( 1 ) \Omega(1) , followed by a â€œdescentâ€ phase in which the iterates Î¸ t \theta_{t} converge rapidly to the global optimum Î¸ â‹† \theta^{\star} , driving âŸ¨ Î¸ t , Î¸ â‹† âŸ© \langle{\theta_{t},\theta^{\star}}\rangle arbitrarily close to 1 1 . Beyond statistical learning, single-index models have found applications in interactive decision-making problems, including bandits and reinforcement learning, where the reward is a nonlinear function of the action. An example is manipulation with object interaction, which represents one of the largest open problems in robotics [ BK19 ] and requires designing good sequential decision rules that can deal with sparse and non-linear reward functions and continuous action spaces [ ZGR+19 ] . This setting is known as the generalized linear bandit or ridge bandit in the bandit literature, where the mean reward satisfies ğ”¼ â€‹ [ r t | a t ] = f â€‹ ( âŸ¨ Î¸ â‹† , a t âŸ© ) \mathbb{E}[r_{t}|a_{t}]=f(\langle{\theta^{\star},a_{t}}\rangle) with a known link function f f . Classical results [ FCG+10 , RV14 ] show that when 0 c 1 â‰¤ f â€² â€‹ ( x ) â‰¤ c 2 0 c_{1}\leq f^{\prime}(x)\leq c_{2} everywhere, both the optimal regret and the optimal learner are essentially the same as in the linear bandit case (where f â€‹ ( x ) = x f(x)=x ). Recent studies [ LH21 , HHK+21 , RHJ+24 ] have considered challenging settings where f â€² â€‹ ( x ) f^{\prime}(x) could be small around x = 0 x=0 . This line of work yields two main insights: 1. While the final â€œlearningâ€ phase has the same regret as linear bandits, there could be a long â€œburn-inâ€ period until the learner can identify some action a t a_{t} with âŸ¨ Î¸ â‹† , a t âŸ© = Î© â€‹ ( 1 ) \langle{\theta^{\star},a_{t}}\rangle=\Omega(1) ; 2. New exploration algorithms are necessary during this burn-in period, as classical methods such as UCB are provably suboptimal for minimizing the initial exploration cost. In response to the second point, this line of research has proposed various exploration strategies for the burn-in phase that are often tailored to the specific link function f f and rely on noisy gradient estimates via zeroth-order optimization. In contrast, SGD offers a natural and straightforward alternative, as its intrinsic â€œsearchâ€ and â€œdescentâ€ phases align well with the â€œburn-inâ€ and â€œlearningâ€ phases encountered in interactive decision-making. This paper is devoted to a systematic study of SGD for learning single-index models, including the aforementioned challenging setting where f â€² â€‹ ( x ) f^{\prime}(x) could be small around x â‰ˆ 0 x\approx 0 , within interactive decision-making settings. In these scenarios, the actions a t a_{t} are no longer Gaussian, prompting us to adopt the following exploration strategy: a t = 1 âˆ’ Ïƒ t 2 â€‹ Î¸ t + Ïƒ t â€‹ Z t , Z t âˆ¼ Unif â€‹ ( { a âˆˆ ğ•Š d âˆ’ 1 : âŸ¨ a , Î¸ t âŸ© = 0 } ) , \displaystyle a_{t}=\sqrt{1-\sigma_{t}^{2}}\theta_{t}+\sigma_{t}Z_{t},\qquad Z_{t}\sim\mathrm{Unif}\left(\left\{a\in\mathbb{S}^{d-1}:\langle{a,\theta_{t}}\rangle=0\right\}\right), (2) where an additional hyperparameter Ïƒ t âˆˆ [ 0 , 1 ] \sigma_{t}\in[0,1] governs the exploration-exploitation tradeoff. After playing the action a t a_{t} and observing the reward r t r_{t} , we update the parameter Î¸ t \theta_{t} via the same SGD as ( 1 ): Î¸ t + 1 / 2 = Î¸ t âˆ’ Î· t â€‹ ( f â€‹ ( âŸ¨ Î¸ t , a t âŸ© ) âˆ’ r t ) â€‹ f â€² â€‹ ( âŸ¨ Î¸ t , a t âŸ© ) â‹… ( I âˆ’ Î¸ t â€‹ Î¸ t âŠ¤ ) â€‹ a t , Î¸ t + 1 = Î¸ t + 1 / 2 â€– Î¸ t + 1 / 2 â€– . \displaystyle\begin{split}\theta_{t+1/2}=\theta_{t}-\eta_{t}(f(\langle{\theta_{t},a_{t}}\rangle)-r_{t})f^{\prime}(\langle{\theta_{t},a_{t}}\rangle)\cdot(I-\theta_{t}\theta_{t}^{\top})a_{t},\quad\theta_{t+1}=\frac{\theta_{t+1/2}}{\|\theta_{t+1/2}\|}.\end{split} (3) By simple algebra, the stochastic gradient in ( 3 ) is also an unbiased estimator of the population (spherical) gradient of Î¸ â†¦ 1 2 â€‹ ğ”¼ â€‹ ( f â€‹ ( âŸ¨ Î¸ , a âŸ© ) âˆ’ r ) 2 \theta\mapsto\frac{1}{2}\mathbb{E}\left(f(\langle{\theta,a}\rangle)-r\right)^{2} at Î¸ = Î¸ t \theta=\theta_{t} , with the distribution of a a given by ( 2 ) and the reward r = f â€‹ ( âŸ¨ Î¸ â‹† , a âŸ© ) + Îµ r=f(\langle{\theta^{\star},a}\rangle)+\varepsilon . Our main result will establish that, for a broad class of link functions, this SGD procedure, with appropriately chosen hyperparameters ( Î· t , Ïƒ t ) (\eta_{t},\sigma_{t}) , achieves near-optimal performance in both the burn-in and learning phases. Notation. For x âˆˆ â„ d x\in\mathbb{R}^{d} , let â€– x â€– \|x\| be its â„“ 2 \ell_{2} norm. For x , y âˆˆ â„ d x,y\in\mathbb{R}^{d} , let âŸ¨ x , y âŸ© \langle{x,y}\rangle be their inner product. Let ğ•Š d âˆ’ 1 \mathbb{S}^{d-1} be the unit sphere in â„ d \mathbb{R}^{d} . Throughout this paper we will use Î¸ â‹† âˆˆ ğ•Š d âˆ’ 1 \theta^{\star}\in\mathbb{S}^{d-1} to denote the true parameter, Î¸ t \theta_{t} to denote the current estimate, and m t = âŸ¨ Î¸ â‹† , Î¸ t âŸ© âˆˆ [ âˆ’ 1 , 1 ] m_{t}=\langle{\theta^{\star},\theta_{t}}\rangle\in[-1,1] to denote the correlation . The standard asymptotic notations o , O , Î© o,O,\Omega , etc. are used throughout the paper, and we also use O ~ , Î© ~ \widetilde{O},\widetilde{\Omega} , etc. to denote the respective meanings with hidden poly-logarithmic factors. 1.1 Main results First we give a formal formulation of the single-index model in the interactive setting. Let Î¸ â‹† âˆˆ ğ•Š d âˆ’ 1 \theta^{\star}\in\mathbb{S}^{d-1} be an unknown parameter vector, and ğ’œ = ğ•Š d âˆ’ 1 \mathcal{A}=\mathbb{S}^{d-1} be the action space. Upon choosing an action a t âˆˆ ğ’œ a_{t}\in\mathcal{A} , the learner receives a reward r t = f â€‹ ( âŸ¨ Î¸ â‹† , a t âŸ© ) + Îµ t r_{t}=f(\langle{\theta^{\star},a_{t}}\rangle)+\varepsilon_{t} for a known link function f : [ âˆ’ 1 , 1 ] â†’ â„ f:[-1,1]\to\mathbb{R} and an unobserved noise Îµ t \varepsilon_{t} which is assumed to be zero-mean and 1-subGaussian. Remark 1.1 . The scaling considered here differs crucially from the prior study on learning single-index models under non-interactive environments (such as [ DH18 , BGJ21 ] with Gaussian or i.i.d. features). In the non-interactive setting, it is usually assumed that x t âˆ¼ ğ’© â€‹ ( 0 , I d ) x_{t}\sim\mathcal{N}(0,I_{d}) , so that â€– x t â€– â‰ d \|x_{t}\|\asymp\sqrt{d} . In the interactive setting, we stick to the convention that actions belong to the unit â„“ 2 \ell_{2} ball, in line with settings considered in the bandit literature [ FCG+10 , RV14 , RHJ+24 ] . As a consequence, sample complexity comparisons between the interactive and non-interactive settings must be made with care. We discuss this in more detail in Section 5 and compare with results established for online SGD with Gaussian features [ BGJ21 ] after normalizing for the difference in scaling. Throughout the paper we make the following mild assumptions on the link function f f . Assumption 1.2 . The following conditions hold for the link function f f : 1. (monotonicity) f : [ âˆ’ 1 , 1 ] â†’ [ âˆ’ 1 , 1 ] f:[-1,1]\to[-1,1] is non-decreasing, with â€– f â€– âˆ â‰¤ 1 \|f\|_{\infty}\leq 1 ; 2. (locally linear near x = 1 x=1 ) 0 Î³ 1 â‰¤ f â€² â€‹ ( x ) â‰¤ Î³ 2 0 \gamma_{1}\leq f^{\prime}(x)\leq\gamma_{2} for all x âˆˆ [ 1 âˆ’ Î³ 0 , 1 ] x\in[1-\gamma_{0},1] , with absolute constants Î³ 0 , Î³ 1 , Î³ 2 0 \gamma_{0},\gamma_{1},\gamma_{2} 0 . Without loss of generality we assume that Î³ 0 â‰¤ 0.1 \gamma_{0}\leq 0.1 . In Section 1.1 , the monotonicity condition is taken from [ RHJ+24 ] to ensure that reward maximization is aligned with parameter estimation, where improving the alignment âŸ¨ Î¸ â‹† , a t âŸ© \langle{\theta^{\star},a_{t}}\rangle directly increases the learnerâ€™s reward. In addition, when it comes to SGD, we will show in Section 5 that the population loss associated with the SGD dynamics in ( 3 ) is decreasing in the correlation m t = âŸ¨ Î¸ â‹† , Î¸ t âŸ© m_{t}=\langle{\theta^{\star},\theta_{t}}\rangle only if f f is increasing. Without monotonicity, there also exists a counterexample where the SGD can never make meaningful progress (cf. Section 5 ). Similar to [ RHJ+24 ] , this condition can be generalized to f f being even and non-decreasing on [ 0 , 1 ] [0,1] , which covers, for example, f â€‹ ( x ) = | x | p f(x)=|x|^{p} for all p 0 p 0 . The second condition in section 1.1 is very mild, satisfied by many natural functions, and ensures that the problem locally resembles a linear bandit near the global optimum a t â‰ˆ Î¸ â‹† a_{t}\approx\theta^{\star} . Finally, we emphasize that this local linearity does not exclude the nontrivial scenario where f â€² â€‹ ( x ) f^{\prime}(x) is very small when x â‰ˆ 0 x\approx 0 . Our first result is the SGD dynamics in the learning phase, under Section 1.1 . Theorem 1.3 (Learning Phase) . Let Îµ , Î´ 0 \varepsilon,\delta 0 . Under section 1.1 , let ( a t , Î¸ t ) t â‰¥ 1 (a_{t},\theta_{t})_{t\geq 1} be given by the SGD evolution in ( 2 ) and ( 3 ), with an initialization Î¸ 1 \theta_{1} such that âŸ¨ Î¸ 1 , Î¸ â‹† âŸ© â‰¥ 1 âˆ’ Î³ 0 / 4 \langle{\theta_{1},\theta^{\star}}\rangle\geq 1-\gamma_{0}/4 . 1. (Pure exploration) By choosing Î· t = Î˜ ~ â€‹ ( d t âˆ§ 1 d ) \eta_{t}=\widetilde{\Theta}(\frac{d}{t}\wedge\frac{1}{d}) and Ïƒ t 2 = Î˜ â€‹ ( 1 ) \sigma_{t}^{2}=\Theta(1) , it holds that m T â‰¥ 1 âˆ’ Îµ m_{T}\geq 1-\varepsilon with probability at least 1 âˆ’ Î´ â€‹ T 1-\delta T , with T = O ~ â€‹ ( d 2 Îµ ) T=\widetilde{O}(\frac{d^{2}}{\varepsilon}) . 2. (Regret minimization) By choosing Î· t = Î˜ ~ â€‹ ( 1 t âˆ§ 1 d ) \eta_{t}=\widetilde{\Theta}(\frac{1}{\sqrt{t}}\wedge\frac{1}{d}) and Ïƒ t 2 = Î˜ ~ â€‹ ( d t âˆ§ 1 ) \sigma_{t}^{2}=\widetilde{\Theta}(\frac{d}{\sqrt{t}}\wedge 1) , with probability at least 1 âˆ’ Î´ â€‹ T 1-\delta T it holds that âˆ‘ t = 1 T ( f â€‹ ( 1 ) âˆ’ f â€‹ ( m t ) ) = O ~ â€‹ ( d â€‹ T ) \sum_{t=1}^{T}(f(1)-f(m_{t}