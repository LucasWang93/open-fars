Title: DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training

Abstract: Determinism is indispensable for reproducibility in large language model (LLM) training, yet it often exacts a steep performance cost. In widely used attention implementations such as FlashAttention-3, the deterministic backward pass can incur up to a 37.9% throughput reduction relative to its non-deterministic counterpart, primarily because gradient accumulation operations must be serialized to guarantee numerical consistency. This performance loss stems from suboptimal scheduling of compute and gradient-reduction phases, leading to significant hardware underutilization.   To address this challenge, we formulate the backward pass of deterministic attention as a scheduling problem on a Directed Acyclic Graph (DAG) and derive schedules that minimize the critical path length. Building on this formulation, we present DASH (Deterministic Attention Scheduling for High-Throughput), which encapsulates two complementary scheduling strategies: (i) Descending Q-Tile Iteration, a reversed query-block traversal that shrinks pipeline stalls in causal attention, and (ii) Shift Scheduling, a theoretically optimal schedule within our DAG model that reduces pipeline stalls for both full and causal masks.   Our empirical evaluations on NVIDIA H800 GPUs demonstrate that DASH narrows the performance gap of deterministic attention. The proposed strategies improve the throughput of the attention backward pass by up to 1.28$\times$ compared to the baseline, significantly advancing the efficiency of reproducible LLM training.   Our code is open-sourced at https://github.com/SJTU-Liquid/deterministic-FA3.

Body: DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training 1 Introduction 2 Background 2.1 Deterministic FlashAttention Backward Pass 2.2 GPU Architecture 2.3 Determinism in Other Operations of the Transformer 3 DASH: Scheduling Strategies for Deterministic Attention 3.1 Problem Formulation Graph Construction. Optimization Constraint. 3.2 Analysis of FlashAttention-3 Deterministic Backward Schedule 3.3 Descending Q-Tile Iteration: A Robust Heuristic for Causal Masks 3.4 Shift Scheduling Optimal Schedule for Full Masks Symmetric Shift Scheduling for Causal Masks Summary of Optimal Performance 4 Experiments 4.1 Experimental Setup Hardware and Software. Baseline and Proposed Methods. Benchmark Settings 4.2 Performance on Full Attention Masks 4.3 Performance on Causal Attention Masks 4.4 End-to-end Performance 4.5 Impact of Determinism on Numerical Stability 5 Related Works FlashAttention and Kernel-Level I/O Optimization Low-Precision Attention Inference-Oriented Attention Kernels Distributed Cyclic Scheduling Deterministic Implementations Determinism in Inference 6 Conclusion A The Use of Large Language Models B Proof of lemma 1 C Exact Algorithm and Modifications DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training Xinwei Qiang 1,3 , Hongmin Chen 2 , Shixuan Sun 1 , Jingwen Leng 1 , Xin Liu 2 , Minyi Guo 1 1 School of Computer Science, Shanghai Jiao Tong University 2 ByteDance Seed, 3 Zhiyuan College, Shanghai Jiao Tong University 1 {qiangxinwei, sunshixuan, leng-jw, guo-my}@sjtu.edu.cn 2 {chenhongmin.will, liuxin.ai}@bytedance.com Shixuan Sun is the corresponding author. Abstract Determinism is indispensable for reproducibility in large language model (LLM) training, yet it often exacts a steep performance cost. In widely used attention implementations such as FlashAttention-3, the deterministic backward pass can incur up to a 37.9% throughput reduction relative to its non‑deterministic counterpart, primarily because gradient accumulation operations must be serialized to guarantee numerical consistency. This performance loss stems from suboptimal scheduling of compute and gradient‑reduction phases, leading to significant hardware underutilization. To address this challenge, we formulate the backward pass of deterministic attention as a scheduling problem on a Directed Acyclic Graph (DAG) and derive schedules that minimize the critical path length. Building on this formulation, we present DASH(Deterministic Attention Scheduling for High-Throughput), which encapsulates two complementary scheduling strategies: (i) Descending Q‑Tile Iteration, a reversed query‑block traversal that shrinks pipeline stalls in causal attention, and (ii) Shift Scheduling, a theoretically optimal schedule within our DAG model that reduces pipeline stalls for both full and causal masks. Our empirical evaluations on NVIDIA H800 GPUs demonstrate that DASH narrows the performance gap of deterministic attention. The proposed strategies improve the throughput of the attention backward pass by up to 1.28 × \times compared to the baseline, significantly advancing the efficiency of reproducible LLM training. Our code is open-sourced at https://github.com/SJTU-Liquid/deterministic-FA3 . 1 Introduction The pursuit of consistent and verifiable outcomes is a cornerstone of rigorous scientific research and large-scale engineering. In the domain of large language model (LLM) training (Wu et al. , 2024 ) , where experiments span thousands of GPUs (Grattafiori et al. , 2024 ; DeepSeek-AI et al. , 2025 ) and incur enormous costs, this principle of reproducibility becomes indispensable. Reproducibility empowers practitioners to diagnose training instabilities, such as loss divergence, and to evaluate the impact of architectural modifications. Consequently, deterministic training, which guarantees bitwise identical results across runs, is increasingly adopted as a standard practice for industry. The origin of the non-determinism in attention of LLM training can be traced back to a fundamental yet often overlooked characteristic of computer arithmetic: the non-associativity of floating-point (FP) operations (Villa et al. , 2009 ) . For instance, ( 10 8 + 10 − 6 ) − 10 8 (10^{8}+10^{-6})-10^{8} evaluates to 0.0 0.0 in single-precision, whereas 10 8 − 10 8 + 10 − 6 10^{8}-10^{8}+10^{-6} yields the correct 10 − 6 10^{-6} . This sensitivity is magnified in the massively parallel environment of GPUs (Shanmugavelu et al. , 2024 ) . Figure 1: Overview of the deterministic FlashAttention. Left: Tiled computation structure of the backward pass, highlighting the local and global reductions. Middle: Comparison between the non-deterministic (atomic-based) and deterministic (ordered) global reduction. Right: Performance degradation under causal and full attention masks, HD stands for head dimension. In high-performance attention (Vaswani et al. , 2023 ) mechanisms like FlashAttention (Dao et al. , 2022 ) , the backward pass computation is parallelized across hundreds of GPU Streaming Multiprocessors (SMs) (NVIDIA, 2022 ) to maximize throughput. Each SM, running a Cooperative Thread Array (CTA), accumulates a partial contribution to gradient tensors (e.g., the gradient for the query matrix, d ​ Q \mathrm{d}Q ). The default high-speed approach allows these CTAs to concurrently update the final gradient in global memory via non-deterministic atomicAdd operations, as shown in Figure 1 middle. This creates a non-deterministic accumulation order: the final accumulated value depends on the uncontrolled completion order of the CTAs, leading to bit-wise variations between runs. To enforce reproducibility, FlashAttention-3 (Shah et al. , 2024 ) provides a deterministic mode. It enforces a fixed accumulation order by using synchronization barriers to force CTAs to perform their additions in a serialized order (e.g., ordered by CTA index). However, this guarantee of consistency imposes a significant performance penalty. As illustrated in Figure 1 right, enabling deterministic mode may lower throughput by up to 37.9%, leading to severe training costs when scaling LLMs across hundreds of thousands of GPUs. This performance gap is not an inherent consequence of serialization itself. Instead, it stems from a direct conflict between the tile scheduling and a rigid, pre-determined accumulation order. As illustrated in the middle of Figure 1 , the full mask scenario, commonly employed in multi-modal tasks, highlights a key inefficiency: the naive schedule creates a bottleneck by forcing reductions to start sequentially. An ideal schedule, however, would parallelize this process, allowing CTAs to begin reduction on different tiles concurrently. Crucially, this reveals that the computation schedule and the accumulation order are tightly coupled and cannot be optimized in isolation. To address this, we introduce D eterministic A ttention S cheduling for H igh-throughput (DASH), a framework that formulates deterministic attention backward execution as an explicit scheduling optimization problem. We model the deterministic backward pass as a Directed Acyclic Graph (DAG), and formalize the objective as minimizing the DAG’s critical path length. Based on this model, we design two complementary scheduling strategies. The first, Descending Q-Tile Iteration , is a heuristic that processes query tiles in reverse order to advance dependency resolution and shrink pipeline bubbles in causal attention. The second strategy, a theoretically optimal algorithm we term Shift Scheduling is provably optimal under our DAG model. It employs a phase-shifted assignment of computational tasks to GPU multiprocessors, creating a perfectly staggered execution pattern. This ensures that the workload is perfectly balanced and that the serialized reduction operations proceed without contention while approaching the model’s theoretical utilization bound. Our empirical evaluations on NVIDIA H800 GPUs show that DASH significantly narrows the performance gap relative to the FlashAttention-3 deterministic baseline. The two strategies deliver up to a 1.28 × 1.28\times speedup for the deterministic attention backward pass, significantly improving the efficiency of reproducible LLM training. In summary, we made the following contributions in this paper: • We identify the misalignment between tile execution and accumulation ordering as the principal source of performance degradation in deterministic attention. • We provide the first DAG-based formalization of deterministic attention backward scheduling, enabling principled optimization of critical path length. • We introduce two complementary scheduling strategies, Descending Q-Tile Iteration and Shift Scheduling, that achieve up to a 1.28 × \times speedup over the FlashAttention-3 deterministic baseline on H800 GPUs. 2 Background 2.1 Deterministic FlashAttention Backward Pass We first outline the core gradient computations in the FlashAttention backward pass: d ​ Q \mathrm{d}Q , d ​ K \mathrm{d}K , and d ​ V \mathrm{d}V (Figure 1 , left). During backpropagation, the gradients d ​ K \mathrm{d}K and d ​ V \mathrm{d}V are accumulated across all queries for each key (or value) position, i.e., they are reduced along the Q Q axis. In contrast, d ​ Q \mathrm{d}Q requires a reduction across all key–value (KV) positions for each query, i.e., along the KV axis. To expose parallelism, the implementation partitions the KV dimension across SMs, allowing d ​ K dK and d ​ V dV to be computed within each SM via a local reduction. However, this strategy distributes partial contributions to d ​ Q \mathrm{d}Q over multiple SMs, necessitating a global reduction to produce the final gradient. A conventional implementation performs this reduction using atomic additions (Figure 1 , middle), which induces run-to-run variation because floating-point addition is non-associative. The resulting numerical nondeterminism undermines strict reproducibility in large-scale training. To guarantee determinism, one must enforce a prescribed accumulation order. FlashAttention-3 achieves this by performing a tile-wise sequential accumulation of d ​ Q \mathrm{d}Q along the KV dimension. 2.2 GPU Architecture On modern GPUs, the memory hierarchy comprises registers, shared memory, L2 cache, and global memory (NVIDIA, 2022 ) , reflecting a fundamental capacity–latency trade-off: smaller and faster storage resides closer to the compute units. Shared memory is private to each SM, enabling low-latency intra-SM data reuse, whereas the L2 cache is globally shared, mediating inter-SM data exchange and coherence. In datacenter-class GPUs, the L2 cache may be physically segmented, with each segment preferentially serving a subset of SMs; remote-segment accesses typically incur higher latency than local ones. This hierarchical organization materially shapes the attainable performance and the efficiency of memory-bound GPU kernels. 2.3 Determinism in Other Operations of the Transformer Other components, such as GEMMs, attention forward and normalization, also involve reduction operations; however, the computational cost of enforcing determinism in these cases is generally minimal during typical LLM training. GEMMs may exhibit nondeterministic behavior only when the reduction axis (i.e., the K-dimension) is partitioned across multiple blocks, as in split-K (NVIDIA Corporation, 2025 ) or stream-K (Osama et al. , 2023 ) parallelization modes. In large-batch LLM training, parallelism along the M and N dimensions is typically sufficient to fully utilize the GPU, rendering split-K or stream-K modes unnecessary; therefore, disabling these modes generally results in only a minor reduction in throughput. Similarly, other operations involving reduction, such as attention forward passes and normalizations, typically perform reductions within a single block, thereby ensuring a deterministic reduction order. Purely elementwise operations, including activation functions and bias additions, are inherently deterministic. 3 DASH: Scheduling Strategies for Deterministic Attention In this section, we introduce optimized scheduling strategies for deterministic attention. Without loss of generality, we assume that the number of KV tiles equals the number of SMs, denoted by n n . When the actual number of KV tiles differs from the number of SMs, we conceptually refine or aggregate attention heads so that all SMs remain fully utilized under the same analytical framework. 3.1 Problem Formulation Figure 2: Visualization of the Deterministic Scheduling Problem. The Gantt chart (left) shows a naive execution schedule for a problem with two KV-tiles ( i i -index) and two Q-tiles ( j j -index). Each task consists of a compute phase C ​ ( i , j ) C(i,j) and a reduction phase R ​ ( i , j ) R(i,j) . Local reductions enforce contiguous execution on a single SM (e.g., all tasks for i = 0 i=0 on SM0). A deterministic global reduction order introduces a cross-SM dependency (red arrow), forcing SM1 to idle and creating a pipeline bubble. The corresponding DAG (right) abstracts this schedule, where the critical path determines the end-to-end latency. We formalize the deterministic attention backward scheduling problem as an optimization over a directed acyclic graph (DAG), as shown in Figure 2 . The DAG’s structure is constrained jointly by the dataflow of FlashAttention and the architectural characteristics of the target GPU. Our model represents a simplified abstraction of actual GPU execution; its primary purpose is to offer insights into more effective scheduling decisions, rather than to accurately predict real execution times. As such, there remain significant differences between our theoretical model and the complexities of real-world GPU behavior. Graph Construction. Each tile-processing task is modeled as a linear path of nodes connected by edges that encode two successive phases: (i) the tile’s computation and (ii) the subsequent global reduction. These phase edges are weighted by their respective execution times, which are assumed to be constants. To encode legal accumulation orderings and data dependencies across tiles, we insert zero-weight dependency edges between nodes of different task paths. In this way, edge weights capture quantitative duration, while the topology captures qualitative ordering constraints. The scheduling objective is to minimize the critical-path length of the resulting DAG, thereby reducing end-to-end latency and improving overall execution efficiency. Optimization Constraint. Data movement across different memory levels incurs substantial overhead, while registers provide the fastest storage in GPUs. To leverage fast register-resident accumulation of d ​ K \mathrm{d}K and d ​ V \mathrm{d}V , all operations for a given KV tile must run contiguously on a single SM. Consequently, the edges associated with this tile form an unbroken chain, which imposes a key constraint on 