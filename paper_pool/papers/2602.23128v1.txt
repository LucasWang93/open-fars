Title: Bound to Disagree: Generalization Bounds via Certifiable Surrogates

Abstract: Generalization bounds for deep learning models are typically vacuous, not computable or restricted to specific model classes. In this paper, we tackle these issues by providing new disagreement-based certificates for the gap between the true risk of any two predictors. We then bound the true risk of the predictor of interest via a surrogate model that enjoys tight generalization guarantees, and evaluating our disagreement bound on an unlabeled dataset. We empirically demonstrate the tightness of the obtained certificates and showcase the versatility of the approach by training surrogate models leveraging three different frameworks: sample compression, model compression and PAC-Bayes theory. Importantly, such guarantees are achieved without modifying the target model, nor adapting the training procedure to the generalization framework.

Body: Bound to Disagree: Generalization Bounds via Certifiable Surrogates 1 INTRODUCTION 2 BACKGROUND AND NOTATION 2.1 Sample compression theory 2.2 Model compression theory 2.3 PAC-Bayes theory 2.4 Other theoretical frameworks 3 Disagreement-based Bounds 3.1 Disagreement with the zero-one loss 3.2 Disagreement with Lipschitz losses 3.3 Disagreement with non-Lipschitz losses 4 EXPERIMENTS 4.1 Certifying the target model 4.2 Minimizing the disagreement 4.3 Bounding the true risk gap 5 CONCLUSION A DEFINITIONS AND THEORETICAL RESULTS FROM THE LITERATURE B PROOFS OF THE MAIN RESULTS C ADDITIONAL RESULTS C.1 Special cases of the main results C.2 Computable PAC-Bayesian disagreement bounds C.3 Using other comparator functions D EXPERIMENTS Devices. Libraries. Datasets. D.1 Target model D.2 Partition-based bounds D.2.1 Ablation study on the partitioning method D.3 Norm-based bounds D.4 Sample compression experiments D.4.1 Ablation study for the bounded cross-entropy loss D.5 Model compression experiments D.6 PAC-Bayes experiments D.7 Model distillation on Amazon polarity D.8 Quantization experiments on Amazon polarity Bound to Disagree: Generalization Bounds via Certifiable Surrogates Mathieu Bazinet DÃ©partement dâ€™informatique et gÃ©nie logiciel UniversitÃ© Laval QuÃ©bec, Qc, Canada Valentina Zantedeschi DÃ©partement dâ€™informatique et gÃ©nie logiciel UniversitÃ© Laval QuÃ©bec, Qc, Canada ServiceNow Research Montreal, Qc, Canada Pascal Germain DÃ©partement dâ€™informatique et gÃ©nie logiciel UniversitÃ© Laval QuÃ©bec, Qc, Canada Abstract Generalization bounds for deep learning models are typically vacuous, not computable or restricted to specific model classes. In this paper, we tackle these issues by providing new disagreement-based certificates for the gap between the true risk of any two predictors. We then bound the true risk of the predictor of interest via a surrogate model that enjoys tight generalization guarantees, and evaluating our disagreement bound on an unlabeled dataset. We empirically demonstrate the tightness of the obtained certificates and showcase the versatility of the approach by training surrogate models leveraging three different frameworks: sample compression, model compression and PAC-Bayes theory. Importantly, such guarantees are achieved without modifying the target model, nor adapting the training procedure to the generalization framework. 1 INTRODUCTION Deep neural networks have been the stars of the machine learning field for the last decade. Their empirical performance challenges the once-common wisdom that over-parameterized models should overfit the training data [Zhang et al., 2017 , 2021 ] . This gap between practice and theory calls for refined theoretical frameworks for studying generalization. The community has mainly turned to statistical learning theory to understand this phenomenon, producing generalization bounds based on the VC dimension [Vapnik and Chervonenkis, 1971 ] , the Rademacher and Gaussian complexities [e.g., Bartlett and Mendelson, 2002 , Pinto et al., 2025 ] , information theory [e.g., HellstrÃ¶m et al., 2025 ] , PAC-Bayes theory [McAllester, 1998 ] , sample compression theory [Littlestone and Warmuth, 1986 ] and model compression theory [e.g., Zhou et al., 2019 ] . Despite this breadth of approaches, progress has remained limited: most bounds are vacuous when applied to medium-to-large neural networks or apply only to a modified version of the model rather than the original predictor. Figure 1 : Generalization bounds. Comparison between bounds from the literature (Norm-based and Partition-based) and our new disagreement-based bounds, using surrogates from sample compression (SC), model compression (MC) and PAC-Bayes (PB) theory. A recent line of work sidesteps these issues by leveraging a simpler surrogate model with similar behavior to the network of interest. The strategy is then to bound the true risk of the original model through its link with the surrogate. In particular, Suzuki et al. [ 2020 ] and Hsu et al. [ 2021 ] leverage a smaller compressed neural network as surrogate models, and derive generalization bounds either based on distillation or on the Minkowski difference between the two models. However, both results depend on universal constants that cannot be computed. In contrast, Dziugaite and Roy [ 2025 ] provides PAC-Bayes generalization bounds by pruning neural networks and finding a â€œteacherâ€ model of smaller size with small risk within them. While the resulting bounds are computable, this approach is restricted to neural networks with gated activations and residual connections, limiting its applicability. In this paper, we present a framework that fulfills all desiderata simultaneously: we provide fully computable, non-vacuous bounds that hold with high probability and apply to any predictor, with no restrictions on architecture, activation functions, training objective, or optimizer (see Table Ëœ 1 for an overview of desiderata fulfilled by each framework). Like prior work, we leverage a surrogate model that is simple enough to enjoy tight generalization guarantees on its own, while remaining close enough to the target network that their predictions align on most inputs. The performance gap between the two models is measured through a disagreement bound, which we evaluate on a small unlabeled dataset not used during training, the only additional requirement of our approach. In comparison to labeled data, acquiring unlabeled data is much cheaper and faster to obtain [Liao et al., 2021 ] , making this requirement much less stringent than labeled data. Our key theoretical contributions are reported in Section 3 , where we present a sketch of our disagreement bound, and before specializing this result to the zero-one loss and to Lipschitz-continuous losses. We further provide generalization certificates for the case where the disagreement term is optimized on the available data (e.g., via model distillation). The resulting certificates are much stronger than the other bounds in the literature that are computable and that hold for the target model without modification, as illustrated in Fig. Ëœ 1 . Unlike Hsu et al. [ 2021 ] and Suzuki et al. [ 2020 ] , our guarantees can be derived for virtually any proxy model. We showcase this versatility in Section 4 , where we apply three different approaches (sample compression, model compression, and PAC-Bayes theory) to train and certify the proxy model, and study various families of neural networks: a CNN on MNIST [LeCun et al., 1998 ] , a ResNet18 [He et al., 2016 ] on CIFAR10 [Krizhevsky et al., 2009 ] , and a DistilBERT [Sanh et al., 2019 ] and a GPT2 [Radford et al., 2019 ] on Amazon polarity [Zhang et al., 2015 ] . Table 1 : Certification frameworks comparison. We seek deep neural network bounds that are computable, non-vacuous, and do not require assumptions about the target model. The last column shows whether an additional dataset beyond the train set is required (and should it be labeled or unlabeled). Computable Non-vacuous No assumptions No additional data PAC-Bayes âœ“ \boldsymbol{\checkmark} âœ“ \boldsymbol{\checkmark} Ã— \boldsymbol{\times} âœ“ \boldsymbol{\checkmark} Sample Compression âœ“ \boldsymbol{\checkmark} âœ“ \boldsymbol{\checkmark} Ã— \boldsymbol{\times} âœ“ \boldsymbol{\checkmark} Model Compression âœ“ \boldsymbol{\checkmark} âœ“ \boldsymbol{\checkmark} Ã— \boldsymbol{\times} âœ“ \boldsymbol{\checkmark} VC dimension Ã— \boldsymbol{\times} Ã— \boldsymbol{\times} âœ“ \boldsymbol{\checkmark} âœ“ \boldsymbol{\checkmark} Information-theoretic âœ“ \boldsymbol{\checkmark} âœ“ \boldsymbol{\checkmark} Ã— \boldsymbol{\times} Ã— \boldsymbol{\times} (need labeled data) Norm-based âœ“ \boldsymbol{\checkmark} Ã— \boldsymbol{\times} âœ“ \boldsymbol{\checkmark} âœ“ \boldsymbol{\checkmark} Partition-based âœ“ \boldsymbol{\checkmark} Ã— \boldsymbol{\times} âœ“ \boldsymbol{\checkmark} âœ“ \boldsymbol{\checkmark} Our bound âœ“ \boldsymbol{\checkmark} âœ“ \boldsymbol{\checkmark} âœ“ \boldsymbol{\checkmark} Ã— \boldsymbol{\times} (need unlabeled data) 2 BACKGROUND AND NOTATION Let ( ğ’™ 1 , y 1 ) , â€¦ , ( ğ’™ n , y n ) (\boldsymbol{x}_{1},y_{1}),\ldots,(\boldsymbol{x}_{n},y_{n}) be a sequence of n n independently and identically distributed ( i.i.d. ) datapoints sampled from an unknown distribution ğ’Ÿ \operatorname{\mathcal{D}} over ğ’³ Ã— ğ’´ \operatorname{\mathcal{X}}\times\operatorname{\mathcal{Y}} . Let S = { ( ğ’™ i , y i ) } i = 1 n S=\{(\boldsymbol{x}_{i},y_{i})\}_{i=1}^{n} be the dataset that contains these datapoints. In this paper, we consider a feature space ğ’³ âŠ† â„ d \operatorname{\mathcal{X}}\subseteq\operatorname{\mathbb{R}}^{d} and a label space ğ’´ = { 1 , 2 , â€¦ , C } \operatorname{\mathcal{Y}}=\{1,2,\ldots,C\} for C C -class classification tasks. The hypothesis class â„‹ \operatorname{\mathcal{H}} contains predictors h : ğ’³ â†’ â„ C h:\operatorname{\mathcal{X}}\to\operatorname{\mathbb{R}}^{C} with h â€‹ ( ğ’™ ) = ( h â€‹ ( ğ’™ ) 1 , â€¦ , h â€‹ ( ğ’™ ) C ) h(\boldsymbol{x})=(h(\boldsymbol{x})_{1},\ldots,h(\boldsymbol{x})_{C}) . A learning algorithm A : â‹ƒ k = 1 âˆ ( ğ’³ Ã— ğ’´ ) k â†’ â„‹ A:\bigcup_{k=1}^{\infty}(\operatorname{\mathcal{X}}\times\operatorname{\mathcal{Y}})^{k}\to\operatorname{\mathcal{H}} outputs a predictor A â€‹ ( S ) âˆˆ â„‹ A(S)\in\operatorname{\mathcal{H}} when applied to S S . Let â„“ : â„ C Ã— ğ’´ â†’ [ B â„“ , T â„“ ] \ell:\operatorname{\mathbb{R}}^{C}\times\operatorname{\mathcal{Y}}\to[B_{\ell},T_{\ell}] denote a loss function with a range Î» â„“ = T â„“ âˆ’ B â„“ \lambda_{\ell}=T_{\ell}-B_{\ell} . Given a loss function â„“ \ell , the true loss of a predictor h h is â„’ ğ’Ÿ â¡ ( h ) = ğ”¼ ( ğ’™ , y ) âˆ¼ ğ’Ÿ â„“ â€‹ ( h â€‹ ( ğ’™ ) , y ) . \operatorname{\mathcal{L}}_{\operatorname{\mathcal{D}}}(h)=\operatorname*{\mathbb{E}}_{(\boldsymbol{x},y)\sim\operatorname{\mathcal{D}}}\ell(h(\boldsymbol{x}),y). The true loss cannot be computed, as ğ’Ÿ \operatorname{\mathcal{D}} is unknown, while, given a dataset S âˆ¼ ğ’Ÿ n S{\,\sim}\operatorname{\mathcal{D}}^{n} , the empirical loss of a predictor is defined as â„’ ^ S â¡ ( h ) = 1 n â€‹ âˆ‘ i = 1 n â„“ â€‹ ( h â€‹ ( ğ’™ i ) , y i ) . \operatorname{\widehat{\mathcal{L}}}_{S}(h)=\frac{1}{n}\sum_{i=1}^{n}\ell(h(\boldsymbol{x}_{i}),y_{i}). In this paper, we are mainly interested in two loss functions: the zero-one loss â„“ 0 â€‹ - â€‹ 1 \ell^{0\textrm{-}1} and the cross-entropy loss â„“ x-e \ell^{\textrm{x-e}} . Given a predictor h h and a pair ( ğ’™ , y ) (\boldsymbol{x},y) , the zero-one loss is defined as â„“ 0 â€‹ - â€‹ 1 â€‹ ( h â€‹ ( ğ’™ ) , y ) = ğ•€ â¡ [ argmax j h â€‹ ( ğ’™ ) j â‰  y ] \ell^{0\textrm{-}1}(h(\boldsymbol{x}),y)=\operatorname{\mathbb{I}}[\operatorname*{argmax}_{j}h(\boldsymbol{x})_{j}\neq y] , where the indicator function ğ•€ â¡ [ a ] \operatorname{\mathbb{I}}[a] takes the value of 1 1 if the predicate a a is true and 0 otherwise. This loss has its own notation for the true risk and empirical risk, respectively denoted R ğ’Ÿ â€‹ ( h ) R_{\operatorname{\mathcal{D}}}(h) and R ^ S â€‹ ( h ) \widehat{R}_{S}(h) . The cross-entropy loss is widely used to train neural networks and has recently become an object of interest in statistical learning theory [e.g., PÃ©rez-Ortiz et al., 2021 , Lotfi et al., 2024 ] . The cross-entropy loss is defined as â„“ x-e â€‹ ( h â€‹ ( ğ’™ ) , y ) = âˆ’ ln â¡ ( Ïƒ â€‹ ( h â€‹ ( ğ’™ ) , y ) ) \ell^{\textrm{x-e}}(h(\boldsymbol{x}),y)=-\ln\left(\sigma(h(\boldsymbol{x}),y)\right) with the softmax function Ïƒ â€‹ ( h â€‹ ( ğ’™ ) , y ) = exp â¡ ( h â€‹ ( ğ’™ ) y ) âˆ‘ c = 1 C exp â¡ ( h â€‹ ( ğ’™ ) c ) . \sigma(h(\boldsymbol{x}),y)=\tfrac{\exp\left(h(\boldsymbol{x})_{y}\right)}{\sum_{c=1}^{C}\exp\left(h(\boldsymbol{x})_{c}\right)}. We denote ğˆ â€‹ ( f â€‹ ( ğ’™ ) ) = ( Ïƒ â€‹ ( f â€‹ ( ğ’™ ) , 1 ) , â€¦ , Ïƒ â€‹ ( f â€‹ ( ğ’™ ) , C ) ) \boldsymbol{\sigma}(f(\boldsymbol{x}))=\left(\sigma(f(\boldsymbol{x}),1),\ldots,\sigma(f(\boldsymbol{x}),C)\right) the softmax vector. By forcing the softmax to always be greater than some positive constant, it is possible to bound the cross-entropy loss, which otherwise tends to infinity when the softmax tends to zero. To do so, we consider the clamped softmax of PÃ©rez-Ortiz et al. [ 2021 ] (see Definition Ëœ S1 ) and the smoothed softmax of Lotfi et al. [ 2024 ] (see Definition Ëœ S2 ). We now review the frameworks we leverage to derive the results in Section Ëœ 3 or experiment with in Section Ëœ 4 . 2.1 Sample compression theory Introduced by Littlestone and Warmuth [ 1986 ] , sample compression theory provides generalization guarantees for data-dependent predictors that can be fully described by a small subset of the training data. Intuitively, if a modelâ€™s parameters depend on only a few training points, the model is unlikely to have overfit, and sample compression theory formalizes this intuition by providing an upper bound of the true risk of the model. Some examples of algorithms compatible with the sample compression framework include the support vector machine [Boser et al., 1992 ] , the perceptron [Rosenblatt, 1958 , Moran et al., 2020 ] , the decision tree [Shah, 2007 ] , the set covering machine [Marchand and Shawe-Taylor, 2002 , Marchand et al., 2003 , Marchand and Sokolova, 2005 , Laviolette et al., 2005 ] and Pick-To-Learn [Paccagnan et al., 2024 , Marks and Paccagnan, 2025 , Paccagnan et al., 2025 ] . The latter was shown to be very effective at deriving tight deep learning bounds [Bazinet et al., 2025 , Comeau et al., 2025 ] . We denote the compression set S ğ¢ = { ( ğ’™ i , y i ) } i âˆˆ ğ¢ S_{\operatorname{\mathbf{i}}}=\{(\boldsymbol{x}_{i},y_{i})\}_{i\in\operatorname{\mathbf{i}}} , which is defined using a strictly increasing sequence of | ğ¢ | \operatorname{|\mathbf{i}|} indices ğ¢ = ( i 1 , â€¦ , i | ğ¢ | ) \operatorname{\mathbf{i}}=(i_{1},\ldots,i_{\operatorname{|\mathbf{i}|}}) . All sequences ğ¢ \operatorname{\mathbf{i}} belong to ğ’« â¡ ( n ) \operatorname{\mathscr{P}}(n) , the set of all the 2 n 2^{n} strictly increasing sequences composed of the numbers 1 1 through n n . We denote the complement set S ğ¢ c = S âˆ– S ğ¢ S_{\operatorname{\mathbf{i}}^{c}}=S\setminus S_{\operatorname{\mathbf{i}}} with | ğ¢ c | = n âˆ’ | ğ¢ | |\mathbf{i}^{c}|=n-\operatorname{|\mathbf{i}|} . The subset of sample-compressed predictors is denoted â„‹ S âŠ† â„‹ \operatorname{\mathcal{H}}_{S}\subseteq\operatorname{\mathcal{H}} . A predictor h = A â€‹ ( S ) h=A(S) is called sample-compressed if there exists a function â„› : â‹ƒ m â‰¤ n ( ğ’³ Ã— ğ’´ ) m â†’ â„‹ S \operatorname{\mathscr{R}}:\bigcup_{m\leq n}(\operatorname{\mathcal{X}}\times\operatorname{\mathcal{Y}})^{m}\to\operatorname{\mathcal{H}}_{S} and a sequence ğ¢ âˆˆ ğ’« â¡ ( n ) \operatorname{\mathbf{i}}\in\operatorname{\mathscr{P}}(n) such that â„› â¡ ( S ğ¢ ) \operatorname{\mathscr{R}}(S_{\operatorname{\mathbf{i}}}) would return the same predictor as the original algorithm ( A â€‹ ( S ) = â„› â¡ ( S ğ¢ ) A(S)=\operatorname{\mathscr{R}}(S_{\operatorname{\mathbf{i}}}) ). For any such predictor, the sample-compression bound of Bazinet et al. [ 2025 ] can be used to upper bound the true risk of the predictor. Theorem 1 ( Bazinet et al. [ 2025 ] ) . For a distribution ğ’Ÿ \operatorname{\mathcal{D}} over ğ’³ Ã— ğ’´ \operatorname{\mathcal{X}}\times\operatorname{\m