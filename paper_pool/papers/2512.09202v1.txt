Title: Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising paradigm for solving partial differential equations (PDEs) by embedding physical laws into neural network training objectives. However, their deployment on resource-constrained platforms is hindered by substantial computational and memory overhead, primarily stemming from higher-order automatic differentiation, intensive tensor operations, and reliance on full-precision arithmetic. To address these challenges, we present a framework that enables scalable and energy-efficient PINN training on edge devices. This framework integrates fully quantized training, Stein's estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition for weight compression. It contributes three key innovations: (1) a mixed-precision training method that use a square-block MX (SMX) format to eliminate data duplication during backpropagation; (2) a difference-based quantization scheme for the Stein's estimator that mitigates underflow; and (3) a partial-reconstruction scheme (PRS) for TT-Layers that reduces quantization-error accumulation. We further design PINTA, a precision-scalable hardware accelerator, to fully exploit the performance of the framework. Experiments on the 2-D Poisson, 20-D Hamilton-Jacobi-Bellman (HJB), and 100-D Heat equations demonstrate that the proposed framework achieves accuracy comparable to or better than full-precision, uncompressed baselines while delivering 5.5x to 83.5x speedups and 159.6x to 2324.1x energy savings. This work enables real-time PDE solving on edge devices and paves the way for energy-efficient scientific computing at scale.

Body: Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers I Introduction II Background II-A Physics-Informed Neural Networks II-A1 Steinâ€™s Derivative Estimator for PDE Residuals II-B Tensor-Train Decomposition III Method III-A Overview III-B Fully Quantized Training for PINNs III-C Difference-based Quantization for Steinâ€™s Estimator III-D Partial-Reconstruction Computing Scheme for TT Layers IV Hardware Architecture V Experiments V-A Evaluation Setting V-B Accuracy Evaluation V-B1 Ablation Study V-C Hardware Performance VI Conclusion Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers Jinming Lu Jiayi Tian Yequan Zhao Hai Li Zheng Zhang Abstract Physics-Informed Neural Networks (PINNs) have emerged as a promising paradigm for solving partial differential equations (PDEs) by embedding physical laws into neural network training objectives. However, their deployment on resource-constrained platforms is hindered by substantial computational and memory overhead, primarily stemming from higher-order automatic differentiation, intensive tensor operations, and reliance on full-precision arithmetic. To address these challenges, we present a framework that enables scalable and energy-efficient PINN training on edge devices. This framework integrates fully quantized training, Steinâ€™s estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition for weight compression. It contributes three key innovations: (1) a mixed-precision training method that use a square-block MX (SMX) format to eliminate data duplication during backpropagation; (2) a difference-based quantization scheme for the Steinâ€™s estimator that mitigates underflow; and (3) a partial-reconstruction scheme (PRS) for TT-Layers that reduces quantization-error accumulation. We further design PINTA, a precision-scalable hardware accelerator, to fully exploit the performance of the framework. Experiments on the 2-D Poisson, 20-D Hamiltonâ€“Jacobiâ€“Bellman (HJB), and 100-D Heat equations demonstrate that the proposed framework achieves accuracy comparable to or better than full-precision, uncompressed baselines while delivering 5.5 Ã— 5.5\times to 83.5 Ã— 83.5\times speedups and 159.6 Ã— 159.6\times to 2324.1 Ã— 2324.1\times energy savings. This work enables real-time PDE solving on edge devices and paves the way for energy-efficient scientific computing at scale. I Introduction Physics-Informed Neural Networks (PINNs) [ raissi2019physics ] have emerged as a promising approach to solving partial differential equations (PDE) by embedding physical laws directly into the training objective of neural networks. They have been successfully applied across diverse domains, including inverse-scattering problems in nano-optics [ chen2020physics , chen2022physics ] , thermomechanical modeling [ nguyen2022physics ] , and control of dynamical systems in robotics [ antonelo2024physics , velioglu2025physics ] . Unlike traditional PDE solvers, PINNs leverage data-driven learning and automatic differentiation (AD) to enforce PDE constraints, yielding mesh-free solutions that generalize across problem settings. Despite these advantages, training PINNs is computationally and memory intensive. This is primarily due to three factors: (1) reliance on higher-order AD to compute PDE residuals; (2) the large model sizes required to capture complex physical behavior; and (3) pervasive use of high-precision floating-point arithmetic during training because of the higher sensitivity of PINN with respect to quantization errors. For example, second-order PDEs require computing and storing numerous Jacobian and Hessian terms at every collocation point [ he2023learning , hu2024hutchinson ] , often consuming 10 Ã— 10\times to 100 Ã— 100\times more memory than standard neural-network training. These costs are a major barrier to deploying PINNs on resource-contained edge platforms (e.g., autonomous robotics and embedded scientific instrumentation), where memory, latency, and power budgets are tightly limited. To mitigate the computational and memory burdens, prior work has proposed algorithmic and hardware-focused solutions. For instance, methods such as [ liu2022tt , vemuri2025functional ] leverage tensor decomposition to compress network weights and reducing model size. However, these methods remain software-only and still rely on full-precision AD, providing limited end-to-end acceleration. Other efforts, such as [ zhao2025scalable , zhaoreal ] propose a backpropagation-free training framework that employs zeroth-order optimization to eliminate AD and gradient backpropagation. These methods are implemented on photonic hardware accelerators, achieving notable gains in efficiency. Nevertheless, their reliance on specialized photonic ASICs restricts portability and broader adoption on conventional digital platforms. Motivated by these limitations, we propose a holistic framework that enables scalable and energy-efficient PINN training on edge devices. Our key contributions are as follows. â€¢ We propose an efficient on-device PINN training framework by integrating three core techniques: fully quantized training, Steinâ€™s estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition. The framework features a novel mixed-precision strategy that leverages Square-block MX-INT (SMX) formats to avoid redundant data duplication while preserving representational fidelity. â€¢ We introduce two accuracy-preserving techniques: (i) a difference-based quantization method (DiffQuant) that decouples quantization noise from perturbations within Steinâ€™s estimator computation; and (ii) a partial-reconstruction scheme (PRS) for TT-Layers that mitigates quantization error accumulation across tensor contractions. â€¢ We design and evaluate a precision-scalable hardware accelerator optimized for the proposed training pipeline. Implemented on 7-nm technology, the design provides up to 83.5 Ã— 83.5\times speedup and 2324.1 Ã— 2324.1\times energy reduction compared to an AD-based baseline, and 18.3 Ã— 18.3\times speedup and 16.0 Ã— 16.0\times energy reduction compared to an SE-based baseline. This work brings PINNs closer to practical deployment in constrained environments, opening the door to real-time PDE solving on edge devices and energy-efficient scientific modeling at scale. II Background II-A Physics-Informed Neural Networks Figure 1 : Training flow of PINN with (a) Automatic Differentiation and (b) Steinâ€™s Estimator. Physics-Informed Neural Networks (PINNs) [ raissi2019physics ] are a class of deep learning models that integrate physical knowledge into the architecture of neural networks to solve forward and inverse problems of PDEs. Formally, we consider a generic PDE defined over a D D -dimensional domain Î© âŠ‚ â„ D \Omega\subset\mathbb{R}^{D} : â„± â€‹ [ u â€‹ ( ğ± ) ] \displaystyle\mathcal{F}[u(\mathbf{x})] = 0 , ğ± âˆˆ Î© , \displaystyle=0,\quad\mathbf{x}\in\Omega, (1) â„¬ â€‹ [ u â€‹ ( ğ± ) ] \displaystyle\mathcal{B}[u(\mathbf{x})] = 0 , ğ± âˆˆ âˆ‚ Î© , \displaystyle=0,\quad\mathbf{x}\in\partial\Omega, (2) where â„± \mathcal{F} denotes a differential operator and â„¬ \mathcal{B} enforces boundary or initial conditions. A neural network u Î¸ â€‹ ( ğ± ) u_{\mathbf{\theta}}(\mathbf{x}) , parameterized by weights Î¸ \mathbf{\theta} , is trained to approximate the solution u â€‹ ( ğ± ) u(\mathbf{x}) . The training loss typically consists of physics-driven and optionally data-driven components: â„’ â€‹ ( Î¸ ) = \displaystyle\mathcal{L}(\theta)= w c N c â€‹ âˆ‘ i = 1 N c â€– â„± â€‹ [ u Î¸ â€‹ ( ğ± c i ) ] â€– 2 + w b N b â€‹ âˆ‘ i = 1 N b â€– â„¬ â€‹ [ u Î¸ â€‹ ( ğ± b i ) ] â€– 2 \displaystyle\,\frac{w_{c}}{N_{c}}\sum_{i=1}^{N_{c}}\left\|\mathcal{F}[u_{\theta}(\mathbf{x}_{c}^{i})]\right\|^{2}+\frac{w_{b}}{N_{b}}\sum_{i=1}^{N_{b}}\left\|\mathcal{B}[u_{\theta}(\mathbf{x}_{b}^{i})]\right\|^{2} (3) + w d N d â€‹ âˆ‘ i = 1 N d â€– u Î¸ â€‹ ( ğ± d i ) âˆ’ u â€‹ ( ğ± d i ) â€– 2 , \displaystyle+\frac{w_{d}}{N_{d}}\sum_{i=1}^{N_{d}}\left\|u_{\theta}(\mathbf{x}_{d}^{i})-u(\mathbf{x}_{d}^{i})\right\|^{2}, where w c , w b , w d w_{c},w_{b},w_{d} are loss weights, and N c , N b , N d N_{c},N_{b},N_{d} denote the number of data points respectively. The first two terms correspond to PDE residuals loss and boundary/initial condition loss, while the third is the regular data loss to fit the dataset. PINNs typically employ automatic differentiation (AD) to compute derivatives required for enforcing PDE constraints. However, as shown in Fig. 1 (a), computing high-order derivatives via AD entails repeated backpropagation, making the process both memory- and compute-intensive, especially for high-dimensional or higher-order PDEs. II-A1 Steinâ€™s Derivative Estimator for PDE Residuals To alleviate the computational burden of AD, He et al. [ he2023learning ] introduced Steinâ€™s derivative estimator (SE), a sampling-based forward-mode technique for derivative approximation. As shown in Fig. 1 (b), it enables derivative computation without explicit backpropagation. For a differentiable function u : â„ d â†’ â„ u:\mathbb{R}^{d}\rightarrow\mathbb{R} , the first-order derivative can be estimated via: âˆ‡ ğ± u â€‹ ( ğ± ) \displaystyle\nabla_{\mathbf{x}}u(\mathbf{x}) = ğ”¼ ğœ¹ â€‹ [ ğœ¹ 2 â€‹ Ïƒ 2 â€‹ ( u â€‹ ( ğ± + ğœ¹ ) âˆ’ u â€‹ ( ğ± âˆ’ ğœ¹ ) ) ] \displaystyle=\mathbb{E}_{\bm{\delta}}\left[\frac{\bm{\delta}}{2\sigma^{2}}\left(u(\mathbf{x}+\bm{\delta})-u(\mathbf{x}-\bm{\delta})\right)\right] (4) â‰ˆ 1 K â€‹ âˆ‘ i = 1 K ğœ¹ i 2 â€‹ Ïƒ 2 â€‹ ( u â€‹ ( ğ± + ğœ¹ i ) âˆ’ u â€‹ ( ğ± âˆ’ ğœ¹ i ) ) , \displaystyle\approx\frac{1}{K}\sum_{i=1}^{K}\frac{\bm{\delta}_{i}}{2\sigma^{2}}\left(u(\mathbf{x}+\bm{\delta}_{i})-u(\mathbf{x}-\bm{\delta}_{i})\right), where ğœ¹ \bm{\delta} is a random perturbation sampled from ğ’© â€‹ ( 0 , Ïƒ 2 â€‹ ğˆ ) \mathcal{N}(0,\sigma^{2}\mathbf{I}) . Higher-order derivatives such as the Laplacian can also be approximated through Eq. ( 5 ). Î” â€‹ u â€‹ ( ğ± ) = ğ”¼ ğœ¹ â€‹ [ â€– ğœ¹ â€– 2 âˆ’ Ïƒ 2 â€‹ D 2 â€‹ Ïƒ 4 â€‹ ( u â€‹ ( ğ± + ğœ¹ ) + u â€‹ ( ğ± âˆ’ ğœ¹ ) âˆ’ 2 â€‹ u â€‹ ( ğ± ) ) ] \displaystyle\Delta u(\mathbf{x})=\mathbb{E}_{\bm{\delta}}\left[\frac{\|\bm{\delta}\|^{2}-\sigma^{2}D}{2\sigma^{4}}\left(u\left(\mathbf{x}+\bm{\delta}\right)+u\left(\mathbf{x}-\bm{\delta}\right)-2u(\mathbf{x})\right)\right] (5) â‰ˆ 1 K â€‹ âˆ‘ i = 1 K â€– ğœ¹ i â€– 2 âˆ’ Ïƒ 2 â€‹ D 2 â€‹ Ïƒ 4 â‹… ( u â€‹ ( ğ± + ğœ¹ i ) + u â€‹ ( ğ± âˆ’ ğœ¹ i ) âˆ’ 2 â€‹ u â€‹ ( ğ± ) ) . \displaystyle\approx\frac{1}{K}\sum_{i=1}^{K}\frac{\|\bm{\delta}_{i}\|^{2}-\sigma^{2}D}{2\sigma^{4}}\cdot\left(u\left(\mathbf{x}+\bm{\delta}_{i}\right)+u\left(\mathbf{x}-\bm{\delta}_{i}\right)-2u(\mathbf{x})\right). Figure 2 : Illustration of Tensor-Train Decomposition. Figure 3 : Overview of the proposed efficient on-device PINN training framework. II-B Tensor-Train Decomposition Tensor-Train (TT) decomposition [ oseledets2011tensor , tian2025ultra ] is a low-rank tensor factorization technique that efficiently represents high-dimensional tensors using a sequence of lower-dimensional core tensors. Originally introduced to address the curse of dimensionality in numerical computations, TT decomposition has found wide applications in machine learning for model compression and efficient inference. In the context of neural networks, TT decomposition is typically applied to fully connected layers by reshaping weight matrices into high-order tensors, followed by a low-rank decomposition in a chain structure. This approach drastically reduces the number of parameters and enables memory- and compute-efficient implementation on hardware accelerators. Formally, consider a weight matrix ğ– âˆˆ â„ M Ã— N \mathbf{W}\in\mathbb{R}^{M\times N} , which is first reshaped into a 2 â€‹ d 2d -dimensional tensor ğ“¦ âˆˆ â„› â‡• âˆ â£ Ã— â‹¯ â£ Ã— â£ â‡• âŒˆ â£ Ã— â£ \ âˆ â£ Ã— â‹¯ â£ Ã— \ âŒˆ \mathbfcal{W}\in\mathbb{R}^{m_{1}\times\cdots\times m_{d}\times n_{1}\times\cdots\times n_{d}} , where N = âˆ i = 1 d n i N=\prod_{i=1}^{d}n_{i} and M = âˆ i = 1 d m i M=\prod_{i=1}^{d}m_{i} . As shown in Fig. 2 , TT factorizes ğ“¦ \mathbfcal{W} into the product of 2 â€‹ d 2d third-order core tensors { ğ“– ( âˆ¥ ) } âˆ¥ = âˆ âˆˆ âŒˆ \{\mathbfcal{G}^{(k)}\}_{k=1}^{2d} such that: ğ“¦ [ âŸ© âˆ , â€¦ âŸ© âŒˆ , | âˆ , â€¦ | âŒˆ ] = \displaystyle\mathbfcal{W}_{[i_{1},...i_{d},j_{1},...j_{d}]}= (6) âˆ‘ r 1 â€‹ â€¦ â€‹ r d ğ“– [ âˆ‡ â€² , âŸ© âˆ , âˆ‡ âˆ ] ( âˆ ) â€‹ â€¦ â€‹ ğ“– [ âˆ‡ âŒˆ âˆ’ âˆ , âŸ© âŒˆ , âˆ‡ âŒˆ ] ( âŒˆ ) â€‹ ğ“– [ âˆ‡ âŒˆ , | âˆ , âˆ‡ âŒˆ + âˆ ] ( âŒˆ + âˆ ) â€‹ â€¦ â€‹ ğ“– [ âˆ‡ âˆˆ âŒˆ âˆ’ âˆ , | âŒˆ , âˆ‡ âˆˆ âŒˆ ] ( âˆˆ âŒˆ ) . \displaystyle\sum_{r_{1}...r_{d}}\mathbfcal{G}^{(1)}_{[r_{0},i_{1},r_{1}]}.\mathbfcal{G}^{(d)}_{[r_{d-1},i_{d},r_{d}]}\mathbfcal{G}^{(d+1)}_{[r_{d},j_{1},r_{d+1}]}.\mathbfcal{G}^{(2d)}_{[r_{2d-1},j_{d},r_{2d}]}. where ğ“– ( âˆ¥ ) âˆˆ â„› âˆ‡ âˆ¥ âˆ’ âˆ â£ Ã— â£ â‡• âˆ¥ â£ Ã— âˆ‡ âˆ¥ \mathbfcal{G}^{(k)}\in\mathbb{R}^{r_{k-1}\times m_{k}\times r_{k}} for 1 â‰¤ k â‰¤ d 1\leq k\leq d , and ğ“– ( âˆ¥ ) âˆˆ â„› âˆ‡ âˆ¥ âˆ’ âˆ â£ Ã— â£ \ âˆ¥ âˆ’ âŒˆ â£ Ã— âˆ‡ âˆ¥ \mathbfcal{G}^{(k)}\in\mathbb{R}^{r_{k-1}\times n_{k-d}\times r_{k}} for d k â‰¤ 2 â€‹ d d k\leq 2d . { r k } k = 1 2 â€‹ d \{r_{k}\}_{k=1}^{2d} are known as TT-ranks, which determine the compression rate and expressiveness of the decomposition. By applying the TT decomposition, the standard linear layer ğ˜ = ğ—ğ– T \mathbf{Y}=\mathbf{X}\mathbf{W}^{T} is replaced with a TT-Layer. The TT-Layer can be expressed as: ğ“¨ [ âŒŠ , âŸ© âˆ â‹¯ âŸ© âŒˆ ] = âˆ‘ | âˆ â€¦ | âŒˆ ğ“– ( âˆ ) [ âŸ© âˆ ] ğ“– ( âˆˆ ) [ âŸ© âˆˆ ] â€¦ ğ“– ( âˆˆ âŒˆ ) [ | âŒˆ ] ğ“§ [ âŒŠ , | âˆ â€¦ | âŒˆ ] , \mathbfcal{Y}_{[b,i_{1}\cdots i_{d}]}=\sum_{j_{1}...j_{d}}\mathbfcal{G}^{(1)}{[i_{1}]}\mathbfcal{G}^{(2)}{[i_{2}]}...\mathbfcal{G}^{(2d)}{[j_{d}]}\mathbfcal{X}_{[b,j_{1}...j_{d}]}, (7) where b b is the batch dimension, ğ“– ( âŸ© ) [ âŸ© âˆ¥ ] âˆˆ â„› âˆ‡ âŸ© âˆ’ âˆ Ã— âˆ‡ âŸ© \mathbfcal{G}^{(i)}[i_{k}]\in\mathbb{R}^{r_{i-1}\times r_{i}} is the i k i_{k} -th slice of the TT-core ğ“– ( âŸ© ) \mathbfcal{G}^{(i)} by fixing its second index as i k i_{k} . III Method III-A Overview In this work, we develop an efficient framework for training PINNs on resource-constrained devices. To reduce the computational and memory overhead of PINN training, as illustrated in Fig. 3 , the framework integrates three key components: fully quantized training to reduce memory footprint and arithmetic cost, Steinâ€™s estimator for low-cost derivative computation without backpropagation, and TT decomposition for compact weight representation. However, naively combining these techniques into a cohesive, high-performance pipeline introduces several challenges. âŠ Compared with models in vision and natural language processing, PINNs are particularly sensitive to quantization-induced errors, especially due to their use in scientific domains [ tu2023guaranteed , Dool2023EfficientNP , hayford2024speeding ] . â‹ The small perturbations used by SE-based training can be obscured by quantization noise, leading to inaccurate gradient estimates âŒ Tensorization reduces parameter count but increases the number of tensor contractions, which can exacerbate the accumulation of quantization errors. We address these issues with three key innovations : (1) a mixed-precision training strategy that uses Square-block MX-INT (SMX) format to balance accuracy and efficiency; (2) a difference-based quantization scheme (DiffQuant) to preserve the sensitivity of Steinâ€™s estimator under low-bit arithmetic; and (3) a partia