Title: Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach

Abstract: Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.

Body: Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach. I Introduction II Related Work II-A Positioning vs. State-of-the-Art III Dataset and Preprocessing III-A FER-2013 Overview III-B Train/Validation/Test Split III-C Preprocessing and Augmentation IV Methodology IV-A Model Architecture IV-B Training Regime IV-C Regularization and Callbacks V Experimental Setup VI Results VI-A Learning Dynamics VI-B Test Performance VI-C Per-class Metrics VI-D Confusion Matrix Analysis VI-E Comparison with Prior Work VII Analysis and Discussion VII-A Why the recipe works VII-B Error analysis and qualitative failure modes VII-C Ablation (recommended) VIII Ethics and Limitations IX Reproducibility X Conclusion Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach. Sahil Naik (Roll No. 22101B0071) Department of Information Technology VIT, Mumbai sahil.naik@vit.edu.in Soham Bagayatkar (Roll No. 22101B0054) Department of Information Technology VIT, Mumbai soham.bagayatkar@vit.edu.in Pavankumar Singh (Roll No. 22101B0050) Department of Information Technology VIT, Mumbai pavankumar.singh@vit.edu.in Abstract Detection of human emotions based on facial images in real-world scenarios is a difficult task because of the low quality of images, variations in lighting, change in pose, background distractions, small inter-class variations, poor crowd-sourced labels, and a severe imbalance in classes, as the FER-2013 dataset of 48 × 48 48\times 48 grayscale images. Although recent methods using large CNNs like VGG and ResNet are scaled to reasonable accuracy, they are costly in terms of computational and memory and apply only to practice in the future. We suggest addressing the outlined issues with the help of a light and efficient facial emotion recognition pipeline that is based on EfficientNetB2 and is trained through the two stage warm-up and fine-tuning approach. It is made robust with AdamW optimization and decoupling weight decay, label smoothing ( ϵ = 0.06 \epsilon=0.06 ) to reduce annotation noise, and differently decoupled class weights to address imbalance together with extra deterrents such as dropout, mixed-precision training, and comprehensive real time data augmentation. The model is trained with stratified 87.5% / 12.5% train-validation divided original data and keeps the official test set intact, achieving a test accuracy of 68.78% while using only a moderate number of parameters, almost ten times fewer than the VGG16-based baselines. The experimental findings, such as per-class measurements and the learning dynamics, prove the stable training and high generalization, which is why the suggested approach can be used in the real-time and edge-based applications. I Introduction Facial expressions are very important in human communication as they present emotions, intentions as well as other hidden social cues. Automatic Facial Emotion Recognition (FER) technologies grow to extract these emotional states of images or video footage and find applications in intelligent tutoring systems, emotion-sensitive human computer interface, mental health, social robotics, surveillance and customer behavior analysis. A trustworthy FER system is able to enhance the quality of interaction by making technology more sensitive to human emotions. Even with the substantial progress in deep learning, it is difficult to create FER systems that can work in uncontrolled real-life environments. Captured and photographed images usually lack high-resolution, distorted lighting, occlusions, differences in the position of the head and extremely subtle changes in facial expression. Along with it, emotion labels are subjective by nature, and crowdsourcing-generated datasets usually have sporadic and noisy annotations. This is also worsened by gross disproportion in the classes where some emotions like disgust or fear are underrepresented than the neutral ones or happy face. Collectively, these variables have adverse effects on generalization and could be used to provide biased predictions. FER-2013 data set, made available as a part of ICML 2013 challenge, has been used as one of the most widely referred benchmarks to perform facial emotion recognition. It is made up of about 35,000 grayscale harmonic facial images with dimensions 48 × 48 48\times 48 and grouped into seven categories of emotions. Although rather popular, the dataset is famously challenging since its quality of images and labels is low, and there is a definite lack of a balance between the classes. Much of the previous research on FER-2013 uses heavyweight models (VGG16 or the different variants of ResNet). Despite the competitive accuracy of these models, they consume only tens or sometimes hundred and millions of parameters, which is why they consume a lot of memory and cost to compute. Consequently, they cannot be applied practically in real-time or edge deployment. As a case in point, Jahangir et al. indicated that a VGG16 model that was trained from scratch achieved a test accuracy of 67.23%. Although the performance that was achieved is impressive, the style mainly relies on the huge model capacity and does not directly mention the efficiency or reproducibility. This is where a key gap in the literature lies: there is the necessity of techniques that could also reach that level of accuracy but consume significantly less memory and less effort to run them. Inspired by this fact, we pay attention to efficiency-oriented model design. We use the EfficientNetB2 that follows a powerful compromise between the accuracy and parameter efficiency, utilizing the compound scaling strategy. We integrate this architecture with well-informed optimization and regularization strategies depending on the peculiarities of the FER-2013 set of data. The resultant system not only offers enhanced recognition, but is also compact, stable and reproducible in whole and therefore makes it suitable to be used in practice. Our principal contributions are listed as following: • A compact EfficientNetB2-based facial expression emotion detector with a test accuracy of 68.78 per cent on the FER-2013 dataset and only a tenth of the parameters of the standard VGG-based models. • Intensive two-stage training process with AdamW loss and label smoothing, clipped class weight averaging, and large scale data augmentation to be able to deal with label noise and label imbalance. • The entire and reproducible experimental framework, such as distinctly assigned information divisions, hyper-parameters, and a comprehensive examination based on per-class statistics as well as confusion tables. II Related Work Olden time methods of facial emotion recognition were based on handcrafted features, i.e., Local Binary Patterns (LBP), Histograms of Oriented Gradients (HOG), and Gabor filters with traditional classifier cum mode. This paradigm was changed with the introduction of deep learning where convolutional neural networks are able to learn discriminative features directly on the data and this marked a significant increase in performance. The VGG type of architecture gained popularity since it was relatively simple and had good performance, as well as because of its residual connections that enabled deeper networks as introduced by the ResNet models [ 19 , 2 ] . But these architectures tend to be significant since a dataset of this size of FER-2013 can be easily overfitted. EfficientNet proposed the compound scaling method which refers to the joint scaling of network depth, width and resolution to enable the attainment of improved accuracy-efficiency trade-offs [ 5 ] . At the same time, a variety of enhancements to optimization algorithms, such as AdamW with adaptive learning resources, such as label smoothing, are proved to be able to lead to generalization improvement in computer vision tasks. These developments are incorporated into one, united, and reproducible framework in our work that is specifically oriented to the issues addressed by the FER-2013 dataset. II-A Positioning vs. State-of-the-Art Most recent state-of-the-art FER methods use ensembles or attention mechanisms or transformer-based architectures to enhance the maximization of accuracy. These procedures may provide a powerful performance, but they are generally expensive in terms of both computational and energy expenses. Conversely, we have a work ethic aimed at efficiency, reproducibility, and ease of deployment and a competitive performance on a limited parameter budget. This is consistent with the current focus on Green AI and sustainable model design, and therefore the offered approach seems to be appropriate to the real-world and edge-based uses of facial emotion recognition applications. III Dataset and Preprocessing III-A FER-2013 Overview FER-2013 (Kaggle / ICML challenge) contains 35,887 images labeled across seven categories: angry , disgust , fear , happy , neutral , sad , surprise . III-B Train/Validation/Test Split We keep the official test partition intact and split the original training set into 87.5% training and 12.5% validation using a fixed random seed for reproducibility. III-C Preprocessing and Augmentation Images were converted to RGB and resized to 260 × 260 260\times 260 (EfficientNetB2 input). Real-time augmentation (rotation ± \pm 25 ∘ , width/height shift 15%, zoom 25%, shear 0.1, horizontal flip) was applied during training via Keras ImageDataGenerator . No additional denoising or face alignment was applied to keep the pipeline simple and reproducible. IV Methodology IV-A Model Architecture We use EfficientNetB2 (ImageNet pretrained) as feature extractor, followed by: • GlobalAveragePooling2D • Dropout(0.5) • Dense(7, softmax) Total trainable parameters ≈ 9.2 \approx 9.2 M. This is an order-of-magnitude smaller than VGG16 (138M). IV-B Training Regime We adopt a two-phase schedule: 1. Warmup (3 epochs): EfficientNet backbone frozen. Optimizer: Adam, lr=1e-3. 2. Fine-tuning (7 epochs): Unfreeze all layers except BatchNorm. Optimizer: AdamW, lr=3e-5, weight_decay=1e-4. Loss: Categorical Crossentropy with label smoothing = 0.06. Class weights computed from training frequencies and clipped to a maximum of 4.0 to avoid extremely large weights for minority classes. IV-C Regularization and Callbacks We use: • EarlyStopping (patience=3, restore_best_weights) • ReduceLROnPlateau (factor=0.3, patience=2) • ModelCheckpoint saving the best validation accuracy • CSVLogger for training logs • Mixed precision when available to speed up training V Experimental Setup Training was executed on an NVIDIA GPU (RTX 30-series or similar). Batch size = 32; total epochs = 10 (3 + 7). All experiments use seed=42 where applicable. The full training script is provided in the appendix. VI Results VI-A Learning Dynamics Figures 1 and 2 illustrate the training dynamics of the proposed EfficientNetB2-based FER model over 10 epochs, consisting of a 3-epoch warm-up phase followed by a 7-epoch fine-tuning phase. Figure 1 shows the evolution of training and validation accuracy. During the initial epochs, training accuracy increases rapidly as the classification head adapts to the FER-2013 emotion distribution while the backbone remains frozen. Validation accuracy shows a similar pattern, which suggests that the features transferred from the ImageNet-pretrained weights are working well. Once the fine-tuning phase starts, both training and validation accuracy continue to rise steadily and stay very close to each other. This strong correspondence shows that there is no great variation in convergence and minimal evidence of overfitting. The near coincidence of the training and validation curves indicate that the regularization strategies used, namely, label smoothing, dropout, and widespread data augmentation, are effective in the prevention of overfitting and provide stable and robust learning. Figure 1 : It is a depiction of Training vs. validation accuracy compared with an epoch. The close alignment of curves indicates stable learning and good generalization. Figure 2 presents the corresponding training and validation loss curves. Training loss decreases consistently throughout training, while validation loss shows a smooth downward trend without sudden spikes. This behavior indicates that the model avoids overfitting despite fine-tuning all layers and confirms that the learning rate schedule, AdamW optimizer, and label smoothing collectively stabilize optimization. Figure 2 : Training vs. validation loss across epochs, showing smooth convergence and stable optimization. VI-B Test Performance The best-performing model checkpoint, selected based on highest validation accuracy, was evaluated on the official FER-2013 test set. The quantitative results are summarized in Table I . TABLE I : Primary Test Performance Metric Value Test accuracy 68.78% Test loss (best checkpoint) see logs Number of parameters ∼ \sim 9.2M The achieved test accuracy demonstrates that the proposed compact model is competitive with significantly larger architectures while maintaining a much smaller parameter footprint. This confirms the suitability of the approach for efficiency-critical deployments. VI-C Per-class Metrics Table II reports precision, recall, and F1-score for each emotion class on the test set. High F1-scores for happy and surprise indicate strong recognition of visually distinctive expressions. Lower performance for fear and sad reflects the inherent ambiguity and visual similarity between these emotions, which is a known limitation of FER-2013. TABLE II : Per-class Precision, Recall, and F1-score on Test Set Class Precision Recall F1 Angry 0.59 0.65 0.62 Disgust 0.55 0.70 0.62 Fear 0.57 0.42 0.48 Happy 0.89 0.88 0.88 Neutral 0.63 0.70 0.66 Sad 0.58 0.56 0.57 Surprise 0.77 0.82 0.79 Overall 0.69 0.69 0.68 VI-D Confusion Matrix Analysis Figure 3 presents the confusion matrix in absolute counts, revealing common misclassification patterns. The model correctly classifies a large proportion of happy , neutral , and surprise samples. However, confusions are frequent between fear and sad , as well as between sad and neutral , due to overlapping facial cues in low-resolution images. Figure 3 : Confusion matrix (absolute counts) on the FER-2013 test set. Figure 4 shows the row-normalized confusion matrix, which highlights per-class recall. The normalized view confirms high recall for happy (88.1%) and surprise (82.3%), while fear exhibits the lowest recall (41.7%), indicating the need for additional discriminative cues such as facial landmarks or attention mechanisms in future work. Figure 4 : Row-normalized confusion matrix showing per-class recall. VI-E Comparison with Prior Work Table III presents a quantitative comparison between the proposed EfficientNetB2-based approach and representative methods reported on the FER-2013 dataset. Prior works predominantly rely on large convolutional neural networks such as VGG16 trained from scratch, which achieve comp