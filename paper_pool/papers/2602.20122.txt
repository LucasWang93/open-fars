Title: NanoKnow: How to Know What Your Language Model Knows

Abstract: How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a "black box" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.

Body: NanoKnow: How to Know What Your Language Model Knows 1 Introduction 2 NanoKnow 2.1 Projection Results 2.1.1 Relevance Judgments 2.1.2 Relevance Judgments Format and Corpus Access Validation. Released Artifacts. 2.2 Pipeline Step 1: BM25 Retrieval Step 2: Answer String Matching Step 3: LLM-Based Verification 3 Experimental Setup 4 Results 4.1 Impact of Answer Frequency in Pre-training 4.2 Closed-Book QA vs. Open-Book QA 4.3 Open-Book QA: Supported vs. Unsupported 4.4 Influence of Distractors 5 Related Work Tracing an LLM’s capabilities to its pre-training data Interplay of parametric versus external knowledge 6 Conclusion NanoKnow: How to Know What Your Language Model Knows Lingwei Gu University of Waterloo Waterloo ON Canada lingwei.gu@uwaterloo.ca , Nour Jedidi University of Waterloo Waterloo ON Canada njedidi@uwaterloo.ca and Jimmy Lin University of Waterloo Waterloo ON Canada jimmylin@uwaterloo.ca Abstract. How do large language models (LLMs) know what they know ? Answering this question has been difficult because pre-training data is often a “black box” – unknown or inaccessible. The recent release of nanochat – a family of small LLMs with fully open pre-training data – addresses this as it provides a transparent view into where a model’s parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow , a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat’s pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow’s utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow . Figure 1. An overview of how NanoKnow was built. If any of the retrieved passages from the FineWeb-Edu corpus are deemed to properly answer the question, we label the question as “supported”. Otherwise, the question is considered “unsupported”. 1. Introduction Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet it is unclear how they know what they know . While LLMs ultimately express their knowledge through their outputs at inference time, how and where this knowledge is acquired remains an open question. Knowledge expressed by LLMs can originate from various, potentially entangled, sources. There exists knowledge stored within their parameters (Roberts et al. , 2020 ) , which can be probed via closed-book question answering (Jiang et al. , 2020 ) , but this only tells us what LLMs know, not necessarily how that knowledge was acquired. For example, did this knowledge come from memorization of its pre-training data (Carlini et al. , 2022 ) or is the model performing a sort of multi-hop reasoning over facts encoded within its parameters (Yang et al. , 2024 ) ? Alternatively, external knowledge can be injected into the LLM using retrieval-augmented generation (RAG), but in this case does the model’s output solely represent facts present in the external context or does the output represent a latent interaction between the external context and the LLM’s parametric knowledge (Zhao et al. , 2025 ) ? Ultimately, answering these questions requires understanding the model’s pre-training data, but this has been difficult as such data is often unknown or inaccessible (Liu et al. , 2025 ) . Recently, this changed with developments in fully open LLMs, making the understanding of the pre-training data now possible. A notable example is the release of nanochat (Karpathy, 2025 ) , which, by being pre-trained on the open FineWeb-Edu corpus – a 100-billion-token collection of educational web content (Penedo et al. , 2024 ) – provides a completely transparent and self-contained environment for tracing the information an LLM has seen . Such transparency allows us to answer questions like: does seeing facts more often make it easier to recall? When does RAG actually make a difference? However, transparency of data is only the first step. Before we can answer these questions systematically, we require a resource which can not only identify questions an LLM has seen the answer to during pre-training but also questions beyond its knowledge. Such a resource is a necessary step toward properly disentangling and understanding the various sources of knowledge LLMs rely on when producing their outputs. To address this, we release NanoKnow, a benchmark dataset of questions from Natural Questions (NQ) (Kwiatkowski et al. , 2019 ) and SQuAD (Rajpurkar et al. , 2016 ) projected onto the FineWeb-Edu corpus. NanoKnow partitions each dataset into two splits – “supported” (questions for which the answer exists in the pre-training data) and “unsupported” (questions for which the answer does not exist in the pre-training data) – enabling a controlled evaluation of knowledge in LLMs, like nanochat, which were pre-trained entirely on FineWeb-Edu. To generate these relevance judgments, NanoKnow was built in three stages. In the first stage, we build a searchable BM25 index over the corpus using Anserini (Yang et al. , 2017 ) and retrieve candidate documents for each question. Next, we check for exact match answer strings across the retrieved documents. In the last stage, we use LLM-based verification to filter out coincidental matches, keeping only documents that genuinely answer the questions. With NanoKnow in hand, we run comprehensive experiments using eight nanochat checkpoints across three different model scales. Our various experiments demonstrate the value of NanoKnow as a tool for confidently disentangling and evaluating the contributions of different knowledge sources underlying an LLM’s outputs. Using NanoKnow, we were able to confirm and replicate a range of results across the literature (Carlini et al. , 2022 ; Biderman et al. , 2023 ; Cuconasu et al. , 2024 ; Liu et al. , 2024 ; Wang et al. , 2025 ; Kandpal et al. , 2023 ) , highlighting its reliability: (1) Closed-book question answering effectiveness is highly related to answer frequency in the pre-training corpus. We found a clear increase in nanochat’s accuracy when it has “seen” the answer more often. (2) Integrating external evidence mitigates this dependence on “memorization”, but even with external evidence, nanochat is more effective on questions with a higher answer frequency in the pre-training corpus. (3) Even when provided the oracle answer document, nanochat was more accurate on “supported” versus “unsupported” questions, demonstrating that parametric knowledge can complement external knowledge. (4) Despite nanochat having “seen” the answer to a question, it is negatively impacted by distractor documents (i.e., non-relevant documents). We found a clear decline in accuracy based on where it is positioned with respect to distractors as well as how many distractors are present. We hope NanoKnow provides a foundation for future explorations in understanding how LLMs know what they know . 2. NanoKnow At a high-level, NanoKnow is a benchmark dataset that partitions NQ and SQuAD questions into supported and unsupported splits based on the presence of their answers within the FineWeb-Edu corpus. In this section, we present NanoKnow and describe our process for producing it. 2.1. Projection Results We apply NanoKnow to the shuffled version of the FineWeb-Edu corpus released by Karpathy ( 2025 ) . 1 1 1 https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle FineWeb-Edu comes as 1,823 parquet shards, each containing thousands of web documents. The total corpus size is about 171GB and contains 97,230,848 documents. After indexing with Anserini (Yang et al. , 2017 ) , the index size is about 326GB. The FineWeb-Edu corpus was chosen in our experiments primarily due to the recent release of nanochat (Karpathy, 2025 ) , a family of small LLMs that utilized it for pre-training. We project the following QA benchmarks onto FineWeb-Edu: • Natural Questions (NQ) (Kwiatkowski et al. , 2019 ) : Open-domain questions from Google search queries. We use the validation set (3,610 questions). Each question has one or more short answers. • SQuAD (Rajpurkar et al. , 2016 ) : Reading comprehension questions where answers are spans from Wikipedia passages. We use the validation set (10,570 questions). 2.1.1. Relevance Judgments From these QA benchmarks, we build relevance judgments that split each dataset in two: • Supported : Questions where the answer appears in FineWeb-Edu in a relevant context. • Unsupported : Questions where the answer does not appear in any retrieved document or only shows up in unrelated contexts. Table 1 shows examples of each from the NQ dataset. For each supported question, the relevance judgments record which documents contain the answer. These two splits allow us to compare model effectiveness on questions it saw during pre-training versus questions which are outside the model’s knowledge. Table 2 shows the projection results. The “String Match” column shows the percentage of questions which have an answer in at least one retrieved document. “LLM Verified” is how many survive after filtering out coincidental matches. The overlap is high for both benchmarks. For NQ, 73.9% of questions have the answer string in a retrieved document; after LLM verification, 66.2% are confirmed to be supported. SQuAD is even higher at 70.9% verified. This is not surprising: SQuAD answers come from Wikipedia, and FineWeb-Edu has a lot of Wikipedia-derived content. About 11% of string matches turn out to be coincidental. The LLM verification process catches most of these. 2.1.2. Relevance Judgments Format and Corpus Access For each question with a match in the corpus, we store relevance judgments linking the question to its answer locations in FineWeb-Edu. Each entry contains the question ID, question and answer text, document ID (shard and row offset, e.g., shard_00151_20323 ), and the character offset where the answer appears. Each document gets a unique ID that encodes its location in the corpus. The format is shard_XXXXX_YYYYY , where XXXXX is the zero-padded shard number and YYYYY is the row offset within that shard. For example, shard_00151_20323 refers to row 20,323 in shard 151. This encoding lets us trace any retrieved document back to its exact location in the original parquet files. The document ID encoding enables efficient corpus access. Since each ID contains the shard number and row offset, we can fetch the original document directly from the parquet files without scanning. For example, using DuckDB, given shard_00151_20323 , we parse the shard number (151) and row offset (20,323), then run a single-row query on the corresponding parquet file. This gives sub-millisecond latency for document retrieval, which matters when running thousands of RAG experiments. The character offsets let us extract answer passages of any length. For our experiments, we pull 256 words before and after the answer match, but researchers can adjust this window as needed. Since the relevance judgments store both the document ID and the character offset, the context can be efficiently retrieved by DuckDB without scanning the full corpus. Validation. To check that our unsupported labels are accurate, we prompt the official d32 nanochat checkpoints with unsupported questions in a closed-book setting. 2 2 2 karpathy/nanochat-d32 We found an accuracy of 1.5% and 0.8% for SQuAD and NQ when evaluating using the answer string matching approach we discuss in Section 2.2 . Released Artifacts. We release the following to support reproducibility and future research: • Qrels : Relevance judgments mapping questions in NQ and SQuAD to FineWeb-Edu documents. For each data split (“supported” or “unsupported”), we provide a file with corresponding question IDs. • Lucene Index : Pre-built index over FineWeb-Edu (326GB). 3 3 3 https://huggingface.co/datasets/LingweiGu/NanoKnow-Fineweb-Edu-Index • Evaluation Code : Scripts to reproduce all experiments, including LLM-Judge prompts and evaluation metrics. All artifacts are available at https://github.com/castorini/NanoKnow . Table 1. Examples of supported and unsupported NQ questions. Supported example includes the matching passage from FineWeb-Edu. Question Answer Evidence Supported When was the last time anyone was on the moon? December 1972 “No one has walked on the Moon since December 1972.” What is the main artery that takes blood from the heart to the body? The aorta “Arteries begin with the aorta, the large artery leaving the heart.” Why does kerosene rise up in the wick of a lantern? Capillary action “…the kerosene burns, capillary action in the wick draws more kerosene up from the fuel tank.” Unsupported Who sang I ran all the way home? The Impalas – Who plays Gram on The Young and the Restless? Max Shippee – Love Yourself by Justin Bieber is about who? Rihanna – Table 2. Projection rates for NQ and SQuAD on FineWeb-Edu. The reported percentage represents how many questions are supported after the string matching (String Match) and subsequent LLM-based verification (LLM Verified) steps. Dataset Samples String Match LLM Verified NQ 3,610 73.9% 66.2% SQuAD 10,570 78.9% 70.9% 2.2. Pipeline We now discuss how we projected NQ and SQuAD onto FineWeb-Edu. A high-level overview of the pipeline is shown in Figure 1 . Given a question-answer pair from NQ or SQuAD, we project it using the following three steps: Step 1: BM25 Retrieval Using BM25, we first search the index to retrieve documents that may contain the answer. We retrieve the top 100 candidate documents, leveraging Pyserini (Lin et al. , 2021 ) for retrieval. Step 2: Answer String Matching Next, we check if any retrieved document contains the answer. We lowercase everything and strip extra whitespace, then look for the answer as a substring. If it shows up, we flag the question as a candidate match . This is fast, but returns many false positives. For example, for the question “What is the best bakery in Paris?” the word “Paris” might appear in a document about the song Paris and not Paris, France. Another step is needed to filter these out. System: You verify whether answer knowledge exists in text. Check if test Q A pairs appear in the pre-training corpus. Be STRICT. Only mark TRUE if the context DIRECTLY answers the question. User: QUESTION: {question} ANSWER FOUND