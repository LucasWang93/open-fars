Title: Uniform Spectral Growth and Convergence of Muon in LoRA-Style Matrix Factorization

Abstract: Spectral gradient descent (SpecGD) orthogonalizes the matrix parameter updates and has inspired practical optimizers such as Muon. They often perform well in large language model (LLM) training, but their dynamics remain poorly understood. In the low-rank adaptation (LoRA) setting, where weight updates are parameterized as a product of two low-rank factors, we find a distinctive spectral phenomenon under Muon in LoRA fine-tuning of LLMs: singular values of the LoRA product show near-uniform growth across the spectrum, despite orthogonalization being performed on the two factors separately. Motivated by this observation, we analyze spectral gradient flow (SpecGF)-a continuous-time analogue of SpecGD-in a simplified LoRA-style matrix factorization setting and prove "equal-rate" dynamics: all singular values grow at equal rates up to small deviations. Consequently, smaller singular values attain their target values earlier than larger ones, sharply contrasting with the largest-first stepwise learning observed in standard gradient flow. Moreover, we prove that SpecGF in our setting converges to global minima from almost all initializations, provided the factor norms remain bounded; with $\ell_2$ regularization, we obtain global convergence. Lastly, we corroborate our theory with experiments in the same setting.

Body: Uniform Spectral Growth and Convergence of Muon in LoRA-Style Matrix Factorization 1 Introduction 2 Related Works 3 Empirical Observation: Uniform Growth in LoRA with Muon Experimental Setup. LLM Fine-tuning Results. Modeling for Theory. Key Question. 4 Theoretical Setup Notation. 4.1 Problem Setup 5 Uniform Growth of Singular Values 5.1 Alignment Yields Decoupled Dynamics 5.2 General case: Approximate as Near-Diagonal Core Variables. Key Concepts. Why Near-Diagonal? 5.2.1 Alignment from Small Initialization 5.2.2 Square-Root Dynamics 5.2.3 Main Result: Uniform Growth Implication. 6 Convergence Analysis of SpecGF Stability of Global Minima. Convergence Rate. Global Convergence via Regularization. 7 Empirical Validation of Our Theory Setup. Results. 8 Conclusion A Speed of singular values of ğ€ \mathbf{A} and ğ \mathbf{B} A.1 Rank-1 Case A.1.1 SpecGF with ğ’¯ Î² \mathcal{T}_{\beta} Synchronized speed of singular values. A.1.2 SpecGF with ğ’¯ \mathcal{T} Synchronized speed of singular values. Fast rotation of ğ \mathbf{B} towards ğ¯ \mathbf{v} . A.2 Spectral initialization Case B Uniform Growth of Singular Values B.1 Setup and Notation Î² \beta -regularized orthogonalization. Core variables. B.2 Alignment from Small Initialization B.3 Initial Growth and Non-degeneracy B.4 Invertibility and Near-Diagonal Factors B.5 Square-Root Dynamics B.6 Non-degeneracy Persistence B.7 Main Theorem: Uniform Growth C Extension to Underparameterized Case ( r r â‹† r r^{\star} ) C.1 Setup and Extended Core Variables C.2 Column Space Invariance C.3 Invariant Manifold C.4 Lipschitz Stability near the Invariant Manifold C.5 Reduction to the Square Case C.6 Extended Uniform Growth Theorem D Convergence to Global Minima of SpecGF D.1 Analyticity of SpecGF D.2 Convergence Analysis of SpecGF Invariance. D.3 Saddle Point Avoidance D.4 Stability Analysis of SpecGF Conjecture: The basin of attraction is large. D.5 Convergence Rate of SpecGF Finite-time convergence of ğ’¯ \mathcal{T} . D.6 SpecGF with â„“ 2 \ell_{2} Regularization E Experiments E.1 Matrix Factorization Experiments Under Various Settings E.2 LLM Experiments E.2.1 Experimental Details E.2.2 Singular Value Dynamics E.2.3 Effective Rank Uniform Spectral Growth and Convergence of Muon in LoRA-Style Matrix Factorization Changmin Kang Jihun Yun Baekrok Shin Yeseul Cho Chulhee Yun Abstract Spectral gradient descent (SpecGD) orthogonalizes the matrix parameter updates and has inspired practical optimizers such as Muon. They often perform well in large language model (LLM) training, but their dynamics remain poorly understood. In the low-rank adaptation (LoRA) setting, where weight updates are parameterized as a product of two low-rank factors, we find a distinctive spectral phenomenon under Muon in LoRA fine-tuning of LLMs: singular values of the LoRA product show near-uniform growth across the spectrum, despite orthogonalization being performed on the two factors separately. Motivated by this observation, we analyze spectral gradient flow (SpecGF)â€”a continuous-time analogue of SpecGDâ€”in a simplified LoRA-style matrix factorization setting and prove â€œequal-rateâ€ dynamics: all singular values grow at equal rates up to small deviations. Consequently, smaller singular values attain their target values earlier than larger ones, sharply contrasting with the largest-first stepwise learning observed in standard gradient flow. Moreover, we prove that SpecGF in our setting converges to global minima from almost all initializations, provided the factor norms remain bounded; with â„“ 2 \ell_{2} regularization, we obtain global convergence. Lastly, we corroborate our theory with experiments in the same setting. 1 Introduction Classical optimization algorithms for training neural networks, such as stochastic gradient descent (Robbins and Monro, 1951 ) and Adam(W) (Kingma and Ba, 2015 ; Loshchilov and Hutter, 2019 ) update network parameters in a coordinate-wise manner, ignoring the structure of parameters. In contrast, several modern approaches in deep learning exploit the structure of matrix parameters, focusing on how or what to update during training. One line of work follows the former direction, and the Muon optimizer (Jordan et al. , 2024 ) is one of representative examples. At each iteration t t , Muon minimizes a loss function f â€‹ ( ğ– ) f(\mathbf{W}) in the weight matrix ğ– \mathbf{W} as ğŒ t = âˆ‡ f â€‹ ( ğ– t ) + Î¼ â€‹ ğŒ t âˆ’ 1 , ğ– t + 1 = ğ– t âˆ’ Î· t â€‹ ğ’¯ â€‹ ( ğŒ t ) , \displaystyle\begin{split}\mathbf{M}_{t} =\nabla f(\mathbf{W}_{t})+\mu\mathbf{M}_{t-1},\\ \mathbf{W}_{t+1} =\mathbf{W}_{t}-\eta_{t}\mathcal{T}(\mathbf{M}_{t}),\end{split} (1) where Î¼ âˆˆ [ 0 , 1 ] \mu\in[0,1] tunes the momentum, Î· t \eta_{t} is the step size, and ğ’¯ \mathcal{T} orthogonalizes the momentum (see Equation 5 ). In practice, Newton-Schulz iteration approximates ğ’¯ \mathcal{T} by mapping most singular values of normalized ğŒ t \mathbf{M}_{t} to an interval [ 0.7 , 1.3 ] [0.7,1.3] . If Î¼ = 0 \mu=0 , the update follows spectral gradient descent (SpecGD, Bernstein and Newhouse ( 2024 ); Ravi et al. ( 2024 ) ). Orthogonalized updates induce the spectrum of matrices to be more isotropic (Wang et al. , 2025 ; Anonymous, 2026a ) . By leveraging such geometric structure, Muon has gained significant attention for training large language models (LLM), often outperforming Adam(W) (Liu et al. , 2025 ; Wang et al. , 2025 ) . Another line of work targets what parameters are updated. This category includes Low-rank adaptation (LoRA, Hu et al. , 2022 ), which freezes the pretrained model and injects a low-rank update. LoRA parameterizes a weight update ğ– âˆˆ â„ m Ã— n \mathbf{W}\in\mathbb{R}^{m\times n} as a product of low-rank factor matrices ğ€ğ \mathbf{A}\mathbf{B} where ğ€ âˆˆ â„ m Ã— r \mathbf{A}\in\mathbb{R}^{m\times r} and ğ âˆˆ â„ r Ã— n \mathbf{B}\in\mathbb{R}^{r\times n} with r â‰ª min â¡ { m , n } r\ll\min\{m,n\} . This substantially reduces the number of trainable parameters while preserving strong fine-tuning performance. Combining these two intriguing approaches modifies both the parameterization and optimizer, potentially leading to distinct training dynamics and implicit biases. While recent studies combining Muon-style optimizers with LoRA have also reported promising empirical results (Ahn et al. , 2025 ; Anonymous, 2026b ) , theoretical explanations about their dynamics and implicit bias remain absent. In Section 3 , we fine-tune language models using the Muon optimizer together with the LoRA parameterization to examine the dynamics, and we observe the Muonâ€™s unique feature persists even after applying LoRA. Although each factor receives its own orthogonalized updates, all spectral components of the product ğ€ğ \mathbf{A}\mathbf{B} evolve nearly uniformly. On the contrary, the dynamics of linear models with multiple layers trained under gradient descent exhibit incremental learning caused by saddle-to-saddle dynamicsâ€”the model learns singular values in a stepwise manner, from largest to smallest (Arora et al. , 2019 ; Gidel et al. , 2019 ; Li et al. , 2020 ) . Motivated by this empirical observation, in subsequent sections, we deliver rigorous analyses of the dynamics induced from the orthogonalized updates under the LoRA setting. To this end, we model the matrix factorization objective induced by LoRA fine-tuning through the lens of spectral gradient flow (SpecGF) , a continuous-time model of SpecGD. We summarize our contributions as follows: Uniform Spectral Growth. Even though SpecGF orthogonalizes the gradients of the LoRA components ğ€ \mathbf{A} and ğ \mathbf{B} rather than their product, we show that the singular values of ğ€ğ \mathbf{A}\mathbf{B} still evolve at a uniform rate under SpecGF ( Theorem 5.7 ). The dynamics make smaller singular values converge before larger ones. This sharply contrasts with standard gradient flow which exhibits largest-first stepwise learning dynamics (Arora et al. , 2019 ; Gidel et al. , 2019 ) . Convergence Guarantee. We prove that if SpecGF converges, it almost surely converges to global minima provided that the factor norms remain bounded ( Theorem 6.2 ); with â„“ 2 \ell_{2} regularization, SpecGF globally converges. In addition, every global minimum is Lyapunov stable, and SpecGF converges to global minima exponentially fast ( Proposition 6.7 ). Empirical Validation. On matrix factorization, we confirm that SpecGF exhibits uniform growth and smallest-first convergence order, in contrast to the largest-first behavior of standard gradient flow. 2 Related Works Orthogonalized Optimizers. A line of research on the optimizers for matrix parameters, rather than vectorized ones, has been spotlighted for its empirical success. This includes Muon (Jordan et al. , 2024 ) and its variants, such as Dion (Ahn et al. , 2025 ) and MuonBP (Khaled et al. , 2025 ) that enhanced the efficiency, as well as preconditioner-equipped ones like Shampoo (Gupta et al. , 2018 ) and SOAP (Vyas et al. , 2025 ) . Muon normalizes the singular values of the matrix-valued updates, leading to a faster convergence and higher performance (Liu et al. , 2025 ; Wang et al. , 2025 ) compared to Adam (Kingma and Ba, 2015 ) . Recent studies have begun to analyze orthogonalized gradients through the lens of spectral gradient descent (SpecGD, Bernstein and Newhouse ( 2024 ) ), which can be viewed as Muon without momentum. Specifically, Fan et al. ( 2025 ) study implicit bias and max-margin behavior under spectral descent, while Anonymous ( 2026a ) analyze generalization benefits in imbalanced classification. These results are informative but limited to data assumptions. Low-Rank Adaptation (LoRA). Low-rank adaptation (LoRA) is an efficient approach for fine-tuning large language models (LLM). This approach injects trainable low-rank factors into weight updates while keeping the base model fixed (Hu et al. , 2022 ) , motivated by the observation that fine-tuning updates often lie in a low intrinsic-dimensional subspace (Aghajanyan et al. , 2021 ) . LoRA reduces trainable parameters from this low-rank structure without sacrificing much downstream performance. Several recent studies report improved performance when applying Muon-style optimizers in LoRA fine-tuning (Ahn et al. , 2025 ; Anonymous, 2026b ) . However, theoretical guarantees for convergence and training dynamics in this combined setting also remain largely unexplored. Training Dynamics. For deep linear models with standard gradient flow, large singular values are typically learned earlier than smaller ones (Arora et al. , 2019 ; Gidel et al. , 2019 ) , aligning with broader notions of spectral bias (Cao et al. , 2021 ; Xu et al. , 2020 ) . Gradient-based methods can also exhibit saddle-to-saddle trajectories in matrix factorization problems (Jacot et al. , 2021 ) . Orthogonalized optimizers such as Muon or SpecGD induce different dynamics. Anonymous ( 2026a ) prove that, for linear and bilinear models under classification tasks with MSE loss, every singular value of the parameters increases at the same rate until saturation. Such a uniform growth across all spectral components leads to a better generalization of SpecGD compared to gradient descent under data imbalance. However, their analysis is limited to a strong initialization such as spectral initialization (Zhang et al. , 2025b ) . Qiu et al. ( 2025 ) demonstrate that Muon encourages a more uniform spectrum than AdamW in the trainable matrices. (a) RoBERTa-Base with Muon (b) RoBERTa-Base with AdamW (c) LLaMA-3.2-1B with Muon (d) LLaMA-3.2-1B with AdamW Figure 1 : Evolution of the singular values of the LoRA ğ€ğ \mathbf{A}\mathbf{B} adapter applied to the query matrix in the first self-attention layer. 3 Empirical Observation: Uniform Growth in LoRA with Muon We begin with an empirical observation that motivates our theoretical analysis. Given pretrained weight ğ– 0 âˆˆ â„ m Ã— n \mathbf{W}_{0}\in\mathbb{R}^{m\times n} , we consider the fine-tuning problem min ğ– âˆˆ â„ m Ã— n â¡ â„“ â€‹ ( ğ– 0 + ğ– ) , \min_{\mathbf{W}\in\mathbb{R}^{m\times n}}\ell(\mathbf{W}_{0}+\mathbf{W}), (2) where â„“ : â„ m Ã— n â†’ â„ \ell:\mathbb{R}^{m\times n}\to\mathbb{R} denotes the loss function and ğ– \mathbf{W} represents the trainable weight update. The LoRA approach reparameterizes ğ– \mathbf{W} as ğ€ğ \mathbf{A}\mathbf{B} with low-rank factors ğ€ âˆˆ â„ m Ã— r \mathbf{A}\in\mathbb{R}^{m\times r} and ğ âˆˆ â„ r Ã— n \mathbf{B}\in\mathbb{R}^{r\times n} , where r â‰¤ min â¡ { m , n } r\leq\min\{m,n\} . The factors are initialized as ğ€ â€‹ ( 0 ) = ğŸ \mathbf{A}(0)=\bm{0} , and ğ â€‹ ( 0 ) \mathbf{B}(0) is a small random matrix (Hu et al. , 2022 ; Hayou et al. , 2024 ) . When training LoRA adapters with Muon, we observe that the singular values of ğ€ğ \mathbf{A}\mathbf{B} grow considerably more uniformly across the spectrum. This is a non-trivial phenomenon because Muon orthogonalizes the updates of ğ€ \mathbf{A} and ğ \mathbf{B} separately. Such a spectral growth in the product ğ€ğ \mathbf{A}\mathbf{B} suggests some form of alignment between the matrices ğ€ \mathbf{A} and ğ \mathbf{B} , established during the training process. Experimental Setup. We fine-tune RoBERTa-Base (Liu et al. , 2019 ) on SST-2 datasets from the GLUE benchmark (Wang et al. , 2018 ) . For a larger-scale experiment, we train LLaMA-3.2-1B (Grattafiori et al. , 2024 ) on the Alpaca dataset (Taori et al. , 2023 ) . For both cases, LoRA is applied to the query and value matrices with rank r = 8 r=8 . We compare Muon and AdamW optimizers, tracking the singular values of the LoRA product throughout training. Additional training details are described in Section E.2 . LLM Fine-tuning Results. Figure 1 illustrates the evolution of the singular values of the LoRA ğ€ğ \mathbf{A}\mathbf{B} adapter applied to the query matrix in the first self-attention layer. Figures 1(c) and 1(a) show results with the Muon optimizer: all singular values exhibit near-uniform evolution, maintaining parallel trajectories throughout training. In contrast, Figures 1(d) and 1(b) show that the AdamW optimizer does not necessarily attend to every spectral component equally; it may focus on larger singular values, aligning with the â€œlargest-firstâ€ learning dynamics observed in standard gradient descent. The complete singular value trajectories and the effective rank (Roy and Vetterli, 2007 ) for all matrices are provided in Section E.2 . Across all layers, Muon promotes all singular values of LoRA ğ€ğ \mathbf{A}\mathbf{B} adapter to evolve near-uniformly; thus, their effective rank is consistently close to the LoRA rank r = 8 r=8 . Such dynamics are not always observed for AdamW across all matrices; the consistency is degraded. This empirical evidence confirms qualitatively different learning behavior of Muon from conventional optimizers. Modeling for Theory. The near-uniform evolution of singular values for all LoRA ğ€ğ \mathbf{A}\mathbf{B} adapters stems from Muon, but such interactions are highly intractable due to the intricate interactions among the components of LLM. To rigorously a