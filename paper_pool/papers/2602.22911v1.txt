Title: NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion

Abstract: Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) outperforms LoRA at rank 512 (PPL 3.90), demonstrating superior spectral efficiency. This advantage generalizes to mathematical reasoning, where NoRA achieves a perplexity of 1.97 on MathInstruct, significantly surpassing LoRA's saturation point of 2.07. Mechanism analysis via Singular Value Decomposition (SVD) confirms that NoRA activates the dormant tail of the singular value spectrum, effectively preventing the rank collapse observed in linear methods.

Body: NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion 1 Introduction 2 NoRA: Non-linear Rank Adaptation 2.1 Preliminaries: The Linear Confinement 2.2 The NoRA Architecture Weight-Level Granularity. SiLU Gating. Structural Dropout as Manifold Expander. 2.3 The Mergeability Trade-off 2.4 Spectral Analysis Framework: Effective Rank 3 Experimental Setup 3.1 Base Model and Implementation 3.2 Datasets 3.3 Baselines 3.4 Evaluation Metrics 4 Empirical Analysis 4.1 The Capacity Scaling Law The Linear Ceiling. Breaking the Ceiling via Spectral Efficiency. 4.2 Generalization on Mathematical Reasoning Consistent Gains via Non-linearity. The Role of Data Diversity. 4.3 Qualitative Analysis: Escaping the Linear Trap Rank Collapse vs. Dynamic Tracking. Interpretation. 5 Mechanism Design Analysis 5.1 Spectral Signature of Non-linearity 5.1.1 Visualizing Expansion 5.1.2 Quantifying Manifold Expansion 5.2 Deconstructing NoRA: An Ablation Study 5.2.1 Granularity: The Weight-Level Advantage 5.2.2 Activation: The Necessity of Non-linearity 5.2.3 Dropout: Structural Manifold Expansion 5.3 Efficiency Trade-off: Capacity per Parameter 5.3.1 Parameter Efficiency 5.3.2 Inference Latency 6 Related Work 6.1 LoRA and Linear Variants 6.2 Granularity in Parallel Adapters 6.3 The Expressivity-Mergeability Trade-off 6.4 Linear Re-parameterization vs. Structural Expansion 7 Limitations 8 Conclusion A Extended Spectral Analysis on SlimOrca AUC-90 Analysis. B Comprehensive Spectral Evaluation on MathInstruct B.1 Singular Value Spectrum Analysis (SVSA) The Heavy Tail Phenomenon B.2 Effective Rank and Information Capacity Effective Rank Analysis B.3 Spectral Energy Distribution Energy Cumulative Density NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion Hung-Hsuan Chen Computer Science and Information Engineering National Central University Taoyuan, Taiwan hhchen1105@acm.org Abstract Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ‚Äúlinear ceiling‚Äù in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) outperforms LoRA at rank 512 (PPL 3.90), demonstrating superior spectral efficiency. This advantage generalizes to mathematical reasoning, where NoRA achieves a perplexity of 1.97 on MathInstruct, significantly surpassing LoRA‚Äôs saturation point of 2.07. Mechanism analysis via Singular Value Decomposition (SVD) confirms that NoRA activates the dormant tail of the singular value spectrum, effectively preventing the rank collapse observed in linear methods. K eywords PEFT ‚ãÖ \cdot LoRA ‚ãÖ \cdot LLM 1 Introduction Parameter-Efficient Fine-Tuning (PEFT) has evolved from an optimization trick into the backbone of Large Language Model (LLM) deployment. Among various techniques, Low-Rank Adaptation (LoRA) [ 5 ] has established itself as the de facto standard. Its popularity rests on a specific ‚Äúmergeability dogma‚Äù: the assumption that weight updates must be inherently linear ( Œî ‚Äã W = B ‚Äã A \Delta W=BA ) to allow seamless merging with the base model for zero-latency inference. We challenge this structural orthodoxy. Although recent variants have attempted to refine LoRA through weight decomposition [ 7 ] or adaptive rank allocation [ 11 ] , they primarily focus on optimizing the learning dynamics of the linear subspace. Crucially, they leave the underlying hypothesis space unchanged. Consequently, these methods remain bounded by the expressivity limits of linear transformations, creating a hard ceiling for reasoning-intensive tasks such as mathematics and logic. We present empirical evidence of this ‚Äúlinear ceiling‚Äù effect, where LoRA suffers from ‚Äúrank saturation‚Äù. In complex reasoning benchmarks, simply increasing the parameter budget yields diminishing returns. In our experiment, a high-rank LoRA ( r = 512 r=512 ) performs no better than a low-rank counterpart ( r = 64 r=64 ), indicating that the bottleneck is not the parameter count, but the structural rigidity of the linearity itself. To shatter this ceiling, we introduce NoRA (Non-linear Rank Adaptation). NoRA marks a paradigm shift from linear subspace optimization to non-linear manifold deformation. By injecting SiLU gating and structural dropout directly into a fine-grained, parallel adapter architecture, NoRA unlocks the high-dimensional expressivity required for complex reasoning. We argue that for high-value vertical tasks, the performance gains from non-linearity outweigh the convenience of weight merging. Furthermore, in the era of cloud-scale multi-tenant serving (e.g., S-LoRA [ 9 ] , Punica [ 1 ] ), unmerged adapters are already the architectural standard, rendering the cost of non-linearity negligible in practice. Our contributions are fourfold: ‚Ä¢ Architecture: We propose NoRA, a fine-grained, weight-level parallel adapter that integrates non-linear gating to capture complex functional updates beyond linear approximations. ‚Ä¢ Empirical Scaling: We demonstrate that NoRA breaks the linear ceiling. On the large-scale SlimOrca benchmark [ 6 ] , NoRA continues to scale, with NoRA at rank 64 significantly outperforming LoRA at Rank 512. ‚Ä¢ Domain Generalization: We validate the robustness of our approach on the MathInstruct dataset [ 10 ] , showing consistent performance gains in mathematical reasoning and confirming that the manifold expansion is not dataset-specific. ‚Ä¢ Theoretical Mechanism: Through Singular Value Decomposition (SVD) analysis, we provide a spectral proof of manifold expansion. We show that NoRA activates the dormant tail of the singular value spectrum, preventing the rank collapse that constrains linear methods. 2 NoRA: Non-linear Rank Adaptation We introduce NoRA, a methodology that reconciles the efficiency of parameter-efficient fine-tuning with the high-dimensional expressivity required for complex reasoning. 2.1 Preliminaries: The Linear Confinement LoRA operates under the restrictive hypothesis that the intrinsic dimension of weight updates for downstream tasks is low. For a pre-trained weight matrix W 0 ‚àà ‚Ñù d √ó k W_{0}\in\mathbb{R}^{d\times k} , LoRA constrains the update Œî ‚Äã W \Delta W to a low-rank decomposition B ‚Äã A BA , where B ‚àà ‚Ñù d √ó r B\in\mathbb{R}^{d\times r} , A ‚àà ‚Ñù r √ó k A\in\mathbb{R}^{r\times k} , and r ‚â™ min ‚Å° ( d , k ) r\ll\min(d,k) . The forward pass is: h = W 0 ‚Äã x + Œ± r ‚Äã B ‚Äã A ‚Äã x , h=W_{0}x+\frac{\alpha}{r}BAx, (1) where x x and h h are the input and output of the layer, and Œ± \alpha is a scaling hyperparameter. This formulation is purely linear. Although computationally convenient ‚Äì allowing Œî ‚Äã W \Delta W to merge into W 0 W_{0} ‚Äì it imposes a linear constraint. The model can rotate the feature space, but it cannot twist or fold it. We argue that this limitation is the root cause of the observed under-utilization of ranks in complex tasks. 2.2 The NoRA Architecture NoRA breaks this linear constraint. We retain the parallel bottleneck structure to preserve synchronization with the main branch but inject non-linearity to unlock high-rank capacity, as illustrated in Figure 1 (bottom-right). Formally, NoRA is defined as: h = W 0 ‚Äã x + s ‚ãÖ W d ‚Äã o ‚Äã w ‚Äã n ‚Äã ( ùíü ‚Äã ( œÉ ‚Äã ( W u ‚Äã p ‚Äã x ) ) ) h=W_{0}x+s\cdot W_{down}(\mathcal{D}(\sigma(W_{up}x))) (2) where W u ‚Äã p ‚àà ‚Ñù r √ó k W_{up}\in\mathbb{R}^{r\times k} projects input to a latent dimension r r , œÉ ‚Äã ( ‚ãÖ ) \sigma(\cdot) is the SiLU activation function, ùíü ‚Äã ( ‚ãÖ ) \mathcal{D}(\cdot) denotes structural dropout, W d ‚Äã o ‚Äã w ‚Äã n ‚àà ‚Ñù d √ó r W_{down}\in\mathbb{R}^{d\times r} projects back to the output dimension, and s s is a scaling scalar. Figure 1: Architectural evolution of adaptation methods. Top Row (Module-Level): Traditional adapters operate on the output of the entire attention block. (Top-left) Serial Adapters create latency bottlenecks. (Top-right) Parallel Adapters improve efficiency but lack fine-grained control over internal projections. Bottom Row (Weight-Level): (Bottom-left) LoRA injects linear updates ( Œî ‚Äã W = B ‚Äã A \Delta W=BA ) directly into W q W_{q} and W v W_{v} . (Bottom-right) NoRA (Ours) operates at the same fine-grained level but introduces non-linear modeling via SiLU gating ( œÉ \sigma ) and structural Dropout ( ùíü \mathcal{D} ). This architecture combines the efficiency of weight-level injection with the high-capacity expressivity of non-linear manifolds. This architecture introduces three critical design pivots: Weight-Level Granularity. As shown in Figure 1 , a critical distinction between NoRA and parallel adapters lies in the insertion point. Unlike traditional Parallel Adapters (top-right), which operate at the module level by processing the aggregate output of an attention block, NoRA operates at the weight level (bottom-right). By injecting updates directly into the internal query ( W q W_{q} ) and value ( W v W_{v} ) projections, NoRA fundamentally alters the internal feature dynamics of the attention mechanism rather than simply correcting its output. SiLU Gating. Linearity forces the adapter to process all input features uniformly. By introducing SiLU ( œÉ ‚Äã ( x ) = x ‚ãÖ sigmoid ‚Äã ( x ) \sigma(x)=x\cdot\text{sigmoid}(x) ), NoRA gains a gating mechanism. This non-linearity allows the adapter to selectively suppress noise or amplify specific feature directions in the latent space, approximating complex decision boundaries that linear low-rank updates cannot represent. Structural Dropout as Manifold Expander. In standard PEFT, dropout is often omitted. In NoRA, we utilize dropout ùíü \mathcal{D} not only as a regularizer, but as a mechanism for manifold expansion. By stochastically blocking latent paths during training, we force the model to distribute information across the entire rank spectrum, preventing the optimization from collapsing into a narrow subspace (rank collapse). 2.3 The Mergeability Trade-off A common critique of nonlinear adapters is the loss of ‚Äúmergeability‚Äù ‚Äì the ability to collapse Œî ‚Äã W \Delta W into W 0 W_{0} for zero-latency inference. We challenge the relevance of this constraint in modern deployment. In cloud-scale multi-tenant serving systems (e.g., S-LoRA [ 9 ] , Punica [ 1 ] ), merging is structurally impossible. To serve thousands of users with different fine-tuned models, the system holds one shared backbone and dynamically fetches unmerged adapter weights for each request. Merging weights would necessitate duplicating the massive backbone for every user, exploding VRAM usage. Therefore, the industrial standard is unmerged inference . In this paradigm, NoRA fits seamlessly. It requires no architectural changes to the serving infrastructure, only the execution of an additional activation kernel. We deliberately trade the theoretical mergeability‚Äîwhich is rarely used in multi-tenant scaling‚Äîfor tangible gains in reasoning capacity. 2.4 Spectral Analysis Framework: Effective Rank To quantify NoRA and LoRA‚Äôs ‚Äúmanifold expansion‚Äù capability, we adopt the effective rank (ER) metric [ 8 ] . Unlike the parametric rank (which is fixed), the effective rank measures the actual dimensionality of the information encoded in the activation space. Let H ‚àà ‚Ñù N √ó d H\in\mathbb{R}^{N\times d} be the matrix of adapter activations collected from N N samples. The singular values of H H are œÉ 1 ‚â• œÉ 2 ‚â• ‚ãØ ‚â• œÉ k ‚â• 0 \sigma_{1}\geq\sigma_{2}\geq\dots\geq\sigma_{k}\geq 0 . We define the normalized singular value distribution as p i = œÉ i ‚àë j = 1 k œÉ j p_{i}=\frac{\sigma_{i}}{\sum_{j=1}^{k}\sigma_{j}} . The effective rank is defined as the exponential of the Shannon entropy: ER ‚Äã ( H ) = exp ‚Å° ( ‚àí ‚àë i = 1 k p i ‚Äã ln ‚Å° p i ) \text{ER}(H)=\exp\left(-\sum_{i=1}^{k}p_{i}\ln p_{i}\right) (3) A higher ER implies a more uniform distribution of energy across dimensions, indicating that the non-linear gating successfully forces the model to utilize the full rank budget. Conversely, a low ER indicates rank collapse. 3 Experimental Setup We design our experiments to rigorously test and analyze the scaling limits of linear versus non-linear adaptation. 3.1 Base Model and Implementation We utilize Llama-3-8B as the backbone for all experiments. Training is conducted in bfloat16 precision using the AdamW optimizer with a cosine learning rate schedule. To ensure a fair comparison, we freeze all backbone parameters and strictly control the trainable parameter budget, varying only the adapter rank and architecture. 3.2 Datasets We select two distinct datasets to evaluate capacity scaling and domain robustness: ‚Ä¢ SlimOrca (Primary - Scaling): A large-scale collection of ‚àº \sim 300k GPT-4 augmented instruction pairs. Unlike standard datasets that yield concise responses, SlimOrca emphasizes Chain-of-Thought (CoT), prioritizing detailed reasoning steps and logical explanations over short answers. Spanning diverse complex domains such as mathematics, logic, and code interpretation, this dataset allows us to probe the capacity scaling law , as its complexity and scale are sufficient to saturate high-rank adapters and expose the ‚Äúlinear ceiling.‚Äù ‚Ä¢ MathInstruct (Secondary - Reasoning): A composite dataset of 100k mathematical problems (including GSM8K and MATH). We use this to validate NoRA‚Äôs domain adaptation capabilities, specifically its ability to model the complex logical dependencies required for mathematical derivation. 3.3 Baselines We compare NoRA directly with the industry standard LoRA. To investigate rank saturation, we sweep across a logarithmic scale of ranks r ‚àà { 16 , 64 , 128 , 512 } r\in\{16,64,128,512\} . This spectrum covers the transition from parameter-efficient constraints to high-capacity regimes. 3.4 Evaluation Metrics We employ a multi-faceted evaluation strategy combining performance and mechanism analysis: ‚Ä¢ Perplexity (PPL): We use the perplexity of the hold-out test set as the primary measure of predictive performance. ‚Ä¢ Singular Value Decomposition (SVD): We compute the singular value spectrum of the learned adapter weights to visualize rank collapse versus tail activation. ‚Ä¢ Effective Rank (ER): To quantify the ‚Äúmanifold expansion,‚Äù we calculate the effective rank [ 8 ] of the adapter activations. A higher ER indicates a more uniform distribution of information across the available rank budget. 4 Empirical Analysis In this section, we present the main empirical findings, focusing on the scaling behavior on the SlimOrca benchmark and domain generalization on the MathInstruct dataset. 4.1 The Capacity Scaling Law We first investigate whether non-linearity allows the model to overcome the rank saturation observed in linear adapters. We train both LoRA and NoRA on the 300k-sample SlimOrca dataset in ranks r ‚àà { 16 , 64 , 128 , 512 } r\in\{16,64,128,512\}