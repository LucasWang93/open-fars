Title: DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation

Abstract: Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.

Body: DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation 1 Introduction 2 Related Work 2.1 Co-Speech Gesture Generation 2.2 Dyadic Gesture and Reaction Generation 2.3 Diffusion-based Gesture Generation 3 DyaDiT 3.1 Seamless Interaction Dataset 3.2 DiT Backbone 3.3 Audio Orthogonalization Cross Attention (ORCA) 3.4 Motion Dictionary (MD) 3.5 Motion Tokenizer 4 Experiments 4.1 Evaluation Metrics 4.2 Baseline Methods 4.3 Quantitative Results 4.4 Ablation Studies 5 User Study 6 Conclusion 7 Limitation Future Work 8 Clustering of Generated Gestures 9 Implementation Details Diffusion Transformer Motion Tokenizer (VQ-VAE). Seamless Interactive Dataset. 10 Questionnaire DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation Yichen Peng 1,2 Jyun-Ting Song 2 Siyeol Jung 2,3 Ruofan Liu 1 Haiyang Liu 4,5 Xuangeng Chu 4,5 Ruicong Liu 4,5 Erwin Wu 1,4 Hideki Koike 1 Kris Kitani 2 1 Institute of Science Tokyo 2 Carnegie Mellon Unversity 3 UNIST 4 Shanda AI Research Tokyo 5 The University of Tokyo peng.y.ag@m.titech.ac.jp Abstract Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker‚Äôs motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset [ 1 ] , DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner‚Äôs gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance. Figure 1 : DyaDiT generates socially aware conversational gestures from dyadic audio, conditioned on social factors such as relationship and personality traits, achieving natural and contextually appropriate reactions that outperform prior methods in both quantitative and user evaluations. 1 Introduction Building synthetic agents (also known as digital humans, AI agents, avatars or androids) that can interact naturally with people, is essential for the future of human‚Äìcomputer interfaces. Recent language models such as GPT-4.5 [ 33 ] and LLaMA-3.1 [ 12 ] already demonstrate impressive conversational ability, and many users even feel as if they are speaking with another person [ 15 ] . However, currently this illusion still remains confined to a text window. In reality, human interaction involves more than just spoken words. People gesture, respond to each other, and express subtle social cues through body motion. In order for humans to truly feel that a synthetic agent is interactive, the agent must accompany its speech with gestures that evolve naturally with the conversation. However, generating such gestures is challenging. They are strongly shaped by social factors such as personality, the relationship between speakers, and their conversational roles. These cues influence how people move, how they react, and how they coordinate with one another, yet most existing gesture generation models do not explicitly model them [ 42 , 26 , 27 ] . As a result, their gestures often appear generic or fake. In addition to social context, dyadic conversation, which happens between two interacting individuals, exhibits speech dynamics that are difficult for gesture generation models to handle. Two people can speak at the same time, interrupt one another, or rapidly switch between speaking and listening [ 8 , 28 ] . Their audio streams merge together, making it difficult to separate who is speaking and who is responding [ 39 , 30 , 19 ] . Most existing dyadic gesture generation models [ 32 , 29 ] , however, simply fuse the two speech streams into one, making the contribution of each speaker blurry and therefore weakening the correspondence between the generated gestures and the underlying interaction. To tackle these limitations, we introduce DyaDiT, a diffusion based transformer that generates socially aware gestures from dyadic audio. Unlike previous work [ 22 , 23 , 36 , 10 ] that focuses only on the alignment between audio and generated motion, DyaDiT conditions its generation on explicit social cues such as relationship and personality. Furthermore, in order to address the strong entanglement of two overlapping audio streams during conversation, we propose Orthogonalization Cross Attention (ORCA), a simple yet effective module that disambiguates the two audio streams, resulting in a cleaner audio representation for better gesture generation. Lastly, since human motion in a dyadic setting is often affected by the partner‚Äôs gestures, DyaDiT can optionally take the partner‚Äôs movements as an additional input, allowing the model to generate gestures that are more coordinated, responsive, and natural. Our experiments show that DyaDiT consistently outperforms existing dyadic gesture generation methods across standard quantitative metrics, achieving clear improvements in gesture quality and distribution alignment. In addition, we conduct extensive user studies to assess human perceptual preference. The results show that participants strongly favor gestures generated by DyaDiT, indicating that our generated motion appears more socially aware and better suited for real conversational settings. In summary, our main contributions are as follows: ‚Ä¢ We propose DyaDiT, a diffusion transformer (DiT) that generates social context aware gestures in dyadic conversations. ‚Ä¢ We introduce an Orthogonalization Cross Attention (ORCA) module that reduces interference between two individual‚Äôs audio streams, enabling a cleaner audio representation for better gesture generation in dyadic conversation. ‚Ä¢ Through extensive quantitative evaluations and user studies, we show that DyaDiT consistently outperforms existing methods on standard metrics and is preferred by users in terms of perceived realism and social consistency. Figure 2 : Overview of DyaDiT. DyaDiT conditions on multiple input modalities, including audio, partner motion, relationship type, and personality scores. It employs an Audio Orthogonalization Cross Attention (ORCA) module to obtain cleaner audio representations and a motion dictionary to guide style aware gesture generation. 2 Related Work 2.1 Co-Speech Gesture Generation Co-speech gesture generation focuses on synthesizing body movements aligned with a single speaker‚Äôs speech. Early approaches formulate gesture generation as a translation problem from multimodal speech cues, combining text, audio, and speaker identity through recurrent or adversarial models, such as the trimodal framework of Yoon et al. [ 49 ] , which treated gesture generation as a translation problem, combining speech transcripts, audio, and speaker identity through an adversarial recurrent framework. Liu et al. [ 23 ] introduced the BEAT dataset and CaMN, a cascaded multimodal adversarial network capable of generating synchronized body and hand gestures. Other works, such as EMAGE [ 22 ] , TalkSHOW [ 47 ] , MECo [ 5 ] , etc. [ 34 , 24 , 11 , 21 , 45 ] enhanced realism by disentangling rhythmic and semantic motion features or integrating discrete latent spaces via VQ-VAEs. Although these advances are notable, most of them focuses on single-speaker co-speech gestures and overlooks the interactive nature of human communication. More complex settings such as dyadic gesture generation, which require modeling both participants‚Äô behaviors and their interpersonal dynamics, remain largely unexplored. 2.2 Dyadic Gesture and Reaction Generation Unlike co-speech gesture synthesis, dyadic gesture generation must model the coordinated behavior between two participants, including interpersonal timing, mutual attention, and responsiveness. Most prior works in this domain stem from facial reaction generation (FRG), where the goal is to predict non-deterministic listener facial responses to a speaker‚Äôs behavior [ 50 , 16 , 26 , 27 , 25 , 38 , 31 ] . Although FRG provides a basis for modeling two-person interactions, it primarily focuses only on facial or head movements rather than full-body gestures. Due to this limitation, several recent works have begun exploring body gesture generation in dyadic settings. A few efforts such as Audio2Photoreal [ 32 ] , ConvoFusion [ 29 ] , and TAG2G [ 9 ] extend single-speaker frameworks to two-party settings. Yet, these models often either (1) overlook the social context between the two individuals or (2) treat dyadic audio as a single blended signal without explicitly modeling cross-person dynamics, which often leads to ambiguity in the roles and interaction patterns presented to the model. These limitations highlight the need for explicit feature disentanglement and better support for socially contextual reasoning. 2.3 Diffusion-based Gesture Generation Diffusion models have emerged as powerful generative tools for human motion due to their ability to model multi-modal and many-to-many distributions. Several works have adapted diffusion frameworks originally designed for text-conditioned motion generation, such as MotionDiffuse [ 51 ] , FineMoGen [ 52 ] , and MDM [ 41 ] , etc. [ 37 , 40 , 6 , 3 ] , to the speech‚Äìgesture domain. Alexanderson et al. [ 2 ] reformulated DiffWave for co-speech gestures, while Zhu et al. [ 55 ] proposed DiffGesture, integrating noisy gesture sequences with contextual embeddings for temporal modeling. DiffuseStyleGesture+ [ 46 ] further introduced conditioning on audio, text, style, and seed gestures, leveraging transformer-based denoising with attention control. Other works, such as UnifiedGesture [ 44 ] , LivelySpeaker [ 53 ] , and AMUSE [ 7 ] , emphasized semantic and rhythmic consistency, disentangling emotional and stylistic latent factors. Considering the success of diffusion-based approaches in gesture generation, and the inherently nondeterministic nature of dyadic conversation, we build our method upon a diffusion model to better capture the variability and dynamics present in two-person interactions. Figure 3 : ORCA reduces ambiguity between the two audio streams, allowing DyaDiT to generate realistic motion even when one person interrupts the other during the conversation. The example demonstrates the generated motions adjusts naturally as the conversation shifts. 3 DyaDiT We introduce DyaDiT , a diffusion transformer (DiT) with multi-modal input for dyadic gesture generation. Given the conversational audio streams from two individuals (denoted as self and other ), our objective is to generate plausible upper-body gestures for the other speaker in accordance with the conversational context. To further enhance the realism of the generated gestures, DyaDiT optionally incorporates the relationship, personality traits, and gesture sequence of self as auxiliary conditioning signals during inference. We train our model on a subset of the Seamless Interaction dataset [ 1 ] , with dataset specifications and preprocessing procedures detailed in Section 3.1 . An overview of the full architecture is shown in Fig. 2 . Details of the Audio Orthogonalization Cross Attention (ORCA) module are provided in Section 3.3 , while the motion dictionary and motion tokenizer are presented in Sections 3.4 and 3.5 , respectively. 3.1 Seamless Interaction Dataset The Seamless Interaction dataset [ 1 ] is a large-scale corpus of dyadic conversation containing synchronized audio, full-body motion, and facial expressions. In this work, we use a curated subset of 3,000 clips (approximately 182 hours) from the dataset‚Äôs naturalistic scenario collection, which contains rich and spontaneous dyadic conversations suited for modeling conversational gestures. We focus on generating upper-body gestures, represented as g ‚àà ‚Ñù T √ó J √ó 6 g\in\mathbb{R}^{T\times J\times 6} using the 6D rotation representation [ 54 ] , where J J denotes the number of upper-body joints. In addition to motion data, we incorporate two high level social annotations provided in the dataset: a relationship type label f r ‚Äã s ‚àà { 0 , 1 } 4 f_{rs}\in\{0,1\}^{4} indicating whether the speakers are friends, strangers, family members, or dating partners; and a personality score vector f p ‚Äã s ‚àà ‚Ñù 5 f_{ps}\in\mathbb{R}^{5} that quantifies five major personality traits, which are extraversion, agreeableness, conscientiousness, neuroticism, and openness. For audio inputs, we extract the dyadic speech signals from both individuals and process them with a pretrained Wav2Vec2 encoder [ 4 ] to obtain audio feature embeddings for conditioning in our model. 3.2 DiT Backbone Our model adopts a diffusion transformer (DiT) backbone that follows the Denoising Diffusion Probabilistic Model (DDPM) [ 13 ] framework. The network takes a noisy latent pose ùê± t \mathbf{x}_{t} and predicts the added Gaussian noise according to time step t t , œµ Œ∏ ‚Äã ( ùê± t , t , ùêú ) \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t},t,\mathbf{c}) conditioned on multiple contextual inputs ùêú \mathbf{c} , where ùêú = ( O ‚Äã R ‚Äã C ‚Äã A ‚Äã ( a self , a other ) , p self , f relat , f ps ) \mathbf{c}=(ORCA(a_{\text{self}},a_{\text{other}}),p_{\text{self}},f_{\text{relat}},f_{\text{ps}}) including audio, partner‚Äôs motion, relationship type, and personality scores. The training objective follows the standard œµ \epsilon -prediction loss: ‚Ñí diff = ùîº ùê± 0 , t , œµ ‚Äã [ ‚Äñ œµ ‚àí œµ Œ∏ ‚Äã ( ùê± t , t , ùêú ) ‚Äñ 2 2 ] , \mathcal{L}_{\text{diff}}=\mathbb{E}_{\mathbf{x}_{0},t,\boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t},t,\mathbf{c})\right\|_{2}^{2}\right], (1) where ùê± t = Œ± ¬Ø t ‚Äã ùê± 0 + 1 ‚àí Œ± ¬Ø t ‚Äã œµ \mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}}\boldsymbol{\epsilon} . As shown in Figure 2 (a), each DiT block consists of a self-attention layer for modeling temporal dependencies within the latent pose sequence and a cross-attention layer for integrating contextual information from multimodal cues. In addition, the relationship and personality embeddings are injected through both FiLM-based [ 35 ] modulation and cross-attention, enabling DyaDiT to jointly capture social attributes and individual expressive styles in gesture generation. 3.3 Audio Orthogonalization Cross Attention (ORCA) To effectively capture conversational dynamics between two speakers, we introduce a orthogonalization cross attention module for audio fusio