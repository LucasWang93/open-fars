Title: POP: Prior-fitted Optimizer Policies

Abstract: Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning.

Body: POP: Prior-fitted Optimizer Policies 1 Introduction 2 POP: Prior-fitted Optimizer 2.1 MDP for Optimization 2.2 Optimization Step 2.3 Reward Signal 2.4 Meta-Learning Objective 3 A Prior for Optimization Problems 4 In-Distribution Generalization 5 Experimental Protocol 5.1 Architecture and Training 5.2 Benchmarks 5.3 Baselines 6 Evaluation Hypothesis 1: Learned optimizers trained on synthetic priors can generalize to unseen problems from the same distribution. Hypothesis 2: Learned coordinate optimizers trained on a low-dimensional (2D) prior for a fixed number of iterations generalizes to longer optimization horizons and higher-dimensional problems. Hypothesis 3: Learned coordinate optimizers trained on a low-dimensional (2D) synthetic prior can generalize to out-of-distribution problems. 7 Related Work 8 Future Work 9 Conclusion A On the Universality of Gaussian Process Prior for Pretraining Learnable Optimizers A.1 Preliminaries: Gaussian Processes and RKHS A.2 Universal Kernels A.3 Proof of the Theorem B Synthetic Prior B.1 Parameter values of our proposed Prior B.2 Visual Illustration of the Prior C Architecture and Training Details D Baseline Hyperparameter Optimization and Qualitative Analysis D.1 In-Distribution Performance E Virtual Library of Simulation Experiments benchmark POP: Prior-fitted Optimizer Policies Jan Kobiolka Christian Frey Gresa Shala Arlind Kadra Erind Bedalli Josif Grabocka Abstract Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning. Machine Learning, ICML 1 Introduction Optimization lies at the core of modern machine learning, scientific computing, and engineering. In practice, most continuous optimization problems are solved using gradient and momentum-based methods, which scale efficiently to high-dimensional settings (Robbins and Monro, 1951 ; Kingma and Ba, 2015 ; Daoud et al. , 2022 ; Liu et al. , 2020 ) . Despite their success, the performance of these optimizers is highly sensitive to hyperparameter choices, in particular the learning-rate schedule and momentum coefficients (Tian et al. , 2023 ; Keskar and Socher, 2017 ; Hassan et al. , 2022 ) . Selecting suitable hyperparameters often requires extensive hyperparameter optimization, which is costly, time-consuming, and difficult to transfer across tasks and domains (Mary et al. , 2025 ; Liao et al. , 2022 ; Schlotthauer et al. , 2025 ) . Figure 1 : POP (blue) adapts its learning rate based on the optimization landscape, enabling rapid convergence, escape from local minima, and improved global optimization compared to Adam (red). Yellow diamond represents the global minima, the white cross represents the start state, and the square represents the end state. More recently, learned optimizers have gained increasing interest in the community (Tang and Yao, 2024 ; Yang et al. , 2023 ; Metz et al. , 2022 ; Cao et al. , 2019 ; Gomes et al. , 2021 ; Vishnu et al. , 2020 ; Ma et al. , 2024 ; Goldie et al. , 2024 ; Lan et al. , 2023 ) . In contrast to classical optimization techniques, learned optimizers are typically composed of black-box function approximators (Metz et al. , 2020 ; Vishnu et al. , 2020 ) , or they build on hand-designed update rules whose hyperparameters are learned (Kristiansen et al. , 2024 ; Moudgil et al. , 2025 ) . Despite promising results, an open challenge remains how to learn an optimizerâ€™s behavior that (i) reduces the need for extensive tuning across tasks and (ii) maintains effective exploration of the objective landscape under constrained budgets, rather than becoming strongly local once the learning rate is annealed over time. We introduce POP, a meta-learned optimizer trained as a continuous-control agent leveraging RL meta-learning with optimization specific reward shaping. POP learns a policy that outputs coordinate-wise step size decisions conditioned on the optimization history accumulated over time, which enables our model to be adaptive towards exploration-exploitation trade-offs automatically. Our model leverages a transformer-based backbone (Vaswani et al. , 2017 ) to encode historical long-range trajectory context. Inspired by prior-data fitted networks (PFNs) (MÃ¼ller et al. , 2022 ) , we meta-train our model on a distribution of objective functions drawn from a novel prior. We construct this prior using a Gaussian process (Rasmussen and Williams, 2006 ) , approximated via Random Fourier Features (Rahimi and Recht, 2007 ) , that allows scalable training on a set of diverse objective landscapes at low computational cost. We illustrate the benefit of a state-conditioned optimizer in Figure 1 . POP increases the learning rate when the optimization landscape is sufficiently smooth, enabling rapid convergence to a local minimum. Upon reaching such a minimum, POP can either refine the solution through smaller updates or promote exploration by leveraging information from other regions of the landscape. This adaptive behavior emerges from the policy that is meta-learned to escape local optima over a plethora of pretraining functions. A thorough evaluation highlights POPâ€™s generalization on the Virtual Library of Simulation Experiments ( Surjanovic and Bingham, ) benchmark consisting of 47 47 diverse optimization functions. We further evaluate our modelâ€™s capabilities to transfer to longer optimization horizons and higher-dimensional problems. A statistical ranking analysis reveals that our method outperforms classical optimizers as well as a meta-learned optimizer introduced recently under matched budget constraints. The key contributions of this paper are: â€¢ We introduce POP, a novel optimizer that is meta-trained over millions of functions and learns step size policies conditioned on the optimizationâ€™s contextual information, enabling effective explorationâ€“exploitation behavior during optimization. â€¢ A Gaussian processâ€“based prior that combines convex and non-convex objectives, enabling scalable meta-training on diverse optimization landscapes and supporting strong generalization. â€¢ An empirical evaluation on 47 47 benchmark functions showing superior performance over conventional optimizers, with ablations on reward design, evaluation budget, and scalability to higher dimensions. 2 POP: Prior-fitted Optimizer In the following, we consider unconstrained optimization problems of the form x âˆ— âˆˆ argmin x âˆˆ â„ d â€‹ f â€‹ ( x ) , x^{\ast}\in\text{argmin}_{x\in\mathbb{R}^{d}}f(x), where each task instance is an objective function f f sampled from a prior f âˆ¼ p â€‹ ( f ) f\sim p(f) over the space of compact functions. We refer to an episode as optimizing a single sampled function f f for a fixed budget of T T time steps. We define an optimization trajectory as: Definition 2.1 (Optimization trajectory) . For an objective function f f , we define the optimization trajectory that includes the first t t update steps of a maximum budget of T T updates as: Ï„ ( t ) â‰” { ( x i , y i , âˆ‡ f â€‹ ( x i ) , i T ) } i = 1 t , \tau^{(t)}\coloneqq\left\{\left(x_{i},y_{i},\nabla f(x_{i}),\frac{i}{T}\right)\right\}_{i=1}^{t}, (1) where x i x_{i} denotes the parameters of the optimization problem in step i i , while y i â‰” f â€‹ ( x i ) y_{i}{\coloneqq}f(x_{i}) is the function value, âˆ‡ f â€‹ ( x i ) \nabla f(x_{i}) the gradient information, and i T \frac{i}{T} the normalized timestep. First-order optimizers proceed by applying an update rule based on local gradient information, e.g., GD applies x t = x t âˆ’ 1 âˆ’ Î· t â€‹ âˆ‡ f â€‹ ( x t âˆ’ 1 ) x_{t}=x_{t-1}-\eta_{t}\nabla f(x_{t-1}) , where Î· t \eta_{t} is the step size at iteration t t . In this work, we replace a preset step size schedule by a meta-learned policy Î· t âˆ¼ Ï€ Î¸ â€‹ ( Î· t | Ï„ ( t âˆ’ 1 ) ) \eta_{t}\sim\pi_{\theta}(\eta_{t}|\tau^{(t-1)}) , where at each iteration, our optimizer outputs the next step size Î· t \eta_{t} via a state-conditioned control signal Ï„ ( t âˆ’ 1 ) \tau^{(t-1)} based on the optimization trajectory (cf. Definition 2.1 ) up to step t âˆ’ 1 t{-}1 . 2.1 MDP for Optimization We formulate the optimization problem as a Markov Decision Process (MDP) defined by the tuple ( ğ’® , ğ’œ , ğ’« , r , p 0 , Î³ ) (\mathcal{S},\mathcal{A},\mathcal{P},r,p_{0},\gamma) , where ğ’® \mathcal{S} are the states, ğ’œ \mathcal{A} the actions, ğ’« \mathcal{P} the transition dynamics, r : ğ’œ Ã— ğ’® â†’ â„ r:\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R} denotes the reward signal, p 0 p_{0} is the probability distribution of initial elements in the optimization trajectory, and Î³ \gamma denotes the discount factor. Our goal is to train a parameterized policy Ï€ Î¸ : ğ’® Ã— ğ’œ â†’ â„ + \pi_{\theta}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{+} . In our meta-learning setting, an episode corresponds to an optimization trajectory Ï„ \tau on a sampled task instance from a prior, i.e., f âˆ¼ p â€‹ ( f ) f\sim p(f) . In the sequential decision problem, the policy receives a representation of the current optimization context and outputs a continuous coordinate-wise step size action a t = Î· t âˆˆ â„ d 0 a_{t}=\eta_{t}\in\mathbb{R}^{d} 0 . The state s t âˆˆ ğ’® s_{t}\in\mathcal{S} is simply the trajectory of updates so far s t â‰” Ï„ ( t ) , for â€‹ t âˆˆ [ 1 , â€¦ , T ] s_{t}\coloneqq\tau^{(t)},\text{ for }t\in[1,\dots,T] . 2.2 Optimization Step Given an optimization trajectory Ï„ ( t âˆ’ 1 ) \tau^{(t-1)} , our stochastic policy is a network with parameters Î¸ \theta that outputs the update steps at iteration t t for each function dimension Î¼ Î¸ â€‹ ( Ï„ ( t âˆ’ 1 ) ) âˆˆ â„ d \mu_{\theta}\left(\tau^{(t-1)}\right)\in\mathbb{R}^{d} and the standard deviation Ïƒ Î¸ âˆˆ â„ d \sigma_{\theta}\in\mathbb{R}^{d} . Ï€ Î¸ â€‹ ( Î· t | Ï„ ( t âˆ’ 1 ) ) = ğ’© â€‹ ( Î¼ Î¸ â€‹ ( Ï„ ( t âˆ’ 1 ) ) , diag â€‹ ( Ïƒ Î¸ 2 ) ) \displaystyle\pi_{\theta}\left(\eta_{t}\;|\;\tau^{(t-1)}\right)=\mathcal{N}\left(\mu_{\theta}\left(\tau^{(t-1)}\right),\mathrm{diag}\left(\sigma_{\theta}^{2}\right)\right) (2) To conduct an optimization, POP applies three steps. First, it samples a coordinate-wise next step size from our policy network (Equation 3 ). Then, POP conducts a gradient update (Equation 4 ), and finally, it updates the trajectory (Equation 5 ). Î· t \displaystyle\eta_{t} âˆ¼ Ï€ Î¸ â€‹ ( Î· t | Ï„ ( t âˆ’ 1 ) ) \displaystyle\sim\pi_{\theta}\left(\eta_{t}\;|\;\tau^{(t-1)}\right) (3) x t \displaystyle x_{t} â† x t âˆ’ 1 âˆ’ Î· t â€‹ âˆ‡ f â€‹ ( x t âˆ’ 1 ) \displaystyle\leftarrow x_{t-1}-\eta_{t}\nabla f(x_{t-1}) (4) Ï„ ( t ) \displaystyle\tau^{(t)} â† Ï„ ( t âˆ’ 1 ) âˆª { ( x t , y t , âˆ‡ f â€‹ ( x t ) , t T ) } \displaystyle\leftarrow\tau^{(t-1)}\cup\left\{\left(x_{t},y_{t},\nabla f(x_{t}),\frac{t}{T}\right)\right\} (5) 2.3 Reward Signal We use a dedicated reward function for our optimization, defined as the improvement to the best observed function value so far in a trajectory. Let y t âˆ’ 1 âˆ— = min ( â‹… , y , â‹… , â‹… ) âˆˆ Ï„ ( t âˆ’ 1 ) â¡ y y^{*}_{t-1}{=}\min_{\left(\cdot,y,\cdot,\cdot\right)\in\tau^{(t-1)}}{y} , we define: R â€‹ ( y t , Ï„ ( t âˆ’ 1 ) ) = max â¡ ( 0 , y t âˆ’ 1 âˆ— âˆ’ y t ) . \displaystyle R\left(y_{t},\tau^{(t-1)}\right)=\max\left(0,y^{*}_{t-1}-y_{t}\right). (6) This reward assigns a positive signal only when an action yields a new observed minimum, directly linking policy optimization to objective improvement. Since non-improving steps receive no penalty, the agent can explore the function freely, and meaningful improvements guide learning. 2.4 Meta-Learning Objective We optimize the policy parameters Î¸ \theta to maximize the expected return over tasks sampled from a prior (cf. Section 3 ). We formulate the objective as follows: max Î¸ â¡ ğ”¼ f âˆ¼ p â€‹ ( f ) â€‹ ğ”¼ Ï„ âˆ¼ POP â€‹ ( f ; Î¸ ) â€‹ âˆ‘ t = 0 T Î³ t â€‹ R â€‹ ( y t , Ï„ ( t âˆ’ 1 ) ) , \displaystyle\max_{\theta}~\mathbb{E}_{f\sim p(f)}~\mathbb{E}_{\tau\sim\text{POP}(f;\theta)}\sum_{t=0}^{T}\gamma^{t}R\left(y_{t},\tau^{(t-1)}\right), (7) where Ï„ âˆ¼ POP â€‹ ( f ; Î¸ ) \tau\sim\text{POP}(f;\theta) is computed by running Equations 3 - 5 for T T steps on the function f f sampled from the prior. We train the policy parameters using the Proximal Policy Optimization (PPO) algorithm (Schulman et al. , 2017 ) . 3 A Prior for Optimization Problems In order to meta-train our method, we sample optimization problems from a prior distribution f âˆ¼ p â€‹ ( f ) f\sim p(f) . This distribution must be sufficiently diverse and complex to cover a wide range of optimization problems, including both convex and non-convex functions, so that learned optimizers generalize well across problems. Therefore, we define a prior over optimization objectives as a combination of a separable quadratic (convex) component and a Gaussian process prior with an RBF kernel, approximated via Random Fourier Features (RFF) for computational efficiency (Rahimi and Recht, 2007 ) . We define a mixing weight Î± âˆ¼ ğ’° â€‹ ( Î± min , Î± max ) \alpha\sim\mathcal{U}(\alpha_{\min},\alpha_{\max}) that interpolates between the convex and non-convex components. For a given dimensionality D D , functions are constructed as: f â€‹ ( x ) := Î± â€‹ f C â€‹ ( x ) + ( 1 âˆ’ Î± ) â€‹ f RFF â€‹ ( x ) , f(x):=\alpha f_{C}(x)+(1-\alpha)f_{\text{RFF}}(x), (8) with the convex component defined as: f C â€‹ ( x ) = âˆ‘ d = 1 D Î² d 1 â€‹ ( x d âˆ’ Î² d 2 ) 2 , f_{C}(x)=\sum_{d=1}^{D}\beta^{1}_{d}(x_{d}-\beta^{2}_{d})^{2}, (9) where Î² d 1 âˆ¼ ğ’° â€‹ ( Î² min 1 , Î² max 1 ) \beta^{1}_{d}\sim\mathcal{U}(\beta^{1}_{\min},\beta^{1}_{\max}) controls the curvature and ensures convexity, and Î² d 2 âˆ¼ ğ’° â€‹ ( Î² min 2 , Î² max 2 ) \beta^{2}_{d}\sim\mathcal{U}(\beta^{2}_{\min},\beta^{2}_{\max}) determines the location of the quadratic minimum. With probability p convex p_{\text{convex}} we set Î± = 1 \alpha=1 , yielding strictly convex quadratic objectives. The RFF Gaussian process prior is constructed as: f RFF â€‹ ( x ) = 2 â€‹ âˆ‘ m = 1 M Ï‰ m 1 â€‹ cos â¡ ( ( Ï‰ m 2 ) âŠ¤ â€‹ x + Ï‰ m 3 ) . f_{\text{RFF}}(x)=\sqrt{2}\sum_{m=1}^{M}\omega^{1}_{m}\cos((\omega^{2}_{m})^{\top}x+\omega^{3}_{m}). (10) The Gaussian process approximation uses M M Random Fourier Features with phases Ï‰ m 3 âˆ¼ ğ’° â€‹ ( 0 , 2 â€‹ Ï€ ) \omega^{3}_{m}\sim\mathcal{U}(0,2\pi) , weights Ï‰ m 1 âˆ¼ ğ’© â€‹ ( 0 , Ïƒ 2 / M ) \omega^{1}_{m}\sim\mathcal{N}(0,\sigma^{2}/M) , and frequencies 