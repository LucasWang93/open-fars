Title: Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators

Abstract: Computing-in-Memory (CIM) accelerators are a promising solution for accelerating Machine Learning (ML) workloads, as they perform Matrix-Vector Multiplications (MVMs) on crossbar arrays directly in memory. Although the bit widths of the crossbar inputs and cells are very limited, most CIM compilers do not support quantization below 8 bit. As a result, a single MVM requires many compute cycles, and weights cannot be efficiently stored in a single crossbar cell. To address this problem, we propose a mixed-precision training and compilation framework for CIM architectures. The biggest challenge is the massive search space, that makes it difficult to find good quantization parameters. This is why we introduce a reinforcement learning-based strategy to find suitable quantization configurations that balance latency and accuracy. In the best case, our approach achieves up to a 2.48x speedup over existing state-of-the-art solutions, with an accuracy loss of only 0.086 %.

Body: Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators I Introduction II Related Work III Background III-A Analog MVMs on RRAM Crossbars III-B Quantization-Aware Training III-C Reinforcement Learning III-D (HAQ) IV Our Approach IV-A CIM-aware Automated Quantization (CIM-AQ) IV-B CIM Compiler V Results V-A CIM-AQ - Performance Improvements V-B CIM-AQ - Evaluation of Constraints V-C Comparison Against Related Work VI Conclusion RRAM Resistive Random Access Memory CIM Computing-in-Memory MVM Matrix-Vector Multiplication CNN Convolutional Neural Network NN Neural Network OFM Output Feature Map IFM Input Feature Map GPU Graphics Processing Unit CPU Central Processing Unit TPU Tensor Processing Unit ML Machine Learning GPEU General Purpose Execution Unit PE Processing Element DAC Digital-to-Analog Converter QAT Quantization-Aware Training WBS Weight Bit Slicing IBS Input Bit Slicing MPQ Mixed-Precision Quantization ONNX Open Neural Network Exchange HAQ Hardware-aware Automated Quantization CIM-AQ CIM-aware Automated Quantization GeMM General Matrix Multiplication 1T1R 1 Transistor 1 Resistor DDPG Deep Deterministic Policy Gradient API Application Programming Interface ONNX Open Neural Network Exchange QDQ Quantize and DeQuantize TVM Tensor Virtual Machine ADC Analog-to-Digital Converter MLP Multi-Layer Perceptron MHA Multi-Head Attention Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators Rebecca Pelke 1 1, Joel Klein 1 1, Jos√© Cubero-Cascante1, Nils Bosbach1, Jan Moritz Joseph12, Rainer Leupers1 1 Both authors contributed equally to this work. Abstract Computing-in-Memory (CIM) accelerators are a promising solution for accelerating Machine Learning ( ML ) workloads, as they perform Matrix-Vector Multiplications on crossbar arrays directly in memory. Although the bit widths of the crossbar inputs and cells are very limited, most CIM compilers do not support quantization below 8 bit 8\text{\,}\mathrm{bit} . As a result, a single MVM requires many compute cycles, and weights cannot be efficiently stored in a single crossbar cell. To address this problem, we propose a mixed-precision training and compilation framework for CIM architectures. The biggest challenge is the massive search space, that makes it difficult to find good quantization parameters. This is why we introduce a reinforcement learning-based strategy to find suitable quantization configurations that balance latency and accuracy. In the best case, our approach achieves up to a 2.48 √ó \mathbf{2.48\times} speedup over existing state-of-the-art solutions, with an accuracy loss of only 0.086 % 0.086\text{\,}\mathrm{\char 37\relax} . I Introduction To perform ML workloads efficiently, new hardware architectures are required. Promising candidates are Computing-in-Memory ( CIM ) accelerators, which execute MVMs directly in memory. This addresses the von Neumann bottleneck by reducing data movements between memory and processing units. [ 1 ] Resistive Random Access Memory ( RRAM ) is a well-known technology for CIM crossbars because it offers high device density, low power usage, fast switching speed, and compatibility with standard CMOS processes [ 2 , 3 ] . A limitation of RRAM crossbars is the restricted bit precision of both, the input activations and the crossbar cells. The cell resolution is limited by device variability, nonlinearity, and drift [ 4 , 5 , 6 ] . This makes it difficult to reliably distinguish between many resistance states. Typical bit widths for RRAM cells are between 1 bit 1\text{\,}\mathrm{bit} and 4 bits 4\text{\,}\mathrm{bits} [ 7 , 8 , 9 , 10 ] . Input resolution is limited by the area, power consumption, and speed of the Digital-to-Analog Converter ( DAC ) used to convert the digital inputs to analog voltages [ 11 , 12 ] . Higher-resolution DACs demand higher-resolution Analog-to-Digital Converters , which increases power consumption significantly [ 8 ] . As a result, most crossbars use 1 bit 1\text{\,}\mathrm{bit} DACs to reduce circuit complexity and improve robustness and efficiency [ 13 , 14 ] . Existing compilers for CIM only support fixed bit width quantization with a minimum of 8 bit 8\text{\,}\mathrm{bit} for activations and weights [ 15 , 16 , 17 , 18 , 19 , 20 ] . To map such 8 bit 8\text{\,}\mathrm{bit} models onto low-resolution crossbars, two techniques are used: weight bit slicing and input bit slicing [ 21 ] . Weight bit slicing splits high-bit weights across multiple RRAM cells. Input bit slicing splits high-bit inputs across multiple computing cycles. This increases the inference latency drastically [ 21 ] . Mixed-precision Quantization-Aware Training ( QAT ) with low bit widths is a hardware-friendly alternative to fixed-precision quantization [ 22 ] . It combines QAT [ 23 ] , which takes simulation effects into account during training, and Mixed-Precision Quantization ( MPQ ), where different layers use different bit widths for weights and inputs. To make use of this quantization scheme on CIM targets, we present our framework shown in Figure 1 . It has two main contributions: Figure 1 : Overview of the proposed framework. The main contributions are highlighted in red. 1. A MPQ-aware compiler that compiles mixed-precision models from the QAT framework Brevitas [ 24 ] for CIM architectures. The supported architectures are described in Sections II and III-A . It can handle crossbars of any size and applies CIM -specific optimizations. The compiler is described in Section IV-B . Results show speedups ranging from 2.20 √ó 2.20\times to 2.48 √ó 2.48\times over 8 bit 8\text{\,}\mathrm{bit} quantization compilers for ResNet-18, ViT-B/32, and VGG-16 on the ImageNet dataset. 2. A reinforcement learning-guided MPQ optimizer for CIM targets. The exponential search space for MPQ makes manual tuning impractical. Existing automated approaches [ 25 , 26 ] target only conventional hardware, while open-source frameworks for CIM are missing. We close this gap with CIM-aware Automated Quantization ( CIM-AQ ), our CIM -aware reinforcement learning-based optimizer. Therefore, we extend the Hardware-aware Automated Quantization ( HAQ ) framework [ 25 ] to enable mixed-precision optimization for CIM . By integrating Brevitas [ 24 ] as a new QAT backend, CIM-AQ does not only support a broader range of Neural Networks , including transformers, but is also approximately 8 % 8\text{\,}\mathrm{\char 37\relax} faster. CIM-AQ can be found on GitHub 2 2 2 GitHub link: https://github.com/jmkle/cim-aq . II Related Work TABLE I : Comparison with existing CIM compilers. Compiler Cell resolution Crossbar size Data types MPQ support TC-CIM [ 15 ] 4 bit 4\text{\,}\mathrm{bit} 256 √ó 256 256\times 256 8 bit 8\text{\,}\mathrm{bit} ‚úó TDO-CIM [ 16 ] 4 bit 4\text{\,}\mathrm{bit} 256 √ó 256 256\times 256 8 bit 8\text{\,}\mathrm{bit} ‚úó OCC [ 17 ] 4 bit 4\text{\,}\mathrm{bit} variable 8 bit 8\text{\,}\mathrm{bit} ‚úó Jin et al. [ 18 ] 2 bit 2\text{\,}\mathrm{bit} variable 16 bit 16\text{\,}\mathrm{bit} ‚úó CINM [ 19 ] 4 bit 4\text{\,}\mathrm{bit} variable 8 bit 8\text{\,}\mathrm{bit} ‚úó CIM-MLC [ 20 ] any variable 8 bit 8\text{\,}\mathrm{bit} ‚úó Ours any variable 1 bit 1\text{\,}\mathrm{bit} - 8 bit 8\text{\,}\mathrm{bit} ‚úì Many CIM compilers have been proposed in the past [ 15 , 16 , 17 , 18 , 19 , 20 , 27 , 28 ] . They differ in the targeted CIM architecture, crossbar design, compiler framework, implemented optimizations, and overall flexibility. Table I shows compilers that assume a CIM architecture similar to ours. Figure 2 illustrates this architecture, consisting of a host CPU and a single memory-mapped CIM accelerator. Besides the crossbar, the CIM core also includes registers and control logic, which are omitted here for clarity. TC-CIM [ 15 ] uses Tensor Comprehensions [ 29 ] and Loop Tactics [ 30 ] to detect and offload suitable tensor operations like MVMs and General Matrix Multiplications to a CIM accelerator. TDO-CIM [ 16 ] builds on TC-CIM by detecting patterns at the LLVM-IR level for broader language support and uses Polly [ 31 ] and Loop Tactics to offload individual layers to a CIM accelerator. OCC [ 17 ] uses MLIR [ 32 ] to offload GeMMs and convolutions to a CIM accelerator. They improve endurance through reduced writes and better weight reuse. Jin et al. [ 18 ] developed a general-purpose LLVM-based compiler that identifies MVMs , GeMMs , and logic operations, with a runtime application that decides between CPU and CIM execution. CINM [ 19 ] offers an end-to-end compiler for heterogeneous CIM systems, using hierarchical MLIR abstractions for progressive lowering and optimizations. CIM-MLC [ 20 ] proposes a multi-level compilation stack with progressive lowering and scheduling optimizations tailored to different CIM architecture levels. As summarized in Table I , current compilers are restricted to a fixed bit width of 8 bit 8\text{\,}\mathrm{bit} or 16 bit 16\text{\,}\mathrm{bit} . Since most crossbars have only 2 bit 2\text{\,}\mathrm{bit} to 4 bit 4\text{\,}\mathrm{bit} resolution and 1 bit 1\text{\,}\mathrm{bit} DACs , the overhead of each MVM becomes large because multiple cycles per MVM and many cells per weight are required. As a result, latency increases and efficiency drops. To address these issues, our compiler uses the best bit width for each layer. This reduces cycles and write operations and thereby improves latency. III Background This section provides background on RRAM -based CIM architectures, QAT , and the used HAQ framework. Figure 2 : The CIM architecture used in this work. III-A Analog MVMs on RRAM Crossbars RRAM crossbars are used to perform MVMs directly in memory. Figure 2 illustrates the basic architecture of the CIM target used in this work. The topology of the M √ó N M\times N 1 Transistor 1 Resistor ( 1T1R ) crossbar has a high resilience against wire parasitics [ 33 , 34 ] . Each column consists of 2 ‚Äã N 2N cells. Each pair of cells represents a single weight; g j , M + g_{j,M}^{+} is the positive and g j , M ‚àí g_{j,M}^{-} the negative part of the weight. This is called differential mapping [ 21 , 35 ] . The dot product result of a crossbar column i k i_{k} , with k ‚àà [ 1 , M ] k\in\left[1,M\right] , can be written as: i k = i k + ‚àí i k ‚àí = ‚àë j = 1 N v j ‚ãÖ ( g j , k + ‚àí g j , k ‚àí ) \displaystyle i_{k}=i_{k}^{+}-i_{k}^{-}=\sum_{j=1}^{N}v_{j}\cdot\left(g_{j,k}^{+}-g_{j,k}^{-}\right) (1) The input v j v_{j} is used to activate or deactivate row j j . The input is binary and requires one cycle per bit. Each pair of cells can store a multi-bit weight. Weight bit slicing is used if the weight has more bits than the cell can store. III-B Quantization-Aware Training Quantization in ML usually means mapping floating point ranges to integer values, e.g., fp32 ranges to int8 values. In range-based linear quantization , this mapping is described by a scaling factor s s and an offset z z called zero-point . In our setup, weights use symmetric signed quantization, and activations use symmetric signed or unsigned quantization. In symmetric quantization, the zero-point is set to zero. The symmetric quantization of a floating point value x f x_{\mathrm{f}} to an integer x q ‚àà [ ‚àí ( 2 B ‚àí 1 ‚àí 1 ) , 2 B ‚àí 1 ‚àí 1 ] x_{\mathrm{q}}\in[-\left(2^{B-1}-1\right),2^{B-1}-1] is defined as: x q = ‚åä x f ‚ãÖ 2 B ‚àí 1 ‚àí 1 max ‚Å° ( | x f | ) ‚åâ \displaystyle x_{\mathrm{q}}=\left\lfloor x_{\mathrm{f}}\cdot\frac{2^{B-1}-1}{\max\left(|x_{\mathrm{f}}|\right)}\right\rceil (2) For small bit widths B B , the quantization error can be significant. To improve the accuracy, QAT [ 23 ] is commonly used. An important concept in QAT is fake quantization . Fake quantization applies rounding and clipping to weights and activations in the forward pass but lets gradients pass through unchanged, so the NN learns the quantization effects. A well-known framework for QAT is Brevitas [ 24 ] . Brevitas is based on PyTorch and also supports MPQ . III-C Reinforcement Learning Reinforcement learning [ 36 ] is a concept where an agent learns by interacting with an environment over time through actions and rewards . Figure 3 illustrates the agent-environment interaction. Environment Agent s t + 1 s_{t+1} r t r_{t} State ‚Äã s t \mathrm{State}\ s_{t} Reward ‚Äã r t ‚àí 1 \mathrm{Reward}\ r_{t-1} Action a t a_{t} Figure 3 : Agent-environment interaction. At each time step t t , the agent choses an action a t a_{t} according to a policy œÄ \pi , with a t = œÄ ‚Äã ( s t ) a_{t}=\pi(s_{t}) for deterministic policies [ 37 ] and œÄ ‚Äã ( a t | s t ) \pi(a_{t}|s_{t}) for stochastic policies [ 38 ] . The environment responds with a new state s t + 1 s_{t+1} and a reward r t r_{t} . The goal in deterministic reinforcement learning is to find a policy œÄ ‚àó \pi^{*} that maximizes the expected cumulative reward: œÄ ‚àó ( s ) = arg max œÄ ùîº [ ‚àë k = 0 ‚àû Œ≥ k r t + k | s t = s ] , \displaystyle\pi^{*}(s)=\arg\max_{\pi}\ \mathbb{E}\left[\sum_{k=0}^{\infty}\gamma^{k}\,r_{t+k}\,\middle|\,s_{t}=s\right], (3) where Œ≥ ‚àà [ 0 , 1 ) \gamma\in[0,1) is a discount factor that prioritizes rewards. III-D Hardware-aware Automated Quantization (HAQ) The HAQ framework uses a Deep Deterministic Policy Gradient ( DDPG ) [ 39 ] agent. DDPG is an off-policy algorithm, which means that the agent can learn from data generated by a different policy than the one it is currently optimizing. It is based on the actor-critic architecture [ 40 ] , where the actor learns the policy œÄ \pi and the critic learns the action-value function Q ‚Äã ( s , a ) Q(s,a) . In HAQ , a state is denoted as o k o_{k} and contains the layer information of layer k k . The policy and the action-value function are learned by NNs . Figure 4 breaks down the main steps of the learning flow of the HAQ framework: 1. In each episode , the actor determines the quantization parameters a k = œÄ Œ∏ ‚Äã ( o k ) a_{k}=\pi_{\theta}\left(o_{k}\right) for each layer separately. 2. It is checked if the quantized NN fits into the resource budget for latency, memory, and power consumption. 3. If the resource budget is exceeded, the bit widths are decreased sequentially to enforce the constraints. 4. The NN is trained for only one epoch. The reward ‚Ñõ \mathcal{R} is calculated based on the top-1 accuracy of this epoch. 5. All tuples T k = ( o k , a k , ‚Ñõ , o k + 1 ) T_{k}=\left(o_{k},a_{k},\mathcal{R},o_{k+1}\right) are stored in the replay buffer , which collects previous experiences. 6. The critic network is trained by minimizing a loss based on a variant of the Bellman equation [ 41 ] . Therefore, a batch of random samples from the input buffer is used. The actor is updated using gradients from the critic. After training, a full QAT must be performed using the learned parameters. We will use Brevitas for this step, as its trained NN can be directly imported into the compiler. Figure 4 : Learning flow in the HAQ framework. IV Our Approach Section IV-A describes the CIM-AQ framework, which adapts the HAQ framework to CIM targets an