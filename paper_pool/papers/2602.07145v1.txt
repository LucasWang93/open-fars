Title: Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate

Abstract: Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly becomes weakly convex after a short period of training, and the loss is predicable by an upper bound on the last iterate, which further informs the scaling of optimal learning rate. Through the lens of convexity, we build scaling laws of learning rates and losses that extrapolate as much as 80X across training horizons and 70X across model sizes.

Body: Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate 1 Introduction 1.1 Related work Convex to deep learning. Scaling laws. Learning rate transfer. 1.2 Contributions 2 Convergence of SGD under convex loss 2.1 Revisiting non-asymptotic bound of SGD 2.2 Asymptotic loss bound at last iteration 2.3 Optimal convergence under scaled learning rate 2.4 Qualifying exam for learning rate schedules 3 Generalizations to deep learning and adaptive optimizers 3.1 Abstract form of any-iteration loss 3.2 Generalizing to deep learning 3.3 Generalizing to adaptive optimizers 4 Loss characterization at last iteration 4.1 Loss under any peak learning rate 4.2 Loss under optimal peak learning rate 5 Two-dimensional scaling law for learning rate 5.1 Experiment settings 5.2 Scaling across training horizons 5.3 Scaling across training horizons and model sizes 5.4 Scaling on multi-modal models 6 Conclusion A Proofs A.1 Derivation of Table Ëœ 1 Constant learning rate Square-root inverse learning rate Linear decay learning rate Cosine decay learning rate Warmup-stable-decay learning rate A.2 Derivation of Ëœ 2.5 A.3 Summary of generalizations B More experiment settings Figure Ëœ 1 : Figure Ëœ 2 : Figure Ëœ 3 Figure Ëœ 4 and Figure Ëœ 5 Section Ëœ 5 C Details of Section Ëœ 4 C.1 Figure Ëœ 6 C.2 Figure Ëœ 7 and Section Ëœ 4.2 D Additional experiments D.1 Effect of overfitting D.2 Parameter-efficient v.s. full model training D.3 Ablation study on hyperparameters \contribution [â€ ]Joint second authors. 1]FAIR, Meta Superintelligence Labs 2]Independent Researcher \correspondence Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate Zhiqi Bu Shiyun Xu Jialin Mao [ [ zhiqibu@meta.com Abstract Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly becomes weakly convex after a short period of training, and the loss is predicable by an upper bound on the last iterate, which further informs the scaling of optimal learning rate. Through the lens of convexity, we build scaling laws of learning rates and losses that extrapolate as much as 80 Ã— 80\times across training horizons and 70 Ã— 70\times across model sizes. 1 Introduction Deep learning has highly non-convex and complex loss landscape, e.g. the global minimum may not be unique, and there may be many local minima and saddle points where the optimization can be trapped ( garipov2018loss ; choromanska2015loss ; dauphin2014identifying ; jin2017escape ; sagun2016eigenvalues ) . Nevertheless, the empirical success of optimization in deep learning has implied that some benign properties may hold and be leveraged. In fact, there has been a long history of empirically exploring convexity in deep learning. For example, Llama training (non-convex) with AdamW ( loshchilovdecoupled ) is closely similar to convex optimization with SGD, in terms of the shapes of loss curves in Figure 1 of ( schaippsurprising ) . Empirical evidence by ( zhousgd ) has shown that SGD follows a star-convex path during the optimization of neural networks. In addition, convexity is universally observed along the direction of gradients in vision and language models by ( bu2025gradient ) . Another line of research has focused on the two-dimensional loss landscape, by ( li2018visualizing ; im2016empirical ) (along two random and normalized directions) and ( allen2019convergence ) (along the gradient and the negatively curved direction of the Hessian), which is approximately locally convex for residual neural networks with various depth and width. Furthermore, these convex-like loss landscapes are also observed on large language models such as RoBERTa, LLaMA, Qwen, and Mistral in ( zhong2022improving ; chen2025understanding ; lee2024fp8 ) . Besides these low-dimensional loss landscapes, the Hessian spectrum provides a rigorous local notion of curvature: a positive-definite Hessian (all eigenvalues positive) indicates that the loss landscape is locally convex around that point. Empirical observations show that at initialization, the Hessian often contains many large negative eigenvalues, which quickly shift toward zero and become much smaller in magnitude than the positive eigenvalues. Consequently, the spectrum becomes dominated by positive eigenvalues and the loss landscape becomes approximately convex ( yao2020pyhessian ; papyan2018full ; sankar2021deeper ; zhang2024transformers ) . In addition, theoretical analysis shows that two-layer and deeper neural networks can be regarded as convex in the wide-network limit, under neural tangent kernel ( jacot2018neural ; lee2019wide ; du2018gradient ; du2019gradient ; li2018learning ; allen2019learning ) , neural repopulation ( fang2019over ; fang2022convex ) , and mean field ( mei2018mean ; chizat2018global ) . As a matter of fact, many works in deep learning have been drawing useful understanding from convex analysis to non-convex deep learning: sutskever2013importance shows that momentum significantly accelerates the convergence; defazio2023optimal explains the effectiveness of learning rate warmup and decay from a convex viewpoint; the effect of weight decay ( krogh1991simple ) is first understood in ridge regression ( hoerl1970ridge ) . Specifically,the relationship between the learning rate sequence { Î· t } \{\eta_{t}\} and the upper bounds of loss sequence { L t } \{L_{t}\} has been heavily studied. These bounds come from different settings, studying convex or strongly convex loss, Lipschitz continuous or smooth loss, finite-iteration or asymptotic bound, averaged or last iterate, etc. Our work is directly based on Corollary 12 of ( defazio2023optimal ) (re-stated in ( 2.4 )), and a simplified version can be found in ( 2.3 ), which already provides some insights as we will summarize in Example Ëœ 2.3 . 1.1 Related work Convex to deep learning. It is well-known in convex regime that loss can converge at O â€‹ ( 1 / T ) O(1/\sqrt{T}) and optimal learning rate is O â€‹ ( 1 / T ) O(1/\sqrt{T}) . However, whether or when these conclusions hold in deep learning is largely unclear. From this perspective, our work is most closely related to ( schaippsurprising ) , both following from ( defazio2023optimal ) . In contrast, we not only extend major findings of ( schaippsurprising ) theoretically (e.g. our qualifying exam in Ëœ 2.5 covers any schedule), but also focus more on empirical validation. For example, O â€‹ ( 1 / T ) O(1/\sqrt{T}) convergence of loss and consistent patterns across models and optimizers in our Figures 6-12 are not presented in ( schaippsurprising ) , which focuses on one model and single training horizon like our Figures 2-5. In particular, our empirical approach is data-driven and practically applicable in large-scale deep learning. Scaling laws. Current popular scaling laws are primarily about loss ( kaplan2020scaling ; hoffmann2022training ) (i.e. scaling laws of loss), predicting how loss changes as model sizes and training horizons change, assuming optimal learning rate. As a result, learning rate is not explicitly presented in these laws. Other laws (i.e. scaling laws of learning rate) can scale learning rate across training horizons Î· peak âˆ— = Î» â€‹ T âˆ’ Î± \eta_{\textup{peak}}^{*}=\lambda T^{-\alpha} , where Î± = 0.125 \alpha=0.125 in ( bi2024deepseek ) , Î± âˆˆ { 0.32 , 0.38 , 0.42 , 0.65 , 0.70 } \alpha\in\{0.32,0.38,0.42,0.65,0.70\} by Table 5 in ( bjorck2024scaling ) , and some are horizon-unaware in ( porian2024resolving ; wang2024scaling ) (i.e. Î± = 0 \alpha=0 ). Nevertheless, these laws may not predict loss. In this work, we propose a law that simultaneously predict loss and optimal learning rate for the fixed value Î± = 0.5 \alpha=0.5 . Learning rate transfer. Maximal update parameterization (muP) ( yang2022tensor ) is a technique to transfer optimal learning rate across model size. While Table 1 of ( yang2022tensor ) claimed it also transfers across training horizons, this contradicts with empirical evidence in ( bjorck2024scaling ) (see Section 3.3) and our analysis. 1.2 Contributions In this work, we study the scaling law of deep learning loss and learning rate, through the lens of convex loss (non-smooth) and bounded gradient. We will establish a series of generalizations from convex theory to deep learning, which is presented in Table Ëœ 3 and summarized as follows. 1. We study the convex-like behaviors in deep learning for general model architectures, optimizers and learning rate schedules, hence establishing a non-asymptotic mapping from learning rate sequence to loss sequence. 2. We generalize to an asymptotic upper bound of loss, achieving O â€‹ ( 1 / T ) O(1/\sqrt{T}) convergence when (I) the peak learning rate is scaled by 1 / T 1/\sqrt{T} and (II) the learning rate schedule is qualified. 3. We propose a data-driven method to fit the asymptotic bound, establishing a scaling law across training horizons and model sizes. 2 Convergence of SGD under convex loss 2.1 Revisiting non-asymptotic bound of SGD We consider the stochastic gradient descent (SGD) as ğ° t + 1 = ğ° t âˆ’ Î· t + 1 â€‹ ğ  â€‹ ( ğ° t ) \mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta_{t+1}\mathbf{g}(\mathbf{w}_{t}) , where ğ° \mathbf{w} is the parameters, Î· \eta is the learning rate, ğ  \mathbf{g} is the mini-batch gradient with ğ”¼ â€‹ ğ  â€‹ ( ğ° ) = âˆ‡ L \mathbb{E}\mathbf{g}(\mathbf{w})=\nabla L , 0 â‰¤ t T 0\leq t T is the iteration and T T is the training horizon (i.e. number of iterations). The learning rate is defined by two factors via Î· peak â‹… s t â€‹ ( T ) \eta_{\textup{peak}}\cdot s_{t}(T) : (I) the learning rate schedule, which is a function s t âˆˆ [ 0 , 1 ] s_{t}\in[0,1] (e.g. linear decay is s t â€‹ ( T ) = 1 âˆ’ t / T s_{t}(T)=1-t/T ), and (II) the peak learning rate Î· peak âˆˆ â„ + \eta_{\textup{peak}}\in\mathbb{R}^{+} which is a positive scalar. We briefly review the convergence analysis under the convex and bounded gradient conditions. Condition 2.1 . Denoting a differentiable function as L L and its gradient as âˆ‡ L \nabla L , then L L is convex if â€‹ âˆ€ ( ğ° , ğ’™ ) , L â€‹ ( ğ° ) âˆ’ L â€‹ ( ğ’™ ) â‰¤ ( ğ° âˆ’ ğ’™ ) âŠ¤ â€‹ âˆ‡ L â€‹ ( ğ° ) \displaystyle\textit{convex if }\forall(\mathbf{w},\bm{x}),L(\mathbf{w})-L(\bm{x})\leq(\mathbf{w}-\bm{x})^{\top}\nabla L(\mathbf{w}) (2.1) bounded in gradient if â€‹ âˆƒ G â€‹ s.t. â€‹ âˆ€ ğ° , ğ”¼ â€‹ â€– ğ  â€‹ ( ğ° ) â€– 2 â‰¤ G 2 \displaystyle\textit{bounded in gradient if }\exists G\text{ s.t. }\forall\mathbf{w},\mathbb{E}\|\mathbf{g}(\mathbf{w})\|^{2}\leq G^{2} (2.2) Remark 2.2 . In Ëœ 2.1 , the convexity is not necessary for our analysis as it can be replaced by star-convexity and iterate-wise convexity along the optimization path, i.e. L â€‹ ( ğ° t ) âˆ’ L â€‹ ( ğ° âˆ— ) â‰¤ ( ğ° t âˆ’ ğ° âˆ— ) âŠ¤ â€‹ âˆ‡ L â€‹ ( ğ° t ) L(\mathbf{w}_{t})-L(\mathbf{w}_{*})\leq(\mathbf{w}_{t}-\mathbf{w}_{*})^{\top}\nabla L(\mathbf{w}_{t}) where ğ° âˆ— âˆˆ argmin ğ° â€‹ L â€‹ ( ğ° ) \mathbf{w}_{*}\in\text{argmin}_{\mathbf{w}}L(\mathbf{w}) is the minimizer, and L â€‹ ( ğ° t ) âˆ’ L â€‹ ( ğ° s ) â‰¤ ( ğ° t âˆ’ ğ° s ) âŠ¤ â€‹ âˆ‡ L â€‹ ( ğ° t ) . L(\mathbf{w}_{t})-L(\mathbf{w}_{s})\leq(\mathbf{w}_{t}-\mathbf{w}_{s})^{\top}\nabla L(\mathbf{w}_{t}). Additionally, the bounded gradient condition reduces to Lipschitz continuity when SGD is full-batch. In the parameter space, with ğ  t := ğ  â€‹ ( ğ° t ) \mathbf{g}_{t}:=\mathbf{g}(\mathbf{w}_{t}) , â€– ğ° t + 1 âˆ’ ğ° âˆ— â€– 2 \displaystyle\|\mathbf{w}_{t+1}-\mathbf{w}_{*}\|^{2} = â€– ğ° t âˆ’ ğ° âˆ— â€– 2 âˆ’ 2 â€‹ Î· t + 1 â€‹ ( ğ° t âˆ’ ğ° âˆ— ) âŠ¤ â€‹ ğ  t + Î· t + 1 2 â€‹ â€– ğ  t â€– 2 \displaystyle=\|\mathbf{w}_{t}-\mathbf{w}_{*}\|^{2}-2\eta_{t+1}(\mathbf{w}_{t}-\mathbf{w}_{*})^{\top}\mathbf{g}_{t}+\eta_{t+1}^{2}\|\mathbf{g}_{t}\|^{2} For bounded gradient and convex loss, denoting L âˆ— = min ğ° â¡ L â€‹ ( ğ° ) L_{*}=\min_{\mathbf{w}}L(\mathbf{w}) , we have in expectation ğ”¼ â€‹ â€– ğ° t + 1 âˆ’ ğ° âˆ— â€– 2 \displaystyle\mathbb{E}\|\mathbf{w}_{t+1}-\mathbf{w}_{*}\|^{2} â‰¤ ğ”¼ â€‹ â€– ğ° t âˆ’ ğ° âˆ— â€– 2 âˆ’ 2 â€‹ Î· t + 1 â€‹ ( L t âˆ’ L âˆ— ) + Î· t + 1 2 â€‹ G 2 \displaystyle\leq\mathbb{E}\|\mathbf{w}_{t}-\mathbf{w}_{*}\|^{2}-2\eta_{t+1}(L_{t}-L_{*})+\eta_{t+1}^{2}G^{2} Telescoping sum gives 0 \displaystyle 0 â‰¤ ğ”¼ â€‹ â€– ğ° Ï„ âˆ’ ğ° âˆ— â€– 2 â‰¤ â€– ğ° 0 âˆ’ ğ° âˆ— â€– 2 âˆ’ 2 â€‹ âˆ‘ t = 0 Ï„ âˆ’ 1 Î· t + 1 â€‹ ( L t âˆ’ L âˆ— ) + âˆ‘ t = 0 Ï„ âˆ’ 1 Î· t + 1 2 â€‹ G 2 \displaystyle\leq\mathbb{E}\|\mathbf{w}_{\tau}-\mathbf{w}_{*}\|^{2}\leq\|\mathbf{w}_{0}-\mathbf{w}_{*}\|^{2}-2\sum_{t=0}^{\tau-1}\eta_{t+1}(L_{t}-L_{*})+\sum_{t=0}^{\tau-1}\eta_{t+1}^{2}G^{2} Dividing by 2 â€‹ âˆ‘ t Î· t + 1 2\sum_{t}\eta_{t+1} and applying Jensenâ€™s inequality, we obtain an upper bound of L â€‹ ( ğ° Â¯ Ï„ ) L(\bar{\mathbf{w}}_{\tau}) , where ğ° Â¯ Ï„ = âˆ‘ t = 0 Ï„ âˆ’ 1 Î· t + 1 â€‹ ğ° t âˆ‘ t = 0 Ï„ âˆ’ 1 Î· t + 1 \bar{\mathbf{w}}_{\tau}=\frac{\sum_{t=0}^{\tau-1}\eta_{t+1}\mathbf{w}_{t}}{\sum_{t=0}^{\tau-1}\eta_{t+1}} is the averaged iterate and D := â€– ğ° 0 âˆ’ ğ° âˆ— â€– D:=\|\mathbf{w}_{0}-\mathbf{w}_{*}\| , ğ”¼ â€‹ L â€‹ ( ğ° Â¯ Ï„ ) â‰¤ âˆ‘ t = 0 Ï„ âˆ’ 1 Î· t + 1 â€‹ L t âˆ‘ t = 0 Ï„ âˆ’ 1 Î· t + 1 \displaystyle\mathbb{E}L(\bar{\mathbf{w}}_{\tau})\leq\frac{\sum_{t=0}^{\tau-1}\eta_{t+1}L_{t}}{\sum_{t=0}^{\tau-1}\eta_{t+1}} â‰¤ L âˆ— + D 2 2 â€‹ âˆ‘ t = 1 Ï„ Î· t + G 2 â€‹ âˆ‘ t = 1 Ï„ Î· t 2 2 â€‹ âˆ‘ t = 1 Ï„ Î· t := L SGD-ave â€‹ ( { Î· t } ) . \displaystyle\leq L_{*}+\frac{D^{2}}{2\sum_{t=1}^{\tau}\eta_{t}}+\frac{G^{2}\sum_{t=1}^{\tau}\eta_{t}^{2}}{2\sum_{t=1}^{\tau}\eta_{t}}:=L_{\text{SGD-ave}}(\{\eta_{t}\}). (2.3) Example 2.3 . For constant learning rate Î· \eta , the loss upper bound derived from convex analysis in ( 2.3 ) simplifies to L âˆ— + D 2 2 â€‹ T â€‹ Î· + Î· â€‹ G 2 2 L_{*}+\frac{D^{2}}{2T\eta}+\frac{\eta G^{2}}{2} . This aligns with the empirical trade-off in deep learning that larger Î· \eta converges faster but to a higher loss, and vice versa [See Fig 10.14 ( deep_learning_bible_nnfs ) ]. Furthermore, this loss bound is minimized by Î· âˆ— = D T â€‹ G \eta_{*}=\frac{D}{\sqrt{T}G} in convex analysis, which underlies the fast convergence observed in deep learning such as D-adaptation ( defazio2023learning ) , Prodigy ( mishchenko2023prodigy ) , DoG ( ivgi2023dog ) , and DoWG ( khaled2023dowg ) . With one extra term in ( defazio2023optimal , Corollary 12) , we have a bound of any single iterate: ğ”¼ â€‹ L â€‹ ( ğ° Ï„ ) â‰¤ L âˆ— + D 2 2 â€‹ âˆ‘ t = 1 Ï„ Î· t + G 2 â€‹ âˆ‘ t = 1 Ï„ Î· t 2 2 â€‹ âˆ‘ t = 1 Ï„ Î· t + G 2 2 â€‹ âˆ‘ k = 1 Ï„ âˆ’ 1 Î· k âˆ‘ t = k + 1 Ï„ Î· t â€‹ âˆ‘ t = k Ï„ Î· t 2 âˆ‘ t = k Ï„ Î· t \displaystyle\mathbb{E}L(\mathbf{w}_{\tau})\leq L_{*}+\frac{D^{2}}{2\sum_{t=1}^{\tau}\eta_{t}}+\frac{G^{2}\sum_{t=1}^{\tau}\eta_{t}^{2}}{2\sum_{t=1}^{\tau}\eta_{t}}+\frac{G^{2}}{2}\sum_{k=1}^{\tau-1}\frac{\eta_{k}}{\sum_{t=k+1}^{\tau}\eta_{t}}\frac{\sum_{t=k}^{\tau}\eta_{t}^{2}}{\sum_{t=k}^{\tau}\eta_{t}} (2.4) Note that ( 2.3 ) and ( 2.4 ) translate an arbitrary learning rate sequence { Î· t } \{\eta_{t}\} to an upper bound of the loss value. While both bounds shed some insights on the loss dynamics, we focus on the bound from ( 2