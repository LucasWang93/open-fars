Title: Intent Laundering: AI Safety Datasets Are Not What They Seem

Abstract: We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world adversarial attacks based on three key properties: being driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on "triggering cues": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce "intent laundering": a procedure that abstracts away triggering cues from adversarial attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world adversarial behavior due to their overreliance on triggering cues. Once these cues are removed, all previously evaluated "reasonably safe" models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated by existing datasets and how real-world adversaries behave.

Body: Intent Laundering: AI Safety Datasets Are Not What They Seem 1 Introduction 2 Empirical Motivation 2.1 Word Clouds 2.2 Data Duplication 2.3 Motivating Evidence 3 Approach 3.1 Intent Laundering 3.2 Evaluation Safety Evaluation. Practicality Evaluation. LLM as a Judge. Attack Success Rate. 3.3 Intent Laundering as a Jailbreaking Technique 4 Experimental Setup Word Clouds. Data Duplication. Evaluation Criteria Generation. Intent Launderer. LLM as a Judge. Evaluation Datasets. Evaluation Models. 5 Results and Discussion 6 Related Work Safety Alignment. Safety Datasets. 7 Conclusion A Agreement Analysis Between LLM-Based and Human Evaluations B Examples of Intent-Laundered Revisions and Model Responses C Input Prompts Intent Laundering : AI Safety Datasets Are Not What They Seem Shahriar Golchin, Marc Wetter Applied Machine Learning Research {sgolchin, mwetter}@labelbox.com Abstract We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice . In isolation, we examine how well these datasets reflect real-world adversarial attacks based on three key properties: being driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on “ triggering cues ”: words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce “ intent laundering ”: a procedure that abstracts away triggering cues from adversarial attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world adversarial behavior due to their overreliance on triggering cues. Once these cues are removed, all previously evaluated “ reasonably safe ” models become unsafe , including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated by existing datasets and how real-world adversaries behave. Warning: This paper may contain AI-generated sensitive content. 1 Introduction Safety alignment and safety datasets are the two pillars of AI safety (Beyer et al., 2025 ) . Safety alignment focuses on post-training techniques that fortify models against adversarial attacks (Askell et al., 2021 ; Ouyang et al., 2022 ) , while safety datasets serve to evaluate the robustness of these defenses (Mazeika et al., 2024 ; Zou et al., 2023 ) . The credibility of these evaluations, therefore, depends largely on the quality of the safety datasets. As with other evaluation tasks, dataset quality is determined by how well these datasets represent real-world scenarios . However, unlike other datasets that approximate such scenarios through common patterns (e.g., math problems), in the context of AI safety, the real-world scenarios involve ulterior , well-crafted , and out-of-distribution attacks that datasets must capture. This makes the design and development of safety datasets fundamentally different from other datasets. Figure 1: Overview of our intent laundering framework. Without the feedback loop, the framework displays the intent laundering procedure; with the loop, it intent laundering as a jailbreaking technique. The process begins by passing the original malicious request (data point) through the intent launderer to generate an intent-laundered revision. This revision is then used to attack the target model. An LLM judge evaluates the response for safety and practicality. If the response is both unsafe and practical, the attack is considered successful. Otherwise , the revision–regeneration mechanism is triggered, leveraging all previously failed revisions as feedback to generate a new, improved revision. The loop stops when the predefined iteration limit or target success rate is reached. In this work, we systematically study the quality of widely used AI safety datasets—AdvBench (Zou et al., 2023 ) and HarmBench (Mazeika et al., 2024 ) —based on prior research (Kim et al., 2025 ; Xie et al., 2025 ; Zhao et al., 2024 ; Xu et al., 2024 ; Li et al., 2024 ; Anil et al., 2024 , inter alia) . First, we analyze whether these datasets, in isolation , faithfully represent real-world adversarial attacks by evaluating three defining properties of such attacks: being driven by ulterior intent, well-crafted, and out-of-distribution. 1 1 1 Similar to other datasets, diversity is another important consideration in the design of safety datasets. This includes diversity across topics and data points . Topic diversity is generally well accounted for by most dataset creators (Röttger et al., 2024a ; Mazeika et al., 2024 ; Zou et al., 2023 ) and is thus excluded from our analysis. In contrast, diversity at the data-point level is a major issue in these datasets, which we discuss in Section 2.2 . Second, we examine whether these datasets actually measure safety risks in practice when they are used to evaluate model safety. To evaluate the quality of safety datasets in isolation, we begin by analyzing n n -gram word clouds. This helps visualize how the most frequent unigrams recur as higher-order n n -grams. We find that these recurring n n -grams consistently form unrealistic “ triggering cues ” 2 2 2 We use “ triggering language ” interchangeably. : expressions with overt negative/sensitive connotations. These cues fall into two categories: (1) inherent —expressions that carry such connotations by nature (e.g., “ commit suicide ”), and (2) contextual —expressions that signal such connotations in the context of harmful requests (e.g., [malicious intent] + “ without getting caught ,” explicitly signaling evasion). Figure 2 shows examples of triggering cues. These cues undermine two properties—being well-crafted and driven by ulterior intent—as such overt language rarely appears in real-world attacks and seems engineered to trigger safety mechanisms artificially. We also show that the repetitive overuse of these cues creates substantial duplication, producing near-identical data points in both sentence structure and malicious intent. This further degrades the out-of-distribution property and adds to the erosion of well-crafted property. Overall, our in-isolation analysis indicates that current safety datasets fail to faithfully represent real-world adversarial behavior. Next, we evaluate the quality of safety datasets in practice by involving models. In particular, we examine whether these datasets genuinely measure safety risks or merely rely on triggering cues that safety-aligned models are trained to detect and refuse to answer. To explore this, we introduce “ intent laundering ”: a systematic procedure that abstracts away overt triggering cues from attacks (data points) while strictly preserving their malicious intent and all relevant details. This transformation has two complementary components: (1) connotation neutralization , and (2) context transposition . In connotation neutralization, triggering expressions are replaced with neutral/positive or descriptive alternatives. In context transposition, real-world scenarios and referents—such as individuals (e.g., “ immigrants ”) or institutions (e.g., “ charity ”)—that can act as triggering cues in harmful requests are mapped to non-real-world contexts (e.g., a game world). We automate this process using an “ intent launderer ”: a large language model (LLM) with a few-shot in-context learning (ICL) setup. Each ICL demonstration pairs an original data point with its manually crafted, intent-laundered revision. Figure 1 depicts an overview of this framework. Our results reveal a strong and universal effect: once triggering cues are removed, the attack success rate (ASR) jumps from a mean of 5.38% to 86.79% on AdvBench, and from 13.79% to 79.83% on HarmBench. Finally, we propose intent laundering as a standalone jailbreaking technique by adding an iterative revision–regeneration mechanism. In each iteration, the model uses all previously failed revisions as feedback to generate a new, improved revision using the same few-shot ICL setup. Figure 1 shows this mechanism. This iterative process continues until either a predefined number of regeneration attempts is reached or a target ASR is met. Our results show that, with this regeneration loop, intent laundering achieves high ASR ( 90% – 98.55% ) after only a few iterations across all studied models under fully black-box access. This includes recent frontier models reported as among the safest —such as Gemini 3 Pro (Google, 2025a ; b ) and Claude Sonnet 3.7 (Anthropic, 2025a ; b ; Holistic AI, 2025 ) . The key contributions of this paper are as follows: (1) We show that AI safety datasets do not faithfully reflect real-world adversarial behavior due to overuse of unrealistic triggering cues. (2) We introduce intent laundering : a procedure that empirically verifies that revising data points to remove triggering cues sharply increases ASR: from a mean of 5.38% to 86.79% on AdvBench, and from 13.79% to 79.83% on HarmBench. (3) We adapt intent laundering into a novel jailbreaking method by integrating a revision–regeneration step for failed revisions, achieving high ASR (90%–98.55%) across the board. (4) We present evidence that similar triggering cues present in publicly available AI safety datasets affect internal safety evaluations and alignment techniques in a similar way. (5) We identify a major gap between how current datasets evaluate model safety and how adversarial behavior manifests in real-world scenarios. 2 Empirical Motivation To motivate our study, we first analyze the quality of safety datasets in isolation. We use n n -gram word clouds and pairwise similarity analyses to assess whether these datasets reflect real-world adversarial attacks. Specifically, we analyze three key characteristics of such attacks: • Ulterior Intent : Attacks conceal harmful intents behind benign-looking requests. • Well-crafted : Attacks are carefully designed to bypass safety filters. • Out-of-distribution : Attacks differ from everyday user prompts. 2.1 Word Clouds (a) Unigrams (b) Bigrams (c) Trigrams Figure 2: Word clouds of the 40 most frequent n n -grams , where n ∈ { 1 , 2 , 3 } n\in\{1,2,3\} , from the combined AdvBench and HarmBench corpus. Triggering cues are highlighted in red for inherently , and orange for contextually negative/sensitive connotations. This includes triggering words in the unigram cloud and triggering phrases in the bigram and trigram clouds. Neutral -connotation unigrams that contribute to triggering cues in higher-order n n -grams are also shown in green. These visualizations reveal an unusual overrepresentation of overtly triggering cues in safety datasets, suggesting that data points are artificially designed to trigger safety mechanisms. For example, expressions such as “ tutorial ” and “ step-by-step instructions ” are used explicitly to trigger infohazard safeguards (Bostrom, 2011 ) , which is unrealistic . Even minimally skilled bad actors rarely use such overt and self-incriminating language (e.g., “ commit identity theft ”). These findings indicate that safety datasets fail to capture two of the key properties of real-world attacks: being well-crafted and driven by ulterior intent. Note that, for clarity, only a representative subset of triggering cues is marked in the word clouds above. We use n n -gram word clouds to assess the quality of safety datasets intuitively. Specifically, we generate word clouds of the 40 most frequent n n -grams in the combined AdvBench and HarmBench corpus, with n ∈ { 1 , 2 , 3 } n\in\{1,2,3\} . These visualizations help trace how common unigrams evolve into higher-order n n -grams, revealing dominant language patterns. Figure 2 displays the resulting word clouds. At the unigram level, there is an unusual overrepresentation of words with overt negative/sensitive connotations , which we term “ triggering words .” Triggering words fall into two categories: (1) inherent triggering words (in red), which carry negative/sensitive connotations by nature (e.g., “ steal ”), and (2) contextual triggering words (in orange), which imply such connotations when used in harmful contexts (e.g., “ commit ” when used with “ suicide ”). As unigrams evolve into higher-order n n -grams, this overrepresentation intensifies: phrases with negative/sensitive connotations become unusually dominant. Similar to unigrams, these phrases carry such connotations either inherently or contextually within malicious requests. We refer to these as “ triggering phrases ,” and together with triggering words, they form what we call “ triggering cues .” Triggering phrases mainly form in two ways: (1) they build on triggering words, or (2) they consist entirely of words with neutral connotations. For example, the inherent triggering word “ steal ,” evolves into inherent triggering phrases such as “ steal sensitive information ,” “ steal confidential information ,” and “ steal personal information .” Similarly, the contextual triggering word “ commit ,” expands into inherent triggering phrases such as “ commit suicide ,” “ commit insider trading ,” and “ commit identity theft .” Neutral-connotation words can also form triggering phrases. For instance, “ without ,” “ getting ,” and “ caught ,” combine into the triggering phrase “ without getting caught .” This also explains their high frequency as unigrams. However, such explicit and repetitive overuse of triggering cues, along with direct mentions of malicious intent, directly contradicts the behavior of real-world adversaries. Even minimally capable adversaries rarely use such overt language, as it easily triggers safety mechanisms. Taken together, these patterns indicate that existing safety datasets contain contrived data points largely disconnected from real-world behavior, where harmful requests are well-crafted and motivated by ulterior intent. Figure 3: Proportion of duplicated versus unique data points in the AdvBench and HarmBench datasets across varying similarity thresholds. Each safety dataset is compared to a size-matched GSM8K subset shown below its plot. Both safety datasets exhibit considerably higher duplication rates across most thresholds compared to their GSM8K counterparts. This is striking, as safety datasets are intended to approximate real-world attacks—characterized by being out-of-distribution and well-crafted. In contrast, they show more duplication than a regular non-safety dataset, where such duplication is more acceptable. This is particularly alarming for safety d