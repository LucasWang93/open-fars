Title: Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns

Abstract: Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC^{2} (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC^{2} combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC^{2} improves the stability-plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior.

Body: Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns 1 Introduction 2 Related Work 3 Method 3.1 Overview and notation 3.2 Block computation 3.3 Modulation controller and predictive pathway Modulation controller. Predictive pathway. 3.4 Chunked sparse routing Router logits. Topology-aware prior. Routing-logit modulation and top- k k selection. Routing auxiliary loss. 3.5 Associative memory and routed cortical computation Associative memory (optional). Routed cortical computation. Readout, output gating, and routed mixture. Chunk-level lateral propagation. 3.6 Routing-weight refinement and low-rank corrective path Routing-weight refinement (optional). Low-rank corrective path (optional). 3.7 Global gain modulation, output head, and training objective 4 Experiments and Results 4.1 Experimental setup Training data. Models and baselines. Training and evaluation protocol. Metrics. 4.2 Results 5 Discussion 6 Conclusion A Technical Derivations and Implementation Details A.1 Predictive pathway: causal one-step-ahead convolution A.2 Chunked routing and padded execution Router logits and topology term. Routing-logit modulation and top- k k selection. Routing auxiliary term. A.3 Parallel cortical computation Dense projection and routed gather. Excitatory-inhibitory gate remapping. Adaptive state coefficients and within-chunk causal filtering. Readout with optional top-down gating. Output gate, skip connection, and routed mixture. Chunk-level lateral propagation. Activation checkpointing. A.4 Associative memory and routing-weight refinement Associative-memory retrieval. Routing-weight refinement (cortico-thalamic feedback). A.5 Model wrapper, auxiliary losses, and trainer objective A.6 Main training configuration Hardware and precision. Optimization and schedule. Data and tokenization. Logging and checkpointing. Model hyperparameters. B Complexity Modulation controller. Predictive coding. Chunked sparse router. Associative memory. Cortical field, one pass. Corrector feedback. Low-rank corrective pathway. Summary. Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns Afshin Khadangi SnT, University of Luxembourg afshin.khadanki@uni.lu Working Paper Abstract Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC 2 (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC 2 combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC 2 improves the stability–plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior. W B Logs 1 Introduction Large language models are increasingly deployed as long-lived systems that must remain useful under shifting data, shifting user intents, and shifting domains. In practice, this creates a persistent tension: the model must adapt quickly to new distributions while preserving previously learned behavior. The default remedy, periodic retraining or heavy fine-tuning, is expensive and slow. Lightweight updates such as adapters and low-rank tuning reduce cost, but sequential updates still induce interference and forgetting, especially when task boundaries are unclear and storage of prior data is restricted. Recent work has exposed both the opportunity and the limits of current architectures. On the efficiency side, modern state-space models have narrowed the quality gap with Transformers while offering favorable inference scaling; Mamba-3 pushes this line further with improved discretization, richer dynamics, and hardware-aware decoding efficiency Lahoti et al. ( 2026 ) . Hybrid designs such as Jamba combine attention and Mamba-like blocks to trade off long-context capability and throughput Lenz et al. ( 2025 ) . On the stability side, gating has emerged as a surprisingly powerful primitive: Gated Attention shows that a small, structured modification to attention can improve training stability, reduce attention pathologies, and support long-context extrapolation Qiu et al. ( 2025a ) . At the same time, sparse routing and mixtures introduce their own fragility when the data distribution evolves, motivating careful study of router robustness in continual pre-training Thérien et al. ( 2025 ) and new routing schemes for scaling SSMs Zhan et al. ( 2025 ) . In parallel, the community has begun to treat adaptation at inference time as a first-class capability. Test-Time Learning for LLMs frames adaptation as input perplexity minimization on unlabeled test streams and shows large gains under distribution shift when updates are constrained to low-rank subspaces Hu et al. ( 2025 ) . Model-merging approaches provide a complementary lens: local mixtures constructed via model merging can approximate test-time training while amortizing cost to training time Bertolissi et al. ( 2025 ) , and null-space constrained gating can reduce interference during continual merging Qiu et al. ( 2025b ) . These results underscore a key point: useful adaptation signals exist at deployment time, but today they are typically exploited through bolt-on procedures that are not native to the backbone and therefore remain difficult to scale, difficult to stabilize, and hard to compare cleanly across settings. This paper argues that continual learning should be treated as an architectural property. We introduce TRC 2 (Thalamically Routed Cortical Columns), a decoder-only backbone designed around two principles. First, communication should be sparse and controllable, so that new information can be routed to a small subset of computation without globally perturbing the model. Second, plasticity should be localized in fast mechanisms that can update online at low cost, while slower representational structures remain stable and support abstraction across time. TRC 2 implements these principles with a looped layer structure. Each layer contains a thalamic router that selects a top- k k set of cortical columns per token and encourages temporal continuity via a topology-aware prior. Each selected column is a compact microcircuit whose core is a selective state-space update, augmented with explicit excitatory and inhibitory modulation. A cerebellar fast-weight corrector provides a dedicated, low-rank pathway for online updates driven by deployment data, enabling rapid adjustment without rewriting the slow cortical parameters. The resulting layer is linear-time in sequence length within each active column, with constant-time routing overhead, and supports chunked scan implementations that reduce kernel-launch overhead in practice. The architecture is motivated by an empirical gap in current continual learning for LLMs. Replay-free adapter methods such as ELLA show that careful control of update subspaces can substantially reduce forgetting, but they still treat the backbone as a static substrate and rely on external regularizers Biswas et al. ( 2025 ) . TRC 2 instead makes interference control and rapid adaptation part of the computation graph through routing, inhibition, and fast weights. This also aligns with recent evidence that local, iterative learning mechanisms can be scaled in deep networks; predictive-coding style training has reached 100+ layer regimes, suggesting that looped correction dynamics need not be confined to toy scales Innocenti et al. ( 2025 ) . Our contributions are as follows. • We present TRC 2 , a decoder-only backbone for continual learning that combines sparse thalamic top- k k routing over cortical columns with biologically grounded mechanisms for modulation, prediction, memory, feedback, and fast correction. • We develop a sparse, chunk-parallel implementation of TRC 2 that supports efficient training and inference on modern accelerators, including topology-aware routing, chunk-level computation, and memory-aware execution with optional activation checkpointing. • We provide a reproducible continual-learning evaluation stack with distributed multi-GPU training, standardized logging, and task-wise evaluations that track forgetting and forward transfer under streaming domain shifts. The framework includes targeted ablations and strong baselines, enabling direct analysis of which TRC 2 components drive gains in adaptation and retention. The remainder of the paper details the TRC 2 layer, then evaluates efficiency and adaptation across language modeling and continual learning benchmarks, with direct comparisons to strong Transformer, hybrid, and state space model baselines. 2 Related Work Continual learning for large language models has expanded from classic task-incremental settings to broader regimes such as continual pre-training, domain-adaptive pre-training, instruction updates, and lifelong knowledge maintenance. Recent surveys organize this space into internal model updates versus external augmentation, and they highlight open evaluation issues that become more severe at scale Zheng et al. ( 2025 ); Shi et al. ( 2025 ) . This framing motivates backbones that are themselves robust to streaming distribution shift, rather than relying only on training-time interventions. A dominant line of work for post-training adaptation constrains updates to small parameter subspaces. DoRA improves low-rank adaptation by decomposing weight updates into magnitude and direction, narrowing the gap to full fine-tuning without changing inference cost Liu et al. ( 2024 ) . Other work studies composition across many updates, including gated combinations of LoRA modules Wu et al. ( 2024 ) and lifelong mixtures with routing constraints and order sensitivity Wang and Li ( 2024 ) . These results suggest that the structure of the update pathway and the routing mechanism both matter for long adaptation sequences. Mixture-of-Experts remains a practical route to higher capacity under bounded per-token compute. DeepSeekMoE studies expert specialization and shared experts to reduce redundancy and improve routing behavior Dai et al. ( 2024 ) . LLaMA-MoE shows that dense decoders can be converted into sparse expert systems and recovered through continued pre-training Zhu et al. ( 2024 ) . At the tuning stage, sparse expertization can also be made highly parameter-efficient for instruction adaptation Zadouri et al. ( 2024 ) . Work on router design, including mixtures of routers, further emphasizes that routing quality is often the limiting factor in sparse systems Zhang et al. ( 2025 ) . Efficient sequence backbones have also shifted attention away from dense attention-only designs. Mamba established selective state-space computation as a competitive foundation-model backbone with linear-time sequence processing Gu and Dao ( 2024 ) . BlackMamba combines state-space dynamics with sparse experts, showing that routing and recurrent sequence cores can be integrated in one architecture Anthony et al. ( 2024 ) . RWKV-family models provide another recurrent path with stronger state parameterization Peng et al. ( 2024 ) . At the systems level, FlashAttention-3 highlights how strongly performance depends on kernel-level implementation choices, which is directly relevant when evaluating alternative backbones Shah et al. ( 2024 ) . Our design is informed by computational and systems neuroscience as architectural guidance. The predictive branch follows predictive-coding formulations that separate top-down prediction from bottom-up mismatch signals Rao and Ballard ( 1999 ) , while the modulation controller is motivated by classical accounts of reward prediction and uncertainty-dependent gain control Schultz et al. ( 1997 ); Angela and Dayan ( 2005 ) . The gated readout is inspired by compartment-specific integration and coincidence effects in cortical pyramidal neurons Larkum et al. ( 1999 ) , and is further supported by recent evidence that cortical feedback engages active dendritic processing Fişek et al. ( 2023 ) . The routing-weight refinement stage is motivated by reciprocal cortico-thalamic feedback loops that shape thalamic processing Born et al. ( 2021 ) . The associative memory pathway uses modern Hopfield retrieval as a differentiable content-addressable memory mechanism Ramsauer et al. ( 2021 ) , and is broadly consistent with recent work on systems consolidation and predictive reward representations in hippocampal-cortical circuits Lee et al. ( 2023 ); Yaghoubi et al. ( 2026 ) . We also view recent studies on large-scale neurotransmitter-system organization and biologically grounded learning principles as complementary motivation for structured control signals and local computation in scalable sequence models Hansen et al. ( 2022 ); Liu et al. ( 2025 ); Song et al. ( 2024 ) . 3 Method 3.1 Overview and notation Let x 1 : T x_{1:T} be a token sequence from a vocabulary of size V V . TRC 2 is a decoder-only language model with hidden width d d and L L stacked blocks. For batch size B B and sequence length T T , the hidden representation at layer ℓ \ell is X ( ℓ ) ∈ ℝ B × T × d . X^{(\ell)}\in\mathbb{R}^{B\times T\times d}. Token and position embeddings are learned: X b , t ( 0 ) = E ​ [ x b , t ] + P ​ [ t ] . X^{(0)}_{b,t}=E[x_{b,t}]+P[t]. (1) Each block uses pre-normalization, U ( ℓ ) = RMSNorm ​ ( X ( ℓ ) ) . U^{(\ell)}=\mathrm{RMSNorm}(X^{(\ell)}). (2) TRC 2 1 combines chunk-level sparse routing, a routed cortical computation, an optional modulation and predictive pathway, an optional associative memory with top-down gating, an optional routing-weight refinement step, and an optional low-rank corrective path. Each subsystem is independently toggleable. Implementation details that are useful for exact reproduction, including padding and tensor layouts, are summarized in Appendix A . Figure 1: TRC 2 architecture block. 3.2 Block computation For one block, let X ∈ ℝ B × T × d X\in\mathbb{R}^{B\times T\times d} be the input and let X + X^{+} denote the output. The core computation is U \displaystyle U = RMSNorm ​ ( X ) , \displaystyle=\mathrm{RMSNorm}(X), (3) ( s route , s pred , s gain ) \displaystyle(s_{\mathrm{route}},s_{\mathrm{pred}},s_{\mathrm{gain}}) = ModCtrl ​ ( U ) , \displaystyle=\mathrm