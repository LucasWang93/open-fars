Title: Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling

Abstract: Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves 2.31times and 2.07times latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX~3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff.

Body: Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling 1 Introduction 2 Related Work 3 Preliminaries 4 Method 4.1 Overview 4.2 Hybrid Parallel Inference Framework 4.3 Adaptive Switching via Denoising Discrepancy 4.4 Theoretical Analysis of Adaptive Switching 4.5 Extensibility to Many GPU Configurations 5 Experiments 5.1 Experimental Setup 5.2 Main Results 5.3 Ablation Study 5.4 Sensitivity Analysis 6 Conclusion A Evaluation of Hybrid Parallelism B Empirical Visualization of Denoising Discrepancy C Adaptive Parallelism Switching Algorithm D Derivation of Score-Based Interpretation of Denoising Discrepancy E Robustness of Determine ùùâ ùüè \boldsymbol{\tau_{1}} under Stochastic Denoising Noise F Extensibility to Many GPU Configurations Structures G Implementation Details H Quantitative Results on the Parallelism Interval ùíå \boldsymbol{k} I Additional Qualitative Results J Qualitative Comparion Results via Different ùíå \boldsymbol{k} Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling Euisoo Jung Byunghyun Kim Hyunjin Kim Seonghye Cho Jae-Gil Lee * School of Computing, KAIST {jyssys, rooknpown, hjkim1228, orangingq, jaegil}@kaist.ac.kr Abstract Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves 2.31 √ó 2.31\times and 2.07 √ó 2.07\times latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX 3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff. Figure 1 : Summary of the proposed hybrid data-pipeline parallelism. Our method consistently outperforms prior distributed approaches across five key aspects: Speed-up , Image Quality , Generality , High-resolution Synthesis , and Communication Cost , demonstrating robust and balanced acceleration-quality trade-offs. ‚Ä† ‚Ä† ‚àó indicates corresponding author. 1 Introduction Figure 2 : Comparison of parallel strategies for diffusion inference. (a) Patch-based data parallel frameworks suffer from bottlenecks caused by all-gather operations and artifacts at patch boundaries, leading to limited acceleration and quality degradation. (b) Pipeline parallel frameworks incur excessive asynchronous communication overhead and accumulate estimate errors. (c) Our hybrid parallelism , which incorporates condition-based data parallelism, adaptively combines both paradigms to achieve high fidelity and fast generation. Diffusion models have emerged as a powerful family of generative models because of their superior sample quality and broad applicability. However, the inherently iterative nature of diffusion processes, which consists of many denoising steps, leads to significant inference latency and computational bottlenecks. As model sizes continue to scale, these inefficiencies become increasingly limiting, making diffusion inference acceleration a pressing research challenge. Existing approaches have focused mainly on reducing the number of sampling steps [ 9 , 19 , 20 , 26 , 27 , 36 , 34 , 37 , 21 ] , designing optimal architectures [ 12 , 13 , 39 , 14 , 38 , 35 ] , or leveraging mathematical approximations [ 1 , 18 , 40 , 40 , 22 , 17 , 28 ] . Yet, these methods often require additional training or fail to deliver strong acceleration in practice, exhibiting a clear trade-off between generation quality and speed. Distributed parallelism across multiple GPUs offers a promising alternative. Using modern parallel computing resources, one can achieve substantial throughput improvements in diffusion inference without additional training. This direction is especially appealing given the success of distributed strategies in natural language processing, where large-scale language models have already benefited from extensive parallelism research [ 24 , 29 ] . As in other domains, distributed parallelism for generative model inference can be broadly classified into data parallelism and pipeline parallelism [ 11 , 2 ] . Both approaches enhance throughput by distributing either the input data or the model itself across multiple GPUs. Representative existing studies include DistriFusion [ 11 ] for data parallelism and AsyncDiff [ 2 ] for pipeline parallelism. In DistriFusion (Figure 2 a), an input image is divided into N N disjoint patches, and these patches are processed in parallel across N N GPUs, where each device independently handles one patch. In AsyncDiff (Figure 2 b), the entire model is divided into N N sequential components, where each component is assigned to a GPU, and the output from the i i -th GPU is asynchronously fed as the input to the ( i + 1 ) (i+1) -th GPU; thus, AsyncDiff enables pipelined execution across devices. In theory, each form of parallelism can improve throughput linearly with respect to the number of GPUs, up to an ideal N √ó N\times speed-up, but in practice, the gains are often sublinear due to communication overhead and synchronization costs. In this paper, we propose a hybrid strategy that combines data and model parallelism to further increase the throughput of generative model inference, achieving beyond-linear scaling relative to the number of GPUs, while maintaining generation quality. That is, if there are two GPUs, we aim to obtain more than a twofold speed-up without noticeable degradation in output fidelity. In practice, when using two GPUs, data and model parallelism achieved 1.2 √ó \times and 1.3 √ó \times speed-up, respectively, whereas our hybrid approach remarkably achieved a 2.3 √ó \times speed-up under the same configuration, as shown in Figures 1 and 2 . To achieve hybrid parallelism, one could combine the aforementioned representative methods. Specifically, an image is divided into disjoint patches, and each patch is fed into a corresponding model component (not necessarily the first one). As a result, each GPU trains a 1 / N 1/N portion of the model using a 1 / N 1/N portion of an input image. This hybrid approach can potentially achieve beyond-linear scaling; however, it may degrade generation quality for two main reasons. First, since each GPU processes only a portion of the image, artifacts are likely to appear particularly along patch boundaries. Second, this issue is exacerbated by asynchronous communication between model components; that is, errors introduced by asynchronous rather than sequential denoising can worsen the artifacts. In this paper, we aim to propose and further optimize the hybrid parallelism for diffusion inference from two complementary perspectives: (1) from the data parallelism perspective, transitioning from patch-based partitioning to condition-based partitioning ; and (2) from the model parallelism perspective, advancing from static parallelism switching to adaptive parallelism switching . (1) Condition-Based Partitioning. The main limitation of patch-based partitioning is that each patch represents only a local subregion of an image, often leading to boundary artifacts and degraded visual coherence. To address this limitation, we leverage the classifier-free guidance (CFG) [ 7 ] , a technique widely adopted in diffusion models, where the model simultaneously predicts conditional (prompted) and unconditional (unprompted) noise estimates. This dual-path prediction naturally provides a meaningful criterion for data partitioning: as shown in Figure 2 c, the conditioned ( x t , c \textbf{x}_{t},c ) and unconditioned ( x t \textbf{x}_{t} ) inputs form two distinct data-parallel paths. Importantly, unlike patch-based partitioning, each image partition covers the entire image, thereby preserving global consistency. Consequently, condition-based partitioning yields improved visual coherence and reduced communication overhead during feature aggregation. (2) Adaptive Parallelism Switching. Because we revise the data partitioning strategy, the pipeline parallelism must also be adapted to align with it. In the early denoising steps, the conditional and unconditional noise estimates differ substantially due to the presence or absence of the condition. Consequently, asynchronous denoising at this stage can lead to divergence between the two paths. To mitigate this issue, we defer the onset of parallel execution until the noise estimates of the two paths become sufficiently similar, beyond the conventional warm-up phase used in prior works (e.g., [ 2 ] ). Similarly, toward the final denoising steps, the noise estimates from the two paths begin to diverge again; at this point, parallel execution is terminated. The specific switching points between serial and parallel execution are determined automatically based on a novel metric, called the denoising discrepancy , which quantifies the difference between the two noise estimates. This adaptive switching mechanism effectively improves generation quality by reducing error propagation, while only marginally shortening the duration of parallel processing. This novel framework demonstrates consistent acceleration not only on conventional denoising diffusion models but also on recent state-of-the-art generative frameworks such as flow matching [ 16 ] . As long as the model follows a sequential denoising process that allows quantifying the relative influence between conditional and unconditional branches, our framework remains robust and effective. Furthermore, due to the nature of pipeline parallelism, it is not restricted to specific architectures such as U-Net or DiT, showing strong generality across diverse networks. As summarized in Figure 1 , our proposed hybrid parallelism achieves superior performance across the five key aspects. In fact, compared to single-GPU inference, our method achieves a 2.3 √ó \times speed-up with two GPUs (i.e., 2 2 ), while preserving generation fidelity. See Appendix A and Section 5 for details of Figure 1 . Finally, the key contributions are summarized as follows. ‚Ä¢ Hybrid Parallelism Framework for Diffusion Inference. We introduce a novel diffusion inference parallelism framework that integrates condition-based partitioning and adaptive parallelism switching into a unified hybrid parallelism design. ‚Ä¢ Novel Condition-Based Partitioning. At the data parallelism level, we exploit the intrinsic mechanism of diffusion by decoupling conditional and unconditional branches and performing multi-GPU denoising. ‚Ä¢ Adaptive Parallelism Switching. To align pipeline parallelism with the behavior of conditional guidance, our method adaptively switches to hybrid parallelism framework during inference. Switching points are automatically determined based on the denoising discrepancy between conditional and unconditional estimates, ensuring generation efficiency throughout the denoising process. ‚Ä¢ Robustness across Models and Architectures. Our framework consistently demonstrates strong acceleration and generation quality across various architectures (e.g., U-Net, DiT) and recent state-of-the-art generative frameworks, such as flow matching, even under high-resolution synthesis settings. Figure 3 : Overview of the proposed diffusion inference hybrid parallel framework. Our method adaptively switches parallelism modes at œÑ 1 \tau_{1} and œÑ 2 \tau_{2} , optimizing the trade-off between computational efficiency and consistency of conditional guidance, and demonstrates superior inference acceleration performance while preserving high generation quality. 2 Related Work Single -GPU Diffusion Acceleration. Research on the acceleration of single -device diffusion inference can be classified into three categories. The first group focuses on reducing the number of sampling steps required for high-quality generation [ 30 , 9 , 19 , 20 , 26 , 27 , 36 , 34 , 40 , 37 , 21 ] . These approaches enable fast sampling by either reformulating the reverse process as an ordinary differential equation (ODE), distilling multi-step models into fewer steps, or directly predicting the reverse process in latent space. The second group targets model architecture optimization, aiming to reduce computational cost through network compression and efficient design [ 12 , 13 , 39 , 14 , 38 , 35 ] . The third group leverages mathematical and algorithmic strategies, either exploiting the mathematical structure of diffusion processes or reusing intermediate computations to further accelerate inference [ 1 , 18 , 40 , 22 , 33 , 17 , 28 ] . While these methods reduce single-device inference time, they are inherently limited by the computational capacity of individual GPUs. Multi -GPU Diffusion Acceleration. Recent studies have explored various distributed parallelism strategies to accelerate diffusion inference using multiple GPUs [ 11 , 2 , 5 , 4 , 32 ] . DistriFusion [ 11 ] introduces a data-parallel approach that divides the input image into independent patches, performing denoising in parallel across GPUs. This work has established a foundational paradigm for parallel diffusion inference. Building on this parallelization idea, AsyncDiff, [ 2 ] introduces model parallelism by dividing the U-Net into layer-wise segments and employing a stride-based scheduling strategy to balance parallel execution, achieving a notable reduction in latency. Subsequently, PipeFusion [ 5 ] and XDiT [ 4 ] combine patch-level parallelism with transformer-oriented parallelism through ring attention. While additional adaptations such as CFG-based data parallelism have been introduced, these methods remain limited to inter-image processing and lack deeper architectural integration. Moreover, transformer-specific schemes such as ring attention exhibit limited scalability and inconsistent performance when applied to general diffusion architectures. More recently, ParaStep [ 32 ] proposes a reuse-then-predict mechanism that leverages the similarity of noise predictions between adjacent denoising steps. By reusing the noise from previous steps before re-prediction, ParaStep enables inter-step parallel