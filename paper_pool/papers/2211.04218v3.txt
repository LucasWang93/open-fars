Title: Clustered Federated Learning based on Nonconvex Pairwise Fusion

Abstract: This study investigates clustered federated learning (FL), one of the formulations of FL with non-i.i.d. data, where the devices are partitioned into clusters and each cluster optimally fits its data with a localized model. We propose a clustered FL framework that incorporates a nonconvex penalty to pairwise differences of parameters. Without a priori knowledge of the set of devices in each cluster and the number of clusters, this framework can autonomously estimate cluster structures. To implement the proposed framework, we introduce a novel clustered FL method called Fusion Penalized Federated Clustering (FPFC). Building upon the standard alternating direction method of multipliers (ADMM), FPFC can perform partial updates at each communication round and allows parallel computation with variable workload. These strategies significantly reduce the communication cost while ensuring privacy, making it practical for FL. We also propose a new warmup strategy for hyperparameter tuning in FL settings and explore the asynchronous variant of FPFC (asyncFPFC). Theoretical analysis provides convergence guarantees for FPFC with general losses and establishes the statistical convergence rate under a linear model with squared loss. Extensive experiments have demonstrated the superiority of FPFC compared to current methods, including robustness and generalization capability.

Body: Clustered Federated Learning based on Nonconvex Pairwise Fusion 1 Introduction 2 Related work 2.1 Heterogeneous federated learning 2.2 Clustering via pairwise fusion penalty 3 Clustered federated learning via nonconvex pairwise fusion 4 Fusion penalized federated clustering 4.1 Network Lasso 4.2 Fusion penalized federated clustering 4.3 Regularization parameter tuning scheme 5 Theoretical guarantees 6 Performance evaluation 6.1 Experimental settings 6.2 Performance comparison 6.3 Regularization parameter tuning 6.4 Practical considerations 6.4.1 Robustness 6.4.2 Generalization to newcomers 6.4.3 AsyncFPFC 6.4.4 Communication-efficient FPFC 7 Conclusion A Details of Algorithm 1 B Proof of Theorem 1 C Proof of Theorem 2 C.1 Proof of Lemma 1 C.2 Proof of Lemma 2 C.3 Proof of Lemma 3 C.4 Proof of Theorem 2 D Statistical convergence rate under a linear model D.1 Notations and preparation D.2 Proof of Theorem 3 Proof of Result 1. Proof of Result 2 E Simulation details and additional numerical experiments E.1 Datasets and models E.2 Additional experiments and results E.2.1 Synthetic data E.2.2 Sensitivity to Initial Values E.2.3 Complete Results on Robustness E.2.4 Effect of Regularization Parameter E.2.5 Heterogeneous training E.2.6 Effect of Œæ ùúâ \xi italic_Œæ License: arXiv.org perpetual non-exclusive license arXiv:2211.04218v3 [cs.LG] 24 Dec 2023 Clustered Federated Learning based on Nonconvex Pairwise Fusion Xue Yu Center for Applied Statistics, School of Statistics, Renmin University of China, Beijing, China {xueyu_2019, ziyiliu, wu.wang, sunyifan}@ruc.edu.cn. Ziyi Liu Center for Applied Statistics, School of Statistics, Renmin University of China, Beijing, China {xueyu_2019, ziyiliu, wu.wang, sunyifan}@ruc.edu.cn. Wu Wang and Yifan Sun Center for Applied Statistics, School of Statistics, Renmin University of China, Beijing, China {xueyu_2019, ziyiliu, wu.wang, sunyifan}@ruc.edu.cn. Abstract This study investigates clustered federated learning (FL), one of the formulations of FL with non-i.i.d. data, where the devices are partitioned into clusters and each cluster optimally fits its data with a localized model. We propose a clustered FL framework that incorporates a nonconvex penalty to pairwise differences of parameters. Without a priori knowledge of the set of devices in each cluster and the number of clusters, this framework can autonomously estimate cluster structures. To implement the proposed framework, we introduce a novel clustered FL method called Fusion Penalized Federated Clustering (FPFC). Building upon the standard alternating direction method of multipliers (ADMM), FPFC can perform partial updates at each communication round and allows parallel computation with variable workload. These strategies significantly reduce the communication cost while ensuring privacy, making it practical for FL. We also propose a new warmup strategy for hyperparameter tuning in FL settings and explore the asynchronous variant of FPFC (asyncFPFC). Theoretical analysis provides convergence guarantees for FPFC with general losses and establishes the statistical convergence rate under a linear model with squared loss. Extensive experiments have demonstrated the superiority of FPFC compared to current methods, including robustness and generalization capability. 1 Introduction Increasing amounts of data are generated by end users‚Äô own devices (also called clients), such as mobile phones, wearable devices, and autonomous vehicles. The fast-growing storage and computational capacities of these terminal devices, coupled with concerns over privacy, have led to a growing interest in federated learning (FL) [ 33 , 25 ] . Standard FL approaches collaboratively train a shared global model for all devices while maintaining data on each device [ 27 , 43 , 45 , 34 , 26 ] . As such, critical concerns such as privacy, security, and data access rights are well-addressed in the FL framework. However, as discussed in [ 50 ] , the local data stored on each device usually have heterogeneous conditional distributions due to diverse user characteristics. The data and statistical heterogeneity observed across different devices obfuscate the efforts to build a global model in many FL applications. In addressing the limitations of a global model, a recent proposal, clustered FL [ 50 , 17 , 41 , 61 ] , assumes that devices can be grouped into clusters, where devices within the same cluster share identical models. Cluster structures are very common in many applications, such as recommendation systems [ 32 ] and precision medicine [ 29 ] . Thus, clustered FL has drawn increasing attention and is considered a formulation of FL that can address the data heterogeneity problem. However, the key issue regarding clustered FL is that the set of devices in each cluster is unknown a priori . To tackle this challenge, several methods have been proposed. Sattler et al. [ 50 ] proposed a clustered FL (CFL) algorithm under the clustered FL framework in the spirit of the hierarchical divisive clustering algorithm. However, the clustering procedure is implemented at the central server, resulting in high computation costs, especially when the number of devices is large. Inspired by the classical K-means algorithm, Mansour et al. [ 41 ] introduced the HYPCLUSTER algorithm and Ghosh et al. [ 17 ] presented the Iterative Federated Clustering Algorithm (IFCA). More recently, Marfoq et al. [ 42 ] developed the Federated Expectation-Maximization (FedEM) algorithm, which offers a versatile approach for clustered FL and personalized FL. Although the three algorithms eliminate the need for centralized clustering procedures, they require specifying the number of clusters, and the clustering results are usually sensitive to the initial values. Motivated by the limitations mentioned above in current studies, we propose a fusion penalization-based framework for clustered FL and develop a novel clustered FL method called fusion penalized federated clustering (FPFC). In this work, we introduce a nonconvex function, such as the smoothly clipped absolute deviation (SCAD) penalty [ 14 ] , to penalize the pairwise differences of local model parameters. The penalty function can force pairs of local parameters to be exactly equal if they are close to each other, effectively creating clusters within the local parameters. This method places the clustered FL on a solid theoretical footing based on a well-defined objective function. In addition to conventional convergence analysis of algorithms, the statistical properties, e.g., the consistency of identifying the cluster structure of devices, of this method can be rigorously established. The main contributions of our work can be summarized as follows: ‚Ä¢ We formulate clustered FL as a penalized optimization problem, where the nonconvex fusion penalization term shrinks the pairwise differences of model parameters of devices and encourages minimal differences, thus clustering the local devices. ‚Ä¢ Under the proposed framework, we develop a novel method called fusion penalized federated clustering (FPFC) to address general clustered FL problems. This method can automatically determine the cluster membership of devices and derive optimal models specific to each cluster in a distributed setting, without requiring prior knowledge regarding the number of clusters and the composition of devices within each cluster. ‚Ä¢ The proposed method is a general framework that can handle learning tasks with nonconvex losses, such as neural networks. Regarding the algorithm, we extend the standard Alternating Direction Method of Multipliers (ADMM) algorithm to the FL setup. Specifically, we decouple the joint optimization problem into a series of subproblems that can be solved by local devices in parallel, in the meantime, the communication cost is greatly reduced compared to similar methods. Moreover, it allows for inexact minimization of each subproblem, further enhancing computational efficiency. ‚Ä¢ We introduce a novel warmup strategy for hyperparameter tuning in the FL setting. This approach significantly reduces the communication cost compared to the conventional cross-validation method, while simultaneously achieving better performance. ‚Ä¢ Theoretically, we establish the convergence rate of FPFC for general nonconvex losses under standard assumptions. More importantly, unlike IFCA [ 17 ] , which assumes full device participation to achieve convergence, our analysis allows partial participation by selecting a subset of devices to perform local updates at each communication round. Furthermore, the statistical convergence rate of the proposed method has been established under a linear model with squared loss. ‚Ä¢ Finally, we conduct extensive experiments to compare our method with state-of-the-art approaches. We explore the performance of FPFC in aspects of robustness, generalization capability, and communication efficiency. We also introduce several variants of FPFC, including asynchronous updates. The results consistently demonstrate the versatility and effectiveness of FPFC across a diverse range of settings. This paper is structured as follows. Section 2 outlines related works. Section 3 provides the relevant background and formulates the main problem. Section 4 details the proposed FPFC algorithm and the regularization parameter tuning strategy. Section 5 presents the theoretical results of FPFC and its statistical properties. The experimental results are presented in Section 6. Section 7 concludes the paper. 2 Related work As this work addresses several issues of existing clustered FL algorithms via a pairwise fusion penalty, we review the related work in two subtopics: heterogeneous FL and clustering via pairwise fusion penalty. 2.1 Heterogeneous federated learning Federated learning has sparked significant interest since it was first proposed by [ 43 ] . The most popular method is the FederatedAveraging (FedAvg) algorithm, and it has been shown to work well empirically, particularly for non-convex problems. FedAvg converges when the data is i.i.d [ 54 ] , but it can diverge in practical settings when the data on the users‚Äô devices are non-i.i.d, i.e., heterogeneous [ 35 ] . Several formulations have been proposed to tackle these heterogeneity issues, such as adding some adaptation terms to the global model, personalized FL, and clustered FL. Karimireddy et al. [ 26 ] pointed out that FedAvg may suffer from device drift, so they used control variables to reduce the drift between different devices and achieve favorable performance. Li et al. [ 34 ] applied a proximal term to limit the impact of statistical heterogeneity under an assumption of bounded local dissimilarity. When data distributions across devices exhibit significant non-i.i.d. characteristics, it becomes challenging to train a global model that performs well across all devices. Another choice is to build personalized models for each device instead of learning a single shared model. As one of the formulations, personalized FL aims to jointly train personalized models that can work better than the global model and the local models without collaboration. The most common method for personalization involves two steps [ 28 ] . First, a global model is trained under the FL framework, and then each device can train a customized model with its private data based on the global model [ 59 , 16 ] . [ 13 ] proposed Personalized FedAvg (Per-FedAvg) to train a shared model that performs well after each device updates it by running a few steps of gradient descent with respect to its own data. Another efficient technique is to regularize personalized models using the relationships between related devices [ 62 , 20 , 55 , 24 ] . Smith et al. [ 52 ] suggested a multi-task learning (MTL) framework to handle the statistical challenges of FL and proposed MOCHA to address the systems challenges of federated MTL based on the relationships among tasks. Lin et al. [ 37 ] proposed to project local models into a shared-and-fixed low-dimensional random subspace and use infimal convolution to achieve personalization. The formulation of clustered FL has been proposed in recent years. Instead of building a unified model for all devices, clustered FL aims to construct a set of heterogeneous models, each corresponding to one cluster of devices. As one of the earliest clustered FL methods, clustered FL (CFL) [ 50 ] follows the idea of the divisive hierarchical clustering algorithm. Specifically, it divides the devices recursively in a top-down manner, relying on the cosine similarity between the gradient updates of the devices. This method has several favorable properties. In particular, it is applicable to general nonconvex losses, does not require the number of clusters to be known a priori , and comes with theoretical guarantees on the clustering quality. However, it also has several limitations. Practically, optimally bisecting devices requires significant computational resources for the central server, especially when dealing with a large number of devices. Theoretically, its convergence behavior has not been rigorously characterized. Another line of research aims to minimize an objective function (without penalization) involving possible clusterings by using some iterative algorithms that are in the similar spirit of K-means clustering, e.g., Iterative Federated Clustering Algorithm (IFCA) [ 17 ] and HYPCLUSTER [ 41 ] . Although their convergence behaviors have been rigorously characterized, these methods need to know the number of clusters in advance and rely on a relatively good initialization to ensure optimal results. Moreover, convergence analysis of IFCA has only been shown for strongly convex losses, although it has good performances in some nonconvex settings. Federated Expectation-Maximization (FedEM) [ 42 ] is a state-of-the-art federated multi-task learning method with clustered FL as a special case. Similar to IFCA and HYPCLUSTER, it requires specification of the number of clusters and underlying data distribution, which is infeasible in practice. Recently, Cho et al. [ 9 ] proposed a framework named COMET which combines clustered knowledge transfer and personalized federated learning to reduce communication costs and accommodate model heterogeneity. Similar to IFCA, COMET also utilizes the K-means clustering algorithm on the server for clustering. Devices are required to have access to a public dataset in this framework. Additionally, Learning Across Domains and Devices (LADD) [ 51 ] applied K-means clustering to partition devices into clusters based on the image styles of each device in autonomous driving applications. To address the challenges of the straggler effect in synchronous FL and model staleness in asynchronous FL, Zhang et al. [ 64 ] presented a semi-asynchronous clustered federated learning framework. They utilized spectral clustering on an affinity matrix that incorporates information about devices‚Äô 