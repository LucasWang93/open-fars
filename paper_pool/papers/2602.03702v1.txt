Title: Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging

Abstract: Large language models are increasingly trained in continual or open-ended settings, where the total training horizon is not known in advance. Despite this, most existing pretraining recipes are not anytime: they rely on horizon-dependent learning rate schedules and extensive tuning under a fixed compute budget. In this work, we provide a theoretical analysis demonstrating the existence of anytime learning schedules for overparameterized linear regression, and we highlight the central role of weight averaging - also known as model merging - in achieving the minimax convergence rates of stochastic gradient descent. We show that these anytime schedules polynomially decay with time, with the decay rate determined by the source and capacity conditions of the problem. Empirically, we evaluate 150M and 300M parameter language models trained at 1-32x Chinchilla scale, comparing constant learning rates with weight averaging and $1/\sqrt{t}$ schedules with weight averaging against a well-tuned cosine schedule. Across the full training range, the anytime schedules achieve comparable final loss to cosine decay. Taken together, our results suggest that weight averaging combined with simple, horizon-free step sizes offers a practical and effective anytime alternative to cosine learning rate schedules for large language model pretraining.

Body: Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging 1 Introduction 1.1 Main Contributions 2 Related Work Finite dimensional SGD analysis. Nonparameteric Least Squares. Learning rate scheduling and averaging in practice. Continual learning and anytime training. 3 Empirical Findings Architecture and Dataset. Training and evaluation details. Averaging. Choice of γ \gamma . 3.1 Anytime Schedules are Competitive 3.2 Large Batch Setting 4 Theoretical Analysis Setup and notation. 4.1 Main results Constant learning rate. WSD. 5 Discussion and Conclusions A Proofs of Section 4 Helper lemmas. Setup. A.1 Proof of Theorem 1 Bias bound. Variance bound. Variance head. Variance tail. A.2 Proof of Corollary 1 A.3 Proof of Theorem 2 Bias bound. Variance bound. B Additional Figures B.1 Figure 2 at optimal hyperparamters B.2 Figure 3 at optimal hyperparameters B.3 Additional σ 2 \sigma^{2} values for synthetic experiments B.4 Optimal cosine envelope Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging Alexandru Meterez Harvard University Kempner Institute at Harvard University Pranav Ajit Nair Harvard University Kempner Institute at Harvard University Depen Morwani Harvard University Kempner Institute at Harvard University Cengiz Pehlevan Harvard University Kempner Institute at Harvard University Sham Kakade Harvard University Kempner Institute at Harvard University Abstract Large language models are increasingly trained in continual or open-ended settings, where the total training horizon is not known in advance. Despite this, most existing pretraining recipes are not anytime: they rely on horizon-dependent learning rate schedules and extensive tuning under a fixed compute budget. In this work, we provide a theoretical analysis demonstrating the existence of anytime learning schedules for overparameterized linear regression, and we highlight the central role of weight averaging—also known as model merging—in achieving the minimax convergence rates of stochastic gradient descent. We show that these anytime schedules polynomially decay with time, with the decay rate determined by the source and capacity conditions of the problem. Empirically, we evaluate 150M and 300M parameter language models trained at 1–32× Chinchilla scale, comparing constant learning rates with weight averaging and 1 / t 1/\sqrt{t} schedules with weight averaging against a well-tuned cosine schedule. Across the full training range, the anytime schedules achieve comparable final loss to cosine decay. Taken together, our results suggest that weight averaging combined with simple, horizon-free step sizes offers a practical and effective anytime alternative to cosine learning rate schedules for large language model pretraining. 1 Introduction † † footnotetext: ∗ : Equal contribution. Correspondence to: ameterez@g.harvard.edu, pranavnair@g.harvard.edu, dmorwani@g.harvard.edu Since its introduction in the seminal paper by Loshchilov and Hutter ( 2016 ) , cosine decay has become the de facto learning rate scheduler for large language model (LLM) pretraining, being used by practitioners for training frontier models across a wide range of scales and architectures (Dubey et al. , 2024 ; Olmo et al. , 2025a ; Liu et al. , 2024 ) . Despite its widespread adoption, cosine decay is a horizon dependent scheduler, since it requires knowing the length of the training run ahead of time, making it ill suited for continual learning setups in which data is continuously received and mixed into the pretraining run. Recent work (Hu et al. , 2024 ; Wen et al. , 2024 ) has introduced the warmup-stable-decay (WSD) schedule as an anytime - meaning horizon free - alternative, achieving competitive performance to cosine decay (Team et al. , 2025 ) . WSD consists of 3 3 phases, namely a linear learning rate warm-up phase up to a maximum learning rate, then keeping the learning constant until 90 % 90\% of the training length, followed by decaying the learning rate for the rest of the 10 % 10\% . Figure 1: 150M model: Cosine schedules do not transfer across horizons. The cosine envelope (red) is formed by independently tuning a horizon-aware cosine schedule for each terminal compute budget ( 1 × 1\times – 32 × 32\times Chinchilla) and taking the best validation loss at that horizon. Gray curves show the same cosine schedule evaluated at intermediate checkpoints when tuned for a single fixed terminal budget. The gap illustrates why cosine decay is not anytime: tuning for a long horizon can be far from optimal at shorter budgets. An analogous plot for the 300M model appears in Figure 10(b) (Appendix B ). While this is not a strictly anytime schedule, it only requires minimal overhead in the form of storing checkpoints of the training run at sparse intervals, from which one can either extend the constant learning rate phase in case of new data, or decay in order to get the final performance. Another viable alternative to learning rate decay is weight averaging (Polyak and Juditsky, 1992 ) , also known in literature as model merging (Li et al. , 2025 ) . Weight averaging involves maintaining an average of the most recent proportion of iterates, either explicitly by averaging over the last N N iterates, or through an exponential moving average (EMA) controlled by a parameter β \beta such that β ≈ 1 − 1 N \beta\approx 1-\tfrac{1}{N} , and using the averaged model for evaluation, while still using only the last iterate for training. Weight averaging has been used in schedule-free algorithms (Defazio et al. , 2024 ; Morwani et al. , 2025 ) and as an alternative for learning rate decay in order to decrease the variance (Zou et al. , 2023 ; Zhang et al. , 2024b ; Meterez et al. , 2025 ) . In practice, one can maintain several EMAs at a time at various β \beta values, each having the cost of an extra copy of the parameter weights. Anytime learning rate schedules such as 1 / t γ 1/t^{\gamma} for γ 1 \gamma 1 have been studied in the literature (Ge et al. , 2019 ) , however unless weight averaging is used these schedules generally do not achieve minimax optimal rates for SGD in linear regression (Shamir and Zhang, 2013 ; Lacoste-Julien et al. , 2012 ; Rakhlin et al. , 2011 ) . In this work we investigate, theoretically and empirically, how 1 / t γ 1/t^{\gamma} , constant learning rate and WSD compare to cosine decay in long training runs, and which of these schedules provides a viable anytime, or almost-anytime, alternative to cosine annealing. Concretely, we require two properties from an anytime scheduler: (i) the schedule should not depend on the planned number of training steps, and (ii) for any intermediate duration T T , it should be competitive with a well-tuned cosine schedule run for T T steps—i.e., it should track the cosine envelope that these tuned cosine schedules define across checkpoints in a long run. Figure 1 highlights why this is a nontrivial requirement: cosine schedules tuned for a single terminal horizon are far from optimal when evaluated at intermediate checkpoints. Put differently, standard training recipes without knowing the stopping time do not yield an anytime procedure, because the choice of horizon implicitly determines the entire trajectory of losses. To the best of our knowledge, this envelope perspective has not been explicitly studied or used as an evaluation target in prior work. This motivates the central question of this paper: when can a horizon-free (or nearly horizon-free) training procedure match—or even improve upon—the cosine envelope across training time? Our goal is to propose alternatives that are competitive with a single cosine run at a fixed endpoint, and to characterize (theoretically and empirically) when matching the envelope is achievable. We show that simple anytime schedules such as constant or 1 / t γ 1/t^{\gamma} with appropriate averaging, can closely follow the envelope over long runs. Figure 2: Top row: 150M; bottom row: 300M. Left: Validation loss versus training compute for cosine decay, constant LR with averaging, WSD, and a 1 / t 1/\sqrt{t} -type schedule with averaging. Specifically, the 1 / t 1/\sqrt{t} schedule uses a multiplicative factor α / ( t + α ) \sqrt{\alpha/(t+\alpha)} , and we tune α \alpha . Each point corresponds to training for 1 × , 2 × , 4 × , 8 × , 16 × , 1\times,2\times,4\times,8\times,16\times, and 32 × 32\times the Chinchilla compute scale. Cosine baselines are trained as separate runs tuned for each duration; in contrast, the anytime schedules come from a single run trained to 32 × 32\times . Red stars mark the per-duration optimal cosine envelope; the red curve shows the cosine schedule tuned for the full 32 × 32\times run (and 16 × 16\times , respectively). For WSD, we apply a linear decay over the final 90 % 90\% of training, starting from the same run used for constant LR with averaging. Right: Loss difference relative to cosine at each compute multiple (negative = better than cosine). Hyperparameters are chosen to be near-optimal across intermediate checkpoints; per-checkpoint optimal losses are reported in Figure 5 (Appendix B ). 1.1 Main Contributions We first state an informal version of our main theoretical result: Theorem (Informal version of Theorem 1 ) . For an SGD process run on N N samples, a polynomially decaying learning rate of the form η t = 1 / t γ \eta_{t}=1/t^{\gamma} with tail averaging matches the rates of well-tuned SGD with averaging, where 0 γ 1 0 \gamma 1 and the exponent γ \gamma depends on the spectral properties of the data. This result shows that an anytime learning-rate schedule can achieve the same rate as well-tuned SGD. In contrast, Zhang et al. ( 2024b ) show that while a constant learning rate with weight averaging can also attain these rates, for certain source and capacity exponents the learning rate must be scaled as a function of the training horizon (i.e., it depends on the end time) to achieve minimax rates, and is therefore not an anytime scheme. Guided by our theoretical analysis, we empirically compare three anytime schemes— 1 / t 1/\sqrt{t} , WSD, and a constant learning rate with weight averaging—against cosine decay. We train 150M- and 300M-parameter models at power-of-two multiples of the Chinchilla compute budget: from 1 × 1\times to 32 × 32\times for 150M, and from 1 × 1\times to 16 × 16\times for 300M. For cosine decay, each model is trained separately at each compute budget, whereas the anytime methods are trained once at the largest Chinchilla multiple and evaluated at intermediate checkpoints, as shown in Figure 2 . Across all intermediate points, including very long training regimes, the anytime methods closely match cosine annealing, paying only a negligible performance hit near the start and end of training. We expand the discussion on empirical contribution in Section 3 . 2 Related Work Finite dimensional SGD analysis. There is a wide body of literature studying risk bounds in stochastic gradient descent, both in the finite dimensional regime - data covariance has finite rank, and in the infinite dimensional/nonparametric setup. Polyak and Juditsky ( 1992 ); Bach and Moulines ( 2013 ) introduced averaged SGD as an algorithm to achieve improved SGD convergence rates. Défossez and Bach ( 2015 ) analyzed constant learning rate with averaged iterates in the strongly convex case, providing rates for the bias and variance terms, with similar proofs being shown in Jain et al. ( 2017 ); Dieuleveut and Bach ( 2016 ) . This analysis has been extended to minibatch gradient descent (Jain et al. , 2018b ) and streaming algorithms (Frostig et al. , 2015 ) . When the horizon is known in advance, Jain et al. ( 2019 ) have shown that a carefully designed step size sequence can be minimax optimal for last iterate SGD, building up on previous work by Shamir and Zhang ( 2013 ); Harvey et al. ( 2019 ) . Ge et al. ( 2019 ) have shown that geometrically decaying step sizes are only log \log condition number suboptimal. Pan et al. ( 2021 ) have shown that a more nuanced step size design can remove this suboptimality. Nonparameteric Least Squares. Recent work by Zhang et al. ( 2024a ) have shown that for power law spectra and under certain conditions on source and capacity exponents, averaging can be minimax optimal. Other schedules have been analyzed in a similar way (Zou et al. , 2023 ; Wu et al. , 2022a , b ; Meterez et al. , 2025 ) . From a statistical physics point of view, Bordelon and Pehlevan ( 2021 ) have established precise asymptotics for SGD in the overparameterized regime, with a similar analysis being done by Atanasov et al. ( 2024 , 2025 ) using tools from random matrix theory. More recently, (Zhang et al. , 2024b ) have proposed a critical batch size scaling in pretraining, by choosing the batch size that balances the bias and variance rates in the quadratic analysis. Learning rate scheduling and averaging in practice. These analyses have given rise to several new algorithms with practical impact in neural network training. Defazio et al. ( 2023 ) have analyzed the linear decay schedule and have proposed further refinements for this schedule using the gradient norms. Linearly decaying to zero has been studied empirically by Bergsma et al. ( 2025 ) , achieving competitive performance to cosine annealing. Defazio et al. ( 2024 ) have proposed a schedule-free optimizer, which takes advantage of tail averaging to remove the need for learning rate scheduling and an improved variant of momentum (Jain et al. , 2018a ) , as shown by Morwani et al. ( 2025 ) . Hägele et al. ( 2024 ) have shown that using stochastic weight averaging removes the need for learning rate scheduling, being comparable empirically to cosine annealing. It has been empirically observed that averaging can also lead to flatter minima and improved generalization performance on multiple image tasks (Izmailov et al. , 2018 ) . More recently, stochastic weight averaging has obtained impressive results in the AlgoPerf competition (Kasimbeg et al. , 2025 ; Dahl et al. , 2023 ) , across multiple downstream tasks (Ajroldi et al. , 2025 ) . Weight averaging has also been commonly applied in image generation and diffusion (Yazıcı et al. , 2018 ; Karras et al. , 2024 ; Song et al. , 2020 ) . Other variants of weight EMA have been used in practice (Kaddour, 2022 ; Sanyal et al. , 2023 ; Li et al. , 2024 ; Morales-Brotons et al. , 2024 ; Arpit et al. , 2022 ; Wortsman et al. , 2022 ) . Continual learning and anytime training. There is a growing body of literature studying continual learning and pretraining. Generally, continual learning refers to training a model on a sequence of (possibly orthogonal) tasks, ensuring that the model does not forget any of them (Wang et al. , 2024 ; Aljundi et al. , 2019 ; Veniat et al. , 2020 ; Cossu et al. , 2024 ; Ramasesh et al. , 2021 ; Mehta et al. , 2023 ; Joudaki et al. , 2025 ) . Anytime training (Caccia et al. , 2022 ; Ibra