Title: Astra: Activation-Space Tail-Eigenvector Low-Rank Adaptation of Large Language Models

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, especially LoRA, are widely used for adapting pre-trained models to downstream tasks due to their computational and storage efficiency. However, in the context of LoRA and its variants, the potential of activation subspaces corresponding to tail eigenvectors remains substantially under-exploited, which may lead to suboptimal fine-tuning performance. In this work, we propose Astra (Activation-Space Tail-Eigenvector Low-Rank Adaptation), a novel PEFT method that leverages the tail eigenvectors of the model output activations-estimated from a small task-specific calibration set-to construct task-adaptive low-rank adapters. By constraining updates to the subspace spanned by these tail eigenvectors, Astra achieves faster convergence and improved downstream performance with a significantly reduced parameter budget. Extensive experiments across natural language understanding (NLU) and natural language generation (NLG) tasks demonstrate that Astra consistently outperforms existing PEFT baselines across 16 benchmarks and even surpasses full fine-tuning (FFT) in certain scenarios.

Body: Astra: Activation-Space Tail-Eigenvector Low-Rank Adaptation of Large Language Models 1 Introduction 2 Related Work PEFT. LoRA and Its Variants. 3 Method 3.1 Preliminaries of LoRA‚Äôs Initialization 3.2 Activation-Space Tail-Eigenvector Low-Rank Adaptation Step 1: Orthogonal Decomposition. Step 2: Tail-Subspace Projection. 4 Experiments 4.1 Baselines 4.2 Natural Language Understanding Models and Datasets. Implementation Details. Main Results. 4.3 Natural Language Generation Models and Datasets. Implementation Details. Main Results. 4.4 Ablation Studies Eigenvectors. LoRA Rank. Calibration Data. 5 Discussion Enhancing Representation Capacity via Increased Effective Rank. 6 Conclusion A Overview and Comparison of LoRA Variants B Details of Benchmark datasets B.1 Benchmarks of Natural Language Understanding B.2 Benchmarks of Natural Language Generation C Experimental Setup and Implementation Details C.1 Experimental Details of NLU C.2 Experimental Details of NLG D Additional Experimental Results D.1 Experiments on Various Eigenvectors D.2 Experiments on NLG D.2.1 Loss and Gradient-norm Curves for LLaMA2-7B D.2.2 Loss and Gradient-norm Curves for LLaMA3-8B E Case Study F PyTorch-like Pseudocode Astra: Activation-Space Tail-Eigenvector Low-Rank Adaptation of Large Language Models Kainan Liu ‚Ä† , Yong Zhang ‚Ä† , Ning Cheng * , Yun Zhu , Yanmeng Wang , Shaojun Wang , Jing Xiao , Ping An Technology (Shenzhen) Co., Ltd., China github.com/LyoAI/Astra Abstract Parameter-Efficient Fine-Tuning (PEFT) methods, especially LoRA, are widely used for adapting pre-trained models to downstream tasks due to their computational and storage efficiency. However, in the context of LoRA and its variants, the potential of activation subspaces corresponding to tail eigenvectors remains substantially under-exploited, which may lead to suboptimal fine-tuning performance. In this work, we propose Astra ( A ctivation- S pace T ail-Eigenvector Low- R ank A daptation), a novel PEFT method that leverages the tail eigenvectors of the model output activations‚Äîestimated from a small task-specific calibration set‚Äîto construct task-adaptive low-rank adapters. By constraining updates to the subspace spanned by these tail eigenvectors, Astra achieves faster convergence and improved downstream performance with a significantly reduced parameter budget. Extensive experiments across natural language understanding (NLU) and natural language generation (NLG) tasks demonstrate that Astra consistently outperforms existing PEFT baselines across 16 benchmarks and even surpasses full fine-tuning (FFT) in certain scenarios. Astra: Activation-Space Tail-Eigenvector Low-Rank Adaptation of Large Language Models Kainan Liu ‚Ä† , Yong Zhang ‚Ä† , Ning Cheng * , Yun Zhu, Yanmeng Wang , Shaojun Wang , Jing Xiao , Ping An Technology (Shenzhen) Co., Ltd., China github.com/LyoAI/Astra ‚Ä† ‚Ä† footnotemark: ‚Ä† ‚Ä† footnotetext: ‚Ä† Equal contribution. * Corresponding author. 1 Introduction Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks ( Achiam et al. , 2023 ; Dubey et al. , 2024 ; Guo et al. , 2025 ). Adapting these pretrained models typically involves full fine-tuning (FFT). Despite its efficacy, FFT incurs substantial computational and memory overhead, limiting its applicability in resource-constrained settings ( Singh et al. , 2024 ; Liu et al. , 2024a ). To overcome these limitations, parameter-efficient fine-tuning (PEFT) methods have been proposed as an effective alternative. By introducing a small number of trainable parameters while keeping the pretrained backbone frozen, PEFT methods substantially reduce training costs while retaining competitive performance ( Liu et al. , 2021 ; Li and Liang, 2021 ; Hu et al. , 2023a ). Among existing PEFT approaches, Low-Rank Adaptation (LoRA) ( Hu et al. , 2022 ) stands out for its simplicity and strong empirical performance. However, the standard initialization of LoRA often yields extremely small gradients at the early training stage, which can hinder optimization and lead to slow convergence or suboptimal adaptation ( Meng et al. , 2024 ; Wang et al. , 2024b ). To address this issue, recent studies have explored improved initialization strategies, which can be broadly classified into two categories: weight-driven methods ( Meng et al. , 2024 ; Wang et al. , 2024a ) that exploit the structure of pretrained weights, and data-driven methods ( Yang et al. , 2024 ; Wang et al. , 2024b ; Paischer et al. , 2024 ) that leverage data distributions or task-specific signals to guide the initialization of low-rank adapters. However, most existing works overlook an important characteristic of LLM representations: dimensions corresponding to tail eigenvalues remain largely under-utilized during fine-tuning ( Nayak et al. , 2025 ). While these directions contribute little to the dominant pretrained representations, they offer substantial flexibility for task-specific adaptation. Adapting model parameters within such under-explored subspaces can effectively increase the model‚Äôs representational rank ( Roy and Vetterli, 2007 ), thereby enhancing expressive capacity and improving adaptability to downstream tasks. Building on this insight, we propose Astra ( A ctivation- S pace T ail-Eigenvector Low- R ank A daptation), a novel PEFT method that exploits under-explored tail subspaces of output activations to construct task-adaptive low-rank adapters. Specifically, Astra first performs eigendecomposition on the covariance matrix of output activations estimated from a small task-specific calibration dataset D D , i.e., C ‚Äã o ‚Äã v ‚Äã ( Y ) = Q ‚Äã Œõ ‚Äã Q ‚ä§ Cov(Y)=Q\Lambda Q^{\top} , where Q Q denotes the eigenvectors and Œõ \Lambda is the diagonal matrix of corresponding eigenvalues. To explicitly constrain optimization within the under-utilized subspaces, Astra projects the pretrained weight matrix W W onto the subspace spanned by the tail eigenvectors, yielding low-rank adapters aligned with the under-utilized activation directions, i.e., A = Q [ : , ‚àí r : ] ‚ä§ ‚Äã W A=Q_{[:,-r:]}^{\top}W and B = Q [ : , ‚àí r : ] B=Q_{[:,-r:]} , where r r denotes the LoRA rank. This initialization strategy offers several advantages. By focusing adaptation on activation dimensions that are weakly optimized during pretraining, Astra increases the effective representational rank while avoiding interference with dominant pretrained features. As a result, it enables more expressive task-specific updates with improved optimization stability and faster convergence. We conduct extensive experiments across a diverse set of tasks to evaluate the effectiveness of Astra, covering both natural language understanding (NLU) and natural language generation (NLG) benchmarks. Experimental results show that Astra consistently outperforms existing PEFT baselines on 16 benchmarks and even surpasses full fine-tuning (FFT) on several tasks. Our main contributions can be summarized as follows: ‚Ä¢ We propose Astra , a novel LoRA initialization method that exploits under-utilized eigenspaces of output activations to enable effective low-rank adaptation. ‚Ä¢ We conduct extensive evaluations on a broad spectrum of NLU and NLG tasks, including general language understanding, mathematical reasoning, code generation, and commonsense reasoning. The results demonstrate that Astra consistently outperforms strong PEFT baselines and exhibits robust adaptability across tasks. ‚Ä¢ We perform comprehensive ablation studies on eigenvector selection, LoRA rank, and calibration data, systematically validating the effectiveness and efficiency of Astra. ‚Ä¢ We provide an effective-rank analysis that empirically supports our core hypothesis: adapting within under-explored activation subspaces enhances task-specific representational capacity and improves downstream task performance. 2 Related Work PEFT. Parameter-efficient fine-tuning (PEFT) provides a lightweight alternative to full fine-tuning by updating only a small subset of parameters, reducing computational overhead while maintaining strong downstream performance. PEFT methods can be broadly categorized into: (i) prompt-based ( Li and Liang, 2021 ; Liu et al. , 2021 ), which prepend trainable tokens or embeddings to the input; (ii) adapter-based ( Houlsby et al. , 2019 ; R√ºckl√© et al. , 2020 ; Hu et al. , 2023a ), which insert small trainable modules into each transformer layer; and (iii) LoRA-based ( Hu et al. , 2022 ; Dettmers et al. , 2023 ), which employ low-rank reparameterization to enable efficient adaptation. LoRA and Its Variants. Low-rank adaptation (LoRA) has received significant attention for enabling fine-tuning without altering the original architecture or adding inference overhead ( Li et al. , 2018 ; Aghajanyan et al. , 2021 ). Building on LoRA, subsequent research has explored several directions to enhance its flexibility and efficiency: Dynamic rank allocation methods ( Valipour et al. , 2022 ; Liu et al. , 2024c ), such as AdaLoRA ( Zhang et al. , 2023b ), adaptively distribute parameter budgets across weight matrices based on importance scores. Structural modifications ( Liu et al. , 2024b ; Feng et al. , 2024 ; Li et al. , 2024 ) generalize LoRA beyond its original design. For example, DoRA ( Liu et al. , 2024b ) decouples learning into magnitude and direction. Hyperparameter optimization has also been explored to improve fine-tuning efficiency ( Kalajdzievski, 2023 ; Hayou et al. , 2024 ). For instance, LoRA+ ( Hayou et al. , 2024 ) introduces differential learning rates for the low-rank matrices A and B, with a higher learning rate for B to accelerate convergence. Initialization Strategies. Recent efforts have also explored initialization strategies to stabilize training and accelerate convergence ( Meng et al. , 2024 ; Wang et al. , 2024a ; Yang et al. , 2024 ; Wang et al. , 2024b ), which can be divided into weight-driven and data-driven approaches. Weight-driven methods primarily analyze the static geometric properties of pre-trained parameters. For example, PiSSA ( Meng et al. , 2024 ) applies Singular Value Decomposition (SVD) to the pretrained weight matrix W ( 0 ) W^{(0)} , capturing the principal singular values to initialize the adapters while relegating the residual components to the frozen weights. While effective, weight-driven methods neglect the dynamic input distribution encountered during inference. In contrast, data-driven approaches incorporate task-specific information into the initialization process. LoRA-GA ( Wang et al. , 2024b ) introduces a novel initialization scheme that aligns the initial gradients of the low-rank update matrices with those of full fine-tuning. Our proposed framework, Astra, represents a distinct paradigm within the data-driven category. Unlike gradient-based methods like LoRA-GA that require computationally expensive gradient estimation, Astra operates on the activation-space covariance. By identifying the tail subspace of the output activations, Astra ensures that the adapters are initialized within directions that are statistically under-utilized by the pre-trained model yet functionally critical for the downstream task. This unique activation-centric perspective allows Astra to capture task-specific functional redundancies that are overlooked by purely geometric weight analysis during adaptation. A detailed comparison of LoRA variants is provided in Appendix A . 3 Method We introduce Astra, a LoRA initialization method based on the observation that pretrained models exhibit uneven utilization across activation dimensions. In standard LoRA, low-rank adapters are initialized without considering this structural property, which may lead to inefficient early-stage optimization. Astra initializes LoRA updates within activation subspaces associated with smaller eigenvalues, aiming to make use of under-utilized directions during task-specific adaptation. 3.1 Preliminaries of LoRA‚Äôs Initialization LoRA ( Hu et al. , 2022 ) introduces trainable updates by reparameterizing weight modifications as the product of two low-rank matrices. Formally, given a pretrained weight matrix W 0 ‚àà ‚Ñù m √ó n W_{0}\in\mathbb{R}^{m\times n} , LoRA expresses the adapted weight as: W ~ = W 0 + Œî ‚Äã W = W 0 + Œ± r ‚Äã B ‚Äã A \tilde{W}=W_{0}+\Delta W=W_{0}+\frac{\alpha}{r}BA (1) where Œî ‚Äã W \Delta W denotes the weight change, which is decomposed into two low-rank matrices B ‚àà ‚Ñù m √ó r B\in\mathbb{R}^{m\times r} and A ‚àà ‚Ñù r √ó n A\in\mathbb{R}^{r\times n} with an intrinsic rank r ‚â™ min ‚Å° ( m , n ) r\ll\min(m,n) , Œ± \alpha is a scaling constant. This parameterization reduces the number of trainable parameters from m ‚Äã n mn to ( m + n ) ‚Äã r (m+n)r , significantly improving fine-tuning efficiency. In practice, A A is initialized from the Gaussian distribution, while B B is initialized as an all-zero matrix to ensure that the initial model output remains unchanged. However, such random initialization can lead to slower convergence, as the gradients of the trainable adapters can be very small or in random directions during the early stages of fine-tuning ( Meng et al. , 2024 ). 3.2 Activation-Space Tail-Eigenvector Low-Rank Adaptation Astra initializes LoRA adapters by constraining low-rank updates to a task-specific tail subspace of the output activation space. The core mechanism involves two stages: (1) performing orthogonal decomposition of the output activations and (2) projecting the pre-trained weights onto the tail subspace to initialize the LoRA adapters. The detailed formulation of each step is presented below. Step 1: Orthogonal Decomposition. Astra begins by characterizing the structure of the output activations relevant to the downstream task through covariance analysis. Specifically, we construct a calibration dataset X = { x i } i = 1 N X=\{x_{i}\}_{i=1}^{N} by randomly sampling N = 64 N=64 samples from the training data. Let Y = W ‚Äã X + b ‚àà ‚Ñù d out √ó N Y=WX+b\in\mathbb{R}^{d_{\text{out}}\times N} denote the collective output activations of a linear layer. We then compute the corresponding covariance matrix as: Cov ‚Äã ( Y ) = ùîº ‚Äã [ Y ‚Äã Y ‚ä§ ] ‚àí ùîº ‚Äã [ Y ] ‚Äã ùîº ‚Äã [ Y ] ‚ä§ \mathrm{Cov}(Y)=\mathbb{E}[YY^{\top}]-\mathbb{E}[Y]\,\mathbb{E}[Y]^{\top} (2) where ùîº ‚Äã [ ‚ãÖ ] \mathbb{E}[\cdot] is the expectation operator. Since Cov ‚Äã ( Y ) \mathrm{Cov}(Y) is positive semi-definite, it admits an eigendecomposition Cov ‚Äã ( Y ) = Q ‚Äã Œõ ‚Äã Q ‚ä§ \mathrm{Cov}(Y)=Q\Lambda Q^{\top} . Here, the orthogonal matrix Q = [ Q main | Q tail ] Q=[Q_{\text{main}}\,|\,Q_{\text{tail}}] partitions the eigenvectors into two subspaces, where Q main ‚àà ‚Ñù d out √ó ( d out ‚àí r ) Q_{\text{main}}\in\mathbb{R}^{d_{\text{out}}\times(d_{\text{out}}-r)} corresponds to the principal subspace associated with the dominant eigenvalues, and Q tail ‚àà ‚Ñù d out √ó r Q_{\text{tail}}\in\mathbb{R}^{d_{\text{out}}\times r} spans the residual subspace defined by the tail eigenvalues. The two subspaces are orthogonal and satisfy Q main ‚ä§ ‚Äã Q tail = 0 Q_{\text{main}}^{\top}Q_