Title: AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

Body: AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning 1 Introduction 2 Preliminary Agent Definition Information Flow Control Flow 3 Methodology 3.1 Test-Time Rectify-or-Reject Pruning Relevant Indicator Retrieval Rectify-or-Reject Pruning Global Fallback against Structural Degeneration Handling Zero-Shot Scenarios 3.2 Failure-Driven Indicator Pool Construction Offline Indicator Mining Redundancy Elimination 4 Experiment 4.1 Experimental Setup 4.2 Main Results 4.3 Cross-Model and Cross-Domain Transferability Indicator Portability across Models Cross-Domain Generalization 5 Analysis 5.1 Ablation Study Impact of Rectification Iteration Rounds Sensitivity to Retrieved Indicator Count Effectiveness of the Retrieval Mechanism Necessity of Pool Deduplication 5.2 Iteration Dynamics and Adaptability 5.3 Distribution of Retrieved Indicators 6 Related Work Robust MAS Architectures Error Monitoring Mechanisms Utilization of Inference Trajectories A Appendix A.1 Pseudo Codes A.2 Indicator Prompt Design Indicator Design Prompt Design A.3 Dataset Statistics A.4 Case Study AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning Yutong Wang Siyuan Xiong Xuebo Liu Wenkang Zhou Liang Ding Miao Zhang Min Zhang Abstract While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MASâ€™s task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2 . Figure 1 : Overview of AgentDropoutV2 versus AgentDropout. While AgentDropout directly discards erroneous agents, AgentDropoutV2 attempts iterative rectification before elimination. 1 Introduction Large language model (LLM)-based agents have achieved outstanding performance across a wide range of tasks, including reasoning (Yao et al. , 2023 ) , planning (Prasad et al. , 2024 ) , and action (Park et al. , 2023 ) . Despite the sophisticated designs that have enabled these agents to achieve significant gains, the single-model paradigm remains a bottleneck that limits their potential. Consequently, a growing body of research has shifted focus towards designing multi-agent systems (MAS) to address more complex scenarios (Li et al. , 2023 ; Guo et al. , 2024 ) . By harnessing collective intelligence (Zhuge et al. , 2024 ; Wu et al. , 2024 ) and orchestrating cooperative teams (Zhang et al. , 2025d ; Dang et al. , 2025 ) , MAS achieves remarkable performance in complex tasks such as software development (Hong et al. , 2024 ; Qian et al. , 2024 ) , ultra-long context handling (Li et al. , 2024a ; Zhao et al. , 2024 ) , and scientific discovery (Ghafarollahi and Buehler, 2025 ; Ghareeb et al. , 2025 ) . However, the structural complexity of MAS also renders them susceptible to erroneous outputs from individual participants due to error propagation (Zhang et al. , 2025f ; Pan et al. , 2025b ) . This necessitates the timely identification and pruning of incorrect information to prevent it from cascading to downstream agents and ultimately compromising the entire task. To mitigate the impact of errors, current research has predominantly diverged into two main paradigms: Structural Optimization and Parameter Internalization. The former seeks to constrain error pathways by engineering robust communication topologies, such as optimizing directed acyclic graphs (DAG) (Zhang et al. , 2025c ; Wang et al. , 2025b ; Zhang et al. , 2025e ) . The latter focuses on enhancing the intrinsic reasoning of agents by fine-tuning them on failure trajectories (Motwani et al. , 2025 ; Zhao et al. , 2025 ) or utilizing process-supervision data (Lightman et al. , 2024 ; Wang et al. , 2025a ; Zhang et al. , 2025b ) . However, despite their contributions, these paradigms share a critical bottleneck: the reliance on offline optimization at the expense of test-time adaptivity. As illustrated in Figure 1 , methods like AgentDropout rely on pre-determined structural priors derived from training statistics. They enforce a static connectivity graph that permanently excludes certain agents without attempting to rehabilitate their outputs or rectify their errors. Similarly, parameter-based methods depend on frozen weights, rendering them incapable of dynamic correction. This static nature prevents the system from salvaging potentially correctable errors during inference, highlighting the urgent need for a test-time rectification framework that can actively intercept and resolve failures in real-time. To this end, we introduce AgentDropoutV2 , an MAS information flow optimization framework based on test-time rectify-or-reject pruning. During the execution process, our method intercepts the output of each participant agent to perform iterative rectification before it is broadcast to downstream successors. Specifically, a dedicated rectifier is prompted to scrutinize the output using adversarial indicators retrieved from a pre-constructed pool of prior failure patterns, generating targeted feedback if errors are detected. If the rectification fails to resolve the issues, the erroneous output is pruned to strictly prevent error propagation. Experimental results demonstrate that our method significantly enhances MAS performance across diverse mathematical and code generation benchmarks by effectively rectifying and eliminating erroneous agent outputs. Extended analyses further confirm the systemâ€™s adaptability, showing its capability to dynamically retrieve context-aware indicators based on task complexity, and to efficiently resolve distinct error patterns through variable iterative refinement. The observed correlation between pruning rates and reasoning difficulty positions our framework as a potential task difficulty evaluator. Our main contributions are listed as follows: â€¢ We propose a test-time rectify-or-reject pruning method that intercepts and iteratively corrects agent outputs to effectively block error propagation in MAS, thereby safeguarding task performance against cascading degradation. â€¢ We construct a failure-driven indicator pool by distilling error patterns from failed MAS trajectories, providing an off-the-shelf knowledge base that encapsulates a broad spectrum of reasoning pitfalls for precise error identification. â€¢ We demonstrate that our method exhibits robust adaptivity across diverse task complexities and scenarios, confirming its effectiveness and generalization capability as a plug-and-play intervention solution. 2 Preliminary Agent Definition We formulate the MAS workflow as an ordered sequence of N N agents, denoted as ğ’® = ( A 1 , A 2 , â€¦ , A N ) \mathcal{S}=(A_{1},A_{2},\ldots,A_{N}) . Each agent in this sequence A i A_{i} is selected from a candidate set of all available agents ğ’œ \mathcal{A} , and can be defined as a tuple of three primary elements: A i = ( Î¦ i , â„› i , ğ’¦ i ) , \displaystyle A_{i}=\left(\Phi_{i},\mathcal{R}_{i},\mathcal{K}_{i}\right), (1) where: (1) Î¦ i â€‹ ( â‹… ) \Phi_{i}(\cdot) represents the backbone model serving as the reasoning engine, which maps the input context to textual output; (2) â„› i \mathcal{R}_{i} denotes the role specification , a static set of instructions defining the agentâ€™s persona, responsibilities, and constraints; (3) ğ’¦ i \mathcal{K}_{i} represents the knowledge base , a dynamic information repository containing the history of messages observable by agent A i A_{i} (Initially ğ’¦ i = âˆ… \mathcal{K}_{i}=\emptyset ). For an active agent A i A_{i} receiving an input denoted as x i x_{i} , it utilizes its backbone model Î¦ i \Phi_{i} to generate the output o i o_{i} conditioned on its profile â„› i \mathcal{R}_{i} and current knowledge ğ’¦ i \mathcal{K}_{i} : o i = Î¦ i â€‹ ( x i , â„› i , ğ’¦ i ) . \displaystyle o_{i}=\Phi_{i}\left(x_{i},\mathcal{R}_{i},\mathcal{K}_{i}\right). (2) Information Flow Once the output o i o_{i} is generated, its dissemination is determined by the systemâ€™s architecture, which is formalized as a mapping function ğ’© : ğ’œ â†’ 2 ğ’œ \mathcal{N}:\mathcal{A}\rightarrow 2^{\mathcal{A}} , which maps the current agent A i A_{i} to a set of successor agents who are designated to receive the information. Then, the system updates the knowledge base of every successor agent A j âˆˆ ğ’© â€‹ ( A i ) A_{j}\in\mathcal{N}(A_{i}) by integrating the new message o i o_{i} : e â€‹ q : b â€‹ r â€‹ o â€‹ a â€‹ d â€‹ c â€‹ a â€‹ s â€‹ t â€‹ ğ’¦ j â† ğ’¦ j âˆª { ( â„› i , o i ) } , âˆ€ A j âˆˆ ğ’© â€‹ ( A i ) . \displaystyle{eq:broadcast}\mathcal{K}_{j}\leftarrow\mathcal{K}_{j}\cup\left\{\left(\mathcal{R}_{i},o_{i}\right)\right\},\ \forall A_{j}\in\mathcal{N}(A_{i}). (3) Through this mechanism, the framework-specialized mapping ğ’© \mathcal{N} rigidly controls the information flow topology, ranging from a broadcast structure (e.g., AutoGen) where ğ’© â€‹ ( A i ) = ğ’œ \mathcal{N}(A_{i})=\mathcal{A} , to a sequential chain where | ğ’© â€‹ ( A i ) | = 1 \lvert\mathcal{N}(A_{i})\rvert=1 . Control Flow Given a task ğ’¬ \mathcal{Q} , the control flow of the MAS is modeled as the construction of an ordered sequence of agents, referred to as the inference trajectory . Specifically, upon the generation of output o i o_{i} by the current agent A i A_{i} , a routing policy Ï€ \pi determines the next active agent A i + 1 A_{i+1} based on the task, the existing sequence of activated agents, and their historical outputs: A i + 1 = Ï€ â€‹ ( ğ’¬ , A 1 : i , o 1 : i , ğ’œ ) . \displaystyle A_{i+1}=\pi\left(\mathcal{Q},A_{1:i},o_{1:i},\mathcal{A}\right). (4) This iterative process constructs the execution path dynamically or statically, depending on the definition of Ï€ \pi from the MAS framework. The workflow concludes when the sequence reaches the terminal output agent A N A_{N} . Consequently, the final answer ğ’´ \mathcal{Y} to the initial user task ğ’¬ \mathcal{Q} is defined as the output generated by this final agent, namely ğ’´ = o N \mathcal{Y}=o_{N} . Figure 2 : Overview of the proposed framework. The upper block shows the test-time pipeline for iteratively rectifying agent outputs within the MAS. The lower block demonstrates the offline construction of the indicator pool via failure-driven mining and dual-stage deduplication. 3 Methodology We present a test-time framework designed to intercept and refine agent outputs during the MAS execution. Specifically, before transmitting the output from agent A i A_{i} to its successors ğ’© â€‹ ( A i ) \mathcal{N}(A_{i}) (as defined in Eq. LABEL:eq:broadcast ), we actively intercept the message. A dedicated rectifier then scrutinizes the content for potential errors and attempts to resolve them through an iterative refinement process. If the output remains flawed despite these efforts, it is discarded rather than propagated, ensuring that downstream agents are shielded from unreliable information. 3.1 Test-Time Rectify-or-Reject Pruning Blindly prompting an agent to self-correct is often counterproductive; without specific direction on what went wrong, the agent may inadvertently introduce new hallucinations or simply rephrase the original error. To ensure the rectification is effective, it is essential to ground the refinement process on specific, verifiable standards. Therefore, we employ adversarial indicators to scrutinize the output for distinct error patterns. If specific error types are detected, these indicators guide the generation of targeted feedback, providing the agent with a clear roadmap for correction. Relevant Indicator Retrieval To support this targeted supervision, our framework incorporates an Indicator Pool , denoted as â„ \mathcal{I} . Constructed offline via a failure-driven mining strategy (detailed in Â§ 3.2 ), this repository encapsulates empirical knowledge regarding a wide spectrum of potential errors that may emerge during MAS execution. Each indicator within this pool is structured as a tuple I = ( n , d , c ) I=(n,d,c) : â€¢ n n ( Name ): A unique identifier for the specific error type. â€¢ d d ( Error Definition ): A description of the erroneous behavior, which serves as the standard to verify whether the agentâ€™s output has deviated from requirements. â€¢ c c ( Trigger Condition ): A context describing when this specific error is likely to occur, which acts as a filter to ensure the indicator is only retrieved in relevant scenarios. Leveraging this structured repository, we can now retrieve the most pertinent indicators to supervise the current reasoning step. For an active agent A i A_{i} producing an output o i ( t ) o_{i}^{(t)} at the t t -th iteration (initially o i ( 0 ) o_{i}^{(0)} ), we first employ a dedicated Rectifier Model Î¦ rect \Phi_{\text{rect}} to distill the semantic essence of the reasoning context. The rectifier extracts two distinct sets of keywords: (1) ğ’® scen ( t ) \mathcal{S}_{\text{scen}}^{(t)} , summarizing the task scenarios (e.g., geometric coordinates, algebraic operations, etc); (2) ğ’® act ( t ) \mathcal{S}_{\text{act}}^{(t)} , representing the specific action types proposed by the agent. We transform these keywords into a query vector ğª i ( t ) = M emb â€‹ ( ğ’® scen ( t ) âŠ• ğ’® act ( t ) ) \mathbf{q}_{i}^{(t)}=M_{\text{emb}}(\mathcal{S}_{\text{scen}}^{(t)}\oplus\mathcal{S}_{\text{act}}^{(t)}) using an embedding model M emb M_{\text{emb}} . Subsequently, we retrieve the top- K act K_{\text{act}} most relevant indicators from â„ \mathcal{I} whose trigger conditions exhibit the highest semantic similarity to the current query, forming the active indicator set â„ act ( t ) \mathcal{I}_{\text{act}}^{(t)} : â„ act ( t ) = Top- â€‹ K act I j âˆˆ â„ â€‹ ( ğª i ( t ) â‹… ğœ j | ğª i ( t ) | â€‹ | ğœ j | ) , \displaystyle\mathcal{I}_{\text{act}}^{(t)}=\underset{I_{j}\in\mathcal{I}}{\text{Top-}K_{\text{act}}}\left(\frac{\mathbf{q}_{i}^{(t)}\cdot\mathbf{c}_{j}}{|\mathbf{q}_{i}^{(t)}||\mathbf{c}_{j}|}\right), (5) where ğœ j \mathbf{c}_{j} represents the trigger condition c j