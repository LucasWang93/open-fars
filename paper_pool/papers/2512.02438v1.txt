Title: Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources

Abstract: In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2-3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD .

Body: Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources 1 Introduction 2 Methodology 2.1 Problem Setting 2.2 Uni-modal Contrastive Learning 2.3 Multi-modal Contrastive Learning 2.4 Increasing batch size without additional resources 3 Experiments 3.1 Implementation details for evaluations Augmentation Configurations Evaluation details 3.2 Datasets 3.3 Comparison with state-of-the-arts (SOTA) Zero-shot Classification Few-shot Classification Image-to-text retrieval 3.4 Ablation studies 3.4.1 Ablation configurations and computational resources comparison 3.4.2 The effectiveness of momentum self-distillation Image-to-text retrieval 3.4.3 Analysis of the momentum self-distillation formula 4 Conclusion Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources Phuc Pham 1 1 1 Contributed Equally. Nhu Pham 1 1 1 Contributed Equally. Ngoc Quoc Ly Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam Vietnam National University, Ho Chi Minh City, Vietnam {20120351, 20120153}@student.hcmus.edu.vn, lqngoc@fit.hcmus.edu.vn Abstract In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2–3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD . 0 0 footnotetext: Contact email: phphuc612@gmail.com 1 Introduction Medical imaging modalities are essential for diagnosing and managing a wide range of serious diseases. Integrating deep learning models into healthcare can significantly enhance early disease detection [ qin2018computer ] , enabling timely treatment and reducing health risks. However, fully supervised models require substantial annotated data, which is often time-consuming and costly to obtain. In contrast, raw radiograph-report data, such as the MIMIC-CXR [ johnson2019mimic ] dataset with over 200,000 pairs, is abundant. This constraint has spurred interest in self-supervised and weakly supervised learning approaches, which reduce reliance on expensive annotations. Contrastive learning has emerged as a key self-supervised learning strategy that accelerates the development of robust feature representations. It operates by learning representations that are invariant among augmented views of a sample while pushing apart representations of different samples. This is achieved by optimizing the noise contrastive estimation (NCE) loss. Early contrastive learning models focused on improving representation learning in the visual domain. For instance, SimCLR [ chen2020simple ] improved performance through complex augmentation strategies and non-linear projection layers, while other models like SimSiam [ chen2020exploring ] , BYOL [ grill2020bootstrap ] , and MoCo [ he2020momentum ] employed different parameter update strategies to enhance training efficiency. Figure 1 : Our overall framework . Left : Uni-modal learning on images. Right : Multi-modal learning on text-to-image. For simplicity, we illustrate our method using a single sample. The same process applies to uni-modal learning on text and multi-modal learning on image-to-text by substituting the corresponding modules. Extending contrastive learning to multi-modality, vision-language models (VLMs) have recently gained traction in medical AI by leveraging paired image-text data to enhance feature representations. Models such as BioViL [ boecking2022making ] , MedCLIP [ wang2022medclip ] , ConVIRT [ zhang2022contrastive ] and Gloria [ huang2021gloria ] have demonstrated strong performance in medical image-text alignment tasks. These methods align the latent spaces of two modalities by treating one modality as another view of the same data sample. However, these approaches often require large-scale datasets and extensive computational resources, which limits their practicality in medical AI settings. Additionally, some approaches, such as MedKLIP [ wu2023medklip ] and MAVL [ phan2024decomposing ] , integrate domain-specific knowledge to improve retrieval and classification tasks. While contrastive learning has proven effective in medical VLMs, further enhancements are needed to address computational constraints and the issue of false negatives. One promising technique to refine learned representations is self-distillation, which allows a model to transfer knowledge to itself for improved performance. Despite its demonstrated success in vision models such as BEiT [ bao2021beit ] and DINO [ caron2021emerging ] , self-distillation remains underutilized in contrastive learning, particularly in multimodal medical imaging settings. Inspired by MoCo [ he2020momentum ] , we extend momentum contrastive learning to the multimodal domain by introducing dual momentum queues and maintaining separate momentum encoders for images and text, following the approach of multimoco [ multimoco ] , referred to as multi-modal MoCo (MM-MoCo). Based on this architecture, we introduce two key innovations: • Momentum Self-Distillation: We demonstrate that applying self-distillation with a momentum mechanism allows the model to achieve strong performance even with small batch sizes , thus alleviating the reliance on large-batch training. • Resource-Free Batch Enlargement: We propose a novel method that exploits the non-gradient nature of momentum to simulate large batch sizes without requiring additional computational resources , leading to improved learning efficiency. Building on these contributions, we propose a novel framework that offers a computationally efficient solution for medical vision-language representation learning. Our empirical results demonstrate improved performance in medical image-text alignment tasks, validating the effectiveness of our approach. 2 Methodology Figure 2 : Our technique to increase batch size without additional resources. We split the primary batch into smaller sub-batches. The preparation of embedding vectors is divided into two separate steps: first, calculating and concatenating the momentum keys from sub-batches, and second, calculating the query vectors and optimizing the contrastive loss with prepared keys. In the second step, Gradient Accumulation is employed to achieve the effects of a large batch size. 2.1 Problem Setting We begin by defining the problem setting in our work. Given a dataset of size N N containing image-text pairs, denoted as D = { ( x i I , x i T ) } i = 0 N − 1 D=\{(x_{i}^{I},x_{i}^{T})\}^{N-1}_{i=0} , where x i I x_{i}^{I} and x i T x_{i}^{T} represent a medical image and its corresponding text report, respectively. This pairing is a natural characteristic of medical datasets thanks to the routine workflow of radiologists generating textual descriptions of images [ zhang2022contrastive , johnson2019mimic ] . Our goal is training image and text encoders so that their latent spaces align. We verify it by transferring the learned text and image embeddings to classification and retrieval tasks, following previous works [ you2023cxr , wu2023medklip , wang2022medclip ] . 2.2 Uni-modal Contrastive Learning Traditionally, end-to-end contrastive learning requires two gradient update streams for both the query and key encoders [ chen2020simple , radford2021learning ] . In contrast, MoCo [ he2020momentum ] updates only the query branch through backpropagation, while the key branch is updated via an exponential moving average (EMA), thus, reducing computational costs. Denoting the parameters of the key branch as θ k \theta_{k} and those of the query branch as θ q \theta_{q} , the momentum update or EMA is: θ k → m ​ θ k + ( 1 − m ) ​ θ q \theta_{k}\rightarrow m\theta_{k}+(1-m)\theta_{q} . The coefficient m m typically set to a high value (e.g., m = 0.995 m=0.995 ) as suggested by MoCo’s experimental results to ensure gradual updates and minimize discrepancies across different versions of the momentum encoder. Thanks to this momentum mechanism, MoCo [ he2020momentum ] enables matching a gradient-encoded query q q with a large queue of momentum-encoded keys { k 0 , k 1 , … } \left\{k_{0},k_{1},\ldots\right\} . Without loss of generality, we focus on uni-modal contrastive learning for images, as illustrated in Figure 1 . The same process applies to text by replacing the corresponding modules. Let x I q x^{I_{q}} and x I k x^{I_{k}} be two transformed views of an input image, encoded into a query vector q I q^{I} and a key vector k I k^{I} , respectively. Following prior works [ he2020momentum , chen2020simple , chen2020improved , gao2021simcse ] , we define the InfoNCE loss as: ℒ I ​ 2 ​ I = − log ⁡ exp ⁡ ( s ​ ( q I , k I ) / τ ) ∑ k i I ∈ M ​ Q I exp ⁡ ( s ​ ( q I , k i I ) / τ ) \mathcal{L}_{I2I}=-\log\dfrac{\exp\left(s\left(q^{I},k^{I}\right)/\tau\right)}{\sum_{k_{i}^{I}\in MQ^{I}}\exp\left(s\left(q^{I},k_{i}^{I}\right)/\tau\right)} (1) where τ \tau is a learnable temperature parameter for the softmax function, s s is cosine similarity, and M ​ Q I MQ^{I} is the momentum queue storing both the previous key vectors and the current key vector. Similarly, the loss for text is derived in the same manner. The overall loss for uni-modal contrastive learning is then defined as the average of the two modal losses: ℒ uni = ℒ I ​ 2 ​ I + ℒ T ​ 2 ​ T 2 . \mathcal{L}_{\text{uni}}=\dfrac{\mathcal{L}_{I2I}+\mathcal{L}_{T2T}}{2}. (2) 2.3 Multi-modal Contrastive Learning The multi-modal version of MoCo or MM-MoCo, introduced by multimoco [ multimoco ] , utilizes one-hot or exact label of text-image pair for contrastive learning. However, we empirically observe that this approach performs inadequately under low batch size configurations (e.g., batch size = 16), as demonstrated in our ablation study. This suboptimal performance can be explained by the inherent ambiguity in textual descriptions and the potential for multiple images within the dataset to match a given textual query. For example, a medical description such as ”heart size and cardiomediastinal contours are normal” could apply to numerous images across the dataset, thereby challenging the effectiveness of exact labels. To address this limitation, we propose a momentum self-distillation strategy that replaces the exact labels with soft targets derived from similarity distributions. Specifically, we compute two distributions: (1) the key-to-key similarity distribution ( p k ​ 2 ​ k p_{k2k} ), representing similarities between the image key corresponding to the query text and other image keys in the momentum queue, and (2) the query-to-key similarity distribution ( p q ​ 2 ​ k p_{q2k} ), computed between the momentum-encoded query text vector and momentum-encoded image key vectors. This approach effectively establishes direct multi-modal correlations, significantly enhancing performance at low batch sizes and continuing to scale well with larger batch sizes. Crucially, our method reduces reliance on large batch sizes, thus alleviating GPU resource constraints during training. Both similarity measures can be effectively combined by computing the Kullback-Leibler (KL) divergence between the predicted text-to-image similarity distribution p t ​ 2 ​ i p_{t2i} and the momentum-distilled similarities: ℒ T ​ 2 ​ I = α ​ KL ​ ( p q ​ 2 ​ k ∥ p t ​ 2 ​ i ) + β ​ KL ​ ( p k ​ 2 ​ k ∥ p t ​ 2 ​ i ) , \mathcal{L}_{T2I}=\alpha\text{KL}(p_{q2k}\parallel p_{t2i})+\beta\text{KL}(p_{k2k}\parallel p_{t2i}), (3) where we empirically set α = 0.3 \alpha=0.3 and β = 0.7 \beta=0.7 . The corresponding image-to-text loss ℒ I ​ 2 ​ T \mathcal{L}_{I2T} is defined analogously. The overall multi-modal contrastive loss is the average of these two: ℒ multi = ℒ T ​ 2 ​ I + ℒ I ​ 2 ​ T 2 . \mathcal{L}_{\text{multi}}=\frac{\mathcal{L}_{T2I}+\mathcal{L}_{I2T}}{2}. (4) Finally, the full pretraining objective is formulated as a weighted sum of uni-modal and multi-modal losses. We observed that the optimization inherently prioritized uni-modal optimization over multi-modal learning, leading to a significant decrease in total loss primarily driven by the uni-modal component. To balance this, we increased the weight of the multi-modal loss, setting ω uni = 1 \omega_{\text{uni}}=1 and ω multi = 10 \omega_{\text{multi}}=10 : ℒ = ω uni ⋅ ℒ uni + ω multi ⋅ ℒ multi ω uni + ω multi . \mathcal{L}=\frac{\omega_{\text{uni}}\cdot\mathcal{L}_{\text{uni}}+\omega_{\text{multi}}\cdot\mathcal{L}_{\text{multi}}}{\omega_{\text{uni}}+\omega_{\text{multi}}}. (5) 2.4 Increasing batch size without additional resources Another key contribution of our work is leveraging the gradient-free nature of the momentum key branch to efficiently increase the effective batch size without additional computational resources. Our approach is applicable to both uni-modal and multi-modal contrastive learning frameworks, as both share a common structure consisting of a gradient-based query branch and a gradient-free key branch. Specifically, we utilize the momentum encoder’s gradient-free property to compute a large batch of key vectors simultaneously. For the query branch, which requires gradient calculations, we handle large batch sizes by segmenting them into smaller sub-batches that fit within GPU memory constraints. Once all momentum keys are prepared, we apply gradient accumulation across these smaller sub-batches. Gradients from each sub-batch are accumulated sequentially, and the optimizer updates the model parameters only after processing all sub-batches, as illustrated in Fig. 2 . This approach ensures each query interacts simultaneou