Title: GFRRN: Explore the Gaps in Single Image Reflection Removal

Abstract: Prior dual-stream methods with the feature interaction mechanism have achieved remarkable performance in single image reflection removal (SIRR). However, they often struggle with (1) semantic understanding gap between the features of pre-trained models and those of reflection removal models, and (2) reflection label inconsistencies between synthetic and real-world training data. In this work, we first adopt the parameter efficient fine-tuning (PEFT) strategy by integrating several learnable Mona layers into the pre-trained model to align the training directions. Then, a label generator is designed to unify the reflection labels for both synthetic and real-world data. In addition, a Gaussian-based Adaptive Frequency Learning Block (G-AFLB) is proposed to adaptively learn and fuse the frequency priors, and a Dynamic Agent Attention (DAA) is employed as an alternative to window-based attention by dynamically modeling the significance levels across windows (inter-) and within an individual window (intra-). These components constitute our proposed Gap-Free Reflection Removal Network (GFRRN). Extensive experiments demonstrate the effectiveness of our GFRRN, achieving superior performance against state-of-the-art SIRR methods.

Body: GFRRN: Explore the Gaps in Single Image Reflection Removal 1 Introduction 2 Related work 2.1 Image Reflection Removal 2.2 Parameter Efficient Fine Tuning 3 Methodology 3.1 Basic model 3.2 Mona-tuning 3.3 Unified label 3.4 G-AFLB DAA in decoder 3.5 Loss function 4 Experiments 4.1 Implementation details 4.2 Performance evaluation 4.3 Ablation study 5 Conclusion A The details of Mona Layer B The overall structure of decoder C The details of G-AFLB D The details of DAA and LDAA E New benchmark and additional visual comparisons GFRRN: Explore the Gaps in Single Image Reflection Removal Yu Chen 1,2 Zewei He 1,2 ‚Ä† Xingyu Liu 1,2 Zixuan Chen 3 Zheming Lu 1,2 1 Zhejiang University 2 Huanjiang Laboratory 3 The Chinese University of Hong Kong Abstract Prior dual-stream methods with the feature interaction mechanism have achieved remarkable performance in single image reflection removal (SIRR). However, they often struggle with (1) semantic understanding gap between the features of pre-trained models and those of reflection removal models, and (2) reflection label inconsistencies between synthetic and real-world training data. In this work, we first adopt the parameter efficient fine-tuning (PEFT) strategy by integrating several learnable Mona layers into the pre-trained model to align the training directions. Then, a label generator is designed to unify the reflection labels for both synthetic and real-world data. In addition, a Gaussian-based Adaptive Frequency Learning Block (G-AFLB) is proposed to adaptively learn and fuse the frequency priors, and a Dynamic Agent Attention (DAA) is employed as an alternative to window-based attention by dynamically modeling the significance levels across windows (inter-) and within an individual window (intra-). These components constitute our proposed Gap-Free Reflection Removal Network (GFRRN). Extensive experiments demonstrate the effectiveness of our GFRRN, achieving superior performance against state-of-the-art SIRR methods. Figure 1 : Setting (a): Single-stream method like RRW [ 51 ] ; Setting (b): Dual-stream method like IBCLN [ 29 ] ; Setting (c): Dual-stream method with feature interaction mechanism like DSIT [ 22 ] ; Setting (d): Ours; (e) Experimental results with different settings. 2 2 footnotetext: Corresponding author: Zewei He (zeweihe@zju.edu.cn). 1 Introduction When capturing images through glasses or other reflective mediums, reflection artifacts are quite common and manifest as a hybrid mixture of reflected and transmitted components. Undesired reflections will significantly degrade the imaging quality of the target scene (i.e., transmission layer), further impeding downstream tasks such as object detection and segmentation [ 41 , 43 ] . Consequently, the development of robust techniques for transmission-reflection decomposition is highly desired. The task of single image reflection removal (SIRR) is a long-standing challenge within the realm of blind source separation, primarily attributed to the ill-posedness inherent in the process of decoupling two natural image signals. Generally, the observed superimposed image ùêà \mathbf{I} can be formulated as follows [ 21 ] : ùêà = ùêì + ùêë + Œ¶ ‚Äã ( ùêì , ùêë ) , \mathbf{I}=\mathbf{T}+\mathbf{R}+\Phi(\mathbf{T},\mathbf{R}), (1) where ùêì \mathbf{T} and ùêë \mathbf{R} denote the target transmission and reflection layers, respectively. Œ¶ ‚Äã ( ùêì , ùêë ) \Phi(\mathbf{T},\mathbf{R}) is a residual term [ 21 ] . Note that Œ¶ \Phi can represent a group of functions, encompassing a variety of models. Based on the behavior model in Eqn. 1 , various SIRR methods have been developed. For example, some single-stream methods [ 51 , 10 ] treat the reflection layer as a form of noise/degradation and focus solely on restoring the transmission layer (refer to Fig. 1 (a)). Some other approaches [ 29 , 45 ] attempt to simultaneously reconstruct both the reflection layer and the transmission layer (refer to Fig. 1 (b)). According to the definition in [ 22 ] , we classify them as dual-stream methods. Currently, dual-stream scheme is a rising trend for SIRR task. Later, researchers utilized the dual-stream feature interaction mechanism to enhance the information flow [ 20 , 21 , 22 ] . Among these methods, a prevalent practice is to leverage a pre-trained model for providing semantic information (refer to Fig. 1 (c)). Though existing dual-stream methods (with feature interaction mechanism) [ 20 , 21 , 22 ] have achieved promising results, they ignore some gaps in single image reflection removal (SIRR) task. (1) Semantic gap : Semantic understanding of the input image is helpful in SIRR [ 17 , 49 , 20 , 21 , 22 ] . Typically, such semantic features are extracted by a pre-trained model (VGG [ 20 , 21 ] or Swin-Transformer [ 22 ] ), which does not participate in the gradient back-propagation. We argue that there exists a semantic gap between the features of pre-trained models and those of reflection removal models. Aligning the training directions of the pre-trained and the reflection removal models can bridge this gap to some extent, thereby boosting the performance of SIRR task. (2) Training data gap : The training of SIRR models typically involves the concurrent use of both synthetic and real-world data. However, the labels of the estimated reflection layers for these two types of data are often inconsistent (synthetic: ùêë \mathbf{R} , real-world: ùêà ‚àí ùêì \mathbf{I}-\mathbf{T} ), which constitutes the so-called ‚Äútraining data gap‚Äù and exerts an adverse impact on the training. If this gap can be bridged at the data level, it could serve as a general solution, benefiting numerous SIRR approaches. To remedy the deficiencies discussed above, we, for the first time , introduce parameter efficient fine-tuning technique [ 47 ] to adapt the semantic information from the pre-trained model. This strategy avoids the optimization challenges associated with full fine-tuning (FFT), while effectively achieving alignment between the pre-trained network and the reflection removal network. Then, we unify the labels of reflection layers for both synthetic and real-world data during the training phase according to our observations and experiments. Specifically, the low-frequency part of ùêà ‚àí ùêì \mathbf{I}-\mathbf{T} is adopted to pose supervision. Note that, this technique can be applied to existing SIRR models (e.g., DSIT [ 22 ] , DSRNet [ 21 ] ), demonstrating its versatility. In addition, a Gaussian-based adaptive frequency learning block (G-AFLB) is designed to explore and leverage the frequency prior, and a dynamic agent attention (DAA) is employed as an alternative to window-based attention by dynamically modeling the significance levels across windows (inter-window) and within an individual window (intra-window). At last, by combining the improvements above, we present our G ap- F ree R eflection R emoval N etwork (GFRRN). Figure 2 : The overall architecture of our GFRRN. It consists of two parallel encoders (i.e., a pre-trained Swin-Transformer with some learnable Mona layers as the Encoder 1 , and a dual-stream CNN borrowed from DSIT [ 22 ] as the Encoder 2 ) and a single decoder. 2 Related work 2.1 Image Reflection Removal Currently, multi-frame and multimodel methods [ 35 , 1 , 12 , 37 , 30 , 27 , 14 , 15 , 33 , 39 , 44 , 46 , 18 ] have achieved quite good results. But compared to single-frame methods, their practicality is limited. Traditional SIRR methods rely on hand-crafted priors based on statistical or physical assumptions. For example, Li et al. [ 31 ] introduced a smoothness prior; Shih et al. [ 36 ] utilized ghosting cues; Levin and Weiss [ 28 ] allowed user annotations to guide restoration; and Nikolaos et al. [ 2 ] combined a Laplacian data term with gradient sparsity. Although these priors do not always hold in practice, they have significantly advanced SIRR development. With the rise of deep learning, data-driven approaches have gradually replaced traditional methods and become dominant in SIRR. CEILNet [ 10 ] explicitly divides the task into two stages: estimate the edge map and restored image. Zhang et al. [ 49 ] proposed a convolutional network trained with perceptual and exclusion losses. ERRNet [ 42 ] provides a way to utilize non-aligned image pairs for training. Although these methods achieve impressive results, they often discard useful information in the reflection layer. IBCLN [ 29 ] uses LSTM units to iteratively recover both reflection and transmission layers. YTMT [ 20 ] introduces a dual-stream interaction module based on ReLU and negative ReLU. DSRNet [ 21 ] proposes a learnable residual mechanism that unifies existing physical models and designs a MUGI block for dual-stream interaction. DSIT [ 22 ] employs attention mechanisms to further improve interaction accuracy. Dong et al. [ 8 ] precisely locate reflection and remove it. RRW [ 51 ] proposes a cascaded strategy network and an efficient data acquisition pipeline. FIRM [ 5 ] achieves accurate reflection removal by allowing users to manually specify reflection regions. DExNet [ 23 ] introduces a lightweight deep unfolding network. RDNet [ 50 ] designs a reversible network that performs well on SIRR. These methods utilize information from reflection and transmission layers, but they requires ITR-aligned triplet datasets. 2.2 Parameter Efficient Fine Tuning Recent studies have shown that pretrained models from high-level vision tasks can assist low-level restoration tasks. For instance, DSRNet, DSIT in SIRR, or SGGLC-Net [ 11 ] in super-resolution. However, there still exist inevitable gaps between high-level and low-level tasks, making fine-tuning of pretrained models essential. The most straightforward approach is Full Fine-Tuning (FFT), but it incurs high computational costs. Parameter-Efficient Fine-Tuning (PEFT) offers an effective solution to these challenges. The first strategy is to freeze most parameters of the pretrained model and only train a small subset, such as BitFit [ 48 ] only fine-tunes the bias, Norm Tuning [ 13 ] only fine-tunes the norm layer. The second strategy reparameterizes parts of the pretrained model, LoRA [ 19 ] is the most representative example. The third strategy fixes all pretrained parameters while adding small trainable modules, such as AdaptFormer [ 4 ] , Mona [ 47 ] , VPT [ 26 ] and P-Tuning [ 32 ] . 3 Methodology 3.1 Basic model Considering that the reflection layer may contain valuable information and can provide better regularization for the restoration of the transmission layer, we also choose to employ a dual-stream framework. Among the existing approaches, the most representative ones include YTYM [ 20 ] , DSRNet [ 21 ] , and DSIT [ 22 ] . They typically incorporate a pre-trained model (e.g., VGG or Swin-Transformer) to provide high-level semantic information, which is then injected into the dual-stream flows via a certain fusion scheme. Figure 3 : (a) Semantic gap exists between the pre-trained model and the reflection removal model. (b) A cognitive-inspired Mona-tuning technique is proposed to bridge the semantic gap. Following their basic configurations, the overall architecture of our Gap-Free Reflection Removal Network (GFRRN) is illustrated in Fig. 2 , which consists of two parallel encoders (i.e., a pre-trained Swin-Transformer with some learnable Mona layers as the Encoder 1 for extracting the global priors, and a task-specific dual-stream CNN borrowed from DSIT [ 22 ] as the Encoder 2 for extracting the local priors) and a single decoder. The estimated reflection ùêë ^ \hat{\mathbf{R}} and transmission ùêì ^ \hat{\mathbf{T}} are employed to predict the residual term ùêç ^ \hat{\mathbf{N}} via a residual estimator, which mainly composed of a NAFBlock [ 3 ] . Our GFRRN has a similar architecture to the DSIT [ 22 ] , yet with three main differences: (1) An adapter-based tuning strategy is introduced into the SwinBlock [ 34 ] , which fixes the pre-trained weights and solely updates the weights in adapters. Specifically, the M ulti-c o g n itive visual a dapter (Mona) [ 47 ] is adopted with vision-friendly filters to transfer the pre-trained knowledge through multiple cognitive perspectives. (2) The supervision label of the estimated reflection is designed as the low-frequency part of ùêà ‚àí ùêì \mathbf{I}-\mathbf{T} instead of direct ùêà ‚àí ùêì \mathbf{I}-\mathbf{T} , to ensure that it exclusively contains information from the reflection layer. (3) In the decoder part, a Gaussian-based adaptive frequency learning block (G-AFLB) is employed to learn and integrate the frequency priors. Moreover, the window-based multi-head self-attention (W-MSA) is replaced with proposed dynamic agent attention (DAA). These components will be elaborated in detail in the following subsections. 3.2 Mona-tuning The Swin-Transformer [ 34 ] used in our GFRRN is initially designed for the classification task, which primarily emphasize high-level semantic information. This focus differs from that of the image restoration task (precisely, reflection removal), as the latter places greater importance on low-level texture details for dense prediction. Therefore, semantic gap exists between the pre-trained model (i.e., Swin-Transformer) and the reflection removal model (see in Fig. 3 (a)). While full fine-tuning (FFT) of the Swin-Transformer appears to be a straightforward solution, its performance remains sub-optimal (as show in Table 3 ). This may stem from the model‚Äôs excessive trainable parameters, which cannot be effectively optimized given our dataset‚Äôs limited scale compared to ImageNet [ 7 ] . Recent delta-tuning or parameter efficient fine-tuning (PEFT) methods [ 4 , 48 , 47 ] provide new options for this problem. As show in Fig. 3 (b), we opt for a cognitive-inspired Mona-tuning technique, which integrates convolution-based filters to bridge the semantic gap by transferring visual knowledge from the pre-trained model to the reflection removal task. Specifically, we insert Mona layers after MSA and MLP in each SwinBlock of Swin-Transformer [ 34 ] . More details can be found in our supplementary material. During the training phase, only the weights of inserted Mona layers are updated. To the best of our knowledge, this is the first work to apply the PEFT technique to the SIRR task. 3.3 Unified label In-depth analysis demonstrates that the issue of data gap primarily arises from the divergence in supervision labels employed during the training process. Specifically, when handling synthetic datasets, models are typically supervised using reflection images denoted as ùêë \mathbf{R} , but switch to using residual images (constructed as ùêà ‚àí ùêì \mathbf{I}-\mathbf{T} ) 1 1 1 Most real training datasets do not contain reflection images, since the difficulty in acquisition. as supervision for real datasets [ 25 ] . This discrepancy in supervision labels between synthetic and real datasets substantially undermines the model‚Äôs capacity for generalization. We firstly employ ùêà ‚àí ùêì \mathbf{I}-\mathbf{T} as