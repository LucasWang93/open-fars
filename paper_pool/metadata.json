[
  {
    "paper_id": "2602.22953",
    "title": "General Agent Evaluation",
    "abstract": "The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.",
    "authors": [
      "Elron Bandel",
      "Asaf Yehudai",
      "Lilach Eden",
      "Yehoshua Sagron",
      "Yotam Perlitz",
      "Elad Venezian",
      "Natalia Razinkov",
      "Natan Ergas",
      "Shlomit Shachor Ifergan",
      "Segev Shlomov",
      "Michal Jacovi",
      "Leshem Choshen",
      "Liat Ein-Dor",
      "Yoav Katz",
      "Michal Shmueli-Scheuer"
    ],
    "url": "https://arxiv.org/abs/2602.22953",
    "date": "2026-02-26T12:48:02.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.23339",
    "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?",
    "abstract": "Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.",
    "authors": [
      "Tilemachos Aravanis",
      "Vladan Stojnić",
      "Bill Psomas",
      "Nikos Komodakis",
      "Giorgos Tolias"
    ],
    "url": "https://arxiv.org/abs/2602.23339",
    "date": "2026-02-26T18:45:33.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.20332",
    "title": "No One Size Fits All: QueryBandits for Hallucination Mitigation",
    "abstract": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.",
    "authors": [
      "Nicole Cho",
      "William Watson",
      "Alec Koppel",
      "Sumitra Ganesh",
      "Manuela Veloso"
    ],
    "url": "https://arxiv.org/abs/2602.20332",
    "date": "2026-02-23T20:28:48.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.20300",
    "title": "What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance",
    "abstract": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent \"risk landscape\": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.",
    "authors": [
      "William Watson",
      "Nicole Cho",
      "Sumitra Ganesh",
      "Manuela Veloso"
    ],
    "url": "https://arxiv.org/abs/2602.20300",
    "date": "2026-02-23T19:30:08.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.18253",
    "title": "MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data",
    "abstract": "Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.",
    "authors": [
      "Xabier de Zuazo",
      "Vincenzo Verbeni",
      "Eva Navas",
      "Ibon Saratxaga",
      "Mathieu Bourguignon",
      "Nicola Molinaro"
    ],
    "url": "https://arxiv.org/abs/2602.18253",
    "date": "2026-02-20T14:39:50.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.22045",
    "title": "DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain",
    "abstract": "We introduce DLT-Corpus, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), United States Patent and Trademark Office (USPTO) patents (49,023 filings), and social media (22 million posts). Existing Natural Language Processing (NLP) resources for DLT focus narrowly on cryptocurrencies price prediction and smart contracts, leaving domain-specific language under explored despite the sector's ~$3 trillion market capitalization and rapid technological evolution.\n  We demonstrate DLT-Corpus' utility by analyzing technology emergence patterns and market-innovation correlations. Findings reveal that technologies originate in scientific literature before reaching patents and social media, following traditional technology transfer patterns. While social media sentiment remains overwhelmingly bullish even during crypto winters, scientific and patent activity grow independently of market fluctuations, tracking overall market expansion in a virtuous cycle where research precedes and enables economic growth that funds further innovation.\n  We publicly release the full DLT-Corpus; LedgerBERT, a domain-adapted model achieving 23% improvement over BERT-base on a DLT-specific Named Entity Recognition (NER) task; and all associated tools and code.",
    "authors": [
      "Walter Hernandez Cruz",
      "Peter Devine",
      "Nikhil Vadgama",
      "Paolo Tasca",
      "Jiahua Xu"
    ],
    "url": "https://arxiv.org/abs/2602.22045",
    "date": "2026-02-25T15:53:41.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.22638",
    "title": "MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios",
    "abstract": "Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .",
    "authors": [
      "Zhiheng Song",
      "Jingshuai Zhang",
      "Chuan Qin",
      "Chao Wang",
      "Chao Chen",
      "Longfei Xu",
      "Kaikui Liu",
      "Xiangxiang Chu",
      "Hengshu Zhu"
    ],
    "url": "https://arxiv.org/abs/2602.22638",
    "date": "2026-02-26T05:39:38.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.20981",
    "title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models",
    "abstract": "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.",
    "authors": [
      "Christian Simon",
      "Masato Ishii",
      "Wei-Yao Wang",
      "Koichi Saito",
      "Akio Hayakawa",
      "Dongseok Shim",
      "Zhi Zhong",
      "Shuyang Cui",
      "Shusuke Takahashi",
      "Takashi Shibuya",
      "Yuki Mitsufuji"
    ],
    "url": "https://arxiv.org/abs/2602.20981",
    "date": "2026-02-24T15:01:39.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.23205",
    "title": "EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents",
    "abstract": "Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.",
    "authors": [
      "Wenjia Wang",
      "Liang Pan",
      "Huaijin Pi",
      "Yuke Lou",
      "Xuqian Ren",
      "Yifan Wu",
      "Zhouyingcheng Liao",
      "Lei Yang",
      "Rishabh Dabral",
      "Christian Theobalt",
      "Taku Komura"
    ],
    "url": "https://arxiv.org/abs/2602.23205",
    "date": "2026-02-26T16:53:41.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.23363",
    "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
    "abstract": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only sim51K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com",
    "authors": [
      "Sahal Shaji Mullappilly",
      "Mohammed Irfan Kurpath",
      "Omair Mohamed",
      "Mohamed Zidan",
      "Fahad Khan",
      "Salman Khan",
      "Rao Anwer",
      "Hisham Cholakkal"
    ],
    "url": "https://arxiv.org/abs/2602.23363",
    "date": "2026-02-26T18:59:46.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.22675",
    "title": "Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization",
    "abstract": "Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose Search More, Think Less (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6\\%), GAIA (75.7\\%), Xbench (82.0\\%), and DeepResearch Bench (45.9\\%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7\\%, while improving accuracy.",
    "authors": [
      "Qianben Chen",
      "Tianrui Qin",
      "King Zhu",
      "Qiexiang Wang",
      "Chengjun Yu",
      "Shu Xu",
      "Jiaqi Wu",
      "Jiayu Zhang",
      "Xinpeng Liu",
      "Xin Gui",
      "Jingyi Cao",
      "Piaohong Wang",
      "Dingfeng Shi",
      "He Zhu",
      "Tiannan Wang",
      "Yuqing Wang",
      "Maojia Song",
      "Tianyu Zheng",
      "Ge Zhang",
      "Jian Yang",
      "Jiaheng Liu",
      "Minghao Liu",
      "Yuchen Eleanor Jiang",
      "Wangchunshu Zhou"
    ],
    "url": "https://arxiv.org/abs/2602.22675",
    "date": "2026-02-26T06:46:41.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.23258",
    "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning",
    "abstract": "While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.",
    "authors": [
      "Yutong Wang",
      "Siyuan Xiong",
      "Xuebo Liu",
      "Wenkang Zhou",
      "Liang Ding",
      "Miao Zhang",
      "Min Zhang"
    ],
    "url": "https://arxiv.org/abs/2602.23258",
    "date": "2026-02-26T17:31:43.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.22859",
    "title": "From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models",
    "abstract": "As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE.",
    "authors": [
      "Hongrui Jia",
      "Chaoya Jiang",
      "Shikun Zhang",
      "Wei Ye"
    ],
    "url": "https://arxiv.org/abs/2602.22859",
    "date": "2026-02-26T10:53:57.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.21760",
    "title": "Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling",
    "abstract": "Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves 2.31times and 2.07times latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX~3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff.",
    "authors": [
      "Euisoo Jung",
      "Byunghyun Kim",
      "Hyunjin Kim",
      "Seonghye Cho",
      "Jae-Gil Lee"
    ],
    "url": "https://arxiv.org/abs/2602.21760",
    "date": "2026-02-25T10:23:07.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.22479",
    "title": "Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns",
    "abstract": "Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC^{2} (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC^{2} combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC^{2} improves the stability-plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior.",
    "authors": [
      "Afshin Khadangi"
    ],
    "url": "https://arxiv.org/abs/2602.22479",
    "date": "2026-02-25T23:38:16.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.17594",
    "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
    "abstract": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play all conceivable human games, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.",
    "authors": [
      "Lance Ying",
      "Ryan Truong",
      "Prafull Sharma",
      "Kaiya Ivy Zhao",
      "Nathan Cloos",
      "Kelsey R. Allen",
      "Thomas L. Griffiths",
      "Katherine M. Collins",
      "José Hernández-Orallo",
      "Phillip Isola",
      "Samuel J. Gershman",
      "Joshua B. Tenenbaum"
    ],
    "url": "https://arxiv.org/abs/2602.17594",
    "date": "2026-02-19T18:17:25.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.23008",
    "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization",
    "abstract": "Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO^2), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO^2 achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO^2 demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO^2 as a promising framework for building more exploratory and generalizable LLM-based agents.",
    "authors": [
      "Zeyuan Liu",
      "Jeonghye Kim",
      "Xufang Luo",
      "Dongsheng Li",
      "Yuqing Yang"
    ],
    "url": "https://arxiv.org/abs/2602.23008",
    "date": "2026-02-26T13:50:57.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.22766",
    "title": "Imagination Helps Visual Reasoning, But Not Yet in Latent Space",
    "abstract": "Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.",
    "authors": [
      "You Li",
      "Chi Chen",
      "Yanghao Li",
      "Fanhu Zeng",
      "Kaiyu Huang",
      "Jinan Xu",
      "Maosong Sun"
    ],
    "url": "https://arxiv.org/abs/2602.22766",
    "date": "2026-02-26T08:56:23.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.22594",
    "title": "Causal Motion Diffusion Models for Autoregressive Motion Generation",
    "abstract": "Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.",
    "authors": [
      "Qing Yu",
      "Akihisa Watanabe",
      "Kent Fujiwara"
    ],
    "url": "https://arxiv.org/abs/2602.22594",
    "date": "2026-02-26T03:58:25.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.22437",
    "title": "veScale-FSDP: Flexible and High-Performance FSDP at Scale",
    "abstract": "Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.",
    "authors": [
      "Zezhou Wang",
      "Youjie Li",
      "Zhiqi Lin",
      "Jiacheng Yang",
      "Cong Xie",
      "Guanyu Feng",
      "Zheng Zhong",
      "Ziyue Huang",
      "Hongyu Zhu",
      "Zhi Zhang",
      "Yanghua Peng",
      "Xin Liu"
    ],
    "url": "https://arxiv.org/abs/2602.22437",
    "date": "2026-02-25T21:55:43.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.23058",
    "title": "GeoWorld: Geometric World Models",
    "abstract": "Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.",
    "authors": [
      "Zeyu Zhang",
      "Danning Li",
      "Ian Reid",
      "Richard Hartley"
    ],
    "url": "https://arxiv.org/abs/2602.23058",
    "date": "2026-02-26T14:42:53.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.22897",
    "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
    "abstract": "Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.",
    "authors": [
      "Xiaoxi Li",
      "Wenxiang Jiao",
      "Jiarui Jin",
      "Shijian Wang",
      "Guanting Dong",
      "Jiajie Jin",
      "Hao Wang",
      "Yinuo Wang",
      "Ji-Rong Wen",
      "Yuan Lu",
      "Zhicheng Dou"
    ],
    "url": "https://arxiv.org/abs/2602.22897",
    "date": "2026-02-26T11:35:04.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.23165",
    "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation",
    "abstract": "Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.",
    "authors": [
      "Yichen Peng",
      "Jyun-Ting Song",
      "Siyeol Jung",
      "Ruofan Liu",
      "Haiyang Liu",
      "Xuangeng Chu",
      "Ruicong Liu",
      "Erwin Wu",
      "Hideki Koike",
      "Kris Kitani"
    ],
    "url": "https://arxiv.org/abs/2602.23165",
    "date": "2026-02-26T16:30:07.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.23152",
    "title": "The Trinity of Consistency as a Defining Principle for General World Models",
    "abstract": "The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.",
    "authors": [
      "Jingxuan Wei",
      "Siyuan Li",
      "Yuhang Xu",
      "Zheng Sun",
      "Junjie Jiang",
      "Hexuan Jin",
      "Caijun Jia",
      "Honghao He",
      "Xinglong Xu",
      "Xi bai",
      "Chang Yu",
      "Yumou Liu",
      "Junnan Zhu",
      "Xuanhe Zhou",
      "Jintao Chen",
      "Xiaobin Hu",
      "Shancheng Pang",
      "Bihui Yu",
      "Ran He",
      "Zhen Lei",
      "Stan Z. Li",
      "Conghui He",
      "Shuicheng Yan",
      "Cheng Tan"
    ],
    "url": "https://arxiv.org/abs/2602.23152",
    "date": "2026-02-26T16:15:55.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.23259",
    "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
    "abstract": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.",
    "authors": [
      "Jiangxin Sun",
      "Feng Xue",
      "Teng Long",
      "Chang Liu",
      "Jian-Fang Hu",
      "Wei-Shi Zheng",
      "Nicu Sebe"
    ],
    "url": "https://arxiv.org/abs/2602.23259",
    "date": "2026-02-26T17:32:30.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.19424",
    "title": "Hepato-LLaVA: An Expert MLLM with Sparse Topo-Pack Attention for Hepatocellular Pathology Analysis on Whole Slide Images",
    "abstract": "Hepatocellular Carcinoma diagnosis relies heavily on the interpretation of gigapixel Whole Slide Images. However, current computational approaches are constrained by fixed-resolution processing mechanisms and inefficient feature aggregation, which inevitably lead to either severe information loss or high feature redundancy. To address these challenges, we propose Hepato-LLaVA, a specialized Multi-modal Large Language Model designed for fine-grained hepatocellular pathology analysis. We introduce a novel Sparse Topo-Pack Attention mechanism that explicitly models 2D tissue topology. This mechanism effectively aggregates local diagnostic evidence into semantic summary tokens while preserving global context. Furthermore, to overcome the lack of multi-scale data, we present HepatoPathoVQA, a clinically grounded dataset comprising 33K hierarchically structured question-answer pairs validated by expert pathologists. Our experiments demonstrate that Hepato-LLaVA achieves state-of-the-art performance on HCC diagnosis and captioning tasks, significantly outperforming existing methods. Our code and implementation details are available at https://pris-cv.github.io/Hepto-LLaVA/.",
    "authors": [
      "Yuxuan Yang",
      "Zhonghao Yan",
      "Yi Zhang",
      "Bo Yun",
      "Muxi Diao",
      "Guowei Zhao",
      "Kongming Liang",
      "Wenbin Li",
      "Zhanyu Ma"
    ],
    "url": "https://arxiv.org/abs/2602.19424",
    "date": "2026-02-23T01:43:32.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.20933",
    "title": "Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting",
    "abstract": "Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. Project Website: https://sk-fun.fun/DropAnSH-GS",
    "authors": [
      "Shuangkang Fang",
      "I-Chao Shen",
      "Xuanyang Zhang",
      "Zesheng Wang",
      "Yufeng Wang",
      "Wenrui Ding",
      "Gang Yu",
      "Takeo Igarashi"
    ],
    "url": "https://arxiv.org/abs/2602.20933",
    "date": "2026-02-24T14:11:56.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.21456",
    "title": "Revisiting Text Ranking in Deep Research",
    "abstract": "Deep research has emerged as an important task that aims to address hard queries through extensive open-web exploration. To tackle it, most prior work equips large language model (LLM)-based agents with opaque web search APIs, enabling agents to iteratively issue search queries, retrieve external evidence, and reason over it. Despite search's essential role in deep research, black-box web search APIs hinder systematic analysis of search components, leaving the behaviour of established text ranking methods in deep research largely unclear. To fill this gap, we reproduce a selection of key findings and best practices for IR text ranking methods in the deep research setting. In particular, we examine their effectiveness from three perspectives: (i) retrieval units (documents vs. passages), (ii) pipeline configurations (different retrievers, re-rankers, and re-ranking depths), and (iii) query characteristics (the mismatch between agent-issued queries and the training queries of text rankers). We perform experiments on BrowseComp-Plus, a deep research dataset with a fixed corpus, evaluating 2 open-source agents, 5 retrievers, and 3 re-rankers across diverse setups. We find that agent-issued queries typically follow web-search-style syntax (e.g., quoted exact matches), favouring lexical, learned sparse, and multi-vector retrievers; passage-level units are more efficient under limited context windows, and avoid the difficulties of document length normalisation in lexical retrieval; re-ranking is highly effective; translating agent-issued queries into natural-language questions significantly bridges the query mismatch.",
    "authors": [
      "Chuan Meng",
      "Litu Ou",
      "Sean MacAvaney",
      "Jeff Dalton"
    ],
    "url": "https://arxiv.org/abs/2602.21456",
    "date": "2026-02-25T00:18:07.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.21374",
    "title": "Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages",
    "abstract": "Extracting clinical information from medical transcripts in low-resource languages remains a significant challenge in healthcare natural language processing (NLP). This study evaluates a two-step pipeline combining Aya-expanse-8B as a Persian-to-English translation model with five open-source small language models (SLMs) -- Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, and Gemma-3-1B-it -- for binary extraction of 13 clinical features from 1,221 anonymized Persian transcripts collected at a cancer palliative care call center. Using a few-shot prompting strategy without fine-tuning, models were assessed on macro-averaged F1-score, Matthews Correlation Coefficient (MCC), sensitivity, and specificity to account for class imbalance. Qwen2.5-7B-Instruct achieved the highest overall performance (median macro-F1: 0.899; MCC: 0.797), while Gemma-3-1B-it showed the weakest results. Larger models (7B--8B parameters) consistently outperformed smaller counterparts in sensitivity and MCC. A bilingual analysis of Aya-expanse-8B revealed that translating Persian transcripts to English improved sensitivity, reduced missing outputs, and boosted metrics robust to class imbalance, though at the cost of slightly lower specificity and precision. Feature-level results showed reliable extraction of physiological symptoms across most models, whereas psychological complaints, administrative requests, and complex somatic features remained challenging. These findings establish a practical, privacy-preserving blueprint for deploying open-source SLMs in multilingual clinical NLP settings with limited infrastructure and annotation resources, and highlight the importance of jointly optimizing model scale and input language strategy for sensitive healthcare applications.",
    "authors": [
      "Mohammadreza Ghaffarzadeh-Esfahani",
      "Nahid Yousefian",
      "Ebrahim Heidari-Farsani",
      "Ali Akbar Omidvarian",
      "Sepehr Ghahraei",
      "Atena Farangi",
      "AmirBahador Boroumand"
    ],
    "url": "https://arxiv.org/abs/2602.21374",
    "date": "2026-02-24T21:10:29.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.16729",
    "title": "Intent Laundering: AI Safety Datasets Are Not What They Seem",
    "abstract": "We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world adversarial attacks based on three key properties: being driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on \"triggering cues\": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce \"intent laundering\": a procedure that abstracts away triggering cues from adversarial attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world adversarial behavior due to their overreliance on triggering cues. Once these cues are removed, all previously evaluated \"reasonably safe\" models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated by existing datasets and how real-world adversaries behave.",
    "authors": [
      "Shahriar Golchin",
      "Marc Wetter"
    ],
    "url": "https://arxiv.org/abs/2602.16729",
    "date": "2026-02-17T18:29:22.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.20273",
    "title": "The Truthfulness Spectrum Hypothesis",
    "abstract": "Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding's generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization (R^2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models' sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. Code for all experiments is provided in https://github.com/zfying/truth_spec.",
    "authors": [
      "Zhuofan Josh Ying",
      "Shauli Ravfogel",
      "Nikolaus Kriegeskorte",
      "Peter Hase"
    ],
    "url": "https://arxiv.org/abs/2602.20273",
    "date": "2026-02-23T19:01:31.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.19594",
    "title": "ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?",
    "abstract": "We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model.",
    "authors": [
      "Ayush Nangia",
      "Shikhar Mishra",
      "Aman Gokrani",
      "Paras Chopra"
    ],
    "url": "https://arxiv.org/abs/2602.19594",
    "date": "2026-02-23T08:37:53.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.18589",
    "title": "DM4CT: Benchmarking Diffusion Models for Computed Tomography Reconstruction",
    "abstract": "Diffusion models have recently emerged as powerful priors for solving inverse problems. While computed tomography (CT) is theoretically a linear inverse problem, it poses many practical challenges. These include correlated noise, artifact structures, reliance on system geometry, and misaligned value ranges, which make the direct application of diffusion models more difficult than in domains like natural image generation. To systematically evaluate how diffusion models perform in this context and compare them with established reconstruction methods, we introduce DM4CT, a comprehensive benchmark for CT reconstruction. DM4CT includes datasets from both medical and industrial domains with sparse-view and noisy configurations. To explore the challenges of deploying diffusion models in practice, we additionally acquire a high-resolution CT dataset at a high-energy synchrotron facility and evaluate all methods under real experimental conditions. We benchmark ten recent diffusion-based methods alongside seven strong baselines, including model-based, unsupervised, and supervised approaches. Our analysis provides detailed insights into the behavior, strengths, and limitations of diffusion models for CT reconstruction. The real-world dataset is publicly available at zenodo.org/records/15420527, and the codebase is open-sourced at github.com/DM4CT/DM4CT.",
    "authors": [
      "Jiayang Shi",
      "Daniel M. Pelt",
      "K. Joost Batenburg"
    ],
    "url": "https://arxiv.org/abs/2602.18589",
    "date": "2026-02-20T19:54:47.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.21778",
    "title": "From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors",
    "abstract": "Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models.",
    "authors": [
      "Liangbing Zhao",
      "Le Zhuo",
      "Sayak Paul",
      "Hongsheng Li",
      "Mohamed Elhoseiny"
    ],
    "url": "https://arxiv.org/abs/2602.21778",
    "date": "2026-02-25T10:54:46.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.21548",
    "title": "DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference",
    "abstract": "The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput.\n  We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path -- which inherently avoids network congestion and avoids interference with latency-critical model execution communications -- with a global scheduler that dynamically balances load across prefill and decode engines.\n  Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87times on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96times without violating SLO.",
    "authors": [
      "Yongtong Wu",
      "Shaoyuan Chen",
      "Yinmin Zhong",
      "Rilin Huang",
      "Yixuan Tan",
      "Wentao Zhang",
      "Liyue Zhang",
      "Shangyan Zhou",
      "Yuxuan Liu",
      "Shunfeng Zhou",
      "Mingxing Zhang",
      "Xin Jin",
      "Panpan Huang"
    ],
    "url": "https://arxiv.org/abs/2602.21548",
    "date": "2026-02-25T04:10:58.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.18964",
    "title": "Yor-Sarc: A gold-standard dataset for sarcasm detection in a low-resource African language",
    "abstract": "Sarcasm detection poses a fundamental challenge in computational semantics, requiring models to resolve disparities between literal and intended meaning. The challenge is amplified in low-resource languages where annotated datasets are scarce or nonexistent. We present Yor-Sarc, the first gold-standard dataset for sarcasm detection in Yorùbá, a tonal Niger-Congo language spoken by over 50 million people. The dataset comprises 436 instances annotated by three native speakers from diverse dialectal backgrounds using an annotation protocol specifically designed for Yorùbá sarcasm by taking culture into account. This protocol incorporates context-sensitive interpretation and community-informed guidelines and is accompanied by a comprehensive analysis of inter-annotator agreement to support replication in other African languages. Substantial to almost perfect agreement was achieved (Fleiss' κ= 0.7660; pairwise Cohen's κ= 0.6732--0.8743), with 83.3% unanimous consensus. One annotator pair achieved almost perfect agreement (κ= 0.8743; 93.8% raw agreement), exceeding a number of reported benchmarks for English sarcasm research works. The remaining 16.7% majority-agreement cases are preserved as soft labels for uncertainty-aware modelling. Yor-Sarchttps://github.com/toheebadura/yor-sarc is expected to facilitate research on semantic interpretation and culturally informed NLP for low-resource African languages.",
    "authors": [
      "Toheeb Aduramomi Jimoh",
      "Tabea De Wille",
      "Nikola S. Nikolov"
    ],
    "url": "https://arxiv.org/abs/2602.18964",
    "date": "2026-02-21T22:10:18.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.17602",
    "title": "MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models",
    "abstract": "Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.",
    "authors": [
      "Hojung Jung",
      "Rodrigo Hormazabal",
      "Jaehyeong Jo",
      "Youngrok Park",
      "Kyunggeun Roh",
      "Se-Young Yun",
      "Sehui Han",
      "Dae-Woong Jeong"
    ],
    "url": "https://arxiv.org/abs/2602.17602",
    "date": "2026-02-19T18:27:11.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.20857",
    "title": "Functional Continuous Decomposition",
    "abstract": "The analysis of non-stationary time-series data requires insight into its local and global patterns with physical interpretability. However, traditional smoothing algorithms, such as B-splines, Savitzky-Golay filtering, and Empirical Mode Decomposition (EMD), lack the ability to perform parametric optimization with guaranteed continuity. In this paper, we propose Functional Continuous Decomposition (FCD), a JAX-accelerated framework that performs parametric, continuous optimization on a wide range of mathematical functions. By using Levenberg-Marquardt optimization to achieve up to C^1 continuous fitting, FCD transforms raw time-series data into M modes that capture different temporal patterns from short-term to long-term trends. Applications of FCD include physics, medicine, financial analysis, and machine learning, where it is commonly used for the analysis of signal temporal patterns, optimized parameters, derivatives, and integrals of decomposition. Furthermore, FCD can be applied for physical analysis and feature extraction with an average SRMSE of 0.735 per segment and a speed of 0.47s on full decomposition of 1,000 points. Finally, we demonstrate that a Convolutional Neural Network (CNN) enhanced with FCD features, such as optimized function values, parameters, and derivatives, achieved 16.8% faster convergence and 2.5% higher accuracy over a standard CNN.",
    "authors": [
      "Teymur Aghayev"
    ],
    "url": "https://arxiv.org/abs/2602.20857",
    "date": "2026-02-24T12:58:21.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.21461",
    "title": "VecGlypher: Unified Vector Glyph Generation with Language Models",
    "abstract": "Vector glyphs are the atomic units of digital typography, yet most learning-based pipelines still depend on carefully curated exemplar sheets and raster-to-vector postprocessing, which limits accessibility and editability. We introduce VecGlypher, a single multimodal language model that generates high-fidelity vector glyphs directly from text descriptions or image exemplars. Given a style prompt, optional reference glyph images, and a target character, VecGlypher autoregressively emits SVG path tokens, avoiding raster intermediates and producing editable, watertight outlines in one pass. A typography-aware data and training recipe makes this possible: (i) a large-scale continuation stage on 39K noisy Envato fonts to master SVG syntax and long-horizon geometry, followed by (ii) post-training on 2.5K expert-annotated Google Fonts with descriptive tags and exemplars to align language and imagery with geometry; preprocessing normalizes coordinate frames, canonicalizes paths, de-duplicates families, and quantizes coordinates for stable long-sequence decoding. On cross-family OOD evaluation, VecGlypher substantially outperforms both general-purpose LLMs and specialized vector-font baselines for text-only generation, while image-referenced generation reaches a state-of-the-art performance, with marked gains over DeepVecFont-v2 and DualVector. Ablations show that model scale and the two-stage recipe are critical and that absolute-coordinate serialization yields the best geometry. VecGlypher lowers the barrier to font creation by letting users design with words or exemplars, and provides a scalable foundation for future multimodal design tools.",
    "authors": [
      "Xiaoke Huang",
      "Bhavul Gauri",
      "Kam Woh Ng",
      "Tony Ng",
      "Mengmeng Xu",
      "Zhiheng Liu",
      "Weiming Ren",
      "Zhaochong An",
      "Zijian Zhou",
      "Haonan Qiu",
      "Yuyin Zhou",
      "Sen He",
      "Ziheng Wang",
      "Tao Xiang",
      "Xiao Han"
    ],
    "url": "https://arxiv.org/abs/2602.21461",
    "date": "2026-02-25T00:27:23.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.18993",
    "title": "SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models",
    "abstract": "Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs.",
    "authors": [
      "Jiwoo Chung",
      "Sangeek Hyun",
      "MinKyu Lee",
      "Byeongju Han",
      "Geonho Cha",
      "Dongyoon Wee",
      "Youngjun Hong",
      "Jae-Pil Heo"
    ],
    "url": "https://arxiv.org/abs/2602.18993",
    "date": "2026-02-22T00:48:03.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.15030",
    "title": "Image Generation with a Sphere Encoder",
    "abstract": "We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .",
    "authors": [
      "Kaiyu Yue",
      "Menglin Jia",
      "Ji Hou",
      "Tom Goldstein"
    ],
    "url": "https://arxiv.org/abs/2602.15030",
    "date": "2026-02-16T18:59:57.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.20122",
    "title": "NanoKnow: How to Know What Your Language Model Knows",
    "abstract": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.",
    "authors": [
      "Lingwei Gu",
      "Nour Jedidi",
      "Jimmy Lin"
    ],
    "url": "https://arxiv.org/abs/2602.20122",
    "date": "2026-02-23T18:37:49.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.22190",
    "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL",
    "abstract": "Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.",
    "authors": [
      "Rui Yang",
      "Qianhui Wu",
      "Zhaoyang Wang",
      "Hanyang Chen",
      "Ke Yang",
      "Hao Cheng",
      "Huaxiu Yao",
      "Baoling Peng",
      "Huan Zhang",
      "Jianfeng Gao",
      "Tong Zhang"
    ],
    "url": "https://arxiv.org/abs/2602.22190",
    "date": "2026-02-25T18:34:57.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.19004",
    "title": "MoBind: Motion Binding for Fine-Grained IMU-Video Pose Alignment",
    "abstract": "We aim to learn a joint representation between inertial measurement unit (IMU) signals and 2D pose sequences extracted from video, enabling accurate cross-modal retrieval, temporal synchronization, subject and body-part localization, and action recognition. To this end, we introduce MoBind, a hierarchical contrastive learning framework designed to address three challenges: (1) filtering out irrelevant visual background, (2) modeling structured multi-sensor IMU configurations, and (3) achieving fine-grained, sub-second temporal alignment. To isolate motion-relevant cues, MoBind aligns IMU signals with skeletal motion sequences rather than raw pixels. We further decompose full-body motion into local body-part trajectories, pairing each with its corresponding IMU to enable semantically grounded multi-sensor alignment. To capture detailed temporal correspondence, MoBind employs a hierarchical contrastive strategy that first aligns token-level temporal segments, then fuses local (body-part) alignment with global (body-wide) motion aggregation. Evaluated on mRi, TotalCapture, and EgoHumans, MoBind consistently outperforms strong baselines across all four tasks, demonstrating robust fine-grained temporal alignment while preserving coarse semantic consistency across modalities. Code is available at https://github.com/bbvisual/ MoBind.",
    "authors": [
      "Duc Duy Nguyen",
      "Tat-Jun Chin",
      "Minh Hoai"
    ],
    "url": "https://arxiv.org/abs/2602.19004",
    "date": "2026-02-22T01:54:29.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.22144",
    "title": "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors",
    "abstract": "Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.",
    "authors": [
      "Lingfeng Ren",
      "Weihao Yu",
      "Runpeng Yu",
      "Xinchao Wang"
    ],
    "url": "https://arxiv.org/abs/2602.22144",
    "date": "2026-02-25T17:50:41.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.12160",
    "title": "DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation",
    "abstract": "Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.",
    "authors": [
      "Xu Guo",
      "Fulong Ye",
      "Qichao Sun",
      "Liyang Chen",
      "Bingchuan Li",
      "Pengze Zhang",
      "Jiawei Liu",
      "Songtao Zhao",
      "Qian He",
      "Xiangwang Hou"
    ],
    "url": "https://arxiv.org/abs/2602.12160",
    "date": "2026-02-12T16:41:52.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.18283",
    "title": "HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation",
    "abstract": "Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.",
    "authors": [
      "Lei Xin",
      "Yuhao Zheng",
      "Ke Cheng",
      "Changjiang Jiang",
      "Zifan Zhang",
      "Fanhu Zeng"
    ],
    "url": "https://arxiv.org/abs/2602.18283",
    "date": "2026-02-20T15:11:40.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.21472",
    "title": "The Design Space of Tri-Modal Masked Diffusion Models",
    "abstract": "Discrete diffusion models have emerged as strong alternatives to autoregressive language models, with recent work initializing and fine-tuning a base unimodal model for bimodal generation. Diverging from previous approaches, we introduce the first tri-modal masked diffusion model pretrained from scratch on text, image-text, and audio-text data. We systematically analyze multimodal scaling laws, modality mixing ratios, noise schedules, and batch-size effects, and we provide optimized inference sampling defaults. Our batch-size analysis yields a novel stochastic differential equation (SDE)-based reparameterization that eliminates the need for tuning the optimal batch size as reported in recent work. This reparameterization decouples the physical batch size, often chosen based on compute constraints (GPU saturation, FLOP efficiency, wall-clock time), from the logical batch size, chosen to balance gradient variance during stochastic optimization. Finally, we pretrain a preliminary 3B-parameter tri-modal model on 6.4T tokens, demonstrating the capabilities of a unified design and achieving strong results in text generation, text-to-image tasks, and text-to-speech tasks. Our work represents the largest-scale systematic open study of multimodal discrete diffusion models conducted to date, providing insights into scaling behaviors across multiple modalities.",
    "authors": [
      "Louis Bethune",
      "Victor Turrisi",
      "Bruno Kacper Mlodozeniec",
      "Pau Rodriguez Lopez",
      "Lokesh Boominathan",
      "Nikhil Bhendawade",
      "Amitis Shidani",
      "Joris Pelemans",
      "Theo X. Olausson",
      "Devon Hjelm",
      "Paul Dixon",
      "Joao Monteiro",
      "Pierre Ablin",
      "Vishnu Banna",
      "Arno Blaas",
      "Nick Henderson",
      "Kari Noriy",
      "Dan Busbridge",
      "Josh Susskind",
      "Marco Cuturi",
      "Irina Belousova",
      "Luca Zappella",
      "Russ Webb",
      "Jason Ramapuram"
    ],
    "url": "https://arxiv.org/abs/2602.21472",
    "date": "2026-02-25T01:02:11.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.18527",
    "title": "JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments",
    "abstract": "Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D perception, relying on RGB video and monaural audio. This design choice introduces a fundamental dimensionality mismatch that precludes reliable source localization and spatial reasoning in complex 3D environments. We address this limitation by presenting JAEGER, a framework that extends AV-LLMs to 3D space, to enable joint spatial grounding and reasoning through the integration of RGB-D observations and multi-channel first-order ambisonics. A core contribution of our work is the neural intensity vector (Neural IV), a learned spatial audio representation that encodes robust directional cues to enhance direction-of-arrival estimation, even in adverse acoustic scenarios with overlapping sources. To facilitate large-scale training and systematic evaluation, we propose SpatialSceneQA, a benchmark of 61k instruction-tuning samples curated from simulated physical environments. Extensive experiments demonstrate that our approach consistently surpasses 2D-centric baselines across diverse spatial perception and reasoning tasks, underscoring the necessity of explicit 3D modelling for advancing AI in physical environments. Our source code, pre-trained model checkpoints and datasets will be released upon acceptance.",
    "authors": [
      "Zhan Liu",
      "Changli Tang",
      "Yuxin Wang",
      "Zhiyuan Zhu",
      "Youjun Chen",
      "Yiwen Shao",
      "Tianzi Wang",
      "Lei Ke",
      "Zengrui Jin",
      "Chao Zhang"
    ],
    "url": "https://arxiv.org/abs/2602.18527",
    "date": "2026-02-20T04:06:07.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.21534",
    "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning",
    "abstract": "Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.",
    "authors": [
      "Xiaoxuan Wang",
      "Han Zhang",
      "Haixin Wang",
      "Yidan Shi",
      "Ruoyan Li",
      "Kaiqiao Han",
      "Chenyi Tong",
      "Haoran Deng",
      "Renliang Sun",
      "Alexander Taylor",
      "Yanqiao Zhu",
      "Jason Cong",
      "Yizhou Sun",
      "Wei Wang"
    ],
    "url": "https://arxiv.org/abs/2602.21534",
    "date": "2026-02-25T03:43:34.000Z",
    "source": "huggingface_daily"
  },
  {
    "paper_id": "2602.16456v1",
    "title": "Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC",
    "abstract": "Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank updates on top of frozen weights, dramatically reducing trainable parameters and memory. In this work, we address the gap between training with full steps with low-rank projections (SVDLoRA) and LoRA fine-tuning. We propose LoRSum, a memory-efficient subroutine that closes this gap for gradient descent by casting LoRA optimization as a proximal sub-problem and solving it efficiently with alternating least squares updates, which we prove to be an implicit block power method. We recover several recently proposed preconditioning methods for LoRA as special cases, and show that LoRSum can also be used for updating a low-rank momentum. In order to address full steps with preconditioned gradient descent, we propose a scaled variant of LoRSum that uses structured metrics such as K-FAC and Shampoo, and we show that storing the diagonal of these metrics still allows them to perform well while remaining memory-efficient. Experiments on a synthetic task, CIFAR-100, and language-model fine-tuning on GLUE, SQuAD v2, and WikiText-103, show that our method can match or improve LoRA baselines given modest compute overhead, while avoiding full-matrix SVD projections and retaining LoRA-style parameter efficiency.",
    "authors": [
      "Abdulla Jasem Almansoori",
      "Maria Ivanova",
      "Andrey Veprikov",
      "Aleksandr Beznosikov",
      "Samuel Horváth",
      "Martin Takáč"
    ],
    "url": "https://arxiv.org/abs/2602.16456v1",
    "date": "2026-02-18T13:41:41Z",
    "source": "arxiv_search",
    "keyword": "LoRA fine-tuning"
  },
  {
    "paper_id": "2602.09703v1",
    "title": "Maastricht University at AMIYA: Adapting LLMs for Dialectal Arabic using Fine-tuning and MBR Decoding",
    "abstract": "Large Language Models (LLMs) are becoming increasingly multilingual, supporting hundreds of languages, especially high resource ones. Unfortunately, Dialect variations are still underrepresented due to limited data and linguistic variation. In this work, we adapt a pre-trained LLM to improve dialectal performance. Specifically, we use Low Rank Adaptation (LoRA) fine-tuning on monolingual and English Dialect parallel data, adapter merging and dialect-aware MBR decoding to improve dialectal fidelity generation and translation. Experiments on Syrian, Moroccan, and Saudi Arabic show that merging and MBR improve dialectal fidelity while preserving semantic accuracy. This combination provides a compact and effective framework for robust dialectal Arabic generation.",
    "authors": [
      "Abdulhai Alali",
      "Abderrahmane Issam"
    ],
    "url": "https://arxiv.org/abs/2602.09703v1",
    "date": "2026-02-10T12:02:34Z",
    "source": "arxiv_search",
    "keyword": "LoRA fine-tuning"
  },
  {
    "paper_id": "2602.08999v1",
    "title": "CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion",
    "abstract": "With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue",
    "authors": [
      "Mouad Abrini",
      "Mohamed Chetouani"
    ],
    "url": "https://arxiv.org/abs/2602.08999v1",
    "date": "2026-02-09T18:44:35Z",
    "source": "arxiv_search",
    "keyword": "LoRA fine-tuning"
  },
  {
    "paper_id": "2602.08794v2",
    "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
    "abstract": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
    "authors": [
      "SII-OpenMOSS Team",
      " :",
      "Donghua Yu",
      "Mingshu Chen",
      "Qi Chen",
      "Qi Luo",
      "Qianyi Wu",
      "Qinyuan Cheng",
      "Ruixiao Li",
      "Tianyi Liang",
      "Wenbo Zhang",
      "Wenming Tu",
      "Xiangyu Peng",
      "Yang Gao",
      "Yanru Huo",
      "Ying Zhu",
      "Yinze Luo",
      "Yiyang Zhang",
      "Yuerong Song",
      "Zhe Xu",
      "Zhiyu Zhang",
      "Chenchen Yang",
      "Cheng Chang",
      "Chushu Zhou",
      "Hanfu Chen",
      "Hongnan Ma",
      "Jiaxi Li",
      "Jingqi Tong",
      "Junxi Liu",
      "Ke Chen",
      "Shimin Li",
      "Shiqi Jiang",
      "Songlin Wang",
      "Wei Jiang",
      "Zhaoye Fei",
      "Zhiyuan Ning",
      "Chunguo Li",
      "Chenhui Li",
      "Ziwei He",
      "Zengfeng Huang",
      "Xie Chen",
      "Xipeng Qiu"
    ],
    "url": "https://arxiv.org/abs/2602.08794v2",
    "date": "2026-02-09T15:31:54Z",
    "source": "arxiv_search",
    "keyword": "LoRA fine-tuning"
  },
  {
    "paper_id": "2602.06385v1",
    "title": "Uniform Spectral Growth and Convergence of Muon in LoRA-Style Matrix Factorization",
    "abstract": "Spectral gradient descent (SpecGD) orthogonalizes the matrix parameter updates and has inspired practical optimizers such as Muon. They often perform well in large language model (LLM) training, but their dynamics remain poorly understood. In the low-rank adaptation (LoRA) setting, where weight updates are parameterized as a product of two low-rank factors, we find a distinctive spectral phenomenon under Muon in LoRA fine-tuning of LLMs: singular values of the LoRA product show near-uniform growth across the spectrum, despite orthogonalization being performed on the two factors separately. Motivated by this observation, we analyze spectral gradient flow (SpecGF)-a continuous-time analogue of SpecGD-in a simplified LoRA-style matrix factorization setting and prove \"equal-rate\" dynamics: all singular values grow at equal rates up to small deviations. Consequently, smaller singular values attain their target values earlier than larger ones, sharply contrasting with the largest-first stepwise learning observed in standard gradient flow. Moreover, we prove that SpecGF in our setting converges to global minima from almost all initializations, provided the factor norms remain bounded; with $\\ell_2$ regularization, we obtain global convergence. Lastly, we corroborate our theory with experiments in the same setting.",
    "authors": [
      "Changmin Kang",
      "Jihun Yun",
      "Baekrok Shin",
      "Yeseul Cho",
      "Chulhee Yun"
    ],
    "url": "https://arxiv.org/abs/2602.06385v1",
    "date": "2026-02-06T04:47:06Z",
    "source": "arxiv_search",
    "keyword": "LoRA fine-tuning"
  },
  {
    "paper_id": "2602.05988v1",
    "title": "Layer-wise LoRA fine-tuning: a similarity metric approach",
    "abstract": "Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA",
    "authors": [
      "Keith Ando Ogawa",
      "Bruno Lopes Yamamoto",
      "Lucas Lauton de Alcantara",
      "Lucas Pellicer",
      "Rosimeire Pereira Costa",
      "Edson Bollis",
      "Anna Helena Reali Costa",
      "Artur Jordao"
    ],
    "url": "https://arxiv.org/abs/2602.05988v1",
    "date": "2026-02-05T18:38:53Z",
    "source": "arxiv_search",
    "keyword": "LoRA fine-tuning"
  },
  {
    "paper_id": "2602.02855v1",
    "title": "When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models",
    "abstract": "Pre-training on a source task is usually expected to facilitate fine-tuning on similar downstream problems. In this work, we mathematically show that this naive intuition is not always true: excessive pre-training can computationally slow down fine-tuning optimization. We study this phenomenon for low-rank adaptation (LoRA) fine-tuning on single-index models trained under one-pass SGD. Leveraging a summary statistics description of the fine-tuning dynamics, we precisely characterize how the convergence rate depends on the initial fine-tuning alignment and the degree of non-linearity of the target task. The key take away is that even when the pre-training and down- stream tasks are well aligned, strong pre-training can induce a prolonged search phase and hinder convergence. Our theory thus provides a unified picture of how pre-training strength and task difficulty jointly shape the dynamics and limitations of LoRA fine-tuning in a nontrivial tractable model.",
    "authors": [
      "Gibbs Nwemadji",
      "Bruno Loureiro",
      "Jean Barbier"
    ],
    "url": "https://arxiv.org/abs/2602.02855v1",
    "date": "2026-02-02T22:02:52Z",
    "source": "arxiv_search",
    "keyword": "LoRA fine-tuning"
  },
  {
    "paper_id": "2602.15874v1",
    "title": "P-RAG: Prompt-Enhanced Parametric RAG with LoRA and Selective CoT for Biomedical and Multi-Hop QA",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities but remain limited by their reliance on static training data. Retrieval-Augmented Generation (RAG) addresses this constraint by retrieving external knowledge during inference, though it still depends heavily on knowledge base quality. To explore potential improvements, we evaluated three RAG variants-Standard RAG, DA-RAG, and our proposed Prompt-Enhanced Parametric RAG (P-RAG), a hybrid architecture that integrates parametric knowledge within the LLM and retrieved evidence, guided by Chain-of-Thought (CoT) prompting and Low-Rank Adaptation (LoRA) fine-tuning-on both general and biomedical datasets. Using LLaMA-3.2-1B-Instruct fine-tuned via LoRA, we evaluate on PubMedQA and 2WikiMultihopQA. P-RAG outperforms Standard RAG on PubMedQA by 10.47 percentage points in F1 (93.33% vs. 82.86%; 12.64% relative). On 2WikiMultihopQA, P-RAG nearly doubles the overall score vs. Standard RAG (33.44% vs. 17.83%) and achieves 44.03% on the Compare subset (with 42.74% Bridge, 21.84% Inference, 8.60% Compose). CoT prompting substantially improves multi-hop reasoning but yields mixed results for simpler, single-hop queries. These findings underscore P-RAG's potential for accurate, scalable, and contextually adaptive biomedical question answering. Our contributions include: (1) LoRA-based fine-tuning of LLaMA-3.2-1B-Instruct for biomedical QA, (2) introduction of P-RAG with Chain-of-Thought prompting, and (3) state-of-the-art results on PubMedQA and 2WikiMultihopQA.",
    "authors": [
      "Xingda Lyu",
      "Gongfu Lyu",
      "Zitai Yan",
      "Yuxin Jiang"
    ],
    "url": "https://arxiv.org/abs/2602.15874v1",
    "date": "2026-02-02T03:42:45Z",
    "source": "arxiv_search",
    "keyword": "LoRA fine-tuning"
  },
  {
    "paper_id": "2602.01237v1",
    "title": "Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models",
    "abstract": "Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.",
    "authors": [
      "Katrina Brown",
      "Aneesh Muppidi",
      "Rana Shahout"
    ],
    "url": "https://arxiv.org/abs/2602.01237v1",
    "date": "2026-02-01T13:58:23Z",
    "source": "arxiv_search",
    "keyword": "LoRA fine-tuning"
  },
  {
    "paper_id": "2602.01082v1",
    "title": "EvoOpt-LLM: Evolving industrial optimization models with large language models",
    "abstract": "Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.",
    "authors": [
      "Yiliu He",
      "Tianle Li",
      "Binghao Ji",
      "Zhiyuan Liu",
      "Di Huang"
    ],
    "url": "https://arxiv.org/abs/2602.01082v1",
    "date": "2026-02-01T07:58:55Z",
    "source": "arxiv_search",
    "keyword": "LoRA fine-tuning"
  },
  {
    "paper_id": "2602.22681v1",
    "title": "Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement",
    "abstract": "Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.",
    "authors": [
      "Shuchen Zhu",
      "Rizhen Hu",
      "Mingze Wang",
      "Mou Sun",
      "Xue Wang",
      "Kun Yuan",
      "Zaiwen Wen"
    ],
    "url": "https://arxiv.org/abs/2602.22681v1",
    "date": "2026-02-26T06:54:57Z",
    "source": "arxiv_search",
    "keyword": "learning rate schedule"
  },
  {
    "paper_id": "2602.18002v1",
    "title": "Asynchronous Heavy-Tailed Optimization",
    "abstract": "Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.",
    "authors": [
      "Junfei Sun",
      "Dixi Yao",
      "Xuchen Gong",
      "Tahseen Rabbani",
      "Manzil Zaheer",
      "Tian Li"
    ],
    "url": "https://arxiv.org/abs/2602.18002v1",
    "date": "2026-02-20T05:28:48Z",
    "source": "arxiv_search",
    "keyword": "learning rate schedule"
  },
  {
    "paper_id": "2602.17876v1",
    "title": "Interactive Learning of Single-Index Models via Stochastic Gradient Descent",
    "abstract": "Stochastic gradient descent (SGD) is a cornerstone algorithm for high-dimensional optimization, renowned for its empirical successes. Recent theoretical advances have provided a deep understanding of how SGD enables feature learning in high-dimensional nonlinear models, most notably the \\textit{single-index model} with i.i.d. data. In this work, we study the sequential learning problem for single-index models, also known as generalized linear bandits or ridge bandits, where SGD is a simple and natural solution, yet its learning dynamics remain largely unexplored. We show that, similar to the optimal interactive learner, SGD undergoes a distinct ``burn-in'' phase before entering the ``learning'' phase in this setting. Moreover, with an appropriately chosen learning rate schedule, a single SGD procedure simultaneously achieves near-optimal (or best-known) sample complexity and regret guarantees across both phases, for a broad class of link functions. Our results demonstrate that SGD remains highly competitive for learning single-index models under adaptive data.",
    "authors": [
      "Nived Rajaraman",
      "Yanjun Han"
    ],
    "url": "https://arxiv.org/abs/2602.17876v1",
    "date": "2026-02-19T22:22:45Z",
    "source": "arxiv_search",
    "keyword": "learning rate schedule"
  },
  {
    "paper_id": "2602.16340v1",
    "title": "The Implicit Bias of Adam and Muon on Smooth Homogeneous Neural Networks",
    "abstract": "We study the implicit bias of momentum-based optimizers on homogeneous models. We first extend existing results on the implicit bias of steepest descent in homogeneous models to normalized steepest descent with an optional learning rate schedule. We then show that for smooth homogeneous models, momentum steepest descent algorithms like Muon (spectral norm), MomentumGD ($\\ell_2$ norm), and Signum ($\\ell_\\infty$ norm) are approximate steepest descent trajectories under a decaying learning rate schedule, proving that these algorithms too have a bias towards KKT points of the corresponding margin maximization problem. We extend the analysis to Adam (without the stability constant), which maximizes the $\\ell_\\infty$ margin, and to Muon-Signum and Muon-Adam, which maximize a hybrid norm. Our experiments corroborate the theory and show that the identity of the margin maximized depends on the choice of optimizer. Overall, our results extend earlier lines of work on steepest descent in homogeneous models and momentum-based optimizers in linear models.",
    "authors": [
      "Eitan Gronich",
      "Gal Vardi"
    ],
    "url": "https://arxiv.org/abs/2602.16340v1",
    "date": "2026-02-18T10:25:07Z",
    "source": "arxiv_search",
    "keyword": "learning rate schedule"
  },
  {
    "paper_id": "2602.07145v1",
    "title": "Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate",
    "abstract": "Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly becomes weakly convex after a short period of training, and the loss is predicable by an upper bound on the last iterate, which further informs the scaling of optimal learning rate. Through the lens of convexity, we build scaling laws of learning rates and losses that extrapolate as much as 80X across training horizons and 70X across model sizes.",
    "authors": [
      "Zhiqi Bu",
      "Shiyun Xu",
      "Jialin Mao"
    ],
    "url": "https://arxiv.org/abs/2602.07145v1",
    "date": "2026-02-06T19:41:59Z",
    "source": "arxiv_search",
    "keyword": "learning rate schedule"
  },
  {
    "paper_id": "2602.06797v2",
    "title": "Optimal Learning-Rate Schedules under Functional Scaling Laws: Power Decay and Warmup-Stable-Decay",
    "abstract": "We study optimal learning-rate schedules (LRSs) under the functional scaling law (FSL) framework introduced in Li et al. (2025), which accurately models the loss dynamics of both linear regression and large language model (LLM) pre-training. Within FSL, loss dynamics are governed by two exponents: a source exponent $s&gt;0$ controlling the rate of signal learning, and a capacity exponent $β&gt;1$ determining the rate of noise forgetting. Focusing on a fixed training horizon $N$, we derive the optimal LRSs and reveal a sharp phase transition. In the easy-task regime $s \\ge 1 - 1/β$, the optimal schedule follows a power decay to zero, $η^*(z) = η_{\\mathrm{peak}}(1 - z/N)^{2β- 1}$, where the peak learning rate scales as $η_{\\mathrm{peak}} \\eqsim N^{-ν}$ for an explicit exponent $ν= ν(s,β)$. In contrast, in the hard-task regime $s &lt; 1 - 1/β$, the optimal LRS exhibits a warmup-stable-decay (WSD) (Hu et al. (2024)) structure: it maintains the largest admissible learning rate for most of training and decays only near the end, with the decay phase occupying a vanishing fraction of the horizon.   We further analyze optimal shape-fixed schedules, where only the peak learning rate is tuned -- a strategy widely adopted in practiceand characterize their strengths and intrinsic limitations. This yields a principled evaluation of commonly used schedules such as cosine and linear decay. Finally, we apply the power-decay LRS to one-pass stochastic gradient descent (SGD) for kernel regression and show the last iterate attains the exact minimax-optimal rate, eliminating the logarithmic suboptimality present in prior analyses. Numerical experiments corroborate our theoretical predictions.",
    "authors": [
      "Binghui Li",
      "Zilin Wang",
      "Fengling Chen",
      "Shiyang Zhao",
      "Ruiheng Zheng",
      "Lei Wu"
    ],
    "url": "https://arxiv.org/abs/2602.06797v2",
    "date": "2026-02-06T15:52:30Z",
    "source": "arxiv_search",
    "keyword": "learning rate schedule"
  },
  {
    "paper_id": "2602.05813v1",
    "title": "Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers",
    "abstract": "We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.   Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup",
    "authors": [
      "Artem Riabinin",
      "Andrey Veprikov",
      "Arman Bolatov",
      "Martin Takáč",
      "Aleksandr Beznosikov"
    ],
    "url": "https://arxiv.org/abs/2602.05813v1",
    "date": "2026-02-05T16:06:19Z",
    "source": "arxiv_search",
    "keyword": "learning rate schedule"
  },
  {
    "paper_id": "2602.04774v1",
    "title": "Theory of Optimal Learning Rate Schedules and Scaling Laws for a Random Feature Model",
    "abstract": "Setting the learning rate for a deep learning model is a critical part of successful training, yet choosing this hyperparameter is often done empirically with trial and error. In this work, we explore a solvable model of optimal learning rate schedules for a powerlaw random feature model trained with stochastic gradient descent (SGD). We consider the optimal schedule $η_T^\\star(t)$ where $t$ is the current iterate and $T$ is the total training horizon. This schedule is computed both numerically and analytically (when possible) using optimal control methods. Our analysis reveals two regimes which we term the easy phase and hard phase. In the easy phase the optimal schedule is a polynomial decay $η_T^\\star(t) \\simeq T^{-ξ} (1-t/T)^δ$ where $ξ$ and $δ$ depend on the properties of the features and task. In the hard phase, the optimal schedule resembles warmup-stable-decay with constant (in $T$) initial learning rate and annealing performed over a vanishing (in $T$) fraction of training steps. We investigate joint optimization of learning rate and batch size, identifying a degenerate optimality condition. Our model also predicts the compute-optimal scaling laws (where model size and training steps are chosen optimally) in both easy and hard regimes. Going beyond SGD, we consider optimal schedules for the momentum $β(t)$, where speedups in the hard phase are possible. We compare our optimal schedule to various benchmarks in our task including (1) optimal constant learning rates $η_T(t) \\sim T^{-ξ}$ (2) optimal power laws $η_T(t) \\sim T^{-ξ} t^{-χ}$, finding that our schedule achieves better rates than either of these. Our theory suggests that learning rate transfer across training horizon depends on the structure of the model and task. We explore these ideas in simple experimental pretraining setups.",
    "authors": [
      "Blake Bordelon",
      "Francesco Mori"
    ],
    "url": "https://arxiv.org/abs/2602.04774v1",
    "date": "2026-02-04T17:11:36Z",
    "source": "arxiv_search",
    "keyword": "learning rate schedule"
  },
  {
    "paper_id": "2602.03702v1",
    "title": "Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging",
    "abstract": "Large language models are increasingly trained in continual or open-ended settings, where the total training horizon is not known in advance. Despite this, most existing pretraining recipes are not anytime: they rely on horizon-dependent learning rate schedules and extensive tuning under a fixed compute budget. In this work, we provide a theoretical analysis demonstrating the existence of anytime learning schedules for overparameterized linear regression, and we highlight the central role of weight averaging - also known as model merging - in achieving the minimax convergence rates of stochastic gradient descent. We show that these anytime schedules polynomially decay with time, with the decay rate determined by the source and capacity conditions of the problem. Empirically, we evaluate 150M and 300M parameter language models trained at 1-32x Chinchilla scale, comparing constant learning rates with weight averaging and $1/\\sqrt{t}$ schedules with weight averaging against a well-tuned cosine schedule. Across the full training range, the anytime schedules achieve comparable final loss to cosine decay. Taken together, our results suggest that weight averaging combined with simple, horizon-free step sizes offers a practical and effective anytime alternative to cosine learning rate schedules for large language model pretraining.",
    "authors": [
      "Alexandru Meterez",
      "Pranav Ajit Nair",
      "Depen Morwani",
      "Cengiz Pehlevan",
      "Sham Kakade"
    ],
    "url": "https://arxiv.org/abs/2602.03702v1",
    "date": "2026-02-03T16:24:05Z",
    "source": "arxiv_search",
    "keyword": "learning rate schedule"
  },
  {
    "paper_id": "2601.23034v1",
    "title": "Breaking the Stochasticity Barrier: An Adaptive Variance-Reduced Method for Variational Inequalities",
    "abstract": "Stochastic non-convex non-concave optimization, formally characterized as Stochastic Variational Inequalities (SVIs), presents unique challenges due to rotational dynamics and the absence of a global merit function. While adaptive step-size methods (like Armijo line-search) have revolutionized convex minimization, their application to this setting is hindered by the Stochasticity Barrier: the noise in gradient estimation masks the true operator curvature, triggering erroneously large steps that destabilize convergence. In this work, we propose VR-SDA-A (Variance-Reduced Stochastic Descent-Ascent with Armijo), a novel algorithm that integrates recursive momentum (STORM) with a rigorous Same-Batch Curvature Verification mechanism. We introduce a theoretical framework based on a Lyapunov potential tracking the Operator Norm, proving that VR- SDA-A achieves an oracle complexity of O(epsilon -3) for finding an epsilon-stationary point in general Lipschitz continuous operators. This matches the optimal rate for non-convex minimization while uniquely enabling automated step-size adaptation in the saddle-point setting. We validate our approach on canonical rotational benchmarks and non-convex robust regression tasks, demonstrating that our method effectively suppresses limit cycles and accelerates convergence with reduced dependence on manual learning rate scheduling.",
    "authors": [
      "Yungi Jeong",
      "Takumi Otsuka"
    ],
    "url": "https://arxiv.org/abs/2601.23034v1",
    "date": "2026-01-30T14:43:07Z",
    "source": "arxiv_search",
    "keyword": "learning rate schedule"
  },
  {
    "paper_id": "2602.22938v1",
    "title": "pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation",
    "abstract": "Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. However, this approach typically overlooks the potential synergies that could arise from integrating diverse domain knowledge within the same tuning process. In this work, we propose a novel Mixture-of-Experts prompt tuning method called pMoE, which leverages the strengths of multiple expert domains through expert-specialized prompt tokens and the learnable dispatcher, effectively combining their expertise in a unified model framework. Our pMoE introduces expert-specific prompt tokens and utilizes a dynamic token dispatching mechanism at various prompt layers to optimize the contribution of each domain expert during the adaptation phase. By incorporating both domain knowledge from diverse experts, the proposed pMoE significantly enhances the model's versatility and applicability to a broad spectrum of tasks. We conduct extensive experiments across 47 adaptation tasks, including both classification and segmentation in general and medical domains. The results demonstrate that our pMoE not only achieves superior performance with a large margin of improvements but also offers an optimal trade-off between computational efficiency and adaptation effectiveness compared to existing methods.",
    "authors": [
      "Shentong Mo",
      "Xufang Luo",
      "Dongsheng Li"
    ],
    "url": "https://arxiv.org/abs/2602.22938v1",
    "date": "2026-02-26T12:27:06Z",
    "source": "arxiv_search",
    "keyword": "parameter efficient fine-tuning"
  },
  {
    "paper_id": "2602.22911v1",
    "title": "NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion",
    "abstract": "Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) outperforms LoRA at rank 512 (PPL 3.90), demonstrating superior spectral efficiency. This advantage generalizes to mathematical reasoning, where NoRA achieves a perplexity of 1.97 on MathInstruct, significantly surpassing LoRA's saturation point of 2.07. Mechanism analysis via Singular Value Decomposition (SVD) confirms that NoRA activates the dormant tail of the singular value spectrum, effectively preventing the rank collapse observed in linear methods.",
    "authors": [
      "Hung-Hsuan Chen"
    ],
    "url": "https://arxiv.org/abs/2602.22911v1",
    "date": "2026-02-26T11:55:25Z",
    "source": "arxiv_search",
    "keyword": "parameter efficient fine-tuning"
  },
  {
    "paper_id": "2602.22695v1",
    "title": "GFRRN: Explore the Gaps in Single Image Reflection Removal",
    "abstract": "Prior dual-stream methods with the feature interaction mechanism have achieved remarkable performance in single image reflection removal (SIRR). However, they often struggle with (1) semantic understanding gap between the features of pre-trained models and those of reflection removal models, and (2) reflection label inconsistencies between synthetic and real-world training data. In this work, we first adopt the parameter efficient fine-tuning (PEFT) strategy by integrating several learnable Mona layers into the pre-trained model to align the training directions. Then, a label generator is designed to unify the reflection labels for both synthetic and real-world data. In addition, a Gaussian-based Adaptive Frequency Learning Block (G-AFLB) is proposed to adaptively learn and fuse the frequency priors, and a Dynamic Agent Attention (DAA) is employed as an alternative to window-based attention by dynamically modeling the significance levels across windows (inter-) and within an individual window (intra-). These components constitute our proposed Gap-Free Reflection Removal Network (GFRRN). Extensive experiments demonstrate the effectiveness of our GFRRN, achieving superior performance against state-of-the-art SIRR methods.",
    "authors": [
      "Yu Chen",
      "Zewei He",
      "Xingyu Liu",
      "Zixuan Chen",
      "Zheming Lu"
    ],
    "url": "https://arxiv.org/abs/2602.22695v1",
    "date": "2026-02-26T07:17:49Z",
    "source": "arxiv_search",
    "keyword": "parameter efficient fine-tuning"
  },
  {
    "paper_id": "2602.22539v1",
    "title": "Agentic AI for Intent-driven Optimization in Cell-free O-RAN",
    "abstract": "Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.",
    "authors": [
      "Mohammad Hossein Shokouhi",
      "Vincent W. S. Wong"
    ],
    "url": "https://arxiv.org/abs/2602.22539v1",
    "date": "2026-02-26T02:26:58Z",
    "source": "arxiv_search",
    "keyword": "parameter efficient fine-tuning"
  },
  {
    "paper_id": "2602.22462v1",
    "title": "MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation",
    "abstract": "Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.",
    "authors": [
      "Raiyan Jahangir",
      "Nafiz Imtiaz Khan",
      "Amritanand Sudheerkumar",
      "Vladimir Filkov"
    ],
    "url": "https://arxiv.org/abs/2602.22462v1",
    "date": "2026-02-25T22:51:31Z",
    "source": "arxiv_search",
    "keyword": "parameter efficient fine-tuning"
  },
  {
    "paper_id": "2602.22268v1",
    "title": "AutoQRA: Joint Optimization of Mixed-Precision Quantization and Low-rank Adapters for Efficient LLM Fine-Tuning",
    "abstract": "Quantization followed by parameter-efficient fine-tuning has emerged as a promising paradigm for downstream adaptation under tight GPU memory constraints. However, this sequential pipeline fails to leverage the intricate interaction between quantization bit-width and LoRA rank. Specifically, a carefully optimized quantization allocation with low quantization error does not always translate to strong fine-tuning performance, and different bit-width and rank configurations can lead to significantly varying outcomes under the same memory budget. To address this limitation, we propose AutoQRA, a joint optimization framework that simultaneously optimizes the bit-width and LoRA rank configuration for each layer during the mixed quantized fine-tuning process. To tackle the challenges posed by the large discrete search space and the high evaluation cost associated with frequent fine-tuning iterations, AutoQRA decomposes the optimization process into two stages. First, it first conducts a global multi-fidelity evolutionary search, where the initial population is warm-started by injecting layer-wise importance priors. This stage employs specific operators and a performance model to efficiently screen candidate configurations. Second, trust-region Bayesian optimization is applied to locally refine promising regions of the search space and identify optimal configurations under the given memory budget. This approach enables active compensation for quantization noise in specific layers during training. Experiments show that AutoQRA achieves performance close to full-precision fine-tuning with a memory footprint comparable to uniform 4-bit methods.",
    "authors": [
      "Changhai Zhou",
      "Shiyang Zhang",
      "Yuhua Zhou",
      "Qian Qiao",
      "Jun Gao",
      "Cheng Jin",
      "Kaizhou Qin",
      "Weizhong Zhang"
    ],
    "url": "https://arxiv.org/abs/2602.22268v1",
    "date": "2026-02-25T07:18:08Z",
    "source": "arxiv_search",
    "keyword": "parameter efficient fine-tuning"
  },
  {
    "paper_id": "2602.20727v1",
    "title": "ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition",
    "abstract": "LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.",
    "authors": [
      "Xindian Ma",
      "Rundong Kong",
      "Peng Zhang",
      "Ruoxiang Huang",
      "Yongyu Jiang"
    ],
    "url": "https://arxiv.org/abs/2602.20727v1",
    "date": "2026-02-24T09:45:10Z",
    "source": "arxiv_search",
    "keyword": "parameter efficient fine-tuning"
  },
  {
    "paper_id": "2602.20409v1",
    "title": "CLIPoint3D: Language-Grounded Few-Shot Unsupervised 3D Point Cloud Domain Adaptation",
    "abstract": "Recent vision-language models (VLMs) such as CLIP demonstrate impressive cross-modal reasoning, extending beyond images to 3D perception. Yet, these models remain fragile under domain shifts, especially when adapting from synthetic to real-world point clouds. Conventional 3D domain adaptation approaches rely on heavy trainable encoders, yielding strong accuracy but at the cost of efficiency. We introduce CLIPoint3D, the first framework for few-shot unsupervised 3D point cloud domain adaptation built upon CLIP. Our approach projects 3D samples into multiple depth maps and exploits the frozen CLIP backbone, refined through a knowledge-driven prompt tuning scheme that integrates high-level language priors with geometric cues from a lightweight 3D encoder. To adapt task-specific features effectively, we apply parameter-efficient fine-tuning to CLIP's encoders and design an entropy-guided view sampling strategy for selecting confident projections. Furthermore, an optimal transport-based alignment loss and an uncertainty-aware prototype alignment loss collaboratively bridge source-target distribution gaps while maintaining class separability. Extensive experiments on PointDA-10 and GraspNetPC-10 benchmarks show that CLIPoint3D achieves consistent 3-16% accuracy gains over both CLIP-based and conventional encoder-based baselines. Codes are available at https://github.com/SarthakM320/CLIPoint3D.",
    "authors": [
      "Mainak Singha",
      "Sarthak Mehrotra",
      "Paolo Casari",
      "Subhasis Chaudhuri",
      "Elisa Ricci",
      "Biplab Banerjee"
    ],
    "url": "https://arxiv.org/abs/2602.20409v1",
    "date": "2026-02-23T23:17:12Z",
    "source": "arxiv_search",
    "keyword": "parameter efficient fine-tuning"
  },
  {
    "paper_id": "2602.19926v1",
    "title": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models",
    "abstract": "Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\\textbf{L}ocal \\textbf{A}lternating \\textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($ε= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\\% in test accuracy. Code is provided in \\repolink.",
    "authors": [
      "Jin Liu",
      "Yinbin Miao",
      "Ning Xi",
      "Junkang Liu"
    ],
    "url": "https://arxiv.org/abs/2602.19926v1",
    "date": "2026-02-23T15:05:28Z",
    "source": "arxiv_search",
    "keyword": "parameter efficient fine-tuning"
  },
  {
    "paper_id": "2602.19111v1",
    "title": "Astra: Activation-Space Tail-Eigenvector Low-Rank Adaptation of Large Language Models",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods, especially LoRA, are widely used for adapting pre-trained models to downstream tasks due to their computational and storage efficiency. However, in the context of LoRA and its variants, the potential of activation subspaces corresponding to tail eigenvectors remains substantially under-exploited, which may lead to suboptimal fine-tuning performance. In this work, we propose Astra (Activation-Space Tail-Eigenvector Low-Rank Adaptation), a novel PEFT method that leverages the tail eigenvectors of the model output activations-estimated from a small task-specific calibration set-to construct task-adaptive low-rank adapters. By constraining updates to the subspace spanned by these tail eigenvectors, Astra achieves faster convergence and improved downstream performance with a significantly reduced parameter budget. Extensive experiments across natural language understanding (NLU) and natural language generation (NLG) tasks demonstrate that Astra consistently outperforms existing PEFT baselines across 16 benchmarks and even surpasses full fine-tuning (FFT) in certain scenarios.",
    "authors": [
      "Kainan Liu",
      "Yong Zhang",
      "Ning Cheng",
      "Yun Zhu",
      "Yanmeng Wang",
      "Shaojun Wang",
      "Jing Xiao"
    ],
    "url": "https://arxiv.org/abs/2602.19111v1",
    "date": "2026-02-22T09:54:40Z",
    "source": "arxiv_search",
    "keyword": "parameter efficient fine-tuning"
  },
  {
    "paper_id": "2602.22955v1",
    "title": "MM-NeuroOnco: A Multimodal Benchmark and Instruction Dataset for MRI-Based Brain Tumor Diagnosis",
    "abstract": "Accurate brain tumor diagnosis requires models to not only detect lesions but also generate clinically interpretable reasoning grounded in imaging manifestations, yet existing public datasets remain limited in annotation richness and diagnostic semantics. To bridge this gap, we introduce MM-NeuroOnco, a large-scale multimodal benchmark and instruction-tuning dataset for brain tumor MRI understanding, consisting of 24,726 MRI slices from 20 data sources paired with approximately 200,000 semantically enriched multimodal instructions spanning diverse tumor subtypes and imaging modalities. To mitigate the scarcity and high cost of diagnostic semantic annotations, we develop a multi-model collaborative pipeline for automated medical information completion and quality control, enabling the generation of diagnosis-related semantics beyond mask-only annotations. Building upon this dataset, we further construct MM-NeuroOnco-Bench, a manually annotated evaluation benchmark with a rejection-aware setting to reduce biases inherent in closed-ended question formats. Evaluation across ten representative models shows that even the strongest baseline, Gemini 3 Flash, achieves only 41.88% accuracy on diagnosis-related questions, highlighting the substantial challenges of multimodal brain tumor diagnostic understanding. Leveraging MM-NeuroOnco, we further propose NeuroOnco-GPT, which achieves a 27% absolute accuracy improvement on diagnostic questions following fine-tuning. This result demonstrates the effectiveness of our dataset and benchmark in advancing clinically grounded multimodal diagnostic reasoning. Code and dataset are publicly available at: https://github.com/gfnnnb/MM-NeuroOnco",
    "authors": [
      "Feng Guo",
      "Jiaxiang Liu",
      "Yang Li",
      "Qianqian Shi",
      "Mingkun Xu"
    ],
    "url": "https://arxiv.org/abs/2602.22955v1",
    "date": "2026-02-26T12:50:32Z",
    "source": "arxiv_search",
    "keyword": "instruction tuning"
  },
  {
    "paper_id": "2602.22538v1",
    "title": "RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format",
    "abstract": "Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning &amp; general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.",
    "authors": [
      "Zhehao Huang",
      "Yuhang Liu",
      "Baijiong Lin",
      "Yixin Lou",
      "Zhengbao He",
      "Hanling Tian",
      "Tao Li",
      "Xiaolin Huang"
    ],
    "url": "https://arxiv.org/abs/2602.22538v1",
    "date": "2026-02-26T02:26:45Z",
    "source": "arxiv_search",
    "keyword": "instruction tuning"
  },
  {
    "paper_id": "2602.22175v1",
    "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs",
    "abstract": "Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO, a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads--a subset of attention heads specialized for long-context retrieval--to identify task-relevant tokens at each decoding step and explicitly up-weight them. By doing so, DySCO dynamically adjusts attention during generation to better utilize relevant context. The method is training-free and can be applied directly to any off-the-shelf LMs. Across multiple instruction-tuned and reasoning models, DySCO consistently improves performance on challenging long-context reasoning benchmarks, yielding relative gains of up to 25% on MRCR and LongBenchV2 at 128K context length with modest additional compute. Further analysis highlights the importance of both dynamic attention rescaling and retrieval-head-guided selection for the effectiveness of the method, while providing interpretability insights into decoding-time attention behavior. Our code is available at https://github.com/princeton-pli/DySCO.",
    "authors": [
      "Xi Ye",
      "Wuwei Zhang",
      "Fangcong Yin",
      "Howard Yen",
      "Danqi Chen"
    ],
    "url": "https://arxiv.org/abs/2602.22175v1",
    "date": "2026-02-25T18:21:35Z",
    "source": "arxiv_search",
    "keyword": "instruction tuning"
  },
  {
    "paper_id": "2602.21854v1",
    "title": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning",
    "abstract": "As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench",
    "authors": [
      "Mustafa Dogan",
      "Ilker Kesen",
      "Iacer Calixto",
      "Aykut Erdem",
      "Erkut Erdem"
    ],
    "url": "https://arxiv.org/abs/2602.21854v1",
    "date": "2026-02-25T12:30:18Z",
    "source": "arxiv_search",
    "keyword": "instruction tuning"
  },
  {
    "paper_id": "2602.21779v1",
    "title": "Beyond Static Artifacts: A Forensic Benchmark for Video Deepfake Reasoning in Vision Language Models",
    "abstract": "Current Vision-Language Models (VLMs) for deepfake detection excel at identifying spatial artifacts but overlook a critical dimension: temporal inconsistencies in video forgeries. Adapting VLMs to reason about these dynamic cues remains a distinct challenge. To bridge this gap, we propose Forensic Answer-Questioning (FAQ), a large-scale benchmark that formulates temporal deepfake analysis as a multiple-choice task. FAQ introduces a three-level hierarchy to progressively evaluate and equip VLMs with forensic capabilities: (1) Facial Perception, testing the ability to identify static visual artifacts; (2) Temporal Deepfake Grounding, requiring the localization of dynamic forgery artifacts across frames; and (3) Forensic Reasoning, challenging models to synthesize evidence for final authenticity verdicts. We evaluate a range of VLMs on FAQ and generate a corresponding instruction-tuning set, FAQ-IT. Extensive experiments show that models fine-tuned on FAQ-IT achieve advanced performance on both in-domain and cross-dataset detection benchmarks. Ablation studies further validate the impact of our key design choices, confirming that FAQ is the driving force behind the temporal reasoning capabilities of these VLMs.",
    "authors": [
      "Zheyuan Gu",
      "Qingsong Zhao",
      "Yusong Wang",
      "Zhaohong Huang",
      "Xinqi Li",
      "Cheng Yuan",
      "Jiaowei Shao",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "url": "https://arxiv.org/abs/2602.21779v1",
    "date": "2026-02-25T10:54:55Z",
    "source": "arxiv_search",
    "keyword": "instruction tuning"
  },
  {
    "paper_id": "2602.21638v1",
    "title": "Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs",
    "abstract": "Effectively addressing client resistance is a sophisticated clinical skill in psychological counseling, yet practitioners often lack timely and scalable supervisory feedback to refine their approaches. Although current NLP research has examined overall counseling quality and general therapeutic skills, it fails to provide granular evaluations of high-stakes moments where clients exhibit resistance. In this work, we present a comprehensive pipeline for the multi-dimensional evaluation of human counselors' interventions specifically targeting client resistance in text-based therapy. We introduce a theory-driven framework that decomposes counselor responses into four distinct communication mechanisms. Leveraging this framework, we curate and share an expert-annotated dataset of real-world counseling excerpts, pairing counselor-client interactions with professional ratings and explanatory rationales. Using this data, we perform full-parameter instruction tuning on a Llama-3.1-8B-Instruct backbone to model fine-grained evaluative judgments of response quality and generate explanations underlying. Experimental results show that our approach can effectively distinguish the quality of different communication mechanisms (77-81% F1), substantially outperforming GPT-4o and Claude-3.5-Sonnet (45-59% F1). Moreover, the model produces high-quality explanations that closely align with expert references and receive near-ceiling ratings from human experts (2.8-2.9/3.0). A controlled experiment with 43 counselors further confirms that receiving these AI-generated feedback significantly improves counselors' ability to respond effectively to client resistance.",
    "authors": [
      "Anqi Li",
      "Ruihan Wang",
      "Zhaoming Chen",
      "Yuqian Chen",
      "Yu Lu",
      "Yi Zhu",
      "Yuan Xie",
      "Zhenzhong Lan"
    ],
    "url": "https://arxiv.org/abs/2602.21638v1",
    "date": "2026-02-25T07:05:05Z",
    "source": "arxiv_search",
    "keyword": "instruction tuning"
  },
  {
    "paper_id": "2602.21186v1",
    "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning",
    "abstract": "While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.",
    "authors": [
      "Haoyi Jiang",
      "Liu Liu",
      "Xinjie Wang",
      "Yonghao He",
      "Wei Sui",
      "Zhizhong Su",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "url": "https://arxiv.org/abs/2602.21186v1",
    "date": "2026-02-24T18:37:34Z",
    "source": "arxiv_search",
    "keyword": "instruction tuning"
  },
  {
    "paper_id": "2602.21142v1",
    "title": "LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis",
    "abstract": "Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN, that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data.",
    "authors": [
      "Zhifan Jiang",
      "Dong Yang",
      "Vishwesh Nath",
      "Abhijeet Parida",
      "Nishad P. Kulkarni",
      "Ziyue Xu",
      "Daguang Xu",
      "Syed Muhammad Anwar",
      "Holger R. Roth",
      "Marius George Linguraru"
    ],
    "url": "https://arxiv.org/abs/2602.21142v1",
    "date": "2026-02-24T17:42:46Z",
    "source": "arxiv_search",
    "keyword": "instruction tuning"
  },
  {
    "paper_id": "2602.20379v1",
    "title": "Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems",
    "abstract": "Enterprise Retrieval-Augmented Generation (RAG) assistants operate in multi-turn, case-based workflows such as technical support and IT operations, where evaluation must reflect operational constraints, structured identifiers (e.g., error codes, versions), and resolution workflows. Existing RAG evaluation frameworks are primarily designed for benchmark-style or single-turn settings and often fail to capture enterprise-specific failure modes such as case misidentification, workflow misalignment, and partial resolution across turns.   We present a case-aware LLM-as-a-Judge evaluation framework for enterprise multi-turn RAG systems. The framework evaluates each turn using eight operationally grounded metrics that separate retrieval quality, grounding fidelity, answer utility, precision integrity, and case/workflow alignment. A severity-aware scoring protocol reduces score inflation and improves diagnostic clarity across heterogeneous enterprise cases. The system uses deterministic prompting with strict JSON outputs, enabling scalable batch evaluation, regression testing, and production monitoring.   Through a comparative study of two instruction-tuned models across short and long workflows, we show that generic proxy metrics provide ambiguous signals, while the proposed framework exposes enterprise-critical tradeoffs that are actionable for system improvement.",
    "authors": [
      "Mukul Chhabra",
      "Luigi Medrano",
      "Arush Verma"
    ],
    "url": "https://arxiv.org/abs/2602.20379v1",
    "date": "2026-02-23T21:37:06Z",
    "source": "arxiv_search",
    "keyword": "instruction tuning"
  },
  {
    "paper_id": "2602.19063v1",
    "title": "Direction-aware 3D Large Multimodal Models",
    "abstract": "3D large multimodal models (3D LMMs) rely heavily on ego poses for enabling directional question-answering and spatial reasoning. However, most existing point cloud benchmarks contain rich directional queries but lack the corresponding ego poses, making them inherently ill-posed in 3D large multimodal modelling. In this work, we redefine a new and rigorous paradigm that enables direction-aware 3D LMMs by identifying and supplementing ego poses into point cloud benchmarks and transforming the corresponding point cloud data according to the identified ego poses. We enable direction-aware 3D LMMs with two novel designs. The first is PoseRecover, a fully automatic pose recovery pipeline that matches questions with ego poses from RGB-D video extrinsics via object-frustum intersection and visibility check with Z-buffers. The second is PoseAlign that transforms the point cloud data to be aligned with the identified ego poses instead of either injecting ego poses into textual prompts or introducing pose-encoded features in the projection layers. Extensive experiments show that our designs yield consistent improvements across multiple 3D LMM backbones such as LL3DA, LL3DA-SONATA, Chat-Scene, and 3D-LLAVA, improving ScanRefer mIoU by 30.0% and Scan2Cap LLM-as-judge accuracy by 11.7%. In addition, our approach is simple, generic, and training-efficient, requiring only instruction tuning while establishing a strong baseline for direction-aware 3D-LMMs.",
    "authors": [
      "Quan Liu",
      "Weihao Xuan",
      "Junjue Wang",
      "Naoto Yokoya",
      "Ling Shao",
      "Shijian Lu"
    ],
    "url": "https://arxiv.org/abs/2602.19063v1",
    "date": "2026-02-22T06:31:28Z",
    "source": "arxiv_search",
    "keyword": "instruction tuning"
  },
  {
    "paper_id": "2602.23128v1",
    "title": "Bound to Disagree: Generalization Bounds via Certifiable Surrogates",
    "abstract": "Generalization bounds for deep learning models are typically vacuous, not computable or restricted to specific model classes. In this paper, we tackle these issues by providing new disagreement-based certificates for the gap between the true risk of any two predictors. We then bound the true risk of the predictor of interest via a surrogate model that enjoys tight generalization guarantees, and evaluating our disagreement bound on an unlabeled dataset. We empirically demonstrate the tightness of the obtained certificates and showcase the versatility of the approach by training surrogate models leveraging three different frameworks: sample compression, model compression and PAC-Bayes theory. Importantly, such guarantees are achieved without modifying the target model, nor adapting the training procedure to the generalization framework.",
    "authors": [
      "Mathieu Bazinet",
      "Valentina Zantedeschi",
      "Pascal Germain"
    ],
    "url": "https://arxiv.org/abs/2602.23128v1",
    "date": "2026-02-26T15:42:13Z",
    "source": "arxiv_search",
    "keyword": "model compression"
  },
  {
    "paper_id": "2602.21662v1",
    "title": "HybridINR-PCGC: Hybrid Lossless Point Cloud Geometry Compression Bridging Pretrained Model and Implicit Neural Representation",
    "abstract": "Learning-based point cloud compression presents superior performance to handcrafted codecs. However, pretrained-based methods, which are based on end-to-end training and expected to generalize to all the potential samples, suffer from training data dependency. Implicit neural representation (INR) based methods are distribution-agnostic and more robust, but they require time-consuming online training and suffer from the bitstream overhead from the overfitted model. To address these limitations, we propose HybridINR-PCGC, a novel hybrid framework that bridges the pretrained model and INR. Our framework retains distribution-agnostic properties while leveraging a pretrained network to accelerate convergence and reduce model overhead, which consists of two parts: the Pretrained Prior Network (PPN) and the Distribution Agnostic Refiner (DAR). We leverage the PPN, designed for fast inference and stable performance, to generate a robust prior for accelerating the DAR's convergence. The DAR is decomposed into a base layer and an enhancement layer, and only the enhancement layer needed to be packed into the bitstream. Finally, we propose a supervised model compression module to further supervise and minimize the bitrate of the enhancement layer parameters. Based on experiment results, HybridINR-PCGC achieves a significantly improved compression rate and encoding efficiency. Specifically, our method achieves a Bpp reduction of approximately 20.43% compared to G-PCC on 8iVFB. In the challenging out-of-distribution scenario Cat1B, our method achieves a Bpp reduction of approximately 57.85% compared to UniPCGC. And our method exhibits a superior time-rate trade-off, achieving an average Bpp reduction of 15.193% relative to the LINR-PCGC on 8iVFB.",
    "authors": [
      "Wenjie Huang",
      "Qi Yang",
      "Shuting Xia",
      "He Huang",
      "Zhu Li",
      "Yiling Xu"
    ],
    "url": "https://arxiv.org/abs/2602.21662v1",
    "date": "2026-02-25T07:42:27Z",
    "source": "arxiv_search",
    "keyword": "model compression"
  },
  {
    "paper_id": "2602.20933v1",
    "title": "Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting",
    "abstract": "Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. Project Website: https://sk-fun.fun/DropAnSH-GS",
    "authors": [
      "Shuangkang Fang",
      "I-Chao Shen",
      "Xuanyang Zhang",
      "Zesheng Wang",
      "Yufeng Wang",
      "Wenrui Ding",
      "Gang Yu",
      "Takeo Igarashi"
    ],
    "url": "https://arxiv.org/abs/2602.20933v1",
    "date": "2026-02-24T14:11:56Z",
    "source": "arxiv_search",
    "keyword": "model compression"
  },
  {
    "paper_id": "2602.22238v1",
    "title": "TT-SEAL: TTD-Aware Selective Encryption for Adversarially-Robust and Low-Latency Edge AI",
    "abstract": "Cloud-edge AI must jointly satisfy model compression and security under tight device budgets. While Tensor-Train Decomposition (TTD) shrinks on-device models, prior selective-encryption studies largely assume dense weights, leaving its practicality under TTD compression unclear. We present TT-SEAL, a selective-encryption framework for TT-decomposed networks. TT-SEAL ranks TT cores with a sensitivity-based importance metric, calibrates a one-time robustness threshold, and uses a value-DP optimizer to encrypt the minimum set of critical cores with AES. Under TTD-aware, transfer-based threat models (and on an FPGA-prototyped edge processor) TT-SEAL matches the robustness of full (black-box) encryption while encrypting as little as 4.89-15.92% of parameters across ResNet-18, MobileNetV2, and VGG-16, and drives the share of AES decryption in end-to-end latency to low single digits (e.g., 58% -&gt; 2.76% on ResNet-18), enabling secure, low-latency edge AI.",
    "authors": [
      "Kyeongpil Min",
      "Sangmin Jeon",
      "Jae-Jin Lee",
      "Woojoo Lee"
    ],
    "url": "https://arxiv.org/abs/2602.22238v1",
    "date": "2026-02-24T05:48:09Z",
    "source": "arxiv_search",
    "keyword": "model compression"
  },
  {
    "paper_id": "2602.19959v1",
    "title": "Deploying a Hybrid PVFinder Algorithm for Primary Vertex Reconstruction in LHCb's GPU-Resident HLT1",
    "abstract": "LHCb's Run 3 upgrade introduced a fully software-based trigger system operating at 30~MHz, processing an average of 5.6 proton-proton collision vertices per bunch crossing (event). This work presents the development of an inference engine for PVFinder, a hybrid deep neural network for finding primary vertices, the proton-proton collision points from which all subsequent particle decays originate into Allen, LHCb's High Level Trigger (HLT1) framework. The integration addresses critical real-time constraints including fixed memory pools, single-stream execution, and sub-400~$μ$s per-event processing budgets on NVIDIA GPUs. We introduce a translation layer that bridges Allen's Structure-of-Arrays (SoA) data layout with cuDNN's tensor format while maintaining zero-copy semantics and deterministic behavior. Current performance shows the CNN stage contributes significant throughput overhead. We present a roadmap targeting order-of-magnitude improvements through mixed-precision computing, model compression and other techniques.",
    "authors": [
      "Simon Akar",
      "Mohamed Elashri",
      "Conor Henderson",
      "Michael Sokoloff"
    ],
    "url": "https://arxiv.org/abs/2602.19959v1",
    "date": "2026-02-23T15:26:49Z",
    "source": "arxiv_search",
    "keyword": "model compression"
  },
  {
    "paper_id": "2602.18690v1",
    "title": "Neural Fields as World Models",
    "abstract": "How does the brain predict physical outcomes while acting in the world? Machine learning world models compress visual input into latent spaces, discarding the spatial structure that characterizes sensory cortex. We propose isomorphic world models: architectures preserving sensory topology so that physics prediction becomes geometric propagation rather than abstract state transition. We implement this using neural fields with motor-gated channels, where activity evolves through local lateral connectivity and motor commands multiplicatively modulate specific populations. Three experiments support this approach: (1) local connectivity is sufficient to learn ballistic physics, with predictions traversing intermediate locations rather than \"teleporting\"; (2) policies trained entirely in imagination transfer to real physics at nearly twice the rate of latent-space alternatives; and (3) motor-gated channels spontaneously develop body-selective encoding through visuomotor prediction alone. These findings suggest intuitive physics and body schema may share a common origin in spatially structured neural dynamics.",
    "authors": [
      "Joshua Nunley"
    ],
    "url": "https://arxiv.org/abs/2602.18690v1",
    "date": "2026-02-21T01:52:43Z",
    "source": "arxiv_search",
    "keyword": "model compression"
  },
  {
    "paper_id": "2602.18420v1",
    "title": "SPQ: An Ensemble Technique for Large Language Model Compression",
    "abstract": "This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/",
    "authors": [
      "Jiamin Yao",
      "Eren Gultepe"
    ],
    "url": "https://arxiv.org/abs/2602.18420v1",
    "date": "2026-02-20T18:44:16Z",
    "source": "arxiv_search",
    "keyword": "model compression"
  },
  {
    "paper_id": "2602.18116v1",
    "title": "Cut Less, Fold More: Model Compression through the Lens of Projection Geometry",
    "abstract": "Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate &gt;1000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate-high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.",
    "authors": [
      "Olga Saukh",
      "Dong Wang",
      "Haris Šikić",
      "Yun Cheng",
      "Lothar Thiele"
    ],
    "url": "https://arxiv.org/abs/2602.18116v1",
    "date": "2026-02-20T10:09:02Z",
    "source": "arxiv_search",
    "keyword": "model compression"
  },
  {
    "paper_id": "2602.17761v1",
    "title": "Hardware-Aware Design of a GNN-Based Hit Filtering Algorithm for the Belle II Level-1 Trigger",
    "abstract": "The Belle~II experiment operates at high luminosity, where an increasing beam-induced background imposes stringent demands on the hardware Level-1 trigger system, which must operate under tight latency and bandwidth constraints. To achieve online data reduction within the Level-1 trigger system, we have developed a hit-filtering algorithm based on the lightweight Interaction Network architecture. In this work, we present a hardware-aware model-compression workflow for this hit-filtering algorithm targeting deployment on FPGA devices within the Belle~II trigger system. The network is adapted to the detector and trigger conditions through model-size and graph-size reduction, low-precision (4 bit) fixed-point arithmetic, and unstructured pruning. We assess the resulting design using the total number of bit operations as a hardware-aware computational complexity metric. Using this metric, we identify a configuration that decreases this cost by more than two orders of magnitude relative to the full-precision reference implementation. This reduction is achieved while preserving performance close to the reference model in terms of hit efficiency and background rejection, as indicated by only a modest decrease in the AUC score from 97.4 to 96.8, evaluated on Belle~II collision data.",
    "authors": [
      "Greta Heine",
      "Fabio Mayer",
      "Marc Neu",
      "Jürgen Becker",
      "Torben Ferber"
    ],
    "url": "https://arxiv.org/abs/2602.17761v1",
    "date": "2026-02-19T19:00:04Z",
    "source": "arxiv_search",
    "keyword": "model compression"
  },
  {
    "paper_id": "2602.17063v1",
    "title": "Sign Lock-In: Randomly Initialized Weight Signs Persist and Bottleneck Sub-Bit Model Compression",
    "abstract": "Sub-bit model compression seeks storage below one bit per weight; as magnitudes are aggressively compressed, the sign bit becomes a fixed-cost bottleneck. Across Transformers, CNNs, and MLPs, learned sign matrices resist low-rank approximation and are spectrally indistinguishable from an i.i.d. Rademacher baseline. Despite this apparent randomness, most weights retain their initialization signs; flips primarily occur via rare near-zero boundary crossings, suggesting that sign-pattern randomness is largely inherited from initialization. We formalize this behavior with sign lock-in theory, a stopping-time analysis of sign flips under SGD noise. Under bounded updates and a rare re-entry condition into a small neighborhood around zero, the number of effective sign flips exhibits a geometric tail. Building on this mechanism, we introduce a gap-based initialization and a lightweight outward-drift regularizer, reducing the effective flip rate to approximately $10^{-3}$ with only about a one-point increase in perplexity.",
    "authors": [
      "Akira Sakai",
      "Yuma Ichikawa"
    ],
    "url": "https://arxiv.org/abs/2602.17063v1",
    "date": "2026-02-19T04:10:05Z",
    "source": "arxiv_search",
    "keyword": "model compression"
  },
  {
    "paper_id": "2601.19849v1",
    "title": "HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation",
    "abstract": "Data across modalities such as images, text, and graphs often contains hierarchical and relational structures, which are challenging to model within Euclidean geometry. Hyperbolic geometry provides a natural framework for representing such structures. Building on this property, this work introduces HexFormer, a hyperbolic vision transformer for image classification that incorporates exponential map aggregation within its attention mechanism. Two designs are explored: a hyperbolic ViT (HexFormer) and a hybrid variant (HexFormer-Hybrid) that combines a hyperbolic encoder with an Euclidean linear classification head. HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit. Experiments across multiple datasets demonstrate consistent performance improvements over Euclidean baselines and prior hyperbolic ViTs, with the hybrid variant achieving the strongest overall results. Additionally, this study provides an analysis of gradient stability in hyperbolic transformers. The results reveal that hyperbolic models exhibit more stable gradients and reduced sensitivity to warmup strategies compared to Euclidean architectures, highlighting their robustness and efficiency in training. Overall, these findings indicate that hyperbolic geometry can enhance vision transformer architectures by improving gradient stability and accuracy. In addition, relatively simple mechanisms such as exponential map aggregation can provide strong practical benefits.",
    "authors": [
      "Haya Alyoussef",
      "Ahmad Bdeir",
      "Diego Coello de Portugal Mecke",
      "Tom Hanika",
      "Niels Landwehr",
      "Lars Schmidt-Thieme"
    ],
    "url": "https://arxiv.org/abs/2601.19849v1",
    "date": "2026-01-27T17:56:49Z",
    "source": "arxiv_search",
    "keyword": "warmup strategy"
  },
  {
    "paper_id": "2511.22194v1",
    "title": "Controllable 3D Object Generation with Single Image Prompt",
    "abstract": "Recently, the impressive generative capabilities of diffusion models have been demonstrated, producing images with remarkable fidelity. Particularly, existing methods for the 3D object generation tasks, which is one of the fastest-growing segments in computer vision, pre-dominantly use text-to-image diffusion models with textual inversion which train a pseudo text prompt to describe the given image. In practice, various text-to-image generative models employ textual inversion to learn concepts or styles of target object in the pseudo text prompt embedding space, thereby generating sophisticated outputs. However, textual inversion requires additional training time and lacks control ability. To tackle this issues, we propose two innovative methods: (1) using an off-the-shelf image adapter that generates 3D objects without textual inversion, offering enhanced control over conditions such as depth, pose, and text. (2) a depth conditioned warmup strategy to enhance 3D consistency. In experimental results, ours show qualitatively and quantitatively comparable performance and improved 3D consistency to the existing text-inversion-based alternatives. Furthermore, we conduct a user study to assess (i) how well results match the input image and (ii) whether 3D consistency is maintained. User study results show that our model outperforms the alternatives, validating the effectiveness of our approaches. Our code is available at GitHub repository:https://github.com/Seooooooogi/Control3D_IP/",
    "authors": [
      "Jaeseok Lee",
      "Jaekoo Lee"
    ],
    "url": "https://arxiv.org/abs/2511.22194v1",
    "date": "2025-11-27T08:03:56Z",
    "source": "arxiv_search",
    "keyword": "warmup strategy"
  },
  {
    "paper_id": "2506.11300v2",
    "title": "Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning",
    "abstract": "Curriculum learning-organizing training data from easy to hard-has improved efficiency across machine learning domains, yet remains underexplored for language model pretraining. We present the first systematic investigation of curriculum learning in LLM pretraining, with over 200 models trained on up to 100B tokens across three strategies: vanilla curriculum learning, pacing-based sampling, and interleaved curricula, guided by six difficulty metrics spanning linguistic and information-theoretic properties. We evaluate performance on eight benchmarks under three realistic scenarios: limited data, unlimited data, and continual training. Our experiments show that curriculum learning consistently accelerates convergence in early and mid-training phases,reducing training steps by $18-45\\%$ to reach baseline performance. When applied as a warmup strategy before standard random sampling, curriculum learning yields sustained improvements up to $3.5\\%$. We identify compression ratio, lexical diversity (MTLD), and readability (Flesch Reading Ease) as the most effective difficulty signals. Our findings demonstrate that data ordering-orthogonal to existing data selection methods-provides a practical mechanism for more efficient LLM pretraining.",
    "authors": [
      "Yang Zhang",
      "Amr Mohamed",
      "Hadi Abdine",
      "Guokan Shang",
      "Michalis Vazirgiannis"
    ],
    "url": "https://arxiv.org/abs/2506.11300v2",
    "date": "2025-06-12T21:06:57Z",
    "source": "arxiv_search",
    "keyword": "warmup strategy"
  },
  {
    "paper_id": "2506.02975v1",
    "title": "HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation",
    "abstract": "With the advancement of language models, unified multimodal understanding and generation have made significant strides, with model architectures evolving from separated components to unified single-model frameworks. This paper explores an efficient training paradigm to build a single transformer for unified multimodal understanding and generation. Specifically, we propose a multimodal warmup strategy utilizing prior knowledge to extend capabilities. To address cross-modal compatibility challenges, we introduce feature pre-scaling and multimodal AdaLN techniques. Integrating the proposed technologies, we present the HaploOmni, a new single multimodal transformer. With limited training costs, HaploOmni achieves competitive performance across multiple image and video understanding and generation benchmarks over advanced unified models. All codes will be made public at https://github.com/Tencent/HaploVLM.",
    "authors": [
      "Yicheng Xiao",
      "Lin Song",
      "Rui Yang",
      "Cheng Cheng",
      "Zunnan Xu",
      "Zhaoyang Zhang",
      "Yixiao Ge",
      "Xiu Li",
      "Ying Shan"
    ],
    "url": "https://arxiv.org/abs/2506.02975v1",
    "date": "2025-06-03T15:14:00Z",
    "source": "arxiv_search",
    "keyword": "warmup strategy"
  },
  {
    "paper_id": "2406.09723v1",
    "title": "When Will Gradient Regularization Be Harmful?",
    "abstract": "Gradient regularization (GR), which aims to penalize the gradient norm atop the loss function, has shown promising results in training modern over-parameterized deep neural networks. However, can we trust this powerful technique? This paper reveals that GR can cause performance degeneration in adaptive optimization scenarios, particularly with learning rate warmup. Our empirical and theoretical analyses suggest this is due to GR inducing instability and divergence in gradient statistics of adaptive optimizers at the initial training stage. Inspired by the warmup heuristic, we propose three GR warmup strategies, each relaxing the regularization effect to a certain extent during the warmup course to ensure the accurate and stable accumulation of gradients. With experiments on Vision Transformer family, we confirm the three GR warmup strategies can effectively circumvent these issues, thereby largely improving the model performance. Meanwhile, we note that scalable models tend to rely more on the GR warmup, where the performance can be improved by up to 3\\% on Cifar10 compared to baseline GR. Code is available at \\href{https://github.com/zhaoyang-0204/gnp}{https://github.com/zhaoyang-0204/gnp}.",
    "authors": [
      "Yang Zhao",
      "Hao Zhang",
      "Xiuyuan Hu"
    ],
    "url": "https://arxiv.org/abs/2406.09723v1",
    "date": "2024-06-14T05:17:39Z",
    "source": "arxiv_search",
    "keyword": "warmup strategy"
  },
  {
    "paper_id": "2405.05496v1",
    "title": "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis",
    "abstract": "Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment analysis, which aims to extract the aspects and predict their sentiments. Most existing studies focus on improving the performance of the target domain by fine-tuning domain-specific models (trained on source domains) based on the target domain dataset. Few works propose continual learning tasks for ABSA, which aim to learn the target domain's ability while maintaining the history domains' abilities. In this paper, we propose a Large Language Model-based Continual Learning (\\texttt{LLM-CL}) model for ABSA. First, we design a domain knowledge decoupling module to learn a domain-invariant adapter and separate domain-variant adapters dependently with an orthogonal constraint. Then, we introduce a domain knowledge warmup strategy to align the representation between domain-invariant and domain-variant knowledge. In the test phase, we index the corresponding domain-variant knowledge via domain positioning to not require each sample's domain ID. Extensive experiments over 19 datasets indicate that our \\texttt{LLM-CL} model obtains new state-of-the-art performance.",
    "authors": [
      "Xuanwen Ding",
      "Jie Zhou",
      "Liang Dou",
      "Qin Chen",
      "Yuanbin Wu",
      "Chengcai Chen",
      "Liang He"
    ],
    "url": "https://arxiv.org/abs/2405.05496v1",
    "date": "2024-05-09T02:00:07Z",
    "source": "arxiv_search",
    "keyword": "warmup strategy"
  },
  {
    "paper_id": "2211.04218v3",
    "title": "Clustered Federated Learning based on Nonconvex Pairwise Fusion",
    "abstract": "This study investigates clustered federated learning (FL), one of the formulations of FL with non-i.i.d. data, where the devices are partitioned into clusters and each cluster optimally fits its data with a localized model. We propose a clustered FL framework that incorporates a nonconvex penalty to pairwise differences of parameters. Without a priori knowledge of the set of devices in each cluster and the number of clusters, this framework can autonomously estimate cluster structures. To implement the proposed framework, we introduce a novel clustered FL method called Fusion Penalized Federated Clustering (FPFC). Building upon the standard alternating direction method of multipliers (ADMM), FPFC can perform partial updates at each communication round and allows parallel computation with variable workload. These strategies significantly reduce the communication cost while ensuring privacy, making it practical for FL. We also propose a new warmup strategy for hyperparameter tuning in FL settings and explore the asynchronous variant of FPFC (asyncFPFC). Theoretical analysis provides convergence guarantees for FPFC with general losses and establishes the statistical convergence rate under a linear model with squared loss. Extensive experiments have demonstrated the superiority of FPFC compared to current methods, including robustness and generalization capability.",
    "authors": [
      "Xue Yu",
      "Ziyi Liu",
      "Wu Wang",
      "Yifan Sun"
    ],
    "url": "https://arxiv.org/abs/2211.04218v3",
    "date": "2022-11-08T13:04:56Z",
    "source": "arxiv_search",
    "keyword": "warmup strategy"
  },
  {
    "paper_id": "2107.05855v1",
    "title": "Automated Learning Rate Scheduler for Large-batch Training",
    "abstract": "Large-batch training has been essential in leveraging large-scale datasets and models in deep learning. While it is computationally beneficial to use large batch sizes, it often requires a specially designed learning rate (LR) schedule to achieve a comparable level of performance as in smaller batch training. Especially, when the number of training epochs is constrained, the use of a large LR and a warmup strategy is critical in the final performance of large-batch training due to the reduced number of updating steps. In this work, we propose an automated LR scheduling algorithm which is effective for neural network training with a large batch size under the given epoch budget. In specific, the whole schedule consists of two phases: adaptive warmup and predefined decay, where the LR is increased until the training loss no longer decreases and decreased to zero until the end of training. Here, whether the training loss has reached the minimum value is robustly checked with Gaussian process smoothing in an online manner with a low computational burden. Coupled with adaptive stochastic optimizers such as AdamP and LAMB, the proposed scheduler successfully adjusts the LRs without cumbersome hyperparameter tuning and achieves comparable or better performances than tuned baselines on various image classification benchmarks and architectures with a wide range of batch sizes.",
    "authors": [
      "Chiheon Kim",
      "Saehoon Kim",
      "Jongmin Kim",
      "Donghoon Lee",
      "Sungwoong Kim"
    ],
    "url": "https://arxiv.org/abs/2107.05855v1",
    "date": "2021-07-13T05:23:13Z",
    "source": "arxiv_search",
    "keyword": "warmup strategy"
  },
  {
    "paper_id": "2103.15055v1",
    "title": "Friends and Foes in Learning from Noisy Labels",
    "abstract": "Learning from examples with noisy labels has attracted increasing attention recently. But, this paper will show that the commonly used CIFAR-based datasets and the accuracy evaluation metric used in the literature are both inappropriate in this context. An alternative valid evaluation metric and new datasets are proposed in this paper to promote proper research and evaluation in this area. Then, friends and foes are identified from existing methods as technical components that are either beneficial or detrimental to deep learning from noisy labeled examples, respectively, and this paper improves and combines technical components from the friends category, including self-supervised learning, new warmup strategy, instance filtering and label correction. The resulting F&amp;F method significantly outperforms existing methods on the proposed nCIFAR datasets and the real-world Clothing1M dataset.",
    "authors": [
      "Yifan Zhou",
      "Yifan Ge",
      "Jianxin Wu"
    ],
    "url": "https://arxiv.org/abs/2103.15055v1",
    "date": "2021-03-28T06:05:17Z",
    "source": "arxiv_search",
    "keyword": "warmup strategy"
  },
  {
    "paper_id": "2010.09277v2",
    "title": "Modality-Pairing Learning for Brain Tumor Segmentation",
    "abstract": "Automatic brain tumor segmentation from multi-modality Magnetic Resonance Images (MRI) using deep learning methods plays an important role in assisting the diagnosis and treatment of brain tumor. However, previous methods mostly ignore the latent relationship among different modalities. In this work, we propose a novel end-to-end Modality-Pairing learning method for brain tumor segmentation. Paralleled branches are designed to exploit different modality features and a series of layer connections are utilized to capture complex relationships and abundant information among modalities. We also use a consistency loss to minimize the prediction variance between two branches. Besides, learning rate warmup strategy is adopted to solve the problem of the training instability and early over-fitting. Lastly, we use average ensemble of multiple models and some post-processing techniques to get final results. Our method is tested on the BraTS 2020 online testing dataset, obtaining promising segmentation performance, with average dice scores of 0.891, 0.842, 0.816 for the whole tumor, tumor core and enhancing tumor, respectively. We won the second place of the BraTS 2020 Challenge for the tumor segmentation task.",
    "authors": [
      "Yixin Wang",
      "Yao Zhang",
      "Feng Hou",
      "Yang Liu",
      "Jiang Tian",
      "Cheng Zhong",
      "Yang Zhang",
      "Zhiqiang He"
    ],
    "url": "https://arxiv.org/abs/2010.09277v2",
    "date": "2020-10-19T07:42:10Z",
    "source": "arxiv_search",
    "keyword": "warmup strategy"
  },
  {
    "paper_id": "2602.21081v1",
    "title": "Scaling Vision Transformers: Evaluating DeepSpeed for Image-Centric Workloads",
    "abstract": "Vision Transformers (ViTs) have demonstrated remarkable potential in image processing tasks by utilizing self-attention mechanisms to capture global relationships within data. However, their scalability is hindered by significant computational and memory demands, especially for large-scale models with many parameters. This study aims to leverage DeepSpeed, a highly efficient distributed training framework that is commonly used for language models, to enhance the scalability and performance of ViTs. We evaluate intra- and inter-node training efficiency across multiple GPU configurations on various datasets like CIFAR-10 and CIFAR-100, exploring the impact of distributed data parallelism on training speed, communication overhead, and overall scalability (strong and weak scaling). By systematically varying software parameters, such as batch size and gradient accumulation, we identify key factors influencing performance of distributed training. The experiments in this study provide a foundational basis for applying DeepSpeed to image-related tasks. Future work will extend these investigations to deepen our understanding of DeepSpeed's limitations and explore strategies for optimizing distributed training pipelines for Vision Transformers.",
    "authors": [
      "Huy Trinh",
      "Rebecca Ma",
      "Zeqi Yu",
      "Tahsin Reza"
    ],
    "url": "https://arxiv.org/abs/2602.21081v1",
    "date": "2026-02-24T16:45:12Z",
    "source": "arxiv_search",
    "keyword": "gradient accumulation"
  },
  {
    "paper_id": "2602.15473v1",
    "title": "POP: Prior-fitted Optimizer Policies",
    "abstract": "Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning.",
    "authors": [
      "Jan Kobiolka",
      "Christian Frey",
      "Gresa Shala",
      "Arlind Kadra",
      "Erind Bedalli",
      "Josif Grabocka"
    ],
    "url": "https://arxiv.org/abs/2602.15473v1",
    "date": "2026-02-17T10:27:07Z",
    "source": "arxiv_search",
    "keyword": "gradient accumulation"
  },
  {
    "paper_id": "2602.11940v1",
    "title": "Temporally Unified Adversarial Perturbations for Time Series Forecasting",
    "abstract": "While deep learning models have achieved remarkable success in time series forecasting, their vulnerability to adversarial examples remains a critical security concern. However, existing attack methods in the forecasting field typically ignore the temporal consistency inherent in time series data, leading to divergent and contradictory perturbation values for the same timestamp across overlapping samples. This temporally inconsistent perturbations problem renders adversarial attacks impractical for real-world data manipulation. To address this, we introduce Temporally Unified Adversarial Perturbations (TUAPs), which enforce a temporal unification constraint to ensure identical perturbations for each timestamp across all overlapping samples. Moreover, we propose a novel Timestamp-wise Gradient Accumulation Method (TGAM) that provides a modular and efficient approach to effectively generate TUAPs by aggregating local gradient information from overlapping samples. By integrating TGAM with momentum-based attack algorithms, we ensure strict temporal consistency while fully utilizing series-level gradient information to explore the adversarial perturbation space. Comprehensive experiments on three benchmark datasets and four representative state-of-the-art models demonstrate that our proposed method significantly outperforms baselines in both white-box and black-box transfer attack scenarios under TUAP constraints. Moreover, our method also exhibits superior transfer attack performance even without TUAP constraints, demonstrating its effectiveness and superiority in generating adversarial perturbations for time series forecasting models.",
    "authors": [
      "Ruixian Su",
      "Yukun Bao",
      "Xinze Zhang"
    ],
    "url": "https://arxiv.org/abs/2602.11940v1",
    "date": "2026-02-12T13:37:45Z",
    "source": "arxiv_search",
    "keyword": "gradient accumulation"
  },
  {
    "paper_id": "2602.01469v1",
    "title": "P-EAGLE: Parallel-Drafting EAGLE with Scalable Training",
    "abstract": "Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.",
    "authors": [
      "Mude Hui",
      "Xin Huang",
      "Jaime Campos Salas",
      "Yue Sun",
      "Nathan Pemberton",
      "Xiang Song",
      "Ashish Khetan",
      "George Karypis"
    ],
    "url": "https://arxiv.org/abs/2602.01469v1",
    "date": "2026-02-01T22:26:17Z",
    "source": "arxiv_search",
    "keyword": "gradient accumulation"
  },
  {
    "paper_id": "2601.21824v1",
    "title": "DASH: Deterministic Attention Scheduling for High-throughput Reproducible LLM Training",
    "abstract": "Determinism is indispensable for reproducibility in large language model (LLM) training, yet it often exacts a steep performance cost. In widely used attention implementations such as FlashAttention-3, the deterministic backward pass can incur up to a 37.9% throughput reduction relative to its non-deterministic counterpart, primarily because gradient accumulation operations must be serialized to guarantee numerical consistency. This performance loss stems from suboptimal scheduling of compute and gradient-reduction phases, leading to significant hardware underutilization.   To address this challenge, we formulate the backward pass of deterministic attention as a scheduling problem on a Directed Acyclic Graph (DAG) and derive schedules that minimize the critical path length. Building on this formulation, we present DASH (Deterministic Attention Scheduling for High-Throughput), which encapsulates two complementary scheduling strategies: (i) Descending Q-Tile Iteration, a reversed query-block traversal that shrinks pipeline stalls in causal attention, and (ii) Shift Scheduling, a theoretically optimal schedule within our DAG model that reduces pipeline stalls for both full and causal masks.   Our empirical evaluations on NVIDIA H800 GPUs demonstrate that DASH narrows the performance gap of deterministic attention. The proposed strategies improve the throughput of the attention backward pass by up to 1.28$\\times$ compared to the baseline, significantly advancing the efficiency of reproducible LLM training.   Our code is open-sourced at https://github.com/SJTU-Liquid/deterministic-FA3.",
    "authors": [
      "Xinwei Qiang",
      "Hongmin Chen",
      "Shixuan Sun",
      "Jingwen Leng",
      "Xin Liu",
      "Minyi Guo"
    ],
    "url": "https://arxiv.org/abs/2601.21824v1",
    "date": "2026-01-29T15:10:13Z",
    "source": "arxiv_search",
    "keyword": "gradient accumulation"
  },
  {
    "paper_id": "2601.19794v1",
    "title": "Component-Aware Pruning Framework for Neural Network Controllers via Gradient-Based Importance Estimation",
    "abstract": "The transition from monolithic to multi-component neural architectures in advanced neural network controllers poses substantial challenges due to the high computational complexity of the latter. Conventional model compression techniques for complexity reduction, such as structured pruning based on norm-based metrics to estimate the relative importance of distinct parameter groups, often fail to capture functional significance. This paper introduces a component-aware pruning framework that utilizes gradient information to compute three distinct importance metrics during training: Gradient Accumulation, Fisher Information, and Bayesian Uncertainty. Experimental results with an autoencoder and a TD-MPC agent demonstrate that the proposed framework reveals critical structural dependencies and dynamic shifts in importance that static heuristics often miss, supporting more informed compression decisions.",
    "authors": [
      "Ganesh Sundaram",
      "Jonas Ulmen",
      "Daniel Görges"
    ],
    "url": "https://arxiv.org/abs/2601.19794v1",
    "date": "2026-01-27T16:53:19Z",
    "source": "arxiv_search",
    "keyword": "gradient accumulation"
  },
  {
    "paper_id": "2601.08512v1",
    "title": "Algorithmic Stability in Infinite Dimensions: Characterizing Unconditional Convergence in Banach Spaces",
    "abstract": "The distinction between conditional, unconditional, and absolute convergence in infinite-dimensional spaces has fundamental implications for computational algorithms. While these concepts coincide in finite dimensions, the Dvoretzky-Rogers theorem establishes their strict separation in general Banach spaces. We present a comprehensive characterization theorem unifying seven equivalent conditions for unconditional convergence: permutation invariance, net convergence, subseries tests, sign stability, bounded multiplier properties, and weak uniform convergence. These theoretical results directly inform algorithmic stability analysis, governing permutation invariance in gradient accumulation for Stochastic Gradient Descent and justifying coefficient thresholding in frame-based signal processing. Our work bridges classical functional analysis with contemporary computational practice, providing rigorous foundations for order-independent and numerically robust summation processes.",
    "authors": [
      "Przemysław Spyra"
    ],
    "url": "https://arxiv.org/abs/2601.08512v1",
    "date": "2026-01-13T12:51:58Z",
    "source": "arxiv_search",
    "keyword": "gradient accumulation"
  },
  {
    "paper_id": "2512.17570v2",
    "title": "GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping",
    "abstract": "SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B.",
    "authors": [
      "Yishu Yin",
      "Xuehai Qian"
    ],
    "url": "https://arxiv.org/abs/2512.17570v2",
    "date": "2025-12-19T13:36:31Z",
    "source": "arxiv_search",
    "keyword": "gradient accumulation"
  },
  {
    "paper_id": "2512.08211v1",
    "title": "MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones",
    "abstract": "Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.",
    "authors": [
      "Jiaxiang Geng",
      "Lunyu Zhao",
      "Yiyi Lu",
      "Bing Luo"
    ],
    "url": "https://arxiv.org/abs/2512.08211v1",
    "date": "2025-12-09T03:41:01Z",
    "source": "arxiv_search",
    "keyword": "gradient accumulation"
  },
  {
    "paper_id": "2512.02438v1",
    "title": "Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources",
    "abstract": "In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2-3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD .",
    "authors": [
      "Phuc Pham",
      "Nhu Pham",
      "Ngoc Quoc Ly"
    ],
    "url": "https://arxiv.org/abs/2512.02438v1",
    "date": "2025-12-02T05:53:51Z",
    "source": "arxiv_search",
    "keyword": "gradient accumulation"
  },
  {
    "paper_id": "2602.23349v1",
    "title": "FlashOptim: Optimizers for Memory Efficient Training",
    "abstract": "Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.   We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.   Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.",
    "authors": [
      "Jose Javier Gonzalez Ortiz",
      "Abhay Gupta",
      "Chris Renard",
      "Davis Blalock"
    ],
    "url": "https://arxiv.org/abs/2602.23349v1",
    "date": "2026-02-26T18:52:22Z",
    "source": "arxiv_search",
    "keyword": "mixed precision training"
  },
  {
    "paper_id": "2602.01410v1",
    "title": "SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training",
    "abstract": "Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.",
    "authors": [
      "Yunjie Pan",
      "Yongyi Yang",
      "Hanmei Yang",
      "Scott Mahlke"
    ],
    "url": "https://arxiv.org/abs/2602.01410v1",
    "date": "2026-02-01T19:34:27Z",
    "source": "arxiv_search",
    "keyword": "mixed precision training"
  },
  {
    "paper_id": "2601.21737v2",
    "title": "Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators",
    "abstract": "Computing-in-Memory (CIM) accelerators are a promising solution for accelerating Machine Learning (ML) workloads, as they perform Matrix-Vector Multiplications (MVMs) on crossbar arrays directly in memory. Although the bit widths of the crossbar inputs and cells are very limited, most CIM compilers do not support quantization below 8 bit. As a result, a single MVM requires many compute cycles, and weights cannot be efficiently stored in a single crossbar cell. To address this problem, we propose a mixed-precision training and compilation framework for CIM architectures. The biggest challenge is the massive search space, that makes it difficult to find good quantization parameters. This is why we introduce a reinforcement learning-based strategy to find suitable quantization configurations that balance latency and accuracy. In the best case, our approach achieves up to a 2.48x speedup over existing state-of-the-art solutions, with an accuracy loss of only 0.086 %.",
    "authors": [
      "Rebecca Pelke",
      "Joel Klein",
      "Jose Cubero-Cascante",
      "Nils Bosbach",
      "Jan Moritz Joseph",
      "Rainer Leupers"
    ],
    "url": "https://arxiv.org/abs/2601.21737v2",
    "date": "2026-01-29T13:54:55Z",
    "source": "arxiv_search",
    "keyword": "mixed precision training"
  },
  {
    "paper_id": "2601.18228v1",
    "title": "Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach",
    "abstract": "Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.",
    "authors": [
      "Sahil Naik",
      "Soham Bagayatkar",
      "Pavankumar Singh"
    ],
    "url": "https://arxiv.org/abs/2601.18228v1",
    "date": "2026-01-26T07:29:50Z",
    "source": "arxiv_search",
    "keyword": "mixed precision training"
  },
  {
    "paper_id": "2512.22804v1",
    "title": "MoR: Mixture Of Representations For Mixed-Precision Training",
    "abstract": "Mixed-precision training is a crucial technique for scaling deep learning models, but successful mixedprecision training requires identifying and applying the right combination of training methods. This paper presents our preliminary study on Mixture-of-Representations (MoR), a novel, per-tensor and sub-tensor level quantization framework that dynamically analyzes a tensor's numerical properties to select between a variety of different representations. Based on the framework, we have proposed and experimented concrete algorithms that choose dynamically between FP8 and BF16 representations for both per-tensor and sub-tensor level granularities. Our universal approach is designed to preserve model quality across various quantization partition strategies and datasets. Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format. This work highlights the potential of dynamic, property-aware quantization while preserving model quality. We believe this approach can generally improve the robustness of low precision training, as demonstrated by achieving FP8 accuracies that are on par with existing approaches without the need for fine-grain partitioning, or can be used in combination with other training methods to improve the leverage of even lower precision number formats such as NVFP4.",
    "authors": [
      "Bor-Yiing Su",
      "Peter Dykas",
      "Mike Chrzanowski",
      "Jatin Chhugani"
    ],
    "url": "https://arxiv.org/abs/2512.22804v1",
    "date": "2025-12-28T06:28:50Z",
    "source": "arxiv_search",
    "keyword": "mixed precision training"
  },
  {
    "paper_id": "2512.09202v1",
    "title": "Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers",
    "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a promising paradigm for solving partial differential equations (PDEs) by embedding physical laws into neural network training objectives. However, their deployment on resource-constrained platforms is hindered by substantial computational and memory overhead, primarily stemming from higher-order automatic differentiation, intensive tensor operations, and reliance on full-precision arithmetic. To address these challenges, we present a framework that enables scalable and energy-efficient PINN training on edge devices. This framework integrates fully quantized training, Stein's estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition for weight compression. It contributes three key innovations: (1) a mixed-precision training method that use a square-block MX (SMX) format to eliminate data duplication during backpropagation; (2) a difference-based quantization scheme for the Stein's estimator that mitigates underflow; and (3) a partial-reconstruction scheme (PRS) for TT-Layers that reduces quantization-error accumulation. We further design PINTA, a precision-scalable hardware accelerator, to fully exploit the performance of the framework. Experiments on the 2-D Poisson, 20-D Hamilton-Jacobi-Bellman (HJB), and 100-D Heat equations demonstrate that the proposed framework achieves accuracy comparable to or better than full-precision, uncompressed baselines while delivering 5.5x to 83.5x speedups and 159.6x to 2324.1x energy savings. This work enables real-time PDE solving on edge devices and paves the way for energy-efficient scientific computing at scale.",
    "authors": [
      "Jinming Lu",
      "Jiayi Tian",
      "Yequan Zhao",
      "Hai Li",
      "Zheng Zhang"
    ],
    "url": "https://arxiv.org/abs/2512.09202v1",
    "date": "2025-12-10T00:00:34Z",
    "source": "arxiv_search",
    "keyword": "mixed precision training"
  },
  {
    "paper_id": "2512.08992v1",
    "title": "Enhanced Chest Disease Classification Using an Improved CheXNet Framework with EfficientNetV2-M and Optimization-Driven Learning",
    "abstract": "The interpretation of Chest X-ray is an important diagnostic issue in clinical practice and especially in the resource-limited setting where the shortage of radiologists plays a role in delayed diagnosis and poor patient outcomes. Although the original CheXNet architecture has shown potential in automated analysis of chest radiographs, DenseNet-121 backbone is computationally inefficient and poorly single-label classifier. To eliminate such shortcomings, we suggest a better classification framework of chest disease that relies on EfficientNetV2-M and incorporates superior training approaches such as Automatic Mixed Precision training, AdamW, Cosine Annealing learning rate scheduling, and Exponential Moving Average regularization. We prepared a dataset of 18,080 chest X-ray images of three source materials of high authority and representing five key clinically significant disease categories which included Cardiomegaly, COVID-19, Normal, Pneumonia, and Tuberculosis. To achieve statistical reliability and reproducibility, nine independent experimental runs were run. The suggested architecture showed significant gains with mean test accuracy of 96.45 percent compared to 95.30 percent at baseline (p less than 0.001) and macro-averaged F1-score increased to 91.08 percent (p less than 0.001). Critical infectious diseases showed near-perfect classification performance with COVID-19 detection having 99.95 percent accuracy and Tuberculosis detection having 99.97 percent accuracy. Although 6.8 times more parameters are included, the training time was reduced by 11.4 percent and performance stability was increased by 22.7 percent. This framework presents itself as a decision-support tool that can be used to respond to a pandemic, screen tuberculosis, and assess thoracic disease regularly in various healthcare facilities.",
    "authors": [
      "Ali M. Bahram",
      "Saman Muhammad Omer",
      "Hardi M. Mohammed",
      "Sirwan Abdolwahed Aula"
    ],
    "url": "https://arxiv.org/abs/2512.08992v1",
    "date": "2025-12-08T05:02:47Z",
    "source": "arxiv_search",
    "keyword": "mixed precision training"
  },
  {
    "paper_id": "2512.02189v2",
    "title": "Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis",
    "abstract": "As GPU architectures rapidly evolve to meet the growing demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA Blackwell (B200) introduces significant architectural advances, including fifth-generation tensor cores, tensor memory (TMEM), a decompression engine (DE), and a dual-chip design; however, systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that provides practical insights into optimizing workloads to fully utilize the rich feature sets of modern GPU architectures. This work enables application developers to make informed architectural decisions and guides future GPU design directions. We study Blackwell GPUs and compare them to the H200 generation with respect to the memory subsystem, tensor core pipeline, and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense and sparse GEMM, transformer inference, and training workloads shows that B200 tensor core enhancements achieve 1.85x ResNet-50 and 1.55x GPT-1.3B mixed-precision training throughput, with 32 percent better energy efficiency than H200.",
    "authors": [
      "Aaron Jarmusch",
      "Sunita Chandrasekaran"
    ],
    "url": "https://arxiv.org/abs/2512.02189v2",
    "date": "2025-12-01T20:31:10Z",
    "source": "arxiv_search",
    "keyword": "mixed precision training"
  },
  {
    "paper_id": "2511.19496v1",
    "title": "Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM",
    "abstract": "Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \\textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \\emph{drop-in agent core}. Training with maximal-update parameterization ($μ$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \\emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \\textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\\,\\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\\footnote{https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints).} Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5.",
    "authors": [
      "Yang Liu",
      "Xiaolong Zhong",
      "Ling Jiang"
    ],
    "url": "https://arxiv.org/abs/2511.19496v1",
    "date": "2025-11-23T13:00:47Z",
    "source": "arxiv_search",
    "keyword": "mixed precision training"
  },
  {
    "paper_id": "2511.14073v2",
    "title": "Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement",
    "abstract": "Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.",
    "authors": [
      "Zijin Su",
      "Huanzhu Lyu",
      "Yuren Niu",
      "Yiming Liu"
    ],
    "url": "https://arxiv.org/abs/2511.14073v2",
    "date": "2025-11-18T03:06:27Z",
    "source": "arxiv_search",
    "keyword": "mixed precision training"
  }
]